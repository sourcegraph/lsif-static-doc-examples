# Package runtime

Package runtime contains operations that interact with Go's runtime system, such as functions to control goroutines. It also includes the low-level type information used by the reflect package; see reflect's documentation for the programmable interface to the run-time type system. 

### hdr-Environment_VariablesEnvironment Variables
The following environment variables ($name or %name%, depending on the host operating system) control the run-time behavior of Go programs. The meanings and use may change from release to release. 

The GOGC variable sets the initial garbage collection target percentage. A collection is triggered when the ratio of freshly allocated data to live data remaining after the previous collection reaches this percentage. The default is GOGC=100. Setting GOGC=off disables the garbage collector entirely. The runtime/debug package's SetGCPercent function allows changing this percentage at run time. See [https://golang.org/pkg/runtime/debug/#SetGCPercent](https://golang.org/pkg/runtime/debug/#SetGCPercent). 

The GODEBUG variable controls debugging variables within the runtime. It is a comma-separated list of name=val pairs setting these named variables: 

```
allocfreetrace: setting allocfreetrace=1 causes every allocation to be
profiled and a stack trace printed on each object's allocation and free.

clobberfree: setting clobberfree=1 causes the garbage collector to
clobber the memory content of an object with bad content when it frees
the object.

cgocheck: setting cgocheck=0 disables all checks for packages
using cgo to incorrectly pass Go pointers to non-Go code.
Setting cgocheck=1 (the default) enables relatively cheap
checks that may miss some errors.  Setting cgocheck=2 enables
expensive checks that should not miss any errors, but will
cause your program to run slower.

efence: setting efence=1 causes the allocator to run in a mode
where each object is allocated on a unique page and addresses are
never recycled.

gccheckmark: setting gccheckmark=1 enables verification of the
garbage collector's concurrent mark phase by performing a
second mark pass while the world is stopped.  If the second
pass finds a reachable object that was not found by concurrent
mark, the garbage collector will panic.

gcpacertrace: setting gcpacertrace=1 causes the garbage collector to
print information about the internal state of the concurrent pacer.

gcshrinkstackoff: setting gcshrinkstackoff=1 disables moving goroutines
onto smaller stacks. In this mode, a goroutine's stack can only grow.

gcstoptheworld: setting gcstoptheworld=1 disables concurrent garbage collection,
making every garbage collection a stop-the-world event. Setting gcstoptheworld=2
also disables concurrent sweeping after the garbage collection finishes.

gctrace: setting gctrace=1 causes the garbage collector to emit a single line to standard
error at each collection, summarizing the amount of memory collected and the
length of the pause. The format of this line is subject to change.
Currently, it is:
	gc # @#s #%: #+#+# ms clock, #+#/#/#+# ms cpu, #->#-># MB, # MB goal, # P
where the fields are as follows:
	gc #        the GC number, incremented at each GC
	@#s         time in seconds since program start
	#%          percentage of time spent in GC since program start
	#+...+#     wall-clock/CPU times for the phases of the GC
	#->#-># MB  heap size at GC start, at GC end, and live heap
	# MB goal   goal heap size
	# P         number of processors used
The phases are stop-the-world (STW) sweep termination, concurrent
mark and scan, and STW mark termination. The CPU times
for mark/scan are broken down in to assist time (GC performed in
line with allocation), background GC time, and idle GC time.
If the line ends with "(forced)", this GC was forced by a
runtime.GC() call.

inittrace: setting inittrace=1 causes the runtime to emit a single line to standard
error for each package with init work, summarizing the execution time and memory
allocation. No information is printed for inits executed as part of plugin loading
and for packages without both user defined and compiler generated init work.
The format of this line is subject to change. Currently, it is:
	init # @#ms, # ms clock, # bytes, # allocs
where the fields are as follows:
	init #      the package name
	@# ms       time in milliseconds when the init started since program start
	# clock     wall-clock time for package initialization work
	# bytes     memory allocated on the heap
	# allocs    number of heap allocations

madvdontneed: setting madvdontneed=0 will use MADV_FREE
instead of MADV_DONTNEED on Linux when returning memory to the
kernel. This is more efficient, but means RSS numbers will
drop only when the OS is under memory pressure.

memprofilerate: setting memprofilerate=X will update the value of runtime.MemProfileRate.
When set to 0 memory profiling is disabled.  Refer to the description of
MemProfileRate for the default value.

invalidptr: invalidptr=1 (the default) causes the garbage collector and stack
copier to crash the program if an invalid pointer value (for example, 1)
is found in a pointer-typed location. Setting invalidptr=0 disables this check.
This should only be used as a temporary workaround to diagnose buggy code.
The real fix is to not store integers in pointer-typed locations.

sbrk: setting sbrk=1 replaces the memory allocator and garbage collector
with a trivial allocator that obtains memory from the operating system and
never reclaims any memory.

scavtrace: setting scavtrace=1 causes the runtime to emit a single line to standard
error, roughly once per GC cycle, summarizing the amount of work done by the
scavenger as well as the total amount of memory returned to the operating system
and an estimate of physical memory utilization. The format of this line is subject
to change, but currently it is:
	scav # # KiB work, # KiB total, #% util
where the fields are as follows:
	scav #       the scavenge cycle number
	# KiB work   the amount of memory returned to the OS since the last line
	# KiB total  the total amount of memory returned to the OS
	#% util      the fraction of all unscavenged memory which is in-use
If the line ends with "(forced)", then scavenging was forced by a
debug.FreeOSMemory() call.

scheddetail: setting schedtrace=X and scheddetail=1 causes the scheduler to emit
detailed multiline info every X milliseconds, describing state of the scheduler,
processors, threads and goroutines.

schedtrace: setting schedtrace=X causes the scheduler to emit a single line to standard
error every X milliseconds, summarizing the scheduler state.

tracebackancestors: setting tracebackancestors=N extends tracebacks with the stacks at
which goroutines were created, where N limits the number of ancestor goroutines to
report. This also extends the information returned by runtime.Stack. Ancestor's goroutine
IDs will refer to the ID of the goroutine at the time of creation; it's possible for this
ID to be reused for another goroutine. Setting N to 0 will report no ancestry information.

asyncpreemptoff: asyncpreemptoff=1 disables signal-based
asynchronous goroutine preemption. This makes some loops
non-preemptible for long periods, which may delay GC and
goroutine scheduling. This is useful for debugging GC issues
because it also disables the conservative stack scanning used
for asynchronously preempted goroutines.

```
The net, net/http, and crypto/tls packages also refer to debugging variables in GODEBUG. See the documentation for those packages for details. 

The GOMAXPROCS variable limits the number of operating system threads that can execute user-level Go code simultaneously. There is no limit to the number of threads that can be blocked in system calls on behalf of Go code; those do not count against the GOMAXPROCS limit. This package's GOMAXPROCS function queries and changes the limit. 

The GORACE variable configures the race detector, for programs built using -race. See [https://golang.org/doc/articles/race_detector.html](https://golang.org/doc/articles/race_detector.html) for details. 

The GOTRACEBACK variable controls the amount of output generated when a Go program fails due to an unrecovered panic or an unexpected runtime condition. By default, a failure prints a stack trace for the current goroutine, eliding functions internal to the run-time system, and then exits with exit code 2. The failure prints stack traces for all goroutines if there is no current goroutine or the failure is internal to the run-time. GOTRACEBACK=none omits the goroutine stack traces entirely. GOTRACEBACK=single (the default) behaves as described above. GOTRACEBACK=all adds stack traces for all user-created goroutines. GOTRACEBACK=system is like `all' but adds stack frames for run-time functions and shows goroutines created internally by the run-time. GOTRACEBACK=crash is like `system' but crashes in an operating system-specific manner instead of exiting. For example, on Unix systems, the crash raises SIGABRT to trigger a core dump. For historical reasons, the GOTRACEBACK settings 0, 1, and 2 are synonyms for none, all, and system, respectively. The runtime/debug package's SetTraceback function allows increasing the amount of output at run time, but it cannot reduce the amount below that specified by the environment variable. See [https://golang.org/pkg/runtime/debug/#SetTraceback](https://golang.org/pkg/runtime/debug/#SetTraceback). 

The GOARCH, GOOS, GOPATH, and GOROOT environment variables complete the set of Go environment variables. They influence the building of Go programs (see [https://golang.org/cmd/go](https://golang.org/cmd/go) and [https://golang.org/pkg/go/build](https://golang.org/pkg/go/build)). GOARCH, GOOS, and GOROOT are recorded at compile time and made available by constants or functions in this package, but they do not influence the execution of the run-time system. 

## Index

* Subpages
  * [std/runtime/internal](runtime/internal.md)
  * [std/runtime/cgo](runtime/cgo.md)
  * [std/runtime/debug](runtime/debug.md)
  * [std/runtime/debug_test](runtime/debug_test.md)
  * [std/runtime/metrics](runtime/metrics.md)
  * [std/runtime/metrics_test](runtime/metrics_test.md)
  * [std/runtime/pprof](runtime/pprof.md)
  * [std/runtime/race](runtime/race.md)
  * [std/runtime/trace](runtime/trace.md)
  * [std/runtime/trace_test](runtime/trace_test.md)
* [Constants](#const)
    * [const c0](#c0)
    * [const c1](#c1)
    * [const hashRandomBytes](#hashRandomBytes)
    * [const cgoCheckPointerFail](#cgoCheckPointerFail)
    * [const cgoResultFail](#cgoResultFail)
    * [const cgoWriteBarrierFail](#cgoWriteBarrierFail)
    * [const maxAlign](#maxAlign)
    * [const hchanSize](#hchanSize)
    * [const debugChan](#debugChan)
    * [const Compiler](#Compiler)
    * [const offsetX86HasAVX](#offsetX86HasAVX)
    * [const offsetX86HasAVX2](#offsetX86HasAVX2)
    * [const offsetX86HasERMS](#offsetX86HasERMS)
    * [const offsetX86HasSSE2](#offsetX86HasSSE2)
    * [const offsetARMHasIDIVA](#offsetARMHasIDIVA)
    * [const offsetMIPS64XHasMSA](#offsetMIPS64XHasMSA)
    * [const maxCPUProfStack](#maxCPUProfStack)
    * [const debugCallSystemStack](#debugCallSystemStack)
    * [const debugCallUnknownFunc](#debugCallUnknownFunc)
    * [const debugCallRuntime](#debugCallRuntime)
    * [const debugCallUnsafePoint](#debugCallUnsafePoint)
    * [const debugLogBytes](#debugLogBytes)
    * [const debugLogStringLimit](#debugLogStringLimit)
    * [const debugLogUnknown](#debugLogUnknown)
    * [const debugLogBoolTrue](#debugLogBoolTrue)
    * [const debugLogBoolFalse](#debugLogBoolFalse)
    * [const debugLogInt](#debugLogInt)
    * [const debugLogUint](#debugLogUint)
    * [const debugLogHex](#debugLogHex)
    * [const debugLogPtr](#debugLogPtr)
    * [const debugLogString](#debugLogString)
    * [const debugLogConstString](#debugLogConstString)
    * [const debugLogStringOverflow](#debugLogStringOverflow)
    * [const debugLogPC](#debugLogPC)
    * [const debugLogTraceback](#debugLogTraceback)
    * [const debugLogHeaderSize](#debugLogHeaderSize)
    * [const debugLogSyncSize](#debugLogSyncSize)
    * [const dlogEnabled](#dlogEnabled)
    * [const _EINTR](#_EINTR)
    * [const _EFAULT](#_EFAULT)
    * [const _EAGAIN](#_EAGAIN)
    * [const _ETIMEDOUT](#_ETIMEDOUT)
    * [const _PROT_NONE](#_PROT_NONE)
    * [const _PROT_READ](#_PROT_READ)
    * [const _PROT_WRITE](#_PROT_WRITE)
    * [const _PROT_EXEC](#_PROT_EXEC)
    * [const _MAP_ANON](#_MAP_ANON)
    * [const _MAP_PRIVATE](#_MAP_PRIVATE)
    * [const _MAP_FIXED](#_MAP_FIXED)
    * [const _MADV_DONTNEED](#_MADV_DONTNEED)
    * [const _MADV_FREE](#_MADV_FREE)
    * [const _MADV_FREE_REUSABLE](#_MADV_FREE_REUSABLE)
    * [const _MADV_FREE_REUSE](#_MADV_FREE_REUSE)
    * [const _SA_SIGINFO](#_SA_SIGINFO)
    * [const _SA_RESTART](#_SA_RESTART)
    * [const _SA_ONSTACK](#_SA_ONSTACK)
    * [const _SA_USERTRAMP](#_SA_USERTRAMP)
    * [const _SA_64REGSET](#_SA_64REGSET)
    * [const _SIGHUP](#_SIGHUP)
    * [const _SIGINT](#_SIGINT)
    * [const _SIGQUIT](#_SIGQUIT)
    * [const _SIGILL](#_SIGILL)
    * [const _SIGTRAP](#_SIGTRAP)
    * [const _SIGABRT](#_SIGABRT)
    * [const _SIGEMT](#_SIGEMT)
    * [const _SIGFPE](#_SIGFPE)
    * [const _SIGKILL](#_SIGKILL)
    * [const _SIGBUS](#_SIGBUS)
    * [const _SIGSEGV](#_SIGSEGV)
    * [const _SIGSYS](#_SIGSYS)
    * [const _SIGPIPE](#_SIGPIPE)
    * [const _SIGALRM](#_SIGALRM)
    * [const _SIGTERM](#_SIGTERM)
    * [const _SIGURG](#_SIGURG)
    * [const _SIGSTOP](#_SIGSTOP)
    * [const _SIGTSTP](#_SIGTSTP)
    * [const _SIGCONT](#_SIGCONT)
    * [const _SIGCHLD](#_SIGCHLD)
    * [const _SIGTTIN](#_SIGTTIN)
    * [const _SIGTTOU](#_SIGTTOU)
    * [const _SIGIO](#_SIGIO)
    * [const _SIGXCPU](#_SIGXCPU)
    * [const _SIGXFSZ](#_SIGXFSZ)
    * [const _SIGVTALRM](#_SIGVTALRM)
    * [const _SIGPROF](#_SIGPROF)
    * [const _SIGWINCH](#_SIGWINCH)
    * [const _SIGINFO](#_SIGINFO)
    * [const _SIGUSR1](#_SIGUSR1)
    * [const _SIGUSR2](#_SIGUSR2)
    * [const _FPE_INTDIV](#_FPE_INTDIV)
    * [const _FPE_INTOVF](#_FPE_INTOVF)
    * [const _FPE_FLTDIV](#_FPE_FLTDIV)
    * [const _FPE_FLTOVF](#_FPE_FLTOVF)
    * [const _FPE_FLTUND](#_FPE_FLTUND)
    * [const _FPE_FLTRES](#_FPE_FLTRES)
    * [const _FPE_FLTINV](#_FPE_FLTINV)
    * [const _FPE_FLTSUB](#_FPE_FLTSUB)
    * [const _BUS_ADRALN](#_BUS_ADRALN)
    * [const _BUS_ADRERR](#_BUS_ADRERR)
    * [const _BUS_OBJERR](#_BUS_OBJERR)
    * [const _SEGV_MAPERR](#_SEGV_MAPERR)
    * [const _SEGV_ACCERR](#_SEGV_ACCERR)
    * [const _ITIMER_REAL](#_ITIMER_REAL)
    * [const _ITIMER_VIRTUAL](#_ITIMER_VIRTUAL)
    * [const _ITIMER_PROF](#_ITIMER_PROF)
    * [const _EV_ADD](#_EV_ADD)
    * [const _EV_DELETE](#_EV_DELETE)
    * [const _EV_CLEAR](#_EV_CLEAR)
    * [const _EV_RECEIPT](#_EV_RECEIPT)
    * [const _EV_ERROR](#_EV_ERROR)
    * [const _EV_EOF](#_EV_EOF)
    * [const _EVFILT_READ](#_EVFILT_READ)
    * [const _EVFILT_WRITE](#_EVFILT_WRITE)
    * [const _PTHREAD_CREATE_DETACHED](#_PTHREAD_CREATE_DETACHED)
    * [const _F_SETFD](#_F_SETFD)
    * [const _F_GETFL](#_F_GETFL)
    * [const _F_SETFL](#_F_SETFL)
    * [const _FD_CLOEXEC](#_FD_CLOEXEC)
    * [const _O_NONBLOCK](#_O_NONBLOCK)
    * [const boundsIndex](#boundsIndex)
    * [const boundsSliceAlen](#boundsSliceAlen)
    * [const boundsSliceAcap](#boundsSliceAcap)
    * [const boundsSliceB](#boundsSliceB)
    * [const boundsSlice3Alen](#boundsSlice3Alen)
    * [const boundsSlice3Acap](#boundsSlice3Acap)
    * [const boundsSlice3B](#boundsSlice3B)
    * [const boundsSlice3C](#boundsSlice3C)
    * [const boundsConvert](#boundsConvert)
    * [const GOOS](#GOOS)
    * [const GOARCH](#GOARCH)
    * [const fastlogNumBits](#fastlogNumBits)
    * [const m1](#m1)
    * [const m2](#m2)
    * [const m3](#m3)
    * [const m4](#m4)
    * [const m5](#m5)
    * [const fieldKindEol](#fieldKindEol)
    * [const fieldKindPtr](#fieldKindPtr)
    * [const fieldKindIface](#fieldKindIface)
    * [const fieldKindEface](#fieldKindEface)
    * [const tagEOF](#tagEOF)
    * [const tagObject](#tagObject)
    * [const tagOtherRoot](#tagOtherRoot)
    * [const tagType](#tagType)
    * [const tagGoroutine](#tagGoroutine)
    * [const tagStackFrame](#tagStackFrame)
    * [const tagParams](#tagParams)
    * [const tagFinalizer](#tagFinalizer)
    * [const tagItab](#tagItab)
    * [const tagOSThread](#tagOSThread)
    * [const tagMemStats](#tagMemStats)
    * [const tagQueuedFinalizer](#tagQueuedFinalizer)
    * [const tagData](#tagData)
    * [const tagBSS](#tagBSS)
    * [const tagDefer](#tagDefer)
    * [const tagPanic](#tagPanic)
    * [const tagMemProf](#tagMemProf)
    * [const tagAllocSample](#tagAllocSample)
    * [const bufSize](#bufSize)
    * [const typeCacheBuckets](#typeCacheBuckets)
    * [const typeCacheAssoc](#typeCacheAssoc)
    * [const timeHistSubBucketBits](#timeHistSubBucketBits)
    * [const timeHistNumSubBuckets](#timeHistNumSubBuckets)
    * [const timeHistNumSuperBuckets](#timeHistNumSuperBuckets)
    * [const timeHistTotalBuckets](#timeHistTotalBuckets)
    * [const fInf](#fInf)
    * [const fNegInf](#fNegInf)
    * [const itabInitSize](#itabInitSize)
    * [const addrBits](#addrBits)
    * [const cntBits](#cntBits)
    * [const aixAddrBits](#aixAddrBits)
    * [const aixCntBits](#aixCntBits)
    * [const locked](#locked)
    * [const active_spin](#active_spin)
    * [const active_spin_cnt](#active_spin_cnt)
    * [const passive_spin](#passive_spin)
    * [const lockRankDummy](#lockRankDummy)
    * [const lockRankSysmon](#lockRankSysmon)
    * [const lockRankScavenge](#lockRankScavenge)
    * [const lockRankForcegc](#lockRankForcegc)
    * [const lockRankSweepWaiters](#lockRankSweepWaiters)
    * [const lockRankAssistQueue](#lockRankAssistQueue)
    * [const lockRankCpuprof](#lockRankCpuprof)
    * [const lockRankSweep](#lockRankSweep)
    * [const lockRankPollDesc](#lockRankPollDesc)
    * [const lockRankSched](#lockRankSched)
    * [const lockRankDeadlock](#lockRankDeadlock)
    * [const lockRankAllg](#lockRankAllg)
    * [const lockRankAllp](#lockRankAllp)
    * [const lockRankTimers](#lockRankTimers)
    * [const lockRankItab](#lockRankItab)
    * [const lockRankReflectOffs](#lockRankReflectOffs)
    * [const lockRankHchan](#lockRankHchan)
    * [const lockRankFin](#lockRankFin)
    * [const lockRankNotifyList](#lockRankNotifyList)
    * [const lockRankTraceBuf](#lockRankTraceBuf)
    * [const lockRankTraceStrings](#lockRankTraceStrings)
    * [const lockRankMspanSpecial](#lockRankMspanSpecial)
    * [const lockRankProf](#lockRankProf)
    * [const lockRankGcBitsArenas](#lockRankGcBitsArenas)
    * [const lockRankRoot](#lockRankRoot)
    * [const lockRankTrace](#lockRankTrace)
    * [const lockRankTraceStackTab](#lockRankTraceStackTab)
    * [const lockRankNetpollInit](#lockRankNetpollInit)
    * [const lockRankRwmutexW](#lockRankRwmutexW)
    * [const lockRankRwmutexR](#lockRankRwmutexR)
    * [const lockRankSpanSetSpine](#lockRankSpanSetSpine)
    * [const lockRankGscan](#lockRankGscan)
    * [const lockRankStackpool](#lockRankStackpool)
    * [const lockRankStackLarge](#lockRankStackLarge)
    * [const lockRankDefer](#lockRankDefer)
    * [const lockRankSudog](#lockRankSudog)
    * [const lockRankWbufSpans](#lockRankWbufSpans)
    * [const lockRankMheap](#lockRankMheap)
    * [const lockRankMheapSpecial](#lockRankMheapSpecial)
    * [const lockRankGlobalAlloc](#lockRankGlobalAlloc)
    * [const lockRankGFree](#lockRankGFree)
    * [const lockRankHchanLeaf](#lockRankHchanLeaf)
    * [const lockRankPanic](#lockRankPanic)
    * [const lockRankNewmHandoff](#lockRankNewmHandoff)
    * [const lockRankDebugPtrmask](#lockRankDebugPtrmask)
    * [const lockRankFaketimeState](#lockRankFaketimeState)
    * [const lockRankTicks](#lockRankTicks)
    * [const lockRankRaceFini](#lockRankRaceFini)
    * [const lockRankPollCache](#lockRankPollCache)
    * [const lockRankDebug](#lockRankDebug)
    * [const lockRankLeafRank](#lockRankLeafRank)
    * [const debugMalloc](#debugMalloc)
    * [const maxTinySize](#maxTinySize)
    * [const tinySizeClass](#tinySizeClass)
    * [const maxSmallSize](#maxSmallSize)
    * [const pageShift](#pageShift)
    * [const pageSize](#pageSize)
    * [const pageMask](#pageMask)
    * [const maxObjsPerSpan](#maxObjsPerSpan)
    * [const concurrentSweep](#concurrentSweep)
    * [const _PageSize](#_PageSize)
    * [const _PageMask](#_PageMask)
    * [const _64bit](#_64bit)
    * [const _TinySize](#_TinySize)
    * [const _TinySizeClass](#_TinySizeClass)
    * [const _FixAllocChunk](#_FixAllocChunk)
    * [const _StackCacheSize](#_StackCacheSize)
    * [const _NumStackOrders](#_NumStackOrders)
    * [const heapAddrBits](#heapAddrBits)
    * [const maxAlloc](#maxAlloc)
    * [const heapArenaBytes](#heapArenaBytes)
    * [const logHeapArenaBytes](#logHeapArenaBytes)
    * [const heapArenaBitmapBytes](#heapArenaBitmapBytes)
    * [const pagesPerArena](#pagesPerArena)
    * [const arenaL1Bits](#arenaL1Bits)
    * [const arenaL2Bits](#arenaL2Bits)
    * [const arenaL1Shift](#arenaL1Shift)
    * [const arenaBits](#arenaBits)
    * [const arenaBaseOffset](#arenaBaseOffset)
    * [const arenaBaseOffsetUintptr](#arenaBaseOffsetUintptr)
    * [const _MaxGcproc](#_MaxGcproc)
    * [const minLegalPointer](#minLegalPointer)
    * [const persistentChunkSize](#persistentChunkSize)
    * [const bucketCntBits](#bucketCntBits)
    * [const bucketCnt](#bucketCnt)
    * [const loadFactorNum](#loadFactorNum)
    * [const loadFactorDen](#loadFactorDen)
    * [const maxKeySize](#maxKeySize)
    * [const maxElemSize](#maxElemSize)
    * [const dataOffset](#dataOffset)
    * [const emptyRest](#emptyRest)
    * [const emptyOne](#emptyOne)
    * [const evacuatedX](#evacuatedX)
    * [const evacuatedY](#evacuatedY)
    * [const evacuatedEmpty](#evacuatedEmpty)
    * [const minTopHash](#minTopHash)
    * [const iterator](#iterator)
    * [const oldIterator](#oldIterator)
    * [const hashWriting](#hashWriting)
    * [const sameSizeGrow](#sameSizeGrow)
    * [const noCheck](#noCheck)
    * [const maxZero](#maxZero)
    * [const bitPointer](#bitPointer)
    * [const bitScan](#bitScan)
    * [const heapBitsShift](#heapBitsShift)
    * [const wordsPerBitmapByte](#wordsPerBitmapByte)
    * [const bitScanAll](#bitScanAll)
    * [const bitPointerAll](#bitPointerAll)
    * [const clobberdeadPtr](#clobberdeadPtr)
    * [const _ENOMEM](#_ENOMEM)
    * [const heapStatsDep](#heapStatsDep)
    * [const sysStatsDep](#sysStatsDep)
    * [const numStatsDeps](#numStatsDeps)
    * [const metricKindBad](#metricKindBad)
    * [const metricKindUint64](#metricKindUint64)
    * [const metricKindFloat64](#metricKindFloat64)
    * [const metricKindFloat64Histogram](#metricKindFloat64Histogram)
    * [const _DebugGC](#_DebugGC)
    * [const _ConcurrentSweep](#_ConcurrentSweep)
    * [const _FinBlockSize](#_FinBlockSize)
    * [const debugScanConservative](#debugScanConservative)
    * [const sweepMinHeapDistance](#sweepMinHeapDistance)
    * [const _GCoff](#_GCoff)
    * [const _GCmark](#_GCmark)
    * [const _GCmarktermination](#_GCmarktermination)
    * [const gcMarkWorkerNotWorker](#gcMarkWorkerNotWorker)
    * [const gcMarkWorkerDedicatedMode](#gcMarkWorkerDedicatedMode)
    * [const gcMarkWorkerFractionalMode](#gcMarkWorkerFractionalMode)
    * [const gcMarkWorkerIdleMode](#gcMarkWorkerIdleMode)
    * [const gcBackgroundMode](#gcBackgroundMode)
    * [const gcForceMode](#gcForceMode)
    * [const gcForceBlockMode](#gcForceBlockMode)
    * [const gcTriggerHeap](#gcTriggerHeap)
    * [const gcTriggerTime](#gcTriggerTime)
    * [const gcTriggerCycle](#gcTriggerCycle)
    * [const fixedRootFinalizers](#fixedRootFinalizers)
    * [const fixedRootFreeGStacks](#fixedRootFreeGStacks)
    * [const fixedRootCount](#fixedRootCount)
    * [const rootBlockBytes](#rootBlockBytes)
    * [const maxObletBytes](#maxObletBytes)
    * [const drainCheckThreshold](#drainCheckThreshold)
    * [const pagesPerSpanRoot](#pagesPerSpanRoot)
    * [const gcDrainUntilPreempt](#gcDrainUntilPreempt)
    * [const gcDrainFlushBgCredit](#gcDrainFlushBgCredit)
    * [const gcDrainIdle](#gcDrainIdle)
    * [const gcDrainFractional](#gcDrainFractional)
    * [const gcGoalUtilization](#gcGoalUtilization)
    * [const gcBackgroundUtilization](#gcBackgroundUtilization)
    * [const gcCreditSlack](#gcCreditSlack)
    * [const gcAssistTimeSlack](#gcAssistTimeSlack)
    * [const gcOverAssistWork](#gcOverAssistWork)
    * [const defaultHeapMinimum](#defaultHeapMinimum)
    * [const scavengePercent](#scavengePercent)
    * [const retainExtraPercent](#retainExtraPercent)
    * [const maxPagesPerPhysPage](#maxPagesPerPhysPage)
    * [const scavengeCostRatio](#scavengeCostRatio)
    * [const scavengeReservationShards](#scavengeReservationShards)
    * [const stackTraceDebug](#stackTraceDebug)
    * [const numSweepClasses](#numSweepClasses)
    * [const sweepClassDone](#sweepClassDone)
    * [const _WorkbufSize](#_WorkbufSize)
    * [const workbufAlloc](#workbufAlloc)
    * [const minPhysPageSize](#minPhysPageSize)
    * [const maxPhysPageSize](#maxPhysPageSize)
    * [const maxPhysHugePageSize](#maxPhysHugePageSize)
    * [const pagesPerReclaimerChunk](#pagesPerReclaimerChunk)
    * [const physPageAlignedStacks](#physPageAlignedStacks)
    * [const mSpanDead](#mSpanDead)
    * [const mSpanInUse](#mSpanInUse)
    * [const mSpanManual](#mSpanManual)
    * [const numSpanClasses](#numSpanClasses)
    * [const tinySpanClass](#tinySpanClass)
    * [const spanAllocHeap](#spanAllocHeap)
    * [const spanAllocStack](#spanAllocStack)
    * [const spanAllocPtrScalarBits](#spanAllocPtrScalarBits)
    * [const spanAllocWorkBuf](#spanAllocWorkBuf)
    * [const _KindSpecialFinalizer](#_KindSpecialFinalizer)
    * [const _KindSpecialProfile](#_KindSpecialProfile)
    * [const _KindSpecialReachable](#_KindSpecialReachable)
    * [const gcBitsChunkBytes](#gcBitsChunkBytes)
    * [const gcBitsHeaderBytes](#gcBitsHeaderBytes)
    * [const pallocChunkPages](#pallocChunkPages)
    * [const pallocChunkBytes](#pallocChunkBytes)
    * [const logPallocChunkPages](#logPallocChunkPages)
    * [const logPallocChunkBytes](#logPallocChunkBytes)
    * [const summaryLevelBits](#summaryLevelBits)
    * [const summaryL0Bits](#summaryL0Bits)
    * [const pallocChunksL2Bits](#pallocChunksL2Bits)
    * [const pallocChunksL1Shift](#pallocChunksL1Shift)
    * [const pallocSumBytes](#pallocSumBytes)
    * [const maxPackedValue](#maxPackedValue)
    * [const logMaxPackedValue](#logMaxPackedValue)
    * [const freeChunkSum](#freeChunkSum)
    * [const summaryLevels](#summaryLevels)
    * [const pageAlloc32Bit](#pageAlloc32Bit)
    * [const pageAlloc64Bit](#pageAlloc64Bit)
    * [const pallocChunksL1Bits](#pallocChunksL1Bits)
    * [const pageCachePages](#pageCachePages)
    * [const memProfile](#memProfile)
    * [const blockProfile](#blockProfile)
    * [const mutexProfile](#mutexProfile)
    * [const buckHashSize](#buckHashSize)
    * [const maxStack](#maxStack)
    * [const mProfCycleWrap](#mProfCycleWrap)
    * [const msanenabled](#msanenabled)
    * [const spanSetBlockEntries](#spanSetBlockEntries)
    * [const spanSetInitSpineCap](#spanSetInitSpineCap)
    * [const testSmallBuf](#testSmallBuf)
    * [const wbBufEntries](#wbBufEntries)
    * [const wbBufEntryPointers](#wbBufEntryPointers)
    * [const pollNoError](#pollNoError)
    * [const pollErrClosing](#pollErrClosing)
    * [const pollErrTimeout](#pollErrTimeout)
    * [const pollErrNotPollable](#pollErrNotPollable)
    * [const pdReady](#pdReady)
    * [const pdWait](#pdWait)
    * [const pollBlockSize](#pollBlockSize)
    * [const _CTL_HW](#_CTL_HW)
    * [const _HW_NCPU](#_HW_NCPU)
    * [const _HW_PAGESIZE](#_HW_PAGESIZE)
    * [const _NSIG](#_NSIG)
    * [const _SI_USER](#_SI_USER)
    * [const _SIG_BLOCK](#_SIG_BLOCK)
    * [const _SIG_UNBLOCK](#_SIG_UNBLOCK)
    * [const _SIG_SETMASK](#_SIG_SETMASK)
    * [const _SS_DISABLE](#_SS_DISABLE)
    * [const deferHeaderSize](#deferHeaderSize)
    * [const minDeferAlloc](#minDeferAlloc)
    * [const minDeferArgs](#minDeferArgs)
    * [const _GoidCacheBatch](#_GoidCacheBatch)
    * [const freezeStopWait](#freezeStopWait)
    * [const forcePreemptNS](#forcePreemptNS)
    * [const randomizeScheduler](#randomizeScheduler)
    * [const profReaderSleeping](#profReaderSleeping)
    * [const profWriteExtra](#profWriteExtra)
    * [const profBufBlocking](#profBufBlocking)
    * [const profBufNonBlocking](#profBufNonBlocking)
    * [const raceenabled](#raceenabled)
    * [const osRelaxMinNS](#osRelaxMinNS)
    * [const tracebackCrash](#tracebackCrash)
    * [const tracebackAll](#tracebackAll)
    * [const tracebackShift](#tracebackShift)
    * [const _Gidle](#_Gidle)
    * [const _Grunnable](#_Grunnable)
    * [const _Grunning](#_Grunning)
    * [const _Gsyscall](#_Gsyscall)
    * [const _Gwaiting](#_Gwaiting)
    * [const _Gmoribund_unused](#_Gmoribund_unused)
    * [const _Gdead](#_Gdead)
    * [const _Genqueue_unused](#_Genqueue_unused)
    * [const _Gcopystack](#_Gcopystack)
    * [const _Gpreempted](#_Gpreempted)
    * [const _Gscan](#_Gscan)
    * [const _Gscanrunnable](#_Gscanrunnable)
    * [const _Gscanrunning](#_Gscanrunning)
    * [const _Gscansyscall](#_Gscansyscall)
    * [const _Gscanwaiting](#_Gscanwaiting)
    * [const _Gscanpreempted](#_Gscanpreempted)
    * [const _Pidle](#_Pidle)
    * [const _Prunning](#_Prunning)
    * [const _Psyscall](#_Psyscall)
    * [const _Pgcstop](#_Pgcstop)
    * [const _Pdead](#_Pdead)
    * [const gTrackingPeriod](#gTrackingPeriod)
    * [const tlsSlots](#tlsSlots)
    * [const tlsSize](#tlsSize)
    * [const _SigNotify](#_SigNotify)
    * [const _SigKill](#_SigKill)
    * [const _SigThrow](#_SigThrow)
    * [const _SigPanic](#_SigPanic)
    * [const _SigDefault](#_SigDefault)
    * [const _SigGoExit](#_SigGoExit)
    * [const _SigSetStack](#_SigSetStack)
    * [const _SigUnblock](#_SigUnblock)
    * [const _SigIgn](#_SigIgn)
    * [const _TraceRuntimeFrames](#_TraceRuntimeFrames)
    * [const _TraceTrap](#_TraceTrap)
    * [const _TraceJumpStack](#_TraceJumpStack)
    * [const _TracebackMaxFrames](#_TracebackMaxFrames)
    * [const waitReasonZero](#waitReasonZero)
    * [const waitReasonGCAssistMarking](#waitReasonGCAssistMarking)
    * [const waitReasonIOWait](#waitReasonIOWait)
    * [const waitReasonChanReceiveNilChan](#waitReasonChanReceiveNilChan)
    * [const waitReasonChanSendNilChan](#waitReasonChanSendNilChan)
    * [const waitReasonDumpingHeap](#waitReasonDumpingHeap)
    * [const waitReasonGarbageCollection](#waitReasonGarbageCollection)
    * [const waitReasonGarbageCollectionScan](#waitReasonGarbageCollectionScan)
    * [const waitReasonPanicWait](#waitReasonPanicWait)
    * [const waitReasonSelect](#waitReasonSelect)
    * [const waitReasonSelectNoCases](#waitReasonSelectNoCases)
    * [const waitReasonGCAssistWait](#waitReasonGCAssistWait)
    * [const waitReasonGCSweepWait](#waitReasonGCSweepWait)
    * [const waitReasonGCScavengeWait](#waitReasonGCScavengeWait)
    * [const waitReasonChanReceive](#waitReasonChanReceive)
    * [const waitReasonChanSend](#waitReasonChanSend)
    * [const waitReasonFinalizerWait](#waitReasonFinalizerWait)
    * [const waitReasonForceGCIdle](#waitReasonForceGCIdle)
    * [const waitReasonSemacquire](#waitReasonSemacquire)
    * [const waitReasonSleep](#waitReasonSleep)
    * [const waitReasonSyncCondWait](#waitReasonSyncCondWait)
    * [const waitReasonTimerGoroutineIdle](#waitReasonTimerGoroutineIdle)
    * [const waitReasonTraceReaderBlocked](#waitReasonTraceReaderBlocked)
    * [const waitReasonWaitForGCCycle](#waitReasonWaitForGCCycle)
    * [const waitReasonGCWorkerIdle](#waitReasonGCWorkerIdle)
    * [const waitReasonPreempted](#waitReasonPreempted)
    * [const waitReasonDebugCall](#waitReasonDebugCall)
    * [const framepointer_enabled](#framepointer_enabled)
    * [const rwmutexMaxReaders](#rwmutexMaxReaders)
    * [const debugSelect](#debugSelect)
    * [const selectSend](#selectSend)
    * [const selectRecv](#selectRecv)
    * [const selectDefault](#selectDefault)
    * [const semTabSize](#semTabSize)
    * [const semaBlockProfile](#semaBlockProfile)
    * [const semaMutexProfile](#semaMutexProfile)
    * [const _SIG_DFL](#_SIG_DFL)
    * [const _SIG_IGN](#_SIG_IGN)
    * [const sigPreempt](#sigPreempt)
    * [const preemptMSupported](#preemptMSupported)
    * [const sigIdle](#sigIdle)
    * [const sigReceiving](#sigReceiving)
    * [const sigSending](#sigSending)
    * [const sigFixup](#sigFixup)
    * [const _MaxSmallSize](#_MaxSmallSize)
    * [const smallSizeDiv](#smallSizeDiv)
    * [const smallSizeMax](#smallSizeMax)
    * [const largeSizeDiv](#largeSizeDiv)
    * [const _NumSizeClasses](#_NumSizeClasses)
    * [const _PageShift](#_PageShift)
    * [const mantbits64](#mantbits64)
    * [const expbits64](#expbits64)
    * [const bias64](#bias64)
    * [const nan64](#nan64)
    * [const inf64](#inf64)
    * [const neg64](#neg64)
    * [const mantbits32](#mantbits32)
    * [const expbits32](#expbits32)
    * [const bias32](#bias32)
    * [const nan32](#nan32)
    * [const inf32](#inf32)
    * [const neg32](#neg32)
    * [const _StackSystem](#_StackSystem)
    * [const _StackMin](#_StackMin)
    * [const _FixedStack0](#_FixedStack0)
    * [const _FixedStack1](#_FixedStack1)
    * [const _FixedStack2](#_FixedStack2)
    * [const _FixedStack3](#_FixedStack3)
    * [const _FixedStack4](#_FixedStack4)
    * [const _FixedStack5](#_FixedStack5)
    * [const _FixedStack6](#_FixedStack6)
    * [const _FixedStack](#_FixedStack)
    * [const _StackBig](#_StackBig)
    * [const _StackGuard](#_StackGuard)
    * [const _StackSmall](#_StackSmall)
    * [const _StackLimit](#_StackLimit)
    * [const stackDebug](#stackDebug)
    * [const stackFromSystem](#stackFromSystem)
    * [const stackFaultOnFree](#stackFaultOnFree)
    * [const stackPoisonCopy](#stackPoisonCopy)
    * [const stackNoCache](#stackNoCache)
    * [const debugCheckBP](#debugCheckBP)
    * [const uintptrMask](#uintptrMask)
    * [const stackPreempt](#stackPreempt)
    * [const stackFork](#stackFork)
    * [const stackForceMove](#stackForceMove)
    * [const tmpStringBufSize](#tmpStringBufSize)
    * [const maxUint](#maxUint)
    * [const maxInt](#maxInt)
    * [const _PCDATA_UnsafePoint](#_PCDATA_UnsafePoint)
    * [const _PCDATA_StackMapIndex](#_PCDATA_StackMapIndex)
    * [const _PCDATA_InlTreeIndex](#_PCDATA_InlTreeIndex)
    * [const _FUNCDATA_ArgsPointerMaps](#_FUNCDATA_ArgsPointerMaps)
    * [const _FUNCDATA_LocalsPointerMaps](#_FUNCDATA_LocalsPointerMaps)
    * [const _FUNCDATA_StackObjects](#_FUNCDATA_StackObjects)
    * [const _FUNCDATA_InlTree](#_FUNCDATA_InlTree)
    * [const _FUNCDATA_OpenCodedDeferInfo](#_FUNCDATA_OpenCodedDeferInfo)
    * [const _FUNCDATA_ArgInfo](#_FUNCDATA_ArgInfo)
    * [const _ArgsSizeUnknown](#_ArgsSizeUnknown)
    * [const _PCDATA_UnsafePointSafe](#_PCDATA_UnsafePointSafe)
    * [const _PCDATA_UnsafePointUnsafe](#_PCDATA_UnsafePointUnsafe)
    * [const _PCDATA_Restart1](#_PCDATA_Restart1)
    * [const _PCDATA_Restart2](#_PCDATA_Restart2)
    * [const _PCDATA_RestartAtEntry](#_PCDATA_RestartAtEntry)
    * [const funcID_normal](#funcID_normal)
    * [const funcID_abort](#funcID_abort)
    * [const funcID_asmcgocall](#funcID_asmcgocall)
    * [const funcID_asyncPreempt](#funcID_asyncPreempt)
    * [const funcID_cgocallback](#funcID_cgocallback)
    * [const funcID_debugCallV2](#funcID_debugCallV2)
    * [const funcID_gcBgMarkWorker](#funcID_gcBgMarkWorker)
    * [const funcID_goexit](#funcID_goexit)
    * [const funcID_gogo](#funcID_gogo)
    * [const funcID_gopanic](#funcID_gopanic)
    * [const funcID_handleAsyncEvent](#funcID_handleAsyncEvent)
    * [const funcID_jmpdefer](#funcID_jmpdefer)
    * [const funcID_mcall](#funcID_mcall)
    * [const funcID_morestack](#funcID_morestack)
    * [const funcID_mstart](#funcID_mstart)
    * [const funcID_panicwrap](#funcID_panicwrap)
    * [const funcID_rt0_go](#funcID_rt0_go)
    * [const funcID_runfinq](#funcID_runfinq)
    * [const funcID_runtime_main](#funcID_runtime_main)
    * [const funcID_sigpanic](#funcID_sigpanic)
    * [const funcID_systemstack](#funcID_systemstack)
    * [const funcID_systemstack_switch](#funcID_systemstack_switch)
    * [const funcID_wrapper](#funcID_wrapper)
    * [const funcFlag_TOPFRAME](#funcFlag_TOPFRAME)
    * [const funcFlag_SPWRITE](#funcFlag_SPWRITE)
    * [const minfunc](#minfunc)
    * [const pcbucketsize](#pcbucketsize)
    * [const debugPcln](#debugPcln)
    * [const timerNoStatus](#timerNoStatus)
    * [const timerWaiting](#timerWaiting)
    * [const timerRunning](#timerRunning)
    * [const timerDeleted](#timerDeleted)
    * [const timerRemoving](#timerRemoving)
    * [const timerRemoved](#timerRemoved)
    * [const timerModifying](#timerModifying)
    * [const timerModifiedEarlier](#timerModifiedEarlier)
    * [const timerModifiedLater](#timerModifiedLater)
    * [const timerMoving](#timerMoving)
    * [const maxWhen](#maxWhen)
    * [const verifyTimers](#verifyTimers)
    * [const traceEvNone](#traceEvNone)
    * [const traceEvBatch](#traceEvBatch)
    * [const traceEvFrequency](#traceEvFrequency)
    * [const traceEvStack](#traceEvStack)
    * [const traceEvGomaxprocs](#traceEvGomaxprocs)
    * [const traceEvProcStart](#traceEvProcStart)
    * [const traceEvProcStop](#traceEvProcStop)
    * [const traceEvGCStart](#traceEvGCStart)
    * [const traceEvGCDone](#traceEvGCDone)
    * [const traceEvGCSTWStart](#traceEvGCSTWStart)
    * [const traceEvGCSTWDone](#traceEvGCSTWDone)
    * [const traceEvGCSweepStart](#traceEvGCSweepStart)
    * [const traceEvGCSweepDone](#traceEvGCSweepDone)
    * [const traceEvGoCreate](#traceEvGoCreate)
    * [const traceEvGoStart](#traceEvGoStart)
    * [const traceEvGoEnd](#traceEvGoEnd)
    * [const traceEvGoStop](#traceEvGoStop)
    * [const traceEvGoSched](#traceEvGoSched)
    * [const traceEvGoPreempt](#traceEvGoPreempt)
    * [const traceEvGoSleep](#traceEvGoSleep)
    * [const traceEvGoBlock](#traceEvGoBlock)
    * [const traceEvGoUnblock](#traceEvGoUnblock)
    * [const traceEvGoBlockSend](#traceEvGoBlockSend)
    * [const traceEvGoBlockRecv](#traceEvGoBlockRecv)
    * [const traceEvGoBlockSelect](#traceEvGoBlockSelect)
    * [const traceEvGoBlockSync](#traceEvGoBlockSync)
    * [const traceEvGoBlockCond](#traceEvGoBlockCond)
    * [const traceEvGoBlockNet](#traceEvGoBlockNet)
    * [const traceEvGoSysCall](#traceEvGoSysCall)
    * [const traceEvGoSysExit](#traceEvGoSysExit)
    * [const traceEvGoSysBlock](#traceEvGoSysBlock)
    * [const traceEvGoWaiting](#traceEvGoWaiting)
    * [const traceEvGoInSyscall](#traceEvGoInSyscall)
    * [const traceEvHeapAlloc](#traceEvHeapAlloc)
    * [const traceEvHeapGoal](#traceEvHeapGoal)
    * [const traceEvTimerGoroutine](#traceEvTimerGoroutine)
    * [const traceEvFutileWakeup](#traceEvFutileWakeup)
    * [const traceEvString](#traceEvString)
    * [const traceEvGoStartLocal](#traceEvGoStartLocal)
    * [const traceEvGoUnblockLocal](#traceEvGoUnblockLocal)
    * [const traceEvGoSysExitLocal](#traceEvGoSysExitLocal)
    * [const traceEvGoStartLabel](#traceEvGoStartLabel)
    * [const traceEvGoBlockGC](#traceEvGoBlockGC)
    * [const traceEvGCMarkAssistStart](#traceEvGCMarkAssistStart)
    * [const traceEvGCMarkAssistDone](#traceEvGCMarkAssistDone)
    * [const traceEvUserTaskCreate](#traceEvUserTaskCreate)
    * [const traceEvUserTaskEnd](#traceEvUserTaskEnd)
    * [const traceEvUserRegion](#traceEvUserRegion)
    * [const traceEvUserLog](#traceEvUserLog)
    * [const traceEvCount](#traceEvCount)
    * [const traceTickDiv](#traceTickDiv)
    * [const traceStackSize](#traceStackSize)
    * [const traceGlobProc](#traceGlobProc)
    * [const traceBytesPerNumber](#traceBytesPerNumber)
    * [const traceArgCountShift](#traceArgCountShift)
    * [const traceFutileWakeup](#traceFutileWakeup)
    * [const usesLR](#usesLR)
    * [const tflagUncommon](#tflagUncommon)
    * [const tflagExtraStar](#tflagExtraStar)
    * [const tflagNamed](#tflagNamed)
    * [const tflagRegularMemory](#tflagRegularMemory)
    * [const kindBool](#kindBool)
    * [const kindInt](#kindInt)
    * [const kindInt8](#kindInt8)
    * [const kindInt16](#kindInt16)
    * [const kindInt32](#kindInt32)
    * [const kindInt64](#kindInt64)
    * [const kindUint](#kindUint)
    * [const kindUint8](#kindUint8)
    * [const kindUint16](#kindUint16)
    * [const kindUint32](#kindUint32)
    * [const kindUint64](#kindUint64)
    * [const kindUintptr](#kindUintptr)
    * [const kindFloat32](#kindFloat32)
    * [const kindFloat64](#kindFloat64)
    * [const kindComplex64](#kindComplex64)
    * [const kindComplex128](#kindComplex128)
    * [const kindArray](#kindArray)
    * [const kindChan](#kindChan)
    * [const kindFunc](#kindFunc)
    * [const kindInterface](#kindInterface)
    * [const kindMap](#kindMap)
    * [const kindPtr](#kindPtr)
    * [const kindSlice](#kindSlice)
    * [const kindString](#kindString)
    * [const kindStruct](#kindStruct)
    * [const kindUnsafePointer](#kindUnsafePointer)
    * [const kindDirectIface](#kindDirectIface)
    * [const kindGCProg](#kindGCProg)
    * [const kindMask](#kindMask)
    * [const runeError](#runeError)
    * [const runeSelf](#runeSelf)
    * [const maxRune](#maxRune)
    * [const surrogateMin](#surrogateMin)
    * [const surrogateMax](#surrogateMax)
    * [const t1](#t1)
    * [const tx](#tx)
    * [const t2](#t2)
    * [const t3](#t3)
    * [const t4](#t4)
    * [const t5](#t5)
    * [const maskx](#maskx)
    * [const mask2](#mask2)
    * [const mask3](#mask3)
    * [const mask4](#mask4)
    * [const rune1Max](#rune1Max)
    * [const rune2Max](#rune2Max)
    * [const rune3Max](#rune3Max)
    * [const locb](#locb)
    * [const hicb](#hicb)
    * [const DlogEnabled](#DlogEnabled)
    * [const DebugLogBytes](#DebugLogBytes)
    * [const DebugLogStringLimit](#DebugLogStringLimit)
    * [const ENOMEM](#ENOMEM)
    * [const MAP_ANON](#MAP_ANON)
    * [const MAP_PRIVATE](#MAP_PRIVATE)
    * [const MAP_FIXED](#MAP_FIXED)
    * [const PreemptMSupported](#PreemptMSupported)
    * [const PtrSize](#PtrSize)
    * [const ProfBufBlocking](#ProfBufBlocking)
    * [const ProfBufNonBlocking](#ProfBufNonBlocking)
    * [const RuntimeHmapSize](#RuntimeHmapSize)
    * [const PageSize](#PageSize)
    * [const PallocChunkPages](#PallocChunkPages)
    * [const PageAlloc64Bit](#PageAlloc64Bit)
    * [const PallocSumBytes](#PallocSumBytes)
    * [const PageCachePages](#PageCachePages)
    * [const TimeHistSubBucketBits](#TimeHistSubBucketBits)
    * [const TimeHistNumSubBuckets](#TimeHistNumSubBuckets)
    * [const TimeHistNumSuperBuckets](#TimeHistNumSuperBuckets)
    * [const Raceenabled](#Raceenabled)
* [Variables](#var)
    * [var useAeshash](#useAeshash)
    * [var aeskeysched](#aeskeysched)
    * [var hashkey](#hashkey)
    * [var _cgo_init](#_cgo_init)
    * [var _cgo_thread_start](#_cgo_thread_start)
    * [var _cgo_sys_thread_create](#_cgo_sys_thread_create)
    * [var _cgo_notify_runtime_init_done](#_cgo_notify_runtime_init_done)
    * [var _cgo_callers](#_cgo_callers)
    * [var _cgo_set_context_function](#_cgo_set_context_function)
    * [var _cgo_yield](#_cgo_yield)
    * [var iscgo](#iscgo)
    * [var cgoHasExtraM](#cgoHasExtraM)
    * [var cgoAlwaysFalse](#cgoAlwaysFalse)
    * [var cgo_yield](#cgo_yield)
    * [var racecgosync](#racecgosync)
    * [var x86HasPOPCNT](#x86HasPOPCNT)
    * [var x86HasSSE41](#x86HasSSE41)
    * [var x86HasFMA](#x86HasFMA)
    * [var armHasVFPv4](#armHasVFPv4)
    * [var arm64HasATOMICS](#arm64HasATOMICS)
    * [var useAVXmemmove](#useAVXmemmove)
    * [var cpuprof](#cpuprof)
    * [var allDloggers](#allDloggers)
    * [var _cgo_setenv](#_cgo_setenv)
    * [var _cgo_unsetenv](#_cgo_unsetenv)
    * [var boundsErrorFmts](#boundsErrorFmts)
    * [var boundsNegErrorFmts](#boundsNegErrorFmts)
    * [var defaultGOROOT](#defaultGOROOT)
    * [var buildVersion](#buildVersion)
    * [var fastlog2Table](#fastlog2Table)
    * [var inf](#inf)
    * [var dumpfd](#dumpfd)
    * [var tmpbuf](#tmpbuf)
    * [var buf](#buf)
    * [var nbuf](#nbuf)
    * [var typecache](#typecache)
    * [var freemark](#freemark)
    * [var dumphdr](#dumphdr)
    * [var itabLock](#itabLock)
    * [var itabTable](#itabTable)
    * [var itabTableInit](#itabTableInit)
    * [var uint16Eface](#uint16Eface)
    * [var uint32Eface](#uint32Eface)
    * [var uint64Eface](#uint64Eface)
    * [var stringEface](#stringEface)
    * [var sliceEface](#sliceEface)
    * [var uint16Type](#uint16Type)
    * [var uint32Type](#uint32Type)
    * [var uint64Type](#uint64Type)
    * [var stringType](#stringType)
    * [var sliceType](#sliceType)
    * [var staticuint64s](#staticuint64s)
    * [var lockNames](#lockNames)
    * [var lockPartialOrder](#lockPartialOrder)
    * [var physPageSize](#physPageSize)
    * [var physHugePageSize](#physHugePageSize)
    * [var physHugePageShift](#physHugePageShift)
    * [var zerobase](#zerobase)
    * [var globalAlloc](#globalAlloc)
    * [var persistentChunks](#persistentChunks)
    * [var zeroVal](#zeroVal)
    * [var debugPtrmask](#debugPtrmask)
    * [var emptymspan](#emptymspan)
    * [var useCheckmark](#useCheckmark)
    * [var metricsSema](#metricsSema)
    * [var metricsInit](#metricsInit)
    * [var metrics](#metrics)
    * [var sizeClassBuckets](#sizeClassBuckets)
    * [var timeHistBuckets](#timeHistBuckets)
    * [var agg](#agg)
    * [var finlock](#finlock)
    * [var fing](#fing)
    * [var finq](#finq)
    * [var finc](#finc)
    * [var finptrmask](#finptrmask)
    * [var fingwait](#fingwait)
    * [var fingwake](#fingwake)
    * [var allfin](#allfin)
    * [var finalizer1](#finalizer1)
    * [var fingCreate](#fingCreate)
    * [var fingRunning](#fingRunning)
    * [var gcenable_setup](#gcenable_setup)
    * [var gcphase](#gcphase)
    * [var writeBarrier](#writeBarrier)
    * [var gcBlackenEnabled](#gcBlackenEnabled)
    * [var gcMarkWorkerModeStrings](#gcMarkWorkerModeStrings)
    * [var work](#work)
    * [var gcMarkDoneFlushed](#gcMarkDoneFlushed)
    * [var poolcleanup](#poolcleanup)
    * [var oneptrmask](#oneptrmask)
    * [var gcController](#gcController)
    * [var scavenge](#scavenge)
    * [var sweep](#sweep)
    * [var mheap_](#mheap_)
    * [var mSpanStateNames](#mSpanStateNames)
    * [var gcBitsArenas](#gcBitsArenas)
    * [var maxSearchAddr](#maxSearchAddr)
    * [var levelBits](#levelBits)
    * [var levelShift](#levelShift)
    * [var levelLogPages](#levelLogPages)
    * [var proflock](#proflock)
    * [var mbuckets](#mbuckets)
    * [var bbuckets](#bbuckets)
    * [var xbuckets](#xbuckets)
    * [var buckhash](#buckhash)
    * [var bucketmem](#bucketmem)
    * [var mProf](#mProf)
    * [var blockprofilerate](#blockprofilerate)
    * [var mutexprofilerate](#mutexprofilerate)
    * [var MemProfileRate](#MemProfileRate)
    * [var disableMemoryProfiling](#disableMemoryProfiling)
    * [var tracelock](#tracelock)
    * [var minOffAddr](#minOffAddr)
    * [var maxOffAddr](#maxOffAddr)
    * [var spanSetBlockPool](#spanSetBlockPool)
    * [var memstats](#memstats)
    * [var netpollInitLock](#netpollInitLock)
    * [var netpollInited](#netpollInited)
    * [var pollcache](#pollcache)
    * [var netpollWaiters](#netpollWaiters)
    * [var pdEface](#pdEface)
    * [var pdType](#pdType)
    * [var kq](#kq)
    * [var netpollBreakRd](#netpollBreakRd)
    * [var netpollBreakWr](#netpollBreakWr)
    * [var netpollWakeSig](#netpollWakeSig)
    * [var sigNoteRead](#sigNoteRead)
    * [var sigNoteWrite](#sigNoteWrite)
    * [var urandom_dev](#urandom_dev)
    * [var failallocatestack](#failallocatestack)
    * [var failthreadcreate](#failthreadcreate)
    * [var sigset_all](#sigset_all)
    * [var executablePath](#executablePath)
    * [var shiftError](#shiftError)
    * [var divideError](#divideError)
    * [var overflowError](#overflowError)
    * [var floatError](#floatError)
    * [var memoryError](#memoryError)
    * [var deferType](#deferType)
    * [var runningPanicDefers](#runningPanicDefers)
    * [var panicking](#panicking)
    * [var paniclk](#paniclk)
    * [var didothers](#didothers)
    * [var deadlock](#deadlock)
    * [var asyncPreemptStack](#asyncPreemptStack)
    * [var no_pointers_stackmap](#no_pointers_stackmap)
    * [var printBacklog](#printBacklog)
    * [var printBacklogIndex](#printBacklogIndex)
    * [var debuglock](#debuglock)
    * [var minhexdigits](#minhexdigits)
    * [var modinfo](#modinfo)
    * [var m0](#m0)
    * [var g0](#g0)
    * [var mcache0](#mcache0)
    * [var raceprocctx0](#raceprocctx0)
    * [var runtime_inittask](#runtime_inittask)
    * [var main_inittask](#main_inittask)
    * [var main_init_done](#main_init_done)
    * [var mainStarted](#mainStarted)
    * [var runtimeInitTime](#runtimeInitTime)
    * [var initSigmask](#initSigmask)
    * [var badmorestackg0Msg](#badmorestackg0Msg)
    * [var badmorestackgsignalMsg](#badmorestackgsignalMsg)
    * [var allglock](#allglock)
    * [var allgs](#allgs)
    * [var allglen](#allglen)
    * [var allgptr](#allgptr)
    * [var fastrandseed](#fastrandseed)
    * [var freezing](#freezing)
    * [var worldsema](#worldsema)
    * [var gcsema](#gcsema)
    * [var cgoThreadStart](#cgoThreadStart)
    * [var earlycgocallback](#earlycgocallback)
    * [var extram](#extram)
    * [var extraMCount](#extraMCount)
    * [var extraMWaiters](#extraMWaiters)
    * [var execLock](#execLock)
    * [var newmHandoff](#newmHandoff)
    * [var mFixupRace](#mFixupRace)
    * [var inForkedChild](#inForkedChild)
    * [var pendingPreemptSignals](#pendingPreemptSignals)
    * [var prof](#prof)
    * [var sigprofCallers](#sigprofCallers)
    * [var sigprofCallersUse](#sigprofCallersUse)
    * [var forcegcperiod](#forcegcperiod)
    * [var starttime](#starttime)
    * [var stealOrder](#stealOrder)
    * [var inittrace](#inittrace)
    * [var overflowTag](#overflowTag)
    * [var labelSync](#labelSync)
    * [var ticks](#ticks)
    * [var envs](#envs)
    * [var argslice](#argslice)
    * [var traceback_cache](#traceback_cache)
    * [var traceback_env](#traceback_env)
    * [var argc](#argc)
    * [var argv](#argv)
    * [var test_z64](#test_z64)
    * [var test_x64](#test_x64)
    * [var debug](#debug)
    * [var dbgvars](#dbgvars)
    * [var waitReasonStrings](#waitReasonStrings)
    * [var allm](#allm)
    * [var gomaxprocs](#gomaxprocs)
    * [var ncpu](#ncpu)
    * [var forcegc](#forcegc)
    * [var sched](#sched)
    * [var newprocs](#newprocs)
    * [var allpLock](#allpLock)
    * [var allp](#allp)
    * [var idlepMask](#idlepMask)
    * [var timerpMask](#timerpMask)
    * [var gcBgMarkWorkerPool](#gcBgMarkWorkerPool)
    * [var gcBgMarkWorkerCount](#gcBgMarkWorkerCount)
    * [var processorVersionInfo](#processorVersionInfo)
    * [var isIntel](#isIntel)
    * [var lfenceBeforeRdtsc](#lfenceBeforeRdtsc)
    * [var goarm](#goarm)
    * [var islibrary](#islibrary)
    * [var isarchive](#isarchive)
    * [var chansendpc](#chansendpc)
    * [var chanrecvpc](#chanrecvpc)
    * [var semtable](#semtable)
    * [var sigtable](#sigtable)
    * [var fwdSig](#fwdSig)
    * [var handlingSig](#handlingSig)
    * [var disableSigChan](#disableSigChan)
    * [var enableSigChan](#enableSigChan)
    * [var maskUpdatedChan](#maskUpdatedChan)
    * [var signalsOK](#signalsOK)
    * [var crashing](#crashing)
    * [var testSigtrap](#testSigtrap)
    * [var testSigusr1](#testSigusr1)
    * [var badginsignalMsg](#badginsignalMsg)
    * [var sigsetAllExiting](#sigsetAllExiting)
    * [var sig](#sig)
    * [var class_to_size](#class_to_size)
    * [var class_to_allocnpages](#class_to_allocnpages)
    * [var class_to_divmagic](#class_to_divmagic)
    * [var size_to_class8](#size_to_class8)
    * [var size_to_class128](#size_to_class128)
    * [var stackpool](#stackpool)
    * [var stackLarge](#stackLarge)
    * [var maxstacksize](#maxstacksize)
    * [var maxstackceiling](#maxstackceiling)
    * [var ptrnames](#ptrnames)
    * [var abiRegArgsEface](#abiRegArgsEface)
    * [var abiRegArgsType](#abiRegArgsType)
    * [var methodValueCallFrameObjs](#methodValueCallFrameObjs)
    * [var badsystemstackMsg](#badsystemstackMsg)
    * [var hashLoad](#hashLoad)
    * [var intArgRegs](#intArgRegs)
    * [var pinnedTypemaps](#pinnedTypemaps)
    * [var firstmoduledata](#firstmoduledata)
    * [var lastmoduledatap](#lastmoduledatap)
    * [var modulesSlice](#modulesSlice)
    * [var faketime](#faketime)
    * [var trace](#trace)
    * [var gStatusStrings](#gStatusStrings)
    * [var cgoTraceback](#cgoTraceback)
    * [var cgoContext](#cgoContext)
    * [var cgoSymbolizer](#cgoSymbolizer)
    * [var reflectOffs](#reflectOffs)
    * [var Dlog](#Dlog)
    * [var Mmap](#Mmap)
    * [var Munmap](#Munmap)
    * [var Pipe](#Pipe)
    * [var Fadd64](#Fadd64)
    * [var Fsub64](#Fsub64)
    * [var Fmul64](#Fmul64)
    * [var Fdiv64](#Fdiv64)
    * [var F64to32](#F64to32)
    * [var F32to64](#F32to64)
    * [var Fcmp64](#Fcmp64)
    * [var Fintto64](#Fintto64)
    * [var F64toint](#F64toint)
    * [var Entersyscall](#Entersyscall)
    * [var Exitsyscall](#Exitsyscall)
    * [var LockedOSThread](#LockedOSThread)
    * [var Xadduintptr](#Xadduintptr)
    * [var FuncPC](#FuncPC)
    * [var Fastlog2](#Fastlog2)
    * [var Atoi](#Atoi)
    * [var Atoi32](#Atoi32)
    * [var Nanotime](#Nanotime)
    * [var NetpollBreak](#NetpollBreak)
    * [var Usleep](#Usleep)
    * [var PhysPageSize](#PhysPageSize)
    * [var PhysHugePageSize](#PhysHugePageSize)
    * [var NetpollGenericInit](#NetpollGenericInit)
    * [var Memmove](#Memmove)
    * [var MemclrNoHeapPointers](#MemclrNoHeapPointers)
    * [var LockPartialOrder](#LockPartialOrder)
    * [var RunSchedLocalQueueEmptyState](#RunSchedLocalQueueEmptyState)
    * [var StringHash](#StringHash)
    * [var BytesHash](#BytesHash)
    * [var Int32Hash](#Int32Hash)
    * [var Int64Hash](#Int64Hash)
    * [var MemHash](#MemHash)
    * [var MemHash32](#MemHash32)
    * [var MemHash64](#MemHash64)
    * [var EfaceHash](#EfaceHash)
    * [var IfaceHash](#IfaceHash)
    * [var UseAeshash](#UseAeshash)
    * [var HashLoad](#HashLoad)
    * [var Open](#Open)
    * [var Close](#Close)
    * [var Read](#Read)
    * [var Write](#Write)
    * [var BigEndian](#BigEndian)
    * [var ForceGCPeriod](#ForceGCPeriod)
    * [var ReadUnaligned32](#ReadUnaligned32)
    * [var ReadUnaligned64](#ReadUnaligned64)
    * [var BaseChunkIdx](#BaseChunkIdx)
    * [var Semacquire](#Semacquire)
    * [var Semrelease1](#Semrelease1)
    * [var GCTestMoveStackOnNextCall](#GCTestMoveStackOnNextCall)
    * [var NonblockingPipe](#NonblockingPipe)
    * [var SetNonblock](#SetNonblock)
    * [var Closeonexec](#Closeonexec)
    * [var waitForSigusr1](#waitForSigusr1)
* [Types](#type)
    * [type cgoCallers [32]uintptr](#cgoCallers)
    * [type argset struct](#argset)
    * [type hchan struct](#hchan)
        * [func reflect_makechan(t *chantype, size int) *hchan](#reflect_makechan)
        * [func makechan64(t *chantype, size int64) *hchan](#makechan64)
        * [func makechan(t *chantype, size int) *hchan](#makechan)
        * [func (c *hchan) raceaddr() unsafe.Pointer](#hchan.raceaddr)
        * [func (c *hchan) sortkey() uintptr](#hchan.sortkey)
    * [type waitq struct](#waitq)
        * [func (q *waitq) enqueue(sgp *sudog)](#waitq.enqueue)
        * [func (q *waitq) dequeue() *sudog](#waitq.dequeue)
        * [func (q *waitq) dequeueSudoG(sgp *sudog)](#waitq.dequeueSudoG)
    * [type cpuProfile struct](#cpuProfile)
        * [func (p *cpuProfile) add(gp *g, stk []uintptr)](#cpuProfile.add)
        * [func (p *cpuProfile) addNonGo(stk []uintptr)](#cpuProfile.addNonGo)
        * [func (p *cpuProfile) addExtra()](#cpuProfile.addExtra)
    * [type debugCallWrapArgs struct](#debugCallWrapArgs)
    * [type dlogger struct](#dlogger)
        * [func dlog() *dlogger](#dlog)
        * [func getCachedDlogger() *dlogger](#getCachedDlogger)
        * [func (l *dlogger) end()](#dlogger.end)
        * [func (l *dlogger) b(x bool) *dlogger](#dlogger.b)
        * [func (l *dlogger) i(x int) *dlogger](#dlogger.i)
        * [func (l *dlogger) i8(x int8) *dlogger](#dlogger.i8)
        * [func (l *dlogger) i16(x int16) *dlogger](#dlogger.i16)
        * [func (l *dlogger) i32(x int32) *dlogger](#dlogger.i32)
        * [func (l *dlogger) i64(x int64) *dlogger](#dlogger.i64)
        * [func (l *dlogger) u(x uint) *dlogger](#dlogger.u)
        * [func (l *dlogger) uptr(x uintptr) *dlogger](#dlogger.uptr)
        * [func (l *dlogger) u8(x uint8) *dlogger](#dlogger.u8)
        * [func (l *dlogger) u16(x uint16) *dlogger](#dlogger.u16)
        * [func (l *dlogger) u32(x uint32) *dlogger](#dlogger.u32)
        * [func (l *dlogger) u64(x uint64) *dlogger](#dlogger.u64)
        * [func (l *dlogger) hex(x uint64) *dlogger](#dlogger.hex)
        * [func (l *dlogger) p(x interface{}) *dlogger](#dlogger.p)
        * [func (l *dlogger) s(x string) *dlogger](#dlogger.s)
        * [func (l *dlogger) pc(x uintptr) *dlogger](#dlogger.pc)
        * [func (l *dlogger) traceback(x []uintptr) *dlogger](#dlogger.traceback)
        * [func (l *dlogger) End()](#dlogger.End)
        * [func (l *dlogger) B(x bool) *dlogger](#dlogger.B)
        * [func (l *dlogger) I(x int) *dlogger](#dlogger.I)
        * [func (l *dlogger) I16(x int16) *dlogger](#dlogger.I16)
        * [func (l *dlogger) U64(x uint64) *dlogger](#dlogger.U64)
        * [func (l *dlogger) Hex(x uint64) *dlogger](#dlogger.Hex)
        * [func (l *dlogger) P(x interface{}) *dlogger](#dlogger.P)
        * [func (l *dlogger) S(x string) *dlogger](#dlogger.S)
        * [func (l *dlogger) PC(x uintptr) *dlogger](#dlogger.PC)
    * [type debugLogWriter struct](#debugLogWriter)
        * [func (l *debugLogWriter) ensure(n uint64)](#debugLogWriter.ensure)
        * [func (l *debugLogWriter) writeFrameAt(pos, size uint64) bool](#debugLogWriter.writeFrameAt)
        * [func (l *debugLogWriter) writeSync(tick, nano uint64)](#debugLogWriter.writeSync)
        * [func (l *debugLogWriter) writeUint64LE(x uint64)](#debugLogWriter.writeUint64LE)
        * [func (l *debugLogWriter) byte(x byte)](#debugLogWriter.byte)
        * [func (l *debugLogWriter) bytes(x []byte)](#debugLogWriter.bytes)
        * [func (l *debugLogWriter) varint(x int64)](#debugLogWriter.varint)
        * [func (l *debugLogWriter) uvarint(u uint64)](#debugLogWriter.uvarint)
    * [type debugLogBuf [16384]byte](#debugLogBuf)
    * [type debugLogReader struct](#debugLogReader)
        * [func (r *debugLogReader) skip() uint64](#debugLogReader.skip)
        * [func (r *debugLogReader) readUint16LEAt(pos uint64) uint16](#debugLogReader.readUint16LEAt)
        * [func (r *debugLogReader) readUint64LEAt(pos uint64) uint64](#debugLogReader.readUint64LEAt)
        * [func (r *debugLogReader) peek() (tick uint64)](#debugLogReader.peek)
        * [func (r *debugLogReader) header() (end, tick, nano uint64, p int)](#debugLogReader.header)
        * [func (r *debugLogReader) uvarint() uint64](#debugLogReader.uvarint)
        * [func (r *debugLogReader) varint() int64](#debugLogReader.varint)
        * [func (r *debugLogReader) printVal() bool](#debugLogReader.printVal)
    * [type dlogPerM struct{}](#dlogPerM)
    * [type stackt struct](#stackt)
    * [type sigactiont struct](#sigactiont)
    * [type usigactiont struct](#usigactiont)
    * [type siginfo struct](#siginfo)
    * [type timeval struct](#timeval)
        * [func (tv *timeval) set_usec(x int32)](#timeval.set_usec)
    * [type itimerval struct](#itimerval)
    * [type timespec struct](#timespec)
        * [func (ts *timespec) setNsec(ns int64)](#timespec.setNsec)
    * [type fpcontrol struct](#fpcontrol)
    * [type fpstatus struct](#fpstatus)
    * [type regmmst struct](#regmmst)
    * [type regxmm struct](#regxmm)
    * [type regs64 struct](#regs64)
    * [type floatstate64 struct](#floatstate64)
    * [type exceptionstate64 struct](#exceptionstate64)
    * [type mcontext64 struct](#mcontext64)
    * [type regs32 struct](#regs32)
    * [type floatstate32 struct](#floatstate32)
    * [type exceptionstate32 struct](#exceptionstate32)
    * [type mcontext32 struct](#mcontext32)
    * [type ucontext struct](#ucontext)
    * [type keventt struct](#keventt)
    * [type pthread uintptr](#pthread)
        * [func pthread_self() (t pthread)](#pthread_self)
    * [type pthreadattr struct](#pthreadattr)
    * [type pthreadmutex struct](#pthreadmutex)
    * [type pthreadmutexattr struct](#pthreadmutexattr)
    * [type pthreadcond struct](#pthreadcond)
    * [type pthreadcondattr struct](#pthreadcondattr)
    * [type machTimebaseInfo struct](#machTimebaseInfo)
    * [type Error interface](#Error)
    * [type TypeAssertionError struct](#TypeAssertionError)
        * [func (*TypeAssertionError) RuntimeError()](#TypeAssertionError.RuntimeError)
        * [func (e *TypeAssertionError) Error() string](#TypeAssertionError.Error)
    * [type errorString string](#errorString)
        * [func (e errorString) RuntimeError()](#errorString.RuntimeError)
        * [func (e errorString) Error() string](#errorString.Error)
    * [type errorAddressString struct](#errorAddressString)
        * [func (e errorAddressString) RuntimeError()](#errorAddressString.RuntimeError)
        * [func (e errorAddressString) Error() string](#errorAddressString.Error)
        * [func (e errorAddressString) Addr() uintptr](#errorAddressString.Addr)
    * [type plainError string](#plainError)
        * [func (e plainError) RuntimeError()](#plainError.RuntimeError)
        * [func (e plainError) Error() string](#plainError.Error)
    * [type boundsError struct](#boundsError)
        * [func (e boundsError) RuntimeError()](#boundsError.RuntimeError)
        * [func (e boundsError) Error() string](#boundsError.Error)
    * [type boundsErrorCode uint8](#boundsErrorCode)
    * [type stringer interface](#stringer)
    * [type typeCacheBucket struct](#typeCacheBucket)
    * [type childInfo struct](#childInfo)
    * [type timeHistogram struct](#timeHistogram)
        * [func (h *timeHistogram) record(duration int64)](#timeHistogram.record)
    * [type itabTableType struct](#itabTableType)
        * [func (t *itabTableType) find(inter *interfacetype, typ *_type) *itab](#itabTableType.find)
        * [func (t *itabTableType) add(m *itab)](#itabTableType.add)
    * [type uint16InterfacePtr uint16](#uint16InterfacePtr)
    * [type uint32InterfacePtr uint32](#uint32InterfacePtr)
    * [type uint64InterfacePtr uint64](#uint64InterfacePtr)
    * [type stringInterfacePtr string](#stringInterfacePtr)
    * [type sliceInterfacePtr []byte](#sliceInterfacePtr)
    * [type lfstack uint64](#lfstack)
        * [func (head *lfstack) push(node *lfnode)](#lfstack.push)
        * [func (head *lfstack) pop() unsafe.Pointer](#lfstack.pop)
        * [func (head *lfstack) empty() bool](#lfstack.empty)
    * [type lockRank int](#lockRank)
        * [func getLockRank(l *mutex) lockRank](#getLockRank)
        * [func (rank lockRank) String() string](#lockRank.String)
    * [type lockRankStruct struct{}](#lockRankStruct)
    * [type persistentAlloc struct](#persistentAlloc)
    * [type linearAlloc struct](#linearAlloc)
        * [func (l *linearAlloc) init(base, size uintptr, mapMemory bool)](#linearAlloc.init)
        * [func (l *linearAlloc) alloc(size, align uintptr, sysStat *sysMemStat) unsafe.Pointer](#linearAlloc.alloc)
    * [type notInHeap struct{}](#notInHeap)
        * [func persistentalloc1(size, align uintptr, sysStat *sysMemStat) *notInHeap](#persistentalloc1)
        * [func (p *notInHeap) add(bytes uintptr) *notInHeap](#notInHeap.add)
    * [type hmap struct](#hmap)
        * [func makemap64(t *maptype, hint int64, h *hmap) *hmap](#makemap64)
        * [func makemap_small() *hmap](#makemap_small)
        * [func makemap(t *maptype, hint int, h *hmap) *hmap](#makemap)
        * [func reflect_makemap(t *maptype, cap int) *hmap](#reflect_makemap)
        * [func (h *hmap) incrnoverflow()](#hmap.incrnoverflow)
        * [func (h *hmap) newoverflow(t *maptype, b *bmap) *bmap](#hmap.newoverflow)
        * [func (h *hmap) createOverflow()](#hmap.createOverflow)
        * [func (h *hmap) growing() bool](#hmap.growing)
        * [func (h *hmap) sameSizeGrow() bool](#hmap.sameSizeGrow)
        * [func (h *hmap) noldbuckets() uintptr](#hmap.noldbuckets)
        * [func (h *hmap) oldbucketmask() uintptr](#hmap.oldbucketmask)
    * [type mapextra struct](#mapextra)
    * [type bmap struct](#bmap)
        * [func makeBucketArray(t *maptype, b uint8, dirtyalloc unsafe.Pointer) (buckets unsafe.Pointer, nextOverflow *bmap)](#makeBucketArray)
        * [func (b *bmap) overflow(t *maptype) *bmap](#bmap.overflow)
        * [func (b *bmap) setoverflow(t *maptype, ovf *bmap)](#bmap.setoverflow)
        * [func (b *bmap) keys() unsafe.Pointer](#bmap.keys)
    * [type hiter struct](#hiter)
        * [func reflect_mapiterinit(t *maptype, h *hmap) *hiter](#reflect_mapiterinit)
    * [type evacDst struct](#evacDst)
    * [type heapBits struct](#heapBits)
        * [func heapBitsForAddr(addr uintptr) (h heapBits)](#heapBitsForAddr)
        * [func (h heapBits) next() heapBits](#heapBits.next)
        * [func (h heapBits) nextArena() heapBits](#heapBits.nextArena)
        * [func (h heapBits) forward(n uintptr) heapBits](#heapBits.forward)
        * [func (h heapBits) forwardOrBoundary(n uintptr) (heapBits, uintptr)](#heapBits.forwardOrBoundary)
        * [func (h heapBits) bits() uint32](#heapBits.bits)
        * [func (h heapBits) morePointers() bool](#heapBits.morePointers)
        * [func (h heapBits) isPointer() bool](#heapBits.isPointer)
        * [func (h heapBits) initSpan(s *mspan)](#heapBits.initSpan)
    * [type markBits struct](#markBits)
        * [func markBitsForAddr(p uintptr) markBits](#markBitsForAddr)
        * [func markBitsForSpan(base uintptr) (mbits markBits)](#markBitsForSpan)
        * [func (m markBits) isMarked() bool](#markBits.isMarked)
        * [func (m markBits) setMarked()](#markBits.setMarked)
        * [func (m markBits) setMarkedNonAtomic()](#markBits.setMarkedNonAtomic)
        * [func (m markBits) clearMarked()](#markBits.clearMarked)
        * [func (m *markBits) advance()](#markBits.advance)
    * [type mcache struct](#mcache)
        * [func allocmcache() *mcache](#allocmcache)
        * [func getMCache() *mcache](#getMCache)
        * [func (c *mcache) nextFree(spc spanClass) (v gclinkptr, s *mspan, shouldhelpgc bool)](#mcache.nextFree)
        * [func (c *mcache) refill(spc spanClass)](#mcache.refill)
        * [func (c *mcache) allocLarge(size uintptr, needzero bool, noscan bool) (*mspan, bool)](#mcache.allocLarge)
        * [func (c *mcache) releaseAll()](#mcache.releaseAll)
        * [func (c *mcache) prepareForSweep()](#mcache.prepareForSweep)
    * [type gclink struct](#gclink)
    * [type gclinkptr uintptr](#gclinkptr)
        * [func nextFreeFast(s *mspan) gclinkptr](#nextFreeFast)
        * [func stackpoolalloc(order uint8) gclinkptr](#stackpoolalloc)
        * [func (p gclinkptr) ptr() *gclink](#gclinkptr.ptr)
    * [type stackfreelist struct](#stackfreelist)
    * [type mcentral struct](#mcentral)
        * [func (c *mcentral) init(spc spanClass)](#mcentral.init)
        * [func (c *mcentral) partialUnswept(sweepgen uint32) *spanSet](#mcentral.partialUnswept)
        * [func (c *mcentral) partialSwept(sweepgen uint32) *spanSet](#mcentral.partialSwept)
        * [func (c *mcentral) fullUnswept(sweepgen uint32) *spanSet](#mcentral.fullUnswept)
        * [func (c *mcentral) fullSwept(sweepgen uint32) *spanSet](#mcentral.fullSwept)
        * [func (c *mcentral) cacheSpan() *mspan](#mcentral.cacheSpan)
        * [func (c *mcentral) uncacheSpan(s *mspan)](#mcentral.uncacheSpan)
        * [func (c *mcentral) grow() *mspan](#mcentral.grow)
    * [type checkmarksMap [1048576]uint8](#checkmarksMap)
    * [type metricData struct](#metricData)
    * [type statDep uint](#statDep)
    * [type statDepSet [1]uint64](#statDepSet)
        * [func makeStatDepSet(deps ...statDep) statDepSet](#makeStatDepSet)
        * [func (s statDepSet) difference(b statDepSet) statDepSet](#statDepSet.difference)
        * [func (s statDepSet) union(b statDepSet) statDepSet](#statDepSet.union)
        * [func (s *statDepSet) empty() bool](#statDepSet.empty)
        * [func (s *statDepSet) has(d statDep) bool](#statDepSet.has)
    * [type heapStatsAggregate struct](#heapStatsAggregate)
        * [func (a *heapStatsAggregate) compute()](#heapStatsAggregate.compute)
    * [type sysStatsAggregate struct](#sysStatsAggregate)
        * [func (a *sysStatsAggregate) compute()](#sysStatsAggregate.compute)
    * [type statAggregate struct](#statAggregate)
        * [func (a *statAggregate) ensure(deps *statDepSet)](#statAggregate.ensure)
    * [type metricKind int](#metricKind)
    * [type metricSample struct](#metricSample)
    * [type metricValue struct](#metricValue)
        * [func (v *metricValue) float64HistOrInit(buckets []float64) *metricFloat64Histogram](#metricValue.float64HistOrInit)
    * [type metricFloat64Histogram struct](#metricFloat64Histogram)
    * [type finblock struct](#finblock)
    * [type finalizer struct](#finalizer)
    * [type fixalloc struct](#fixalloc)
        * [func (f *fixalloc) init(size uintptr, first func(arg, p unsafe.Pointer), arg unsafe.Pointer, stat *sysMemStat)](#fixalloc.init)
        * [func (f *fixalloc) alloc() unsafe.Pointer](#fixalloc.alloc)
        * [func (f *fixalloc) free(p unsafe.Pointer)](#fixalloc.free)
    * [type mlink struct](#mlink)
    * [type gcMarkWorkerMode int](#gcMarkWorkerMode)
    * [type gcMode int](#gcMode)
    * [type gcTrigger struct](#gcTrigger)
        * [func (t gcTrigger) test() bool](#gcTrigger.test)
    * [type gcTriggerKind int](#gcTriggerKind)
    * [type gcBgMarkWorkerNode struct](#gcBgMarkWorkerNode)
    * [type gcDrainFlags int](#gcDrainFlags)
    * [type gcControllerState struct](#gcControllerState)
        * [func (c *gcControllerState) init(gcPercent int32)](#gcControllerState.init)
        * [func (c *gcControllerState) startCycle()](#gcControllerState.startCycle)
        * [func (c *gcControllerState) revise()](#gcControllerState.revise)
        * [func (c *gcControllerState) endCycle(userForced bool) float64](#gcControllerState.endCycle)
        * [func (c *gcControllerState) enlistWorker()](#gcControllerState.enlistWorker)
        * [func (c *gcControllerState) findRunnableGCWorker(_p_ *p) *g](#gcControllerState.findRunnableGCWorker)
        * [func (c *gcControllerState) commit(triggerRatio float64)](#gcControllerState.commit)
        * [func (c *gcControllerState) effectiveGrowthRatio() float64](#gcControllerState.effectiveGrowthRatio)
        * [func (c *gcControllerState) setGCPercent(in int32) int32](#gcControllerState.setGCPercent)
    * [type stackWorkBuf struct](#stackWorkBuf)
    * [type stackWorkBufHdr struct](#stackWorkBufHdr)
    * [type stackObjectBuf struct](#stackObjectBuf)
        * [func binarySearchTree(x *stackObjectBuf, idx int, n int) (root *stackObject, restBuf *stackObjectBuf, restIdx int)](#binarySearchTree)
    * [type stackObjectBufHdr struct](#stackObjectBufHdr)
    * [type stackObject struct](#stackObject)
        * [func binarySearchTree(x *stackObjectBuf, idx int, n int) (root *stackObject, restBuf *stackObjectBuf, restIdx int)](#binarySearchTree)
        * [func (obj *stackObject) setRecord(r *stackObjectRecord)](#stackObject.setRecord)
    * [type stackScanState struct](#stackScanState)
        * [func (s *stackScanState) putPtr(p uintptr, conservative bool)](#stackScanState.putPtr)
        * [func (s *stackScanState) getPtr() (p uintptr, conservative bool)](#stackScanState.getPtr)
        * [func (s *stackScanState) addObject(addr uintptr, r *stackObjectRecord)](#stackScanState.addObject)
        * [func (s *stackScanState) buildIndex()](#stackScanState.buildIndex)
        * [func (s *stackScanState) findObject(a uintptr) *stackObject](#stackScanState.findObject)
    * [type sweepdata struct](#sweepdata)
    * [type sweepClass uint32](#sweepClass)
        * [func (s *sweepClass) load() sweepClass](#sweepClass.load)
        * [func (s *sweepClass) update(sNew sweepClass)](#sweepClass.update)
        * [func (s *sweepClass) clear()](#sweepClass.clear)
        * [func (s sweepClass) split() (spc spanClass, full bool)](#sweepClass.split)
    * [type sweepLocker struct](#sweepLocker)
        * [func newSweepLocker() sweepLocker](#newSweepLocker)
        * [func (l *sweepLocker) tryAcquire(s *mspan) (sweepLocked, bool)](#sweepLocker.tryAcquire)
        * [func (l *sweepLocker) blockCompletion()](#sweepLocker.blockCompletion)
        * [func (l *sweepLocker) dispose()](#sweepLocker.dispose)
        * [func (l *sweepLocker) sweepIsDone()](#sweepLocker.sweepIsDone)
    * [type sweepLocked struct](#sweepLocked)
        * [func (sl *sweepLocked) sweep(preserve bool) bool](#sweepLocked.sweep)
    * [type gcWork struct](#gcWork)
        * [func (w *gcWork) init()](#gcWork.init)
        * [func (w *gcWork) put(obj uintptr)](#gcWork.put)
        * [func (w *gcWork) putFast(obj uintptr) bool](#gcWork.putFast)
        * [func (w *gcWork) putBatch(obj []uintptr)](#gcWork.putBatch)
        * [func (w *gcWork) tryGet() uintptr](#gcWork.tryGet)
        * [func (w *gcWork) tryGetFast() uintptr](#gcWork.tryGetFast)
        * [func (w *gcWork) dispose()](#gcWork.dispose)
        * [func (w *gcWork) balance()](#gcWork.balance)
        * [func (w *gcWork) empty() bool](#gcWork.empty)
    * [type workbufhdr struct](#workbufhdr)
    * [type workbuf struct](#workbuf)
        * [func getempty() *workbuf](#getempty)
        * [func trygetfull() *workbuf](#trygetfull)
        * [func handoff(b *workbuf) *workbuf](#handoff)
        * [func (b *workbuf) checknonempty()](#workbuf.checknonempty)
        * [func (b *workbuf) checkempty()](#workbuf.checkempty)
    * [type mheap struct](#mheap)
        * [func (h *mheap) sysAlloc(n uintptr) (v unsafe.Pointer, size uintptr)](#mheap.sysAlloc)
        * [func (h *mheap) nextSpanForSweep() *mspan](#mheap.nextSpanForSweep)
        * [func (h *mheap) init()](#mheap.init)
        * [func (h *mheap) reclaim(npage uintptr)](#mheap.reclaim)
        * [func (h *mheap) reclaimChunk(arenas []arenaIdx, pageIdx, n uintptr) uintptr](#mheap.reclaimChunk)
        * [func (h *mheap) alloc(npages uintptr, spanclass spanClass, needzero bool) (*mspan, bool)](#mheap.alloc)
        * [func (h *mheap) allocManual(npages uintptr, typ spanAllocType) *mspan](#mheap.allocManual)
        * [func (h *mheap) setSpans(base, npage uintptr, s *mspan)](#mheap.setSpans)
        * [func (h *mheap) allocNeedsZero(base, npage uintptr) (needZero bool)](#mheap.allocNeedsZero)
        * [func (h *mheap) tryAllocMSpan() *mspan](#mheap.tryAllocMSpan)
        * [func (h *mheap) allocMSpanLocked() *mspan](#mheap.allocMSpanLocked)
        * [func (h *mheap) freeMSpanLocked(s *mspan)](#mheap.freeMSpanLocked)
        * [func (h *mheap) allocSpan(npages uintptr, typ spanAllocType, spanclass spanClass) (s *mspan)](#mheap.allocSpan)
        * [func (h *mheap) grow(npage uintptr) bool](#mheap.grow)
        * [func (h *mheap) freeSpan(s *mspan)](#mheap.freeSpan)
        * [func (h *mheap) freeManual(s *mspan, typ spanAllocType)](#mheap.freeManual)
        * [func (h *mheap) freeSpanLocked(s *mspan, typ spanAllocType)](#mheap.freeSpanLocked)
        * [func (h *mheap) scavengeAll()](#mheap.scavengeAll)
    * [type heapArena struct](#heapArena)
        * [func pageIndexOf(p uintptr) (arena *heapArena, pageIdx uintptr, pageMask uint8)](#pageIndexOf)
    * [type arenaHint struct](#arenaHint)
    * [type mSpanState uint8](#mSpanState)
    * [type mSpanStateBox struct](#mSpanStateBox)
        * [func (b *mSpanStateBox) set(s mSpanState)](#mSpanStateBox.set)
        * [func (b *mSpanStateBox) get() mSpanState](#mSpanStateBox.get)
    * [type mSpanList struct](#mSpanList)
        * [func (list *mSpanList) init()](#mSpanList.init)
        * [func (list *mSpanList) remove(span *mspan)](#mSpanList.remove)
        * [func (list *mSpanList) isEmpty() bool](#mSpanList.isEmpty)
        * [func (list *mSpanList) insert(span *mspan)](#mSpanList.insert)
        * [func (list *mSpanList) insertBack(span *mspan)](#mSpanList.insertBack)
        * [func (list *mSpanList) takeAll(other *mSpanList)](#mSpanList.takeAll)
    * [type mspan struct](#mspan)
        * [func findObject(p, refBase, refOff uintptr) (base uintptr, s *mspan, objIndex uintptr)](#findObject)
        * [func materializeGCProg(ptrdata uintptr, prog *byte) *mspan](#materializeGCProg)
        * [func spanOf(p uintptr) *mspan](#spanOf)
        * [func spanOfUnchecked(p uintptr) *mspan](#spanOfUnchecked)
        * [func spanOfHeap(p uintptr) *mspan](#spanOfHeap)
        * [func (s *mspan) allocBitsForIndex(allocBitIndex uintptr) markBits](#mspan.allocBitsForIndex)
        * [func (s *mspan) refillAllocCache(whichByte uintptr)](#mspan.refillAllocCache)
        * [func (s *mspan) nextFreeIndex() uintptr](#mspan.nextFreeIndex)
        * [func (s *mspan) isFree(index uintptr) bool](#mspan.isFree)
        * [func (s *mspan) divideByElemSize(n uintptr) uintptr](#mspan.divideByElemSize)
        * [func (s *mspan) objIndex(p uintptr) uintptr](#mspan.objIndex)
        * [func (s *mspan) markBitsForIndex(objIndex uintptr) markBits](#mspan.markBitsForIndex)
        * [func (s *mspan) markBitsForBase() markBits](#mspan.markBitsForBase)
        * [func (s *mspan) countAlloc() int](#mspan.countAlloc)
        * [func (s *mspan) ensureSwept()](#mspan.ensureSwept)
        * [func (s *mspan) reportZombies()](#mspan.reportZombies)
        * [func (s *mspan) base() uintptr](#mspan.base)
        * [func (s *mspan) layout() (size, n, total uintptr)](#mspan.layout)
        * [func (span *mspan) init(base uintptr, npages uintptr)](#mspan.init)
        * [func (span *mspan) inList() bool](#mspan.inList)
    * [type spanClass uint8](#spanClass)
        * [func makeSpanClass(sizeclass uint8, noscan bool) spanClass](#makeSpanClass)
        * [func (sc spanClass) sizeclass() int8](#spanClass.sizeclass)
        * [func (sc spanClass) noscan() bool](#spanClass.noscan)
    * [type arenaIdx uint](#arenaIdx)
        * [func arenaIndex(p uintptr) arenaIdx](#arenaIndex)
        * [func (i arenaIdx) l1() uint](#arenaIdx.l1)
        * [func (i arenaIdx) l2() uint](#arenaIdx.l2)
    * [type spanAllocType uint8](#spanAllocType)
        * [func (s spanAllocType) manual() bool](#spanAllocType.manual)
    * [type special struct](#special)
        * [func removespecial(p unsafe.Pointer, kind uint8) *special](#removespecial)
    * [type specialfinalizer struct](#specialfinalizer)
    * [type specialprofile struct](#specialprofile)
    * [type specialReachable struct](#specialReachable)
    * [type specialsIter struct](#specialsIter)
        * [func newSpecialsIter(span *mspan) specialsIter](#newSpecialsIter)
        * [func (i *specialsIter) valid() bool](#specialsIter.valid)
        * [func (i *specialsIter) next()](#specialsIter.next)
        * [func (i *specialsIter) unlinkAndNext() *special](#specialsIter.unlinkAndNext)
    * [type gcBits uint8](#gcBits)
        * [func newMarkBits(nelems uintptr) *gcBits](#newMarkBits)
        * [func newAllocBits(nelems uintptr) *gcBits](#newAllocBits)
        * [func (b *gcBits) bytep(n uintptr) *uint8](#gcBits.bytep)
        * [func (b *gcBits) bitp(n uintptr) (bytep *uint8, mask uint8)](#gcBits.bitp)
    * [type gcBitsHeader struct](#gcBitsHeader)
    * [type gcBitsArena struct](#gcBitsArena)
        * [func newArenaMayUnlock() *gcBitsArena](#newArenaMayUnlock)
        * [func (b *gcBitsArena) tryAlloc(bytes uintptr) *gcBits](#gcBitsArena.tryAlloc)
    * [type chunkIdx uint](#chunkIdx)
        * [func chunkIndex(p uintptr) chunkIdx](#chunkIndex)
        * [func (i chunkIdx) l1() uint](#chunkIdx.l1)
        * [func (i chunkIdx) l2() uint](#chunkIdx.l2)
    * [type pageAlloc struct](#pageAlloc)
        * [func (p *pageAlloc) scavenge(nbytes uintptr, mayUnlock bool) uintptr](#pageAlloc.scavenge)
        * [func (p *pageAlloc) scavengeStartGen()](#pageAlloc.scavengeStartGen)
        * [func (p *pageAlloc) scavengeReserve() (addrRange, uint32)](#pageAlloc.scavengeReserve)
        * [func (p *pageAlloc) scavengeUnreserve(r addrRange, gen uint32)](#pageAlloc.scavengeUnreserve)
        * [func (p *pageAlloc) scavengeOne(work addrRange, max uintptr, mayUnlock bool) (uintptr, addrRange)](#pageAlloc.scavengeOne)
        * [func (p *pageAlloc) scavengeRangeLocked(ci chunkIdx, base, npages uint) uintptr](#pageAlloc.scavengeRangeLocked)
        * [func (p *pageAlloc) init(mheapLock *mutex, sysStat *sysMemStat)](#pageAlloc.init)
        * [func (p *pageAlloc) tryChunkOf(ci chunkIdx) *pallocData](#pageAlloc.tryChunkOf)
        * [func (p *pageAlloc) chunkOf(ci chunkIdx) *pallocData](#pageAlloc.chunkOf)
        * [func (p *pageAlloc) grow(base, size uintptr)](#pageAlloc.grow)
        * [func (p *pageAlloc) update(base, npages uintptr, contig, alloc bool)](#pageAlloc.update)
        * [func (p *pageAlloc) allocRange(base, npages uintptr) uintptr](#pageAlloc.allocRange)
        * [func (p *pageAlloc) findMappedAddr(addr offAddr) offAddr](#pageAlloc.findMappedAddr)
        * [func (p *pageAlloc) find(npages uintptr) (uintptr, offAddr)](#pageAlloc.find)
        * [func (p *pageAlloc) alloc(npages uintptr) (addr uintptr, scav uintptr)](#pageAlloc.alloc)
        * [func (p *pageAlloc) free(base, npages uintptr)](#pageAlloc.free)
        * [func (p *pageAlloc) sysInit()](#pageAlloc.sysInit)
        * [func (p *pageAlloc) sysGrow(base, limit uintptr)](#pageAlloc.sysGrow)
        * [func (p *pageAlloc) allocToCache() pageCache](#pageAlloc.allocToCache)
    * [type pallocSum uint64](#pallocSum)
        * [func packPallocSum(start, max, end uint) pallocSum](#packPallocSum)
        * [func mergeSummaries(sums []pallocSum, logMaxPagesPerSum uint) pallocSum](#mergeSummaries)
        * [func (p pallocSum) start() uint](#pallocSum.start)
        * [func (p pallocSum) max() uint](#pallocSum.max)
        * [func (p pallocSum) end() uint](#pallocSum.end)
        * [func (p pallocSum) unpack() (uint, uint, uint)](#pallocSum.unpack)
    * [type pageCache struct](#pageCache)
        * [func (c *pageCache) empty() bool](#pageCache.empty)
        * [func (c *pageCache) alloc(npages uintptr) (uintptr, uintptr)](#pageCache.alloc)
        * [func (c *pageCache) allocN(npages uintptr) (uintptr, uintptr)](#pageCache.allocN)
        * [func (c *pageCache) flush(p *pageAlloc)](#pageCache.flush)
    * [type pageBits [8]uint64](#pageBits)
        * [func (b *pageBits) get(i uint) uint](#pageBits.get)
        * [func (b *pageBits) block64(i uint) uint64](#pageBits.block64)
        * [func (b *pageBits) set(i uint)](#pageBits.set)
        * [func (b *pageBits) setRange(i, n uint)](#pageBits.setRange)
        * [func (b *pageBits) setAll()](#pageBits.setAll)
        * [func (b *pageBits) clear(i uint)](#pageBits.clear)
        * [func (b *pageBits) clearRange(i, n uint)](#pageBits.clearRange)
        * [func (b *pageBits) clearAll()](#pageBits.clearAll)
        * [func (b *pageBits) popcntRange(i, n uint) (s uint)](#pageBits.popcntRange)
    * [type pallocBits runtime.pageBits](#pallocBits)
        * [func (b *pallocBits) summarize() pallocSum](#pallocBits.summarize)
        * [func (b *pallocBits) find(npages uintptr, searchIdx uint) (uint, uint)](#pallocBits.find)
        * [func (b *pallocBits) find1(searchIdx uint) uint](#pallocBits.find1)
        * [func (b *pallocBits) findSmallN(npages uintptr, searchIdx uint) (uint, uint)](#pallocBits.findSmallN)
        * [func (b *pallocBits) findLargeN(npages uintptr, searchIdx uint) (uint, uint)](#pallocBits.findLargeN)
        * [func (b *pallocBits) allocRange(i, n uint)](#pallocBits.allocRange)
        * [func (b *pallocBits) allocAll()](#pallocBits.allocAll)
        * [func (b *pallocBits) free1(i uint)](#pallocBits.free1)
        * [func (b *pallocBits) free(i, n uint)](#pallocBits.free)
        * [func (b *pallocBits) freeAll()](#pallocBits.freeAll)
        * [func (b *pallocBits) pages64(i uint) uint64](#pallocBits.pages64)
    * [type pallocData struct](#pallocData)
        * [func (m *pallocData) hasScavengeCandidate(min uintptr) bool](#pallocData.hasScavengeCandidate)
        * [func (m *pallocData) findScavengeCandidate(searchIdx uint, min, max uintptr) (uint, uint)](#pallocData.findScavengeCandidate)
        * [func (m *pallocData) allocRange(i, n uint)](#pallocData.allocRange)
        * [func (m *pallocData) allocAll()](#pallocData.allocAll)
    * [type bucketType int](#bucketType)
    * [type bucket struct](#bucket)
        * [func newBucket(typ bucketType, nstk int) *bucket](#newBucket)
        * [func stkbucket(typ bucketType, size uintptr, stk []uintptr, alloc bool) *bucket](#stkbucket)
        * [func (b *bucket) stk() []uintptr](#bucket.stk)
        * [func (b *bucket) mp() *memRecord](#bucket.mp)
        * [func (b *bucket) bp() *blockRecord](#bucket.bp)
    * [type memRecord struct](#memRecord)
    * [type memRecordCycle struct](#memRecordCycle)
        * [func (a *memRecordCycle) add(b *memRecordCycle)](#memRecordCycle.add)
    * [type blockRecord struct](#blockRecord)
    * [type StackRecord struct](#StackRecord)
        * [func (r *StackRecord) Stack() []uintptr](#StackRecord.Stack)
    * [type MemProfileRecord struct](#MemProfileRecord)
        * [func (r *MemProfileRecord) InUseBytes() int64](#MemProfileRecord.InUseBytes)
        * [func (r *MemProfileRecord) InUseObjects() int64](#MemProfileRecord.InUseObjects)
        * [func (r *MemProfileRecord) Stack() []uintptr](#MemProfileRecord.Stack)
    * [type BlockProfileRecord struct](#BlockProfileRecord)
    * [type addrRange struct](#addrRange)
        * [func makeAddrRange(base, limit uintptr) addrRange](#makeAddrRange)
        * [func (a addrRange) size() uintptr](#addrRange.size)
        * [func (a addrRange) contains(addr uintptr) bool](#addrRange.contains)
        * [func (a addrRange) subtract(b addrRange) addrRange](#addrRange.subtract)
        * [func (a addrRange) removeGreaterEqual(addr uintptr) addrRange](#addrRange.removeGreaterEqual)
    * [type offAddr struct](#offAddr)
        * [func levelIndexToOffAddr(level, idx int) offAddr](#levelIndexToOffAddr)
        * [func (l offAddr) add(bytes uintptr) offAddr](#offAddr.add)
        * [func (l offAddr) sub(bytes uintptr) offAddr](#offAddr.sub)
        * [func (l1 offAddr) diff(l2 offAddr) uintptr](#offAddr.diff)
        * [func (l1 offAddr) lessThan(l2 offAddr) bool](#offAddr.lessThan)
        * [func (l1 offAddr) lessEqual(l2 offAddr) bool](#offAddr.lessEqual)
        * [func (l1 offAddr) equal(l2 offAddr) bool](#offAddr.equal)
        * [func (l offAddr) addr() uintptr](#offAddr.addr)
    * [type addrRanges struct](#addrRanges)
        * [func (a *addrRanges) init(sysStat *sysMemStat)](#addrRanges.init)
        * [func (a *addrRanges) findSucc(addr uintptr) int](#addrRanges.findSucc)
        * [func (a *addrRanges) findAddrGreaterEqual(addr uintptr) (uintptr, bool)](#addrRanges.findAddrGreaterEqual)
        * [func (a *addrRanges) contains(addr uintptr) bool](#addrRanges.contains)
        * [func (a *addrRanges) add(r addrRange)](#addrRanges.add)
        * [func (a *addrRanges) removeLast(nBytes uintptr) addrRange](#addrRanges.removeLast)
        * [func (a *addrRanges) removeGreaterEqual(addr uintptr)](#addrRanges.removeGreaterEqual)
        * [func (a *addrRanges) cloneInto(b *addrRanges)](#addrRanges.cloneInto)
    * [type spanSet struct](#spanSet)
        * [func (b *spanSet) push(s *mspan)](#spanSet.push)
        * [func (b *spanSet) pop() *mspan](#spanSet.pop)
        * [func (b *spanSet) reset()](#spanSet.reset)
    * [type spanSetBlock struct](#spanSetBlock)
    * [type spanSetBlockAlloc struct](#spanSetBlockAlloc)
        * [func (p *spanSetBlockAlloc) alloc() *spanSetBlock](#spanSetBlockAlloc.alloc)
        * [func (p *spanSetBlockAlloc) free(block *spanSetBlock)](#spanSetBlockAlloc.free)
    * [type headTailIndex uint64](#headTailIndex)
        * [func makeHeadTailIndex(head, tail uint32) headTailIndex](#makeHeadTailIndex)
        * [func (h headTailIndex) head() uint32](#headTailIndex.head)
        * [func (h headTailIndex) tail() uint32](#headTailIndex.tail)
        * [func (h headTailIndex) split() (head uint32, tail uint32)](#headTailIndex.split)
        * [func (h *headTailIndex) load() headTailIndex](#headTailIndex.load)
        * [func (h *headTailIndex) cas(old, new headTailIndex) bool](#headTailIndex.cas)
        * [func (h *headTailIndex) incHead() headTailIndex](#headTailIndex.incHead)
        * [func (h *headTailIndex) decHead() headTailIndex](#headTailIndex.decHead)
        * [func (h *headTailIndex) incTail() headTailIndex](#headTailIndex.incTail)
        * [func (h *headTailIndex) reset()](#headTailIndex.reset)
    * [type mstats struct](#mstats)
    * [type MemStats struct](#MemStats)
        * [func ReadMemStatsSlow() (base, slow MemStats)](#ReadMemStatsSlow)
    * [type sysMemStat uint64](#sysMemStat)
        * [func (s *sysMemStat) load() uint64](#sysMemStat.load)
        * [func (s *sysMemStat) add(n int64)](#sysMemStat.add)
    * [type heapStatsDelta struct](#heapStatsDelta)
        * [func (a *heapStatsDelta) merge(b *heapStatsDelta)](#heapStatsDelta.merge)
    * [type consistentHeapStats struct](#consistentHeapStats)
        * [func (m *consistentHeapStats) acquire() *heapStatsDelta](#consistentHeapStats.acquire)
        * [func (m *consistentHeapStats) release()](#consistentHeapStats.release)
        * [func (m *consistentHeapStats) unsafeRead(out *heapStatsDelta)](#consistentHeapStats.unsafeRead)
        * [func (m *consistentHeapStats) unsafeClear()](#consistentHeapStats.unsafeClear)
        * [func (m *consistentHeapStats) read(out *heapStatsDelta)](#consistentHeapStats.read)
    * [type wbBuf struct](#wbBuf)
        * [func (b *wbBuf) reset()](#wbBuf.reset)
        * [func (b *wbBuf) discard()](#wbBuf.discard)
        * [func (b *wbBuf) empty() bool](#wbBuf.empty)
        * [func (b *wbBuf) putFast(old, new uintptr) bool](#wbBuf.putFast)
    * [type pollDesc struct](#pollDesc)
        * [func poll_runtime_pollOpen(fd uintptr) (*pollDesc, int)](#poll_runtime_pollOpen)
        * [func (pd *pollDesc) makeArg() (i interface{})](#pollDesc.makeArg)
    * [type pollCache struct](#pollCache)
        * [func (c *pollCache) free(pd *pollDesc)](#pollCache.free)
        * [func (c *pollCache) alloc() *pollDesc](#pollCache.alloc)
    * [type mOS struct](#mOS)
    * [type sigset uint32](#sigset)
    * [type ptabEntry struct](#ptabEntry)
    * [type suspendGState struct](#suspendGState)
        * [func suspendG(gp *g) suspendGState](#suspendG)
    * [type hex uint64](#hex)
    * [type cgothreadstart struct](#cgothreadstart)
    * [type sysmontick struct](#sysmontick)
    * [type pMask []uint32](#pMask)
        * [func (p pMask) read(id uint32) bool](#pMask.read)
        * [func (p pMask) set(id int32)](#pMask.set)
        * [func (p pMask) clear(id int32)](#pMask.clear)
    * [type gQueue struct](#gQueue)
        * [func runqdrain(_p_ *p) (drainQ gQueue, n uint32)](#runqdrain)
        * [func (q *gQueue) empty() bool](#gQueue.empty)
        * [func (q *gQueue) push(gp *g)](#gQueue.push)
        * [func (q *gQueue) pushBack(gp *g)](#gQueue.pushBack)
        * [func (q *gQueue) pushBackAll(q2 gQueue)](#gQueue.pushBackAll)
        * [func (q *gQueue) pop() *g](#gQueue.pop)
        * [func (q *gQueue) popList() gList](#gQueue.popList)
    * [type gList struct](#gList)
        * [func netpoll(delay int64) gList](#netpoll)
        * [func (l *gList) empty() bool](#gList.empty)
        * [func (l *gList) push(gp *g)](#gList.push)
        * [func (l *gList) pushAll(q gQueue)](#gList.pushAll)
        * [func (l *gList) pop() *g](#gList.pop)
    * [type randomOrder struct](#randomOrder)
        * [func (ord *randomOrder) reset(count uint32)](#randomOrder.reset)
        * [func (ord *randomOrder) start(i uint32) randomEnum](#randomOrder.start)
    * [type randomEnum struct](#randomEnum)
        * [func (enum *randomEnum) done() bool](#randomEnum.done)
        * [func (enum *randomEnum) next()](#randomEnum.next)
        * [func (enum *randomEnum) position() uint32](#randomEnum.position)
    * [type initTask struct](#initTask)
    * [type tracestat struct](#tracestat)
    * [type profBuf struct](#profBuf)
        * [func newProfBuf(hdrsize, bufwords, tags int) *profBuf](#newProfBuf)
        * [func (b *profBuf) hasOverflow() bool](#profBuf.hasOverflow)
        * [func (b *profBuf) takeOverflow() (count uint32, time uint64)](#profBuf.takeOverflow)
        * [func (b *profBuf) incrementOverflow(now int64)](#profBuf.incrementOverflow)
        * [func (b *profBuf) canWriteRecord(nstk int) bool](#profBuf.canWriteRecord)
        * [func (b *profBuf) canWriteTwoRecords(nstk1, nstk2 int) bool](#profBuf.canWriteTwoRecords)
        * [func (b *profBuf) write(tagPtr *unsafe.Pointer, now int64, hdr []uint64, stk []uintptr)](#profBuf.write)
        * [func (b *profBuf) close()](#profBuf.close)
        * [func (b *profBuf) wakeupExtra()](#profBuf.wakeupExtra)
        * [func (b *profBuf) read(mode profBufReadMode) (data []uint64, tags []unsafe.Pointer, eof bool)](#profBuf.read)
    * [type profAtomic uint64](#profAtomic)
        * [func (x *profAtomic) load() profIndex](#profAtomic.load)
        * [func (x *profAtomic) store(new profIndex)](#profAtomic.store)
        * [func (x *profAtomic) cas(old, new profIndex) bool](#profAtomic.cas)
    * [type profIndex uint64](#profIndex)
        * [func (x profIndex) dataCount() uint32](#profIndex.dataCount)
        * [func (x profIndex) tagCount() uint32](#profIndex.tagCount)
        * [func (x profIndex) addCountsAndClearFlags(data, tag int) profIndex](#profIndex.addCountsAndClearFlags)
    * [type profBufReadMode int](#profBufReadMode)
    * [type dbgVar struct](#dbgVar)
    * [type mutex struct](#mutex)
    * [type note struct](#note)
    * [type funcval struct](#funcval)
    * [type iface struct](#iface)
        * [func convT2I(tab *itab, elem unsafe.Pointer) (i iface)](#convT2I)
        * [func convT2Inoptr(tab *itab, elem unsafe.Pointer) (i iface)](#convT2Inoptr)
        * [func convI2I(inter *interfacetype, i iface) (r iface)](#convI2I)
        * [func assertI2I2(inter *interfacetype, i iface) (r iface)](#assertI2I2)
        * [func assertE2I2(inter *interfacetype, e eface) (r iface)](#assertE2I2)
    * [type eface struct](#eface)
        * [func convT2E(t *_type, elem unsafe.Pointer) (e eface)](#convT2E)
        * [func convT2Enoptr(t *_type, elem unsafe.Pointer) (e eface)](#convT2Enoptr)
        * [func efaceOf(ep *interface{}) *eface](#efaceOf)
    * [type guintptr uintptr](#guintptr)
        * [func (gp guintptr) ptr() *g](#guintptr.ptr)
        * [func (gp *guintptr) set(g *g)](#guintptr.set)
        * [func (gp *guintptr) cas(old, new guintptr) bool](#guintptr.cas)
    * [type puintptr uintptr](#puintptr)
        * [func (pp puintptr) ptr() *p](#puintptr.ptr)
        * [func (pp *puintptr) set(p *p)](#puintptr.set)
    * [type muintptr uintptr](#muintptr)
        * [func (mp muintptr) ptr() *m](#muintptr.ptr)
        * [func (mp *muintptr) set(m *m)](#muintptr.set)
    * [type gobuf struct](#gobuf)
    * [type sudog struct](#sudog)
        * [func acquireSudog() *sudog](#acquireSudog)
    * [type libcall struct](#libcall)
    * [type stack struct](#stack)
        * [func stackalloc(n uint32) stack](#stackalloc)
    * [type heldLockInfo struct](#heldLockInfo)
    * [type g struct](#g)
        * [func beforeIdle(int64, int64) (*g, bool)](#beforeIdle)
        * [func wakefing() *g](#wakefing)
        * [func netpollunblock(pd *pollDesc, mode int32, ioready bool) *g](#netpollunblock)
        * [func atomicAllG() (**g, uintptr)](#atomicAllG)
        * [func atomicAllGIndex(ptr **g, i uintptr) *g](#atomicAllGIndex)
        * [func findrunnable() (gp *g, inheritTime bool)](#findrunnable)
        * [func stealWork(now int64) (gp *g, inheritTime bool, rnow, pollUntil int64, newWork bool)](#stealWork)
        * [func checkIdleGCNoP() (*p, *g)](#checkIdleGCNoP)
        * [func malg(stacksize int32) *g](#malg)
        * [func newproc1(fn *funcval, argp unsafe.Pointer, narg int32, callergp *g, callerpc uintptr) *g](#newproc1)
        * [func gfget(_p_ *p) *g](#gfget)
        * [func globrunqget(_p_ *p, max int32) *g](#globrunqget)
        * [func runqget(_p_ *p) (gp *g, inheritTime bool)](#runqget)
        * [func runqsteal(_p_, p2 *p, stealRunNextG bool) *g](#runqsteal)
        * [func sigFetchG(c *sigctxt) *g](#sigFetchG)
        * [func getg() *g](#getg)
        * [func traceReader() *g](#traceReader)
        * [func Getg() *G](#Getg)
    * [type m struct](#m)
        * [func allocm(_p_ *p, fn func(), id int64) *m](#allocm)
        * [func lockextra(nilokay bool) *m](#lockextra)
        * [func mget() *m](#mget)
        * [func acquirem() *m](#acquirem)
        * [func traceAcquireBuffer() (mp *m, pid int32, bufp *traceBufPtr)](#traceAcquireBuffer)
    * [type p struct](#p)
        * [func checkRunqsNoP(allpSnapshot []*p, idlepMaskSnapshot pMask) *p](#checkRunqsNoP)
        * [func checkIdleGCNoP() (*p, *g)](#checkIdleGCNoP)
        * [func procresize(nprocs int32) *p](#procresize)
        * [func releasep() *p](#releasep)
        * [func pidleget() *p](#pidleget)
        * [func timeSleepUntil() (int64, *p)](#timeSleepUntil)
        * [func (pp *p) init(id int32)](#p.init)
        * [func (pp *p) destroy()](#p.destroy)
    * [type schedt struct](#schedt)
    * [type _func struct](#_func)
    * [type funcinl struct](#funcinl)
    * [type itab struct](#itab)
        * [func getitab(inter *interfacetype, typ *_type, canfail bool) *itab](#getitab)
        * [func assertI2I(inter *interfacetype, tab *itab) *itab](#assertI2I)
        * [func assertE2I(inter *interfacetype, t *_type) *itab](#assertE2I)
        * [func (m *itab) init() string](#itab.init)
    * [type lfnode struct](#lfnode)
        * [func lfstackUnpack(val uint64) *lfnode](#lfstackUnpack)
    * [type forcegcstate struct](#forcegcstate)
    * [type _defer struct](#_defer)
        * [func newdefer(siz int32) *_defer](#newdefer)
    * [type _panic struct](#_panic)
    * [type stkframe struct](#stkframe)
    * [type ancestorInfo struct](#ancestorInfo)
    * [type waitReason uint8](#waitReason)
        * [func (w waitReason) String() string](#waitReason.String)
    * [type rwmutex struct](#rwmutex)
        * [func (rw *rwmutex) rlock()](#rwmutex.rlock)
        * [func (rw *rwmutex) runlock()](#rwmutex.runlock)
        * [func (rw *rwmutex) lock()](#rwmutex.lock)
        * [func (rw *rwmutex) unlock()](#rwmutex.unlock)
    * [type scase struct](#scase)
    * [type runtimeSelect struct](#runtimeSelect)
    * [type selectDir int](#selectDir)
    * [type semaRoot struct](#semaRoot)
        * [func semroot(addr *uint32) *semaRoot](#semroot)
        * [func (root *semaRoot) queue(addr *uint32, s *sudog, lifo bool)](#semaRoot.queue)
        * [func (root *semaRoot) dequeue(addr *uint32) (found *sudog, now int64)](#semaRoot.dequeue)
        * [func (root *semaRoot) rotateLeft(x *sudog)](#semaRoot.rotateLeft)
        * [func (root *semaRoot) rotateRight(y *sudog)](#semaRoot.rotateRight)
    * [type semaProfileFlags int](#semaProfileFlags)
    * [type notifyList struct](#notifyList)
    * [type sigctxt struct](#sigctxt)
        * [func (c *sigctxt) sigpc() uintptr](#sigctxt.sigpc)
        * [func (c *sigctxt) sigsp() uintptr](#sigctxt.sigsp)
        * [func (c *sigctxt) siglr() uintptr](#sigctxt.siglr)
        * [func (c *sigctxt) fault() uintptr](#sigctxt.fault)
        * [func (c *sigctxt) preparePanic(sig uint32, gp *g)](#sigctxt.preparePanic)
        * [func (c *sigctxt) pushCall(targetPC, resumePC uintptr)](#sigctxt.pushCall)
        * [func (c *sigctxt) regs() *regs64](#sigctxt.regs)
        * [func (c *sigctxt) rax() uint64](#sigctxt.rax)
        * [func (c *sigctxt) rbx() uint64](#sigctxt.rbx)
        * [func (c *sigctxt) rcx() uint64](#sigctxt.rcx)
        * [func (c *sigctxt) rdx() uint64](#sigctxt.rdx)
        * [func (c *sigctxt) rdi() uint64](#sigctxt.rdi)
        * [func (c *sigctxt) rsi() uint64](#sigctxt.rsi)
        * [func (c *sigctxt) rbp() uint64](#sigctxt.rbp)
        * [func (c *sigctxt) rsp() uint64](#sigctxt.rsp)
        * [func (c *sigctxt) r8() uint64](#sigctxt.r8)
        * [func (c *sigctxt) r9() uint64](#sigctxt.r9)
        * [func (c *sigctxt) r10() uint64](#sigctxt.r10)
        * [func (c *sigctxt) r11() uint64](#sigctxt.r11)
        * [func (c *sigctxt) r12() uint64](#sigctxt.r12)
        * [func (c *sigctxt) r13() uint64](#sigctxt.r13)
        * [func (c *sigctxt) r14() uint64](#sigctxt.r14)
        * [func (c *sigctxt) r15() uint64](#sigctxt.r15)
        * [func (c *sigctxt) rip() uint64](#sigctxt.rip)
        * [func (c *sigctxt) rflags() uint64](#sigctxt.rflags)
        * [func (c *sigctxt) cs() uint64](#sigctxt.cs)
        * [func (c *sigctxt) fs() uint64](#sigctxt.fs)
        * [func (c *sigctxt) gs() uint64](#sigctxt.gs)
        * [func (c *sigctxt) sigcode() uint64](#sigctxt.sigcode)
        * [func (c *sigctxt) sigaddr() uint64](#sigctxt.sigaddr)
        * [func (c *sigctxt) set_rip(x uint64)](#sigctxt.set_rip)
        * [func (c *sigctxt) set_rsp(x uint64)](#sigctxt.set_rsp)
        * [func (c *sigctxt) set_sigcode(x uint64)](#sigctxt.set_sigcode)
        * [func (c *sigctxt) set_sigaddr(x uint64)](#sigctxt.set_sigaddr)
        * [func (c *sigctxt) fixsigcode(sig uint32)](#sigctxt.fixsigcode)
    * [type sigTabT struct](#sigTabT)
    * [type gsignalStack struct](#gsignalStack)
    * [type slice struct](#slice)
        * [func growslice(et *_type, old slice, cap int) slice](#growslice)
    * [type notInHeapSlice struct](#notInHeapSlice)
    * [type stackpoolItem struct](#stackpoolItem)
    * [type adjustinfo struct](#adjustinfo)
    * [type bitvector struct](#bitvector)
        * [func makeheapobjbv(p uintptr, size uintptr) bitvector](#makeheapobjbv)
        * [func progToPointerMask(prog *byte, size uintptr) bitvector](#progToPointerMask)
        * [func getStackMap(frame *stkframe, cache *pcvalueCache, debug bool) (locals, args bitvector, objs []stackObjectRecord)](#getStackMap)
        * [func stackmapdata(stkmap *stackmap, n int32) bitvector](#stackmapdata)
        * [func getArgInfoFast(f funcInfo, needArgMap bool) (arglen uintptr, argmap *bitvector, ok bool)](#getArgInfoFast)
        * [func getArgInfo(frame *stkframe, f funcInfo, needArgMap bool, ctxt *funcval) (arglen uintptr, argmap *bitvector)](#getArgInfo)
        * [func (bv *bitvector) ptrbit(i uintptr) uint8](#bitvector.ptrbit)
    * [type stackObjectRecord struct](#stackObjectRecord)
        * [func (r *stackObjectRecord) useGCProg() bool](#stackObjectRecord.useGCProg)
        * [func (r *stackObjectRecord) ptrdata() uintptr](#stackObjectRecord.ptrdata)
    * [type tmpBuf [32]byte](#tmpBuf)
    * [type stringStruct struct](#stringStruct)
        * [func stringStructOf(sp *string) *stringStruct](#stringStructOf)
    * [type stringStructDWARF struct](#stringStructDWARF)
    * [type neverCallThisFunction struct{}](#neverCallThisFunction)
    * [type Frames struct](#Frames)
        * [func CallersFrames(callers []uintptr) *Frames](#CallersFrames)
        * [func (ci *Frames) Next() (frame Frame, more bool)](#Frames.Next)
    * [type Frame struct](#Frame)
    * [type Func struct](#Func)
        * [func FuncForPC(pc uintptr) *Func](#FuncForPC)
        * [func (f *Func) raw() *_func](#Func.raw)
        * [func (f *Func) funcInfo() funcInfo](#Func.funcInfo)
        * [func (f *Func) Name() string](#Func.Name)
        * [func (f *Func) Entry() uintptr](#Func.Entry)
        * [func (f *Func) FileLine(pc uintptr) (file string, line int)](#Func.FileLine)
    * [type funcID uint8](#funcID)
    * [type funcFlag uint8](#funcFlag)
    * [type pcHeader struct](#pcHeader)
    * [type moduledata struct](#moduledata)
        * [func findmoduledatap(pc uintptr) *moduledata](#findmoduledatap)
    * [type modulehash struct](#modulehash)
    * [type functab struct](#functab)
    * [type textsect struct](#textsect)
    * [type findfuncbucket struct](#findfuncbucket)
    * [type funcInfo struct](#funcInfo)
        * [func findfunc(pc uintptr) funcInfo](#findfunc)
        * [func (f funcInfo) valid() bool](#funcInfo.valid)
        * [func (f funcInfo) _Func() *Func](#funcInfo._Func)
    * [type pcvalueCache struct](#pcvalueCache)
    * [type pcvalueCacheEnt struct](#pcvalueCacheEnt)
    * [type stackmap struct](#stackmap)
    * [type inlinedCall struct](#inlinedCall)
    * [type timer struct](#timer)
    * [type traceBufHeader struct](#traceBufHeader)
    * [type traceBuf struct](#traceBuf)
        * [func (buf *traceBuf) varint(v uint64)](#traceBuf.varint)
        * [func (buf *traceBuf) byte(v byte)](#traceBuf.byte)
    * [type traceBufPtr uintptr](#traceBufPtr)
        * [func traceBufPtrOf(b *traceBuf) traceBufPtr](#traceBufPtrOf)
        * [func traceFullDequeue() traceBufPtr](#traceFullDequeue)
        * [func traceAcquireBuffer() (mp *m, pid int32, bufp *traceBufPtr)](#traceAcquireBuffer)
        * [func traceFlush(buf traceBufPtr, pid int32) traceBufPtr](#traceFlush)
        * [func traceString(bufp *traceBufPtr, pid int32, s string) (uint64, *traceBufPtr)](#traceString)
        * [func traceFrameForPC(buf traceBufPtr, pid int32, f Frame) (traceFrame, traceBufPtr)](#traceFrameForPC)
        * [func (tp traceBufPtr) ptr() *traceBuf](#traceBufPtr.ptr)
        * [func (tp *traceBufPtr) set(b *traceBuf)](#traceBufPtr.set)
    * [type traceStackTable struct](#traceStackTable)
        * [func (tab *traceStackTable) put(pcs []uintptr) uint32](#traceStackTable.put)
        * [func (tab *traceStackTable) find(pcs []uintptr, hash uintptr) uint32](#traceStackTable.find)
        * [func (tab *traceStackTable) newStack(n int) *traceStack](#traceStackTable.newStack)
        * [func (tab *traceStackTable) dump()](#traceStackTable.dump)
    * [type traceStack struct](#traceStack)
        * [func (ts *traceStack) stack() []uintptr](#traceStack.stack)
    * [type traceStackPtr uintptr](#traceStackPtr)
        * [func (tp traceStackPtr) ptr() *traceStack](#traceStackPtr.ptr)
    * [type traceFrame struct](#traceFrame)
        * [func traceFrameForPC(buf traceBufPtr, pid int32, f Frame) (traceFrame, traceBufPtr)](#traceFrameForPC)
    * [type traceAlloc struct](#traceAlloc)
        * [func (a *traceAlloc) alloc(n uintptr) unsafe.Pointer](#traceAlloc.alloc)
        * [func (a *traceAlloc) drop()](#traceAlloc.drop)
    * [type traceAllocBlock struct](#traceAllocBlock)
    * [type traceAllocBlockPtr uintptr](#traceAllocBlockPtr)
        * [func (p traceAllocBlockPtr) ptr() *traceAllocBlock](#traceAllocBlockPtr.ptr)
        * [func (p *traceAllocBlockPtr) set(x *traceAllocBlock)](#traceAllocBlockPtr.set)
    * [type reflectMethodValue struct](#reflectMethodValue)
    * [type cgoTracebackArg struct](#cgoTracebackArg)
    * [type cgoContextArg struct](#cgoContextArg)
    * [type cgoSymbolizerArg struct](#cgoSymbolizerArg)
    * [type tflag uint8](#tflag)
    * [type _type struct](#_type)
        * [func resolveTypeOff(ptrInModule unsafe.Pointer, off typeOff) *_type](#resolveTypeOff)
        * [func (t *_type) string() string](#_type.string)
        * [func (t *_type) uncommon() *uncommontype](#_type.uncommon)
        * [func (t *_type) name() string](#_type.name)
        * [func (t *_type) pkgpath() string](#_type.pkgpath)
        * [func (t *_type) nameOff(off nameOff) name](#_type.nameOff)
        * [func (t *_type) typeOff(off typeOff) *_type](#_type.typeOff)
        * [func (t *_type) textOff(off textOff) unsafe.Pointer](#_type.textOff)
    * [type nameOff int32](#nameOff)
    * [type typeOff int32](#typeOff)
    * [type textOff int32](#textOff)
    * [type method struct](#method)
    * [type uncommontype struct](#uncommontype)
    * [type imethod struct](#imethod)
    * [type interfacetype struct](#interfacetype)
    * [type maptype struct](#maptype)
        * [func (mt *maptype) indirectkey() bool](#maptype.indirectkey)
        * [func (mt *maptype) indirectelem() bool](#maptype.indirectelem)
        * [func (mt *maptype) reflexivekey() bool](#maptype.reflexivekey)
        * [func (mt *maptype) needkeyupdate() bool](#maptype.needkeyupdate)
        * [func (mt *maptype) hashMightPanic() bool](#maptype.hashMightPanic)
    * [type arraytype struct](#arraytype)
    * [type chantype struct](#chantype)
    * [type slicetype struct](#slicetype)
    * [type functype struct](#functype)
        * [func (t *functype) in() []*_type](#functype.in)
        * [func (t *functype) out() []*_type](#functype.out)
        * [func (t *functype) dotdotdot() bool](#functype.dotdotdot)
    * [type ptrtype struct](#ptrtype)
    * [type structfield struct](#structfield)
        * [func (f *structfield) offset() uintptr](#structfield.offset)
    * [type structtype struct](#structtype)
    * [type name struct](#name)
        * [func resolveNameOff(ptrInModule unsafe.Pointer, off nameOff) name](#resolveNameOff)
        * [func (n name) data(off int) *byte](#name.data)
        * [func (n name) isExported() bool](#name.isExported)
        * [func (n name) readvarint(off int) (int, int)](#name.readvarint)
        * [func (n name) name() (s string)](#name.name)
        * [func (n name) tag() (s string)](#name.tag)
        * [func (n name) pkgPath() string](#name.pkgPath)
        * [func (n name) isBlank() bool](#name.isBlank)
    * [type _typePair struct](#_typePair)
    * [type LockRank runtime.lockRank](#LockRank)
        * [func (l LockRank) String() string](#LockRank.String)
    * [type LFNode struct](#LFNode)
        * [func LFStackPop(head *uint64) *LFNode](#LFStackPop)
    * [type ProfBuf runtime.profBuf](#ProfBuf)
        * [func NewProfBuf(hdrsize, bufwords, tags int) *ProfBuf](#NewProfBuf)
        * [func (p *ProfBuf) Write(tag *unsafe.Pointer, now int64, hdr []uint64, stk []uintptr)](#ProfBuf.Write)
        * [func (p *ProfBuf) Read(mode profBufReadMode) ([]uint64, []unsafe.Pointer, bool)](#ProfBuf.Read)
        * [func (p *ProfBuf) Close()](#ProfBuf.Close)
    * [type RWMutex struct](#RWMutex)
        * [func (rw *RWMutex) RLock()](#RWMutex.RLock)
        * [func (rw *RWMutex) RUnlock()](#RWMutex.RUnlock)
        * [func (rw *RWMutex) Lock()](#RWMutex.Lock)
        * [func (rw *RWMutex) Unlock()](#RWMutex.Unlock)
    * [type G runtime.g](#G)
        * [func beforeIdle(int64, int64) (*g, bool)](#beforeIdle)
        * [func wakefing() *g](#wakefing)
        * [func netpollunblock(pd *pollDesc, mode int32, ioready bool) *g](#netpollunblock)
        * [func atomicAllG() (**g, uintptr)](#atomicAllG)
        * [func atomicAllGIndex(ptr **g, i uintptr) *g](#atomicAllGIndex)
        * [func findrunnable() (gp *g, inheritTime bool)](#findrunnable)
        * [func stealWork(now int64) (gp *g, inheritTime bool, rnow, pollUntil int64, newWork bool)](#stealWork)
        * [func checkIdleGCNoP() (*p, *g)](#checkIdleGCNoP)
        * [func malg(stacksize int32) *g](#malg)
        * [func newproc1(fn *funcval, argp unsafe.Pointer, narg int32, callergp *g, callerpc uintptr) *g](#newproc1)
        * [func gfget(_p_ *p) *g](#gfget)
        * [func globrunqget(_p_ *p, max int32) *g](#globrunqget)
        * [func runqget(_p_ *p) (gp *g, inheritTime bool)](#runqget)
        * [func runqsteal(_p_, p2 *p, stealRunNextG bool) *g](#runqsteal)
        * [func sigFetchG(c *sigctxt) *g](#sigFetchG)
        * [func getg() *g](#getg)
        * [func traceReader() *g](#traceReader)
        * [func Getg() *G](#Getg)
    * [type Sudog runtime.sudog](#Sudog)
        * [func acquireSudog() *sudog](#acquireSudog)
    * [type PallocSum runtime.pallocSum](#PallocSum)
        * [func PackPallocSum(start, max, end uint) PallocSum](#PackPallocSum)
        * [func SummarizeSlow(b *PallocBits) PallocSum](#SummarizeSlow)
        * [func (m PallocSum) Start() uint](#PallocSum.Start)
        * [func (m PallocSum) Max() uint](#PallocSum.Max)
        * [func (m PallocSum) End() uint](#PallocSum.End)
    * [type PallocBits runtime.pallocBits](#PallocBits)
        * [func (b *PallocBits) Find(npages uintptr, searchIdx uint) (uint, uint)](#PallocBits.Find)
        * [func (b *PallocBits) AllocRange(i, n uint)](#PallocBits.AllocRange)
        * [func (b *PallocBits) Free(i, n uint)](#PallocBits.Free)
        * [func (b *PallocBits) Summarize() PallocSum](#PallocBits.Summarize)
        * [func (b *PallocBits) PopcntRange(i, n uint) uint](#PallocBits.PopcntRange)
    * [type PallocData runtime.pallocData](#PallocData)
        * [func (d *PallocData) FindScavengeCandidate(searchIdx uint, min, max uintptr) (uint, uint)](#PallocData.FindScavengeCandidate)
        * [func (d *PallocData) AllocRange(i, n uint)](#PallocData.AllocRange)
        * [func (d *PallocData) ScavengedSetRange(i, n uint)](#PallocData.ScavengedSetRange)
        * [func (d *PallocData) PallocBits() *PallocBits](#PallocData.PallocBits)
        * [func (d *PallocData) Scavenged() *PallocBits](#PallocData.Scavenged)
    * [type PageCache runtime.pageCache](#PageCache)
        * [func NewPageCache(base uintptr, cache, scav uint64) PageCache](#NewPageCache)
        * [func (c *PageCache) Empty() bool](#PageCache.Empty)
        * [func (c *PageCache) Base() uintptr](#PageCache.Base)
        * [func (c *PageCache) Cache() uint64](#PageCache.Cache)
        * [func (c *PageCache) Scav() uint64](#PageCache.Scav)
        * [func (c *PageCache) Alloc(npages uintptr) (uintptr, uintptr)](#PageCache.Alloc)
        * [func (c *PageCache) Flush(s *PageAlloc)](#PageCache.Flush)
    * [type ChunkIdx runtime.chunkIdx](#ChunkIdx)
    * [type PageAlloc runtime.pageAlloc](#PageAlloc)
        * [func NewPageAlloc(chunks, scav map[ChunkIdx][]BitRange) *PageAlloc](#NewPageAlloc)
        * [func (p *PageAlloc) Alloc(npages uintptr) (uintptr, uintptr)](#PageAlloc.Alloc)
        * [func (p *PageAlloc) AllocToCache() PageCache](#PageAlloc.AllocToCache)
        * [func (p *PageAlloc) Free(base, npages uintptr)](#PageAlloc.Free)
        * [func (p *PageAlloc) Bounds() (ChunkIdx, ChunkIdx)](#PageAlloc.Bounds)
        * [func (p *PageAlloc) Scavenge(nbytes uintptr, mayUnlock bool) (r uintptr)](#PageAlloc.Scavenge)
        * [func (p *PageAlloc) InUse() []AddrRange](#PageAlloc.InUse)
        * [func (p *PageAlloc) PallocData(i ChunkIdx) *PallocData](#PageAlloc.PallocData)
    * [type AddrRange struct](#AddrRange)
        * [func MakeAddrRange(base, limit uintptr) AddrRange](#MakeAddrRange)
        * [func (a AddrRange) Base() uintptr](#AddrRange.Base)
        * [func (a AddrRange) Limit() uintptr](#AddrRange.Limit)
        * [func (a AddrRange) Equals(b AddrRange) bool](#AddrRange.Equals)
        * [func (a AddrRange) Size() uintptr](#AddrRange.Size)
    * [type AddrRanges struct](#AddrRanges)
        * [func NewAddrRanges() AddrRanges](#NewAddrRanges)
        * [func MakeAddrRanges(a ...AddrRange) AddrRanges](#MakeAddrRanges)
        * [func (a *AddrRanges) Ranges() []AddrRange](#AddrRanges.Ranges)
        * [func (a *AddrRanges) FindSucc(base uintptr) int](#AddrRanges.FindSucc)
        * [func (a *AddrRanges) Add(r AddrRange)](#AddrRanges.Add)
        * [func (a *AddrRanges) TotalBytes() uintptr](#AddrRanges.TotalBytes)
    * [type BitRange struct](#BitRange)
    * [type BitsMismatch struct](#BitsMismatch)
    * [type MSpan runtime.mspan](#MSpan)
        * [func AllocMSpan() *MSpan](#AllocMSpan)
    * [type TimeHistogram runtime.timeHistogram](#TimeHistogram)
        * [func (th *TimeHistogram) Count(bucket, subBucket uint) (uint64, bool)](#TimeHistogram.Count)
        * [func (th *TimeHistogram) Record(duration int64)](#TimeHistogram.Record)
    * [type M runtime.m](#M)
        * [func allocm(_p_ *p, fn func(), id int64) *m](#allocm)
        * [func lockextra(nilokay bool) *m](#lockextra)
        * [func mget() *m](#mget)
        * [func acquirem() *m](#acquirem)
        * [func traceAcquireBuffer() (mp *m, pid int32, bufp *traceBufPtr)](#traceAcquireBuffer)
* [Functions](#func)
    * [func memhash0(p unsafe.Pointer, h uintptr) uintptr](#memhash0)
    * [func memhash8(p unsafe.Pointer, h uintptr) uintptr](#memhash8)
    * [func memhash16(p unsafe.Pointer, h uintptr) uintptr](#memhash16)
    * [func memhash128(p unsafe.Pointer, h uintptr) uintptr](#memhash128)
    * [func memhash_varlen(p unsafe.Pointer, h uintptr) uintptr](#memhash_varlen)
    * [func memhash(p unsafe.Pointer, h, s uintptr) uintptr](#memhash)
    * [func memhash32(p unsafe.Pointer, h uintptr) uintptr](#memhash32)
    * [func memhash64(p unsafe.Pointer, h uintptr) uintptr](#memhash64)
    * [func strhash(p unsafe.Pointer, h uintptr) uintptr](#strhash)
    * [func strhashFallback(a unsafe.Pointer, h uintptr) uintptr](#strhashFallback)
    * [func f32hash(p unsafe.Pointer, h uintptr) uintptr](#f32hash)
    * [func f64hash(p unsafe.Pointer, h uintptr) uintptr](#f64hash)
    * [func c64hash(p unsafe.Pointer, h uintptr) uintptr](#c64hash)
    * [func c128hash(p unsafe.Pointer, h uintptr) uintptr](#c128hash)
    * [func interhash(p unsafe.Pointer, h uintptr) uintptr](#interhash)
    * [func nilinterhash(p unsafe.Pointer, h uintptr) uintptr](#nilinterhash)
    * [func typehash(t *_type, p unsafe.Pointer, h uintptr) uintptr](#typehash)
    * [func reflect_typehash(t *_type, p unsafe.Pointer, h uintptr) uintptr](#reflect_typehash)
    * [func memequal0(p, q unsafe.Pointer) bool](#memequal0)
    * [func memequal8(p, q unsafe.Pointer) bool](#memequal8)
    * [func memequal16(p, q unsafe.Pointer) bool](#memequal16)
    * [func memequal32(p, q unsafe.Pointer) bool](#memequal32)
    * [func memequal64(p, q unsafe.Pointer) bool](#memequal64)
    * [func memequal128(p, q unsafe.Pointer) bool](#memequal128)
    * [func f32equal(p, q unsafe.Pointer) bool](#f32equal)
    * [func f64equal(p, q unsafe.Pointer) bool](#f64equal)
    * [func c64equal(p, q unsafe.Pointer) bool](#c64equal)
    * [func c128equal(p, q unsafe.Pointer) bool](#c128equal)
    * [func strequal(p, q unsafe.Pointer) bool](#strequal)
    * [func interequal(p, q unsafe.Pointer) bool](#interequal)
    * [func nilinterequal(p, q unsafe.Pointer) bool](#nilinterequal)
    * [func efaceeq(t *_type, x, y unsafe.Pointer) bool](#efaceeq)
    * [func ifaceeq(tab *itab, x, y unsafe.Pointer) bool](#ifaceeq)
    * [func stringHash(s string, seed uintptr) uintptr](#stringHash)
    * [func bytesHash(b []byte, seed uintptr) uintptr](#bytesHash)
    * [func int32Hash(i uint32, seed uintptr) uintptr](#int32Hash)
    * [func int64Hash(i uint64, seed uintptr) uintptr](#int64Hash)
    * [func efaceHash(i interface{}, seed uintptr) uintptr](#efaceHash)
    * [func ifaceHash(i interface {...](#ifaceHash)
    * [func alginit()](#alginit)
    * [func initAlgAES()](#initAlgAES)
    * [func readUnaligned32(p unsafe.Pointer) uint32](#readUnaligned32)
    * [func readUnaligned64(p unsafe.Pointer) uint64](#readUnaligned64)
    * [func atomicwb(ptr *unsafe.Pointer, new unsafe.Pointer)](#atomicwb)
    * [func atomicstorep(ptr unsafe.Pointer, new unsafe.Pointer)](#atomicstorep)
    * [func sync_atomic_StoreUintptr(ptr *uintptr, new uintptr)](#sync_atomic_StoreUintptr)
    * [func sync_atomic_StorePointer(ptr *unsafe.Pointer, new unsafe.Pointer)](#sync_atomic_StorePointer)
    * [func sync_atomic_SwapUintptr(ptr *uintptr, new uintptr) uintptr](#sync_atomic_SwapUintptr)
    * [func sync_atomic_SwapPointer(ptr *unsafe.Pointer, new unsafe.Pointer) unsafe.Pointer](#sync_atomic_SwapPointer)
    * [func sync_atomic_CompareAndSwapUintptr(ptr *uintptr, old, new uintptr) bool](#sync_atomic_CompareAndSwapUintptr)
    * [func sync_atomic_CompareAndSwapPointer(ptr *unsafe.Pointer, old, new unsafe.Pointer) bool](#sync_atomic_CompareAndSwapPointer)
    * [func cgoUse(interface{})](#cgoUse)
    * [func syscall_cgocaller(fn unsafe.Pointer, args ...uintptr) uintptr](#syscall_cgocaller)
    * [func cgocall(fn, arg unsafe.Pointer) int32](#cgocall)
    * [func cgocallbackg(fn, frame unsafe.Pointer, ctxt uintptr)](#cgocallbackg)
    * [func cgocallbackg1(fn, frame unsafe.Pointer, ctxt uintptr)](#cgocallbackg1)
    * [func unwindm(restore *bool)](#unwindm)
    * [func badcgocallback()](#badcgocallback)
    * [func cgounimpl()](#cgounimpl)
    * [func cgoCheckPointer(ptr interface{}, arg interface{})](#cgoCheckPointer)
    * [func cgoCheckArg(t *_type, p unsafe.Pointer, indir, top bool, msg string)](#cgoCheckArg)
    * [func cgoCheckUnknownPointer(p unsafe.Pointer, msg string) (base, i uintptr)](#cgoCheckUnknownPointer)
    * [func cgoIsGoPointer(p unsafe.Pointer) bool](#cgoIsGoPointer)
    * [func cgoInRange(p unsafe.Pointer, start, end uintptr) bool](#cgoInRange)
    * [func cgoCheckResult(val interface{})](#cgoCheckResult)
    * [func _cgo_panic_internal(p *byte)](#_cgo_panic_internal)
    * [func cgoCheckWriteBarrier(dst *uintptr, src uintptr)](#cgoCheckWriteBarrier)
    * [func cgoCheckMemmove(typ *_type, dst, src unsafe.Pointer, off, size uintptr)](#cgoCheckMemmove)
    * [func cgoCheckSliceCopy(typ *_type, dst, src unsafe.Pointer, n int)](#cgoCheckSliceCopy)
    * [func cgoCheckTypedBlock(typ *_type, src unsafe.Pointer, off, size uintptr)](#cgoCheckTypedBlock)
    * [func cgoCheckBits(src unsafe.Pointer, gcbits *byte, off, size uintptr)](#cgoCheckBits)
    * [func cgoCheckUsingType(typ *_type, src unsafe.Pointer, off, size uintptr)](#cgoCheckUsingType)
    * [func chanbuf(c *hchan, i uint) unsafe.Pointer](#chanbuf)
    * [func full(c *hchan) bool](#full)
    * [func chansend1(c *hchan, elem unsafe.Pointer)](#chansend1)
    * [func chansend(c *hchan, ep unsafe.Pointer, block bool, callerpc uintptr) bool](#chansend)
    * [func send(c *hchan, sg *sudog, ep unsafe.Pointer, unlockf func(), skip int)](#send)
    * [func sendDirect(t *_type, sg *sudog, src unsafe.Pointer)](#sendDirect)
    * [func recvDirect(t *_type, sg *sudog, dst unsafe.Pointer)](#recvDirect)
    * [func closechan(c *hchan)](#closechan)
    * [func empty(c *hchan) bool](#empty)
    * [func chanrecv1(c *hchan, elem unsafe.Pointer)](#chanrecv1)
    * [func chanrecv2(c *hchan, elem unsafe.Pointer) (received bool)](#chanrecv2)
    * [func chanrecv(c *hchan, ep unsafe.Pointer, block bool) (selected, received bool)](#chanrecv)
    * [func recv(c *hchan, sg *sudog, ep unsafe.Pointer, unlockf func(), skip int)](#recv)
    * [func chanparkcommit(gp *g, chanLock unsafe.Pointer) bool](#chanparkcommit)
    * [func selectnbsend(c *hchan, elem unsafe.Pointer) (selected bool)](#selectnbsend)
    * [func selectnbrecv(elem unsafe.Pointer, c *hchan) (selected, received bool)](#selectnbrecv)
    * [func reflect_chansend(c *hchan, elem unsafe.Pointer, nb bool) (selected bool)](#reflect_chansend)
    * [func reflect_chanrecv(c *hchan, nb bool, elem unsafe.Pointer) (selected bool, received bool)](#reflect_chanrecv)
    * [func reflect_chanlen(c *hchan) int](#reflect_chanlen)
    * [func reflectlite_chanlen(c *hchan) int](#reflectlite_chanlen)
    * [func reflect_chancap(c *hchan) int](#reflect_chancap)
    * [func reflect_chanclose(c *hchan)](#reflect_chanclose)
    * [func racesync(c *hchan, sg *sudog)](#racesync)
    * [func racenotify(c *hchan, idx uint, sg *sudog)](#racenotify)
    * [func checkptrAlignment(p unsafe.Pointer, elem *_type, n uintptr)](#checkptrAlignment)
    * [func checkptrArithmetic(p unsafe.Pointer, originals []unsafe.Pointer)](#checkptrArithmetic)
    * [func checkptrBase(p unsafe.Pointer) uintptr](#checkptrBase)
    * [func inf2one(f float64) float64](#inf2one)
    * [func complex128div(n complex128, m complex128) complex128](#complex128div)
    * [func init()](#init)
    * [func SetCPUProfileRate(hz int)](#SetCPUProfileRate)
    * [func CPUProfile() []byte](#CPUProfile)
    * [func runtime_pprof_runtime_cyclesPerSecond() int64](#runtime_pprof_runtime_cyclesPerSecond)
    * [func runtime_pprof_readProfile() ([]uint64, []unsafe.Pointer, bool)](#runtime_pprof_readProfile)
    * [func cputicks() int64](#cputicks)
    * [func GOMAXPROCS(n int) int](#GOMAXPROCS)
    * [func NumCPU() int](#NumCPU)
    * [func NumCgoCall() int64](#NumCgoCall)
    * [func NumGoroutine() int](#NumGoroutine)
    * [func debug_modinfo() string](#debug_modinfo)
    * [func debugCallV2()](#debugCallV2)
    * [func debugCallPanicked(val interface{})](#debugCallPanicked)
    * [func debugCallCheck(pc uintptr) string](#debugCallCheck)
    * [func debugCallWrap(dispatch uintptr)](#debugCallWrap)
    * [func debugCallWrap1()](#debugCallWrap1)
    * [func debugCallWrap2(dispatch uintptr)](#debugCallWrap2)
    * [func printDebugLog()](#printDebugLog)
    * [func printDebugLogPC(pc uintptr, returnPC bool)](#printDebugLogPC)
    * [func putCachedDlogger(l *dlogger) bool](#putCachedDlogger)
    * [func gogetenv(key string) string](#gogetenv)
    * [func envKeyEqual(a, b string) bool](#envKeyEqual)
    * [func lowerASCII(c byte) byte](#lowerASCII)
    * [func syscall_setenv_c(k string, v string)](#syscall_setenv_c)
    * [func syscall_unsetenv_c(k string)](#syscall_unsetenv_c)
    * [func cstring(s string) unsafe.Pointer](#cstring)
    * [func itoa(buf []byte, val uint64) []byte](#itoa)
    * [func appendIntStr(b []byte, v int64, signed bool) []byte](#appendIntStr)
    * [func printany(i interface{})](#printany)
    * [func printanycustomtype(i interface{})](#printanycustomtype)
    * [func panicwrap()](#panicwrap)
    * [func Caller(skip int) (pc uintptr, file string, line int, ok bool)](#Caller)
    * [func Callers(skip int, pc []uintptr) int](#Callers)
    * [func GOROOT() string](#GOROOT)
    * [func Version() string](#Version)
    * [func fastlog2(x float64) float64](#fastlog2)
    * [func isNaN(f float64) (is bool)](#isNaN)
    * [func isFinite(f float64) bool](#isFinite)
    * [func isInf(f float64) bool](#isInf)
    * [func abs(x float64) float64](#abs)
    * [func copysign(x, y float64) float64](#copysign)
    * [func float64bits(f float64) uint64](#float64bits)
    * [func float64frombits(b uint64) float64](#float64frombits)
    * [func memhashFallback(p unsafe.Pointer, seed, s uintptr) uintptr](#memhashFallback)
    * [func memhash32Fallback(p unsafe.Pointer, seed uintptr) uintptr](#memhash32Fallback)
    * [func memhash64Fallback(p unsafe.Pointer, seed uintptr) uintptr](#memhash64Fallback)
    * [func mix(a, b uintptr) uintptr](#mix)
    * [func r4(p unsafe.Pointer) uintptr](#r4)
    * [func r8(p unsafe.Pointer) uintptr](#r8)
    * [func runtime_debug_WriteHeapDump(fd uintptr)](#runtime_debug_WriteHeapDump)
    * [func dwrite(data unsafe.Pointer, len uintptr)](#dwrite)
    * [func dwritebyte(b byte)](#dwritebyte)
    * [func flush()](#flush)
    * [func dumpint(v uint64)](#dumpint)
    * [func dumpbool(b bool)](#dumpbool)
    * [func dumpmemrange(data unsafe.Pointer, len uintptr)](#dumpmemrange)
    * [func dumpslice(b []byte)](#dumpslice)
    * [func dumpstr(s string)](#dumpstr)
    * [func dumptype(t *_type)](#dumptype)
    * [func dumpobj(obj unsafe.Pointer, size uintptr, bv bitvector)](#dumpobj)
    * [func dumpotherroot(description string, to unsafe.Pointer)](#dumpotherroot)
    * [func dumpfinalizer(obj unsafe.Pointer, fn *funcval, fint *_type, ot *ptrtype)](#dumpfinalizer)
    * [func dumpbv(cbv *bitvector, offset uintptr)](#dumpbv)
    * [func dumpframe(s *stkframe, arg unsafe.Pointer) bool](#dumpframe)
    * [func dumpgoroutine(gp *g)](#dumpgoroutine)
    * [func dumpgs()](#dumpgs)
    * [func finq_callback(fn *funcval, obj unsafe.Pointer, nret uintptr, fint *_type, ot *ptrtype)](#finq_callback)
    * [func dumproots()](#dumproots)
    * [func dumpobjs()](#dumpobjs)
    * [func dumpparams()](#dumpparams)
    * [func itab_callback(tab *itab)](#itab_callback)
    * [func dumpitabs()](#dumpitabs)
    * [func dumpms()](#dumpms)
    * [func dumpmemstats(m *MemStats)](#dumpmemstats)
    * [func dumpmemprof_callback(b *bucket, nstk uintptr, pstk *uintptr, size, allocs, frees uintptr)](#dumpmemprof_callback)
    * [func dumpmemprof()](#dumpmemprof)
    * [func mdump(m *MemStats)](#mdump)
    * [func writeheapdump_m(fd uintptr, m *MemStats)](#writeheapdump_m)
    * [func dumpfields(bv bitvector)](#dumpfields)
    * [func float64Inf() float64](#float64Inf)
    * [func float64NegInf() float64](#float64NegInf)
    * [func timeHistogramMetricsBuckets() []float64](#timeHistogramMetricsBuckets)
    * [func itabHashFunc(inter *interfacetype, typ *_type) uintptr](#itabHashFunc)
    * [func itabAdd(m *itab)](#itabAdd)
    * [func itabsinit()](#itabsinit)
    * [func panicdottypeE(have, want, iface *_type)](#panicdottypeE)
    * [func panicdottypeI(have *itab, want, iface *_type)](#panicdottypeI)
    * [func panicnildottype(want *_type)](#panicnildottype)
    * [func convT16(val uint16) (x unsafe.Pointer)](#convT16)
    * [func convT32(val uint32) (x unsafe.Pointer)](#convT32)
    * [func convT64(val uint64) (x unsafe.Pointer)](#convT64)
    * [func convTstring(val string) (x unsafe.Pointer)](#convTstring)
    * [func convTslice(val []byte) (x unsafe.Pointer)](#convTslice)
    * [func reflect_ifaceE2I(inter *interfacetype, e eface, dst *iface)](#reflect_ifaceE2I)
    * [func reflectlite_ifaceE2I(inter *interfacetype, e eface, dst *iface)](#reflectlite_ifaceE2I)
    * [func iterate_itabs(fn func(*itab))](#iterate_itabs)
    * [func unreachableMethod()](#unreachableMethod)
    * [func lfnodeValidate(node *lfnode)](#lfnodeValidate)
    * [func lfstackPack(node *lfnode, cnt uintptr) uint64](#lfstackPack)
    * [func lock(l *mutex)](#lock)
    * [func lock2(l *mutex)](#lock2)
    * [func unlock(l *mutex)](#unlock)
    * [func unlock2(l *mutex)](#unlock2)
    * [func noteclear(n *note)](#noteclear)
    * [func notewakeup(n *note)](#notewakeup)
    * [func notesleep(n *note)](#notesleep)
    * [func notetsleep_internal(n *note, ns int64, gp *g, deadline int64) bool](#notetsleep_internal)
    * [func notetsleep(n *note, ns int64) bool](#notetsleep)
    * [func notetsleepg(n *note, ns int64) bool](#notetsleepg)
    * [func checkTimeouts()](#checkTimeouts)
    * [func lockInit(l *mutex, rank lockRank)](#lockInit)
    * [func lockWithRank(l *mutex, rank lockRank)](#lockWithRank)
    * [func acquireLockRank(rank lockRank)](#acquireLockRank)
    * [func unlockWithRank(l *mutex)](#unlockWithRank)
    * [func releaseLockRank(rank lockRank)](#releaseLockRank)
    * [func lockWithRankMayAcquire(l *mutex, rank lockRank)](#lockWithRankMayAcquire)
    * [func assertLockHeld(l *mutex)](#assertLockHeld)
    * [func assertRankHeld(r lockRank)](#assertRankHeld)
    * [func worldStopped()](#worldStopped)
    * [func worldStarted()](#worldStarted)
    * [func assertWorldStopped()](#assertWorldStopped)
    * [func assertWorldStoppedOrLockHeld(l *mutex)](#assertWorldStoppedOrLockHeld)
    * [func mallocinit()](#mallocinit)
    * [func sysReserveAligned(v unsafe.Pointer, size, align uintptr) (unsafe.Pointer, uintptr)](#sysReserveAligned)
    * [func mallocgc(size uintptr, typ *_type, needzero bool) unsafe.Pointer](#mallocgc)
    * [func memclrNoHeapPointersChunked(size uintptr, x unsafe.Pointer)](#memclrNoHeapPointersChunked)
    * [func newobject(typ *_type) unsafe.Pointer](#newobject)
    * [func reflect_unsafe_New(typ *_type) unsafe.Pointer](#reflect_unsafe_New)
    * [func reflectlite_unsafe_New(typ *_type) unsafe.Pointer](#reflectlite_unsafe_New)
    * [func newarray(typ *_type, n int) unsafe.Pointer](#newarray)
    * [func reflect_unsafe_NewArray(typ *_type, n int) unsafe.Pointer](#reflect_unsafe_NewArray)
    * [func profilealloc(mp *m, x unsafe.Pointer, size uintptr)](#profilealloc)
    * [func nextSample() uintptr](#nextSample)
    * [func fastexprand(mean int) int32](#fastexprand)
    * [func nextSampleNoFP() uintptr](#nextSampleNoFP)
    * [func persistentalloc(size, align uintptr, sysStat *sysMemStat) unsafe.Pointer](#persistentalloc)
    * [func inPersistentAlloc(p uintptr) bool](#inPersistentAlloc)
    * [func isEmpty(x uint8) bool](#isEmpty)
    * [func bucketShift(b uint8) uintptr](#bucketShift)
    * [func bucketMask(b uint8) uintptr](#bucketMask)
    * [func tophash(hash uintptr) uint8](#tophash)
    * [func evacuated(b *bmap) bool](#evacuated)
    * [func mapaccess1(t *maptype, h *hmap, key unsafe.Pointer) unsafe.Pointer](#mapaccess1)
    * [func mapaccess2(t *maptype, h *hmap, key unsafe.Pointer) (unsafe.Pointer, bool)](#mapaccess2)
    * [func mapaccessK(t *maptype, h *hmap, key unsafe.Pointer) (unsafe.Pointer, unsafe.Pointer)](#mapaccessK)
    * [func mapaccess1_fat(t *maptype, h *hmap, key, zero unsafe.Pointer) unsafe.Pointer](#mapaccess1_fat)
    * [func mapaccess2_fat(t *maptype, h *hmap, key, zero unsafe.Pointer) (unsafe.Pointer, bool)](#mapaccess2_fat)
    * [func mapassign(t *maptype, h *hmap, key unsafe.Pointer) unsafe.Pointer](#mapassign)
    * [func mapdelete(t *maptype, h *hmap, key unsafe.Pointer)](#mapdelete)
    * [func mapiterinit(t *maptype, h *hmap, it *hiter)](#mapiterinit)
    * [func mapiternext(it *hiter)](#mapiternext)
    * [func mapclear(t *maptype, h *hmap)](#mapclear)
    * [func hashGrow(t *maptype, h *hmap)](#hashGrow)
    * [func overLoadFactor(count int, B uint8) bool](#overLoadFactor)
    * [func tooManyOverflowBuckets(noverflow uint16, B uint8) bool](#tooManyOverflowBuckets)
    * [func growWork(t *maptype, h *hmap, bucket uintptr)](#growWork)
    * [func bucketEvacuated(t *maptype, h *hmap, bucket uintptr) bool](#bucketEvacuated)
    * [func evacuate(t *maptype, h *hmap, oldbucket uintptr)](#evacuate)
    * [func advanceEvacuationMark(h *hmap, t *maptype, newbit uintptr)](#advanceEvacuationMark)
    * [func reflect_mapaccess(t *maptype, h *hmap, key unsafe.Pointer) unsafe.Pointer](#reflect_mapaccess)
    * [func reflect_mapassign(t *maptype, h *hmap, key unsafe.Pointer, elem unsafe.Pointer)](#reflect_mapassign)
    * [func reflect_mapdelete(t *maptype, h *hmap, key unsafe.Pointer)](#reflect_mapdelete)
    * [func reflect_mapiternext(it *hiter)](#reflect_mapiternext)
    * [func reflect_mapiterkey(it *hiter) unsafe.Pointer](#reflect_mapiterkey)
    * [func reflect_mapiterelem(it *hiter) unsafe.Pointer](#reflect_mapiterelem)
    * [func reflect_maplen(h *hmap) int](#reflect_maplen)
    * [func reflectlite_maplen(h *hmap) int](#reflectlite_maplen)
    * [func mapaccess1_fast32(t *maptype, h *hmap, key uint32) unsafe.Pointer](#mapaccess1_fast32)
    * [func mapaccess2_fast32(t *maptype, h *hmap, key uint32) (unsafe.Pointer, bool)](#mapaccess2_fast32)
    * [func mapassign_fast32(t *maptype, h *hmap, key uint32) unsafe.Pointer](#mapassign_fast32)
    * [func mapassign_fast32ptr(t *maptype, h *hmap, key unsafe.Pointer) unsafe.Pointer](#mapassign_fast32ptr)
    * [func mapdelete_fast32(t *maptype, h *hmap, key uint32)](#mapdelete_fast32)
    * [func growWork_fast32(t *maptype, h *hmap, bucket uintptr)](#growWork_fast32)
    * [func evacuate_fast32(t *maptype, h *hmap, oldbucket uintptr)](#evacuate_fast32)
    * [func mapaccess1_fast64(t *maptype, h *hmap, key uint64) unsafe.Pointer](#mapaccess1_fast64)
    * [func mapaccess2_fast64(t *maptype, h *hmap, key uint64) (unsafe.Pointer, bool)](#mapaccess2_fast64)
    * [func mapassign_fast64(t *maptype, h *hmap, key uint64) unsafe.Pointer](#mapassign_fast64)
    * [func mapassign_fast64ptr(t *maptype, h *hmap, key unsafe.Pointer) unsafe.Pointer](#mapassign_fast64ptr)
    * [func mapdelete_fast64(t *maptype, h *hmap, key uint64)](#mapdelete_fast64)
    * [func growWork_fast64(t *maptype, h *hmap, bucket uintptr)](#growWork_fast64)
    * [func evacuate_fast64(t *maptype, h *hmap, oldbucket uintptr)](#evacuate_fast64)
    * [func mapaccess1_faststr(t *maptype, h *hmap, ky string) unsafe.Pointer](#mapaccess1_faststr)
    * [func mapaccess2_faststr(t *maptype, h *hmap, ky string) (unsafe.Pointer, bool)](#mapaccess2_faststr)
    * [func mapassign_faststr(t *maptype, h *hmap, s string) unsafe.Pointer](#mapassign_faststr)
    * [func mapdelete_faststr(t *maptype, h *hmap, ky string)](#mapdelete_faststr)
    * [func growWork_faststr(t *maptype, h *hmap, bucket uintptr)](#growWork_faststr)
    * [func evacuate_faststr(t *maptype, h *hmap, oldbucket uintptr)](#evacuate_faststr)
    * [func typedmemmove(typ *_type, dst, src unsafe.Pointer)](#typedmemmove)
    * [func reflect_typedmemmove(typ *_type, dst, src unsafe.Pointer)](#reflect_typedmemmove)
    * [func reflectlite_typedmemmove(typ *_type, dst, src unsafe.Pointer)](#reflectlite_typedmemmove)
    * [func reflect_typedmemmovepartial(typ *_type, dst, src unsafe.Pointer, off, size uintptr)](#reflect_typedmemmovepartial)
    * [func reflectcallmove(typ *_type, dst, src unsafe.Pointer, size uintptr, regs *abi.RegArgs)](#reflectcallmove)
    * [func typedslicecopy(typ *_type, dstPtr unsafe.Pointer, dstLen int, srcPtr unsafe.Pointer, srcLen int) int](#typedslicecopy)
    * [func reflect_typedslicecopy(elemType *_type, dst, src slice) int](#reflect_typedslicecopy)
    * [func typedmemclr(typ *_type, ptr unsafe.Pointer)](#typedmemclr)
    * [func reflect_typedmemclr(typ *_type, ptr unsafe.Pointer)](#reflect_typedmemclr)
    * [func reflect_typedmemclrpartial(typ *_type, ptr unsafe.Pointer, off, size uintptr)](#reflect_typedmemclrpartial)
    * [func memclrHasPointers(ptr unsafe.Pointer, n uintptr)](#memclrHasPointers)
    * [func addb(p *byte, n uintptr) *byte](#addb)
    * [func subtractb(p *byte, n uintptr) *byte](#subtractb)
    * [func add1(p *byte) *byte](#add1)
    * [func subtract1(p *byte) *byte](#subtract1)
    * [func badPointer(s *mspan, p, refBase, refOff uintptr)](#badPointer)
    * [func bulkBarrierPreWrite(dst, src, size uintptr)](#bulkBarrierPreWrite)
    * [func bulkBarrierPreWriteSrcOnly(dst, src, size uintptr)](#bulkBarrierPreWriteSrcOnly)
    * [func bulkBarrierBitmap(dst, src, size, maskOffset uintptr, bits *uint8)](#bulkBarrierBitmap)
    * [func typeBitsBulkBarrier(typ *_type, dst, src, size uintptr)](#typeBitsBulkBarrier)
    * [func heapBitsSetType(x, size, dataSize uintptr, typ *_type)](#heapBitsSetType)
    * [func heapBitsSetTypeGCProg(h heapBits, progSize, elemSize, dataSize, allocSize uintptr, prog *byte)](#heapBitsSetTypeGCProg)
    * [func runGCProg(prog, trailer, dst *byte, size int) uintptr](#runGCProg)
    * [func dematerializeGCProg(s *mspan)](#dematerializeGCProg)
    * [func dumpGCProg(p *byte)](#dumpGCProg)
    * [func getgcmaskcb(frame *stkframe, ctxt unsafe.Pointer) bool](#getgcmaskcb)
    * [func reflect_gcbits(x interface{}) []byte](#reflect_gcbits)
    * [func getgcmask(ep interface{}) (mask []byte)](#getgcmask)
    * [func freemcache(c *mcache)](#freemcache)
    * [func startCheckmarks()](#startCheckmarks)
    * [func endCheckmarks()](#endCheckmarks)
    * [func setCheckmark(obj, base, off uintptr, mbits markBits) bool](#setCheckmark)
    * [func sysAlloc(n uintptr, sysStat *sysMemStat) unsafe.Pointer](#sysAlloc)
    * [func sysUnused(v unsafe.Pointer, n uintptr)](#sysUnused)
    * [func sysUsed(v unsafe.Pointer, n uintptr)](#sysUsed)
    * [func sysHugePage(v unsafe.Pointer, n uintptr)](#sysHugePage)
    * [func sysFree(v unsafe.Pointer, n uintptr, sysStat *sysMemStat)](#sysFree)
    * [func sysFault(v unsafe.Pointer, n uintptr)](#sysFault)
    * [func sysReserve(v unsafe.Pointer, n uintptr) unsafe.Pointer](#sysReserve)
    * [func sysMap(v unsafe.Pointer, n uintptr, sysStat *sysMemStat)](#sysMap)
    * [func initMetrics()](#initMetrics)
    * [func readMetrics(samplesp unsafe.Pointer, len int, cap int)](#readMetrics)
    * [func queuefinalizer(p unsafe.Pointer, fn *funcval, nret uintptr, fint *_type, ot *ptrtype)](#queuefinalizer)
    * [func iterate_finq(callback func(*funcval, unsafe.Pointer, uintptr, *_type, *ptrtype))](#iterate_finq)
    * [func createfing()](#createfing)
    * [func runfinq()](#runfinq)
    * [func SetFinalizer(obj interface{}, finalizer interface{})](#SetFinalizer)
    * [func KeepAlive(x interface{})](#KeepAlive)
    * [func gcinit()](#gcinit)
    * [func gcenable()](#gcenable)
    * [func setGCPhase(x uint32)](#setGCPhase)
    * [func pollFractionalWorkerExit() bool](#pollFractionalWorkerExit)
    * [func GC()](#GC)
    * [func gcWaitOnMark(n uint32)](#gcWaitOnMark)
    * [func gcStart(trigger gcTrigger)](#gcStart)
    * [func gcMarkDone()](#gcMarkDone)
    * [func gcMarkTermination(nextTriggerRatio float64)](#gcMarkTermination)
    * [func gcBgMarkStartWorkers()](#gcBgMarkStartWorkers)
    * [func gcBgMarkPrepare()](#gcBgMarkPrepare)
    * [func gcBgMarkWorker()](#gcBgMarkWorker)
    * [func gcMarkWorkAvailable(p *p) bool](#gcMarkWorkAvailable)
    * [func gcMark(startTime int64)](#gcMark)
    * [func gcSweep(mode gcMode)](#gcSweep)
    * [func gcResetMarkState()](#gcResetMarkState)
    * [func sync_runtime_registerPoolCleanup(f func())](#sync_runtime_registerPoolCleanup)
    * [func clearpools()](#clearpools)
    * [func itoaDiv(buf []byte, val uint64, dec int) []byte](#itoaDiv)
    * [func fmtNSAsMS(buf []byte, ns uint64) []byte](#fmtNSAsMS)
    * [func gcTestMoveStackOnNextCall()](#gcTestMoveStackOnNextCall)
    * [func gcTestIsReachable(ptrs ...unsafe.Pointer) (mask uint64)](#gcTestIsReachable)
    * [func gcTestPointerClass(p unsafe.Pointer) string](#gcTestPointerClass)
    * [func gcMarkRootPrepare()](#gcMarkRootPrepare)
    * [func gcMarkRootCheck()](#gcMarkRootCheck)
    * [func markroot(gcw *gcWork, i uint32)](#markroot)
    * [func markrootBlock(b0, n0 uintptr, ptrmask0 *uint8, gcw *gcWork, shard int)](#markrootBlock)
    * [func markrootFreeGStacks()](#markrootFreeGStacks)
    * [func markrootSpans(gcw *gcWork, shard int)](#markrootSpans)
    * [func gcAssistAlloc(gp *g)](#gcAssistAlloc)
    * [func gcAssistAlloc1(gp *g, scanWork int64)](#gcAssistAlloc1)
    * [func gcWakeAllAssists()](#gcWakeAllAssists)
    * [func gcParkAssist() bool](#gcParkAssist)
    * [func gcFlushBgCredit(scanWork int64)](#gcFlushBgCredit)
    * [func scanstack(gp *g, gcw *gcWork)](#scanstack)
    * [func scanframeworker(frame *stkframe, state *stackScanState, gcw *gcWork)](#scanframeworker)
    * [func gcDrain(gcw *gcWork, flags gcDrainFlags)](#gcDrain)
    * [func gcDrainN(gcw *gcWork, scanWork int64) int64](#gcDrainN)
    * [func scanblock(b0, n0 uintptr, ptrmask *uint8, gcw *gcWork, stk *stackScanState)](#scanblock)
    * [func scanobject(b uintptr, gcw *gcWork)](#scanobject)
    * [func scanConservative(b, n uintptr, ptrmask *uint8, gcw *gcWork, state *stackScanState)](#scanConservative)
    * [func shade(b uintptr)](#shade)
    * [func greyobject(obj, base, off uintptr, span *mspan, gcw *gcWork, objIndex uintptr)](#greyobject)
    * [func gcDumpObject(label string, obj, off uintptr)](#gcDumpObject)
    * [func gcmarknewobject(span *mspan, obj, size, scanSize uintptr)](#gcmarknewobject)
    * [func gcMarkTinyAllocs()](#gcMarkTinyAllocs)
    * [func init()](#init)
    * [func setGCPercent(in int32) (out int32)](#setGCPercent)
    * [func readGOGC() int32](#readGOGC)
    * [func heapRetained() uint64](#heapRetained)
    * [func gcPaceScavenger()](#gcPaceScavenger)
    * [func readyForScavenger()](#readyForScavenger)
    * [func wakeScavenger()](#wakeScavenger)
    * [func scavengeSleep(ns int64) int64](#scavengeSleep)
    * [func bgscavenge()](#bgscavenge)
    * [func printScavTrace(gen uint32, released uintptr, forced bool)](#printScavTrace)
    * [func fillAligned(x uint64, m uint) uint64](#fillAligned)
    * [func init()](#init)
    * [func finishsweep_m()](#finishsweep_m)
    * [func bgsweep()](#bgsweep)
    * [func sweepone() uintptr](#sweepone)
    * [func isSweepDone() bool](#isSweepDone)
    * [func deductSweepCredit(spanBytes uintptr, callerSweepPages uintptr)](#deductSweepCredit)
    * [func clobberfree(x unsafe.Pointer, size uintptr)](#clobberfree)
    * [func init()](#init)
    * [func putempty(b *workbuf)](#putempty)
    * [func putfull(b *workbuf)](#putfull)
    * [func prepareFreeWorkbufs()](#prepareFreeWorkbufs)
    * [func freeSomeWbufs(preemptible bool) bool](#freeSomeWbufs)
    * [func recordspan(vh unsafe.Pointer, p unsafe.Pointer)](#recordspan)
    * [func arenaBase(i arenaIdx) uintptr](#arenaBase)
    * [func inheap(b uintptr) bool](#inheap)
    * [func inHeapOrStack(b uintptr) bool](#inHeapOrStack)
    * [func runtime_debug_freeOSMemory()](#runtime_debug_freeOSMemory)
    * [func spanHasSpecials(s *mspan)](#spanHasSpecials)
    * [func spanHasNoSpecials(s *mspan)](#spanHasNoSpecials)
    * [func addspecial(p unsafe.Pointer, s *special) bool](#addspecial)
    * [func addfinalizer(p unsafe.Pointer, f *funcval, nret uintptr, fint *_type, ot *ptrtype) bool](#addfinalizer)
    * [func removefinalizer(p unsafe.Pointer)](#removefinalizer)
    * [func setprofilebucket(p unsafe.Pointer, b *bucket)](#setprofilebucket)
    * [func freeSpecial(s *special, p unsafe.Pointer, size uintptr)](#freeSpecial)
    * [func nextMarkBitArenaEpoch()](#nextMarkBitArenaEpoch)
    * [func chunkBase(ci chunkIdx) uintptr](#chunkBase)
    * [func chunkPageIndex(p uintptr) uint](#chunkPageIndex)
    * [func offAddrToLevelIndex(level int, addr offAddr) int](#offAddrToLevelIndex)
    * [func addrsToSummaryRange(level int, base, limit uintptr) (lo int, hi int)](#addrsToSummaryRange)
    * [func blockAlignSummaryRange(level int, lo, hi int) (int, int)](#blockAlignSummaryRange)
    * [func findBitRange64(c uint64, n uint) uint](#findBitRange64)
    * [func eqslice(x, y []uintptr) bool](#eqslice)
    * [func mProf_NextCycle()](#mProf_NextCycle)
    * [func mProf_Flush()](#mProf_Flush)
    * [func mProf_FlushLocked()](#mProf_FlushLocked)
    * [func mProf_PostSweep()](#mProf_PostSweep)
    * [func mProf_Malloc(p unsafe.Pointer, size uintptr)](#mProf_Malloc)
    * [func mProf_Free(b *bucket, size uintptr)](#mProf_Free)
    * [func SetBlockProfileRate(rate int)](#SetBlockProfileRate)
    * [func blockevent(cycles int64, skip int)](#blockevent)
    * [func blocksampled(cycles, rate int64) bool](#blocksampled)
    * [func saveblockevent(cycles, rate int64, skip int, which bucketType)](#saveblockevent)
    * [func SetMutexProfileFraction(rate int) int](#SetMutexProfileFraction)
    * [func mutexevent(cycles int64, skip int)](#mutexevent)
    * [func defaultMemProfileRate(v int) int](#defaultMemProfileRate)
    * [func MemProfile(p []MemProfileRecord, inuseZero bool) (n int, ok bool)](#MemProfile)
    * [func record(r *MemProfileRecord, b *bucket)](#record)
    * [func iterate_memprof(fn func(*bucket, uintptr, *uintptr, uintptr, uintptr, uintptr))](#iterate_memprof)
    * [func BlockProfile(p []BlockProfileRecord) (n int, ok bool)](#BlockProfile)
    * [func MutexProfile(p []BlockProfileRecord) (n int, ok bool)](#MutexProfile)
    * [func ThreadCreateProfile(p []StackRecord) (n int, ok bool)](#ThreadCreateProfile)
    * [func runtime_goroutineProfileWithLabels(p []StackRecord, labels []unsafe.Pointer) (n int, ok bool)](#runtime_goroutineProfileWithLabels)
    * [func goroutineProfileWithLabels(p []StackRecord, labels []unsafe.Pointer) (n int, ok bool)](#goroutineProfileWithLabels)
    * [func GoroutineProfile(p []StackRecord) (n int, ok bool)](#GoroutineProfile)
    * [func saveg(pc, sp uintptr, gp *g, r *StackRecord)](#saveg)
    * [func Stack(buf []byte, all bool) int](#Stack)
    * [func tracealloc(p unsafe.Pointer, size uintptr, typ *_type)](#tracealloc)
    * [func tracefree(p unsafe.Pointer, size uintptr)](#tracefree)
    * [func tracegc()](#tracegc)
    * [func msanread(addr unsafe.Pointer, sz uintptr)](#msanread)
    * [func msanwrite(addr unsafe.Pointer, sz uintptr)](#msanwrite)
    * [func msanmalloc(addr unsafe.Pointer, sz uintptr)](#msanmalloc)
    * [func msanfree(addr unsafe.Pointer, sz uintptr)](#msanfree)
    * [func msanmove(dst, src unsafe.Pointer, sz uintptr)](#msanmove)
    * [func roundupsize(size uintptr) uintptr](#roundupsize)
    * [func init()](#init)
    * [func ReadMemStats(m *MemStats)](#ReadMemStats)
    * [func readmemstats_m(stats *MemStats)](#readmemstats_m)
    * [func readGCStats(pauses *[]uint64)](#readGCStats)
    * [func readGCStats_m(pauses *[]uint64)](#readGCStats_m)
    * [func updatememstats()](#updatememstats)
    * [func flushmcache(i int)](#flushmcache)
    * [func flushallmcaches()](#flushallmcaches)
    * [func wbBufFlush(dst *uintptr, src uintptr)](#wbBufFlush)
    * [func wbBufFlush1(_p_ *p)](#wbBufFlush1)
    * [func nonblockingPipe() (r, w int32, errno int32)](#nonblockingPipe)
    * [func poll_runtime_pollServerInit()](#poll_runtime_pollServerInit)
    * [func netpollGenericInit()](#netpollGenericInit)
    * [func netpollinited() bool](#netpollinited)
    * [func poll_runtime_isPollServerDescriptor(fd uintptr) bool](#poll_runtime_isPollServerDescriptor)
    * [func poll_runtime_pollClose(pd *pollDesc)](#poll_runtime_pollClose)
    * [func poll_runtime_pollReset(pd *pollDesc, mode int) int](#poll_runtime_pollReset)
    * [func poll_runtime_pollWait(pd *pollDesc, mode int) int](#poll_runtime_pollWait)
    * [func poll_runtime_pollWaitCanceled(pd *pollDesc, mode int)](#poll_runtime_pollWaitCanceled)
    * [func poll_runtime_pollSetDeadline(pd *pollDesc, d int64, mode int)](#poll_runtime_pollSetDeadline)
    * [func poll_runtime_pollUnblock(pd *pollDesc)](#poll_runtime_pollUnblock)
    * [func netpollready(toRun *gList, pd *pollDesc, mode int32)](#netpollready)
    * [func netpollcheckerr(pd *pollDesc, mode int32) int](#netpollcheckerr)
    * [func netpollblockcommit(gp *g, gpp unsafe.Pointer) bool](#netpollblockcommit)
    * [func netpollgoready(gp *g, traceskip int)](#netpollgoready)
    * [func netpollblock(pd *pollDesc, mode int32, waitio bool) bool](#netpollblock)
    * [func netpolldeadlineimpl(pd *pollDesc, seq uintptr, read, write bool)](#netpolldeadlineimpl)
    * [func netpollDeadline(arg interface{}, seq uintptr)](#netpollDeadline)
    * [func netpollReadDeadline(arg interface{}, seq uintptr)](#netpollReadDeadline)
    * [func netpollWriteDeadline(arg interface{}, seq uintptr)](#netpollWriteDeadline)
    * [func netpollinit()](#netpollinit)
    * [func netpollIsPollDescriptor(fd uintptr) bool](#netpollIsPollDescriptor)
    * [func netpollopen(fd uintptr, pd *pollDesc) int32](#netpollopen)
    * [func netpollclose(fd uintptr) int32](#netpollclose)
    * [func netpollarm(pd *pollDesc, mode int)](#netpollarm)
    * [func netpollBreak()](#netpollBreak)
    * [func unimplemented(name string)](#unimplemented)
    * [func semacreate(mp *m)](#semacreate)
    * [func semasleep(ns int64) int32](#semasleep)
    * [func semawakeup(mp *m)](#semawakeup)
    * [func sigNoteSetup(*note)](#sigNoteSetup)
    * [func sigNoteWakeup(*note)](#sigNoteWakeup)
    * [func sigNoteSleep(*note)](#sigNoteSleep)
    * [func osinit()](#osinit)
    * [func sysctlbynameInt32(name []byte) (int32, int32)](#sysctlbynameInt32)
    * [func internal_cpu_getsysctlbyname(name []byte) (int32, int32)](#internal_cpu_getsysctlbyname)
    * [func getncpu() int32](#getncpu)
    * [func getPageSize() uintptr](#getPageSize)
    * [func getRandomData(r []byte)](#getRandomData)
    * [func goenvs()](#goenvs)
    * [func newosproc(mp *m)](#newosproc)
    * [func mstart_stub()](#mstart_stub)
    * [func newosproc0(stacksize uintptr, fn uintptr)](#newosproc0)
    * [func libpreinit()](#libpreinit)
    * [func mpreinit(mp *m)](#mpreinit)
    * [func minit()](#minit)
    * [func unminit()](#unminit)
    * [func mdestroy(mp *m)](#mdestroy)
    * [func osyield_no_g()](#osyield_no_g)
    * [func osyield()](#osyield)
    * [func setsig(i uint32, fn uintptr)](#setsig)
    * [func sigtramp()](#sigtramp)
    * [func cgoSigtramp()](#cgoSigtramp)
    * [func setsigstack(i uint32)](#setsigstack)
    * [func getsig(i uint32) uintptr](#getsig)
    * [func setSignalstackSP(s *stackt, sp uintptr)](#setSignalstackSP)
    * [func sigaddset(mask *sigset, i int)](#sigaddset)
    * [func sigdelset(mask *sigset, i int)](#sigdelset)
    * [func sysargs(argc int32, argv **byte)](#sysargs)
    * [func signalM(mp *m, sig int)](#signalM)
    * [func osStackAlloc(s *mspan)](#osStackAlloc)
    * [func osStackFree(s *mspan)](#osStackFree)
    * [func panicCheck1(pc uintptr, msg string)](#panicCheck1)
    * [func panicCheck2(err string)](#panicCheck2)
    * [func goPanicIndex(x int, y int)](#goPanicIndex)
    * [func goPanicIndexU(x uint, y int)](#goPanicIndexU)
    * [func goPanicSliceAlen(x int, y int)](#goPanicSliceAlen)
    * [func goPanicSliceAlenU(x uint, y int)](#goPanicSliceAlenU)
    * [func goPanicSliceAcap(x int, y int)](#goPanicSliceAcap)
    * [func goPanicSliceAcapU(x uint, y int)](#goPanicSliceAcapU)
    * [func goPanicSliceB(x int, y int)](#goPanicSliceB)
    * [func goPanicSliceBU(x uint, y int)](#goPanicSliceBU)
    * [func goPanicSlice3Alen(x int, y int)](#goPanicSlice3Alen)
    * [func goPanicSlice3AlenU(x uint, y int)](#goPanicSlice3AlenU)
    * [func goPanicSlice3Acap(x int, y int)](#goPanicSlice3Acap)
    * [func goPanicSlice3AcapU(x uint, y int)](#goPanicSlice3AcapU)
    * [func goPanicSlice3B(x int, y int)](#goPanicSlice3B)
    * [func goPanicSlice3BU(x uint, y int)](#goPanicSlice3BU)
    * [func goPanicSlice3C(x int, y int)](#goPanicSlice3C)
    * [func goPanicSlice3CU(x uint, y int)](#goPanicSlice3CU)
    * [func goPanicSliceConvert(x int, y int)](#goPanicSliceConvert)
    * [func panicIndex(x int, y int)](#panicIndex)
    * [func panicIndexU(x uint, y int)](#panicIndexU)
    * [func panicSliceAlen(x int, y int)](#panicSliceAlen)
    * [func panicSliceAlenU(x uint, y int)](#panicSliceAlenU)
    * [func panicSliceAcap(x int, y int)](#panicSliceAcap)
    * [func panicSliceAcapU(x uint, y int)](#panicSliceAcapU)
    * [func panicSliceB(x int, y int)](#panicSliceB)
    * [func panicSliceBU(x uint, y int)](#panicSliceBU)
    * [func panicSlice3Alen(x int, y int)](#panicSlice3Alen)
    * [func panicSlice3AlenU(x uint, y int)](#panicSlice3AlenU)
    * [func panicSlice3Acap(x int, y int)](#panicSlice3Acap)
    * [func panicSlice3AcapU(x uint, y int)](#panicSlice3AcapU)
    * [func panicSlice3B(x int, y int)](#panicSlice3B)
    * [func panicSlice3BU(x uint, y int)](#panicSlice3BU)
    * [func panicSlice3C(x int, y int)](#panicSlice3C)
    * [func panicSlice3CU(x uint, y int)](#panicSlice3CU)
    * [func panicSliceConvert(x int, y int)](#panicSliceConvert)
    * [func panicshift()](#panicshift)
    * [func panicdivide()](#panicdivide)
    * [func panicoverflow()](#panicoverflow)
    * [func panicfloat()](#panicfloat)
    * [func panicmem()](#panicmem)
    * [func panicmemAddr(addr uintptr)](#panicmemAddr)
    * [func deferproc(siz int32, fn *funcval)](#deferproc)
    * [func deferprocStack(d *_defer)](#deferprocStack)
    * [func deferclass(siz uintptr) uintptr](#deferclass)
    * [func totaldefersize(siz uintptr) uintptr](#totaldefersize)
    * [func testdefersizes()](#testdefersizes)
    * [func deferArgs(d *_defer) unsafe.Pointer](#deferArgs)
    * [func deferFunc(d *_defer) func()](#deferFunc)
    * [func init()](#init)
    * [func freedefer(d *_defer)](#freedefer)
    * [func freedeferpanic()](#freedeferpanic)
    * [func freedeferfn()](#freedeferfn)
    * [func deferreturn()](#deferreturn)
    * [func Goexit()](#Goexit)
    * [func preprintpanics(p *_panic)](#preprintpanics)
    * [func printpanics(p *_panic)](#printpanics)
    * [func addOneOpenDeferFrame(gp *g, pc uintptr, sp unsafe.Pointer)](#addOneOpenDeferFrame)
    * [func readvarintUnsafe(fd unsafe.Pointer) (uint32, unsafe.Pointer)](#readvarintUnsafe)
    * [func runOpenDeferFrame(gp *g, d *_defer) bool](#runOpenDeferFrame)
    * [func reflectcallSave(p *_panic, fn, arg unsafe.Pointer, argsize uint32)](#reflectcallSave)
    * [func deferCallSave(p *_panic, fn func())](#deferCallSave)
    * [func gopanic(e interface{})](#gopanic)
    * [func getargp() uintptr](#getargp)
    * [func gorecover(argp uintptr) interface{}](#gorecover)
    * [func sync_throw(s string)](#sync_throw)
    * [func throw(s string)](#throw)
    * [func recovery(gp *g)](#recovery)
    * [func fatalthrow()](#fatalthrow)
    * [func fatalpanic(msgs *_panic)](#fatalpanic)
    * [func startpanic_m() bool](#startpanic_m)
    * [func dopanic_m(gp *g, pc, sp uintptr) bool](#dopanic_m)
    * [func canpanic(gp *g) bool](#canpanic)
    * [func shouldPushSigpanic(gp *g, pc, lr uintptr) bool](#shouldPushSigpanic)
    * [func isAbortPC(pc uintptr) bool](#isAbortPC)
    * [func plugin_lastmoduleinit() (path string, syms map[string]interface{}, errstr string)](#plugin_lastmoduleinit)
    * [func pluginftabverify(md *moduledata)](#pluginftabverify)
    * [func inRange(r0, r1, v0, v1 uintptr) bool](#inRange)
    * [func resumeG(state suspendGState)](#resumeG)
    * [func canPreemptM(mp *m) bool](#canPreemptM)
    * [func asyncPreempt()](#asyncPreempt)
    * [func asyncPreempt2()](#asyncPreempt2)
    * [func init()](#init)
    * [func wantAsyncPreempt(gp *g) bool](#wantAsyncPreempt)
    * [func isAsyncSafePoint(gp *g, pc, sp, lr uintptr) (bool, uintptr)](#isAsyncSafePoint)
    * [func osPreemptExtEnter(mp *m)](#osPreemptExtEnter)
    * [func osPreemptExtExit(mp *m)](#osPreemptExtExit)
    * [func bytes(s string) (ret []byte)](#bytes)
    * [func recordForPanic(b []byte)](#recordForPanic)
    * [func printlock()](#printlock)
    * [func printunlock()](#printunlock)
    * [func gwrite(b []byte)](#gwrite)
    * [func printsp()](#printsp)
    * [func printnl()](#printnl)
    * [func printbool(v bool)](#printbool)
    * [func printfloat(v float64)](#printfloat)
    * [func printcomplex(c complex128)](#printcomplex)
    * [func printuint(v uint64)](#printuint)
    * [func printint(v int64)](#printint)
    * [func printhex(v uint64)](#printhex)
    * [func printpointer(p unsafe.Pointer)](#printpointer)
    * [func printuintptr(p uintptr)](#printuintptr)
    * [func printstring(s string)](#printstring)
    * [func printslice(s []byte)](#printslice)
    * [func printeface(e eface)](#printeface)
    * [func printiface(i iface)](#printiface)
    * [func hexdumpWords(p, end uintptr, mark func(uintptr) byte)](#hexdumpWords)
    * [func main_main()](#main_main)
    * [func main()](#main)
    * [func os_beforeExit()](#os_beforeExit)
    * [func init()](#init)
    * [func forcegchelper()](#forcegchelper)
    * [func Gosched()](#Gosched)
    * [func goschedguarded()](#goschedguarded)
    * [func gopark(unlockf func(*g, unsafe.Pointer) bool, lock unsafe.Pointer, reason waitReason, traceEv byte, traceskip int)](#gopark)
    * [func goparkunlock(lock *mutex, reason waitReason, traceEv byte, traceskip int)](#goparkunlock)
    * [func goready(gp *g, traceskip int)](#goready)
    * [func releaseSudog(s *sudog)](#releaseSudog)
    * [func funcPC(f interface{}) uintptr](#funcPC)
    * [func badmcall(fn func(*g))](#badmcall)
    * [func badmcall2(fn func(*g))](#badmcall2)
    * [func badreflectcall()](#badreflectcall)
    * [func badmorestackg0()](#badmorestackg0)
    * [func badmorestackgsignal()](#badmorestackgsignal)
    * [func badctxt()](#badctxt)
    * [func lockedOSThread() bool](#lockedOSThread)
    * [func allgadd(gp *g)](#allgadd)
    * [func forEachG(fn func(gp *g))](#forEachG)
    * [func forEachGRace(fn func(gp *g))](#forEachGRace)
    * [func cpuinit()](#cpuinit)
    * [func schedinit()](#schedinit)
    * [func dumpgstatus(gp *g)](#dumpgstatus)
    * [func checkmcount()](#checkmcount)
    * [func mReserveID() int64](#mReserveID)
    * [func mcommoninit(mp *m, id int64)](#mcommoninit)
    * [func fastrandinit()](#fastrandinit)
    * [func ready(gp *g, traceskip int, next bool)](#ready)
    * [func freezetheworld()](#freezetheworld)
    * [func readgstatus(gp *g) uint32](#readgstatus)
    * [func casfrom_Gscanstatus(gp *g, oldval, newval uint32)](#casfrom_Gscanstatus)
    * [func castogscanstatus(gp *g, oldval, newval uint32) bool](#castogscanstatus)
    * [func casgstatus(gp *g, oldval, newval uint32)](#casgstatus)
    * [func casgcopystack(gp *g) uint32](#casgcopystack)
    * [func casGToPreemptScan(gp *g, old, new uint32)](#casGToPreemptScan)
    * [func casGFromPreempted(gp *g, old, new uint32) bool](#casGFromPreempted)
    * [func stopTheWorld(reason string)](#stopTheWorld)
    * [func startTheWorld()](#startTheWorld)
    * [func stopTheWorldGC(reason string)](#stopTheWorldGC)
    * [func startTheWorldGC()](#startTheWorldGC)
    * [func stopTheWorldWithSema()](#stopTheWorldWithSema)
    * [func startTheWorldWithSema(emitTraceEvent bool) int64](#startTheWorldWithSema)
    * [func usesLibcall() bool](#usesLibcall)
    * [func mStackIsSystemAllocated() bool](#mStackIsSystemAllocated)
    * [func mstart()](#mstart)
    * [func mstart0()](#mstart0)
    * [func mstart1()](#mstart1)
    * [func mstartm0()](#mstartm0)
    * [func mPark()](#mPark)
    * [func mexit(osStack bool)](#mexit)
    * [func forEachP(fn func(*p))](#forEachP)
    * [func syscall_runtime_doAllThreadsSyscall(fn func(bool) bool)](#syscall_runtime_doAllThreadsSyscall)
    * [func runSafePointFn()](#runSafePointFn)
    * [func needm()](#needm)
    * [func newextram()](#newextram)
    * [func oneNewExtraM()](#oneNewExtraM)
    * [func dropm()](#dropm)
    * [func getm() uintptr](#getm)
    * [func unlockextra(mp *m)](#unlockextra)
    * [func newm(fn func(), _p_ *p, id int64)](#newm)
    * [func newm1(mp *m)](#newm1)
    * [func startTemplateThread()](#startTemplateThread)
    * [func mDoFixup() bool](#mDoFixup)
    * [func mDoFixupAndOSYield()](#mDoFixupAndOSYield)
    * [func templateThread()](#templateThread)
    * [func stopm()](#stopm)
    * [func mspinning()](#mspinning)
    * [func startm(_p_ *p, spinning bool)](#startm)
    * [func handoffp(_p_ *p)](#handoffp)
    * [func wakep()](#wakep)
    * [func stoplockedm()](#stoplockedm)
    * [func startlockedm(gp *g)](#startlockedm)
    * [func gcstopm()](#gcstopm)
    * [func execute(gp *g, inheritTime bool)](#execute)
    * [func pollWork() bool](#pollWork)
    * [func checkTimersNoP(allpSnapshot []*p, timerpMaskSnapshot pMask, pollUntil int64) int64](#checkTimersNoP)
    * [func wakeNetPoller(when int64)](#wakeNetPoller)
    * [func resetspinning()](#resetspinning)
    * [func injectglist(glist *gList)](#injectglist)
    * [func schedule()](#schedule)
    * [func dropg()](#dropg)
    * [func checkTimers(pp *p, now int64) (rnow, pollUntil int64, ran bool)](#checkTimers)
    * [func parkunlock_c(gp *g, lock unsafe.Pointer) bool](#parkunlock_c)
    * [func park_m(gp *g)](#park_m)
    * [func goschedImpl(gp *g)](#goschedImpl)
    * [func gosched_m(gp *g)](#gosched_m)
    * [func goschedguarded_m(gp *g)](#goschedguarded_m)
    * [func gopreempt_m(gp *g)](#gopreempt_m)
    * [func preemptPark(gp *g)](#preemptPark)
    * [func goyield()](#goyield)
    * [func goyield_m(gp *g)](#goyield_m)
    * [func goexit1()](#goexit1)
    * [func goexit0(gp *g)](#goexit0)
    * [func save(pc, sp uintptr)](#save)
    * [func reentersyscall(pc, sp uintptr)](#reentersyscall)
    * [func entersyscall()](#entersyscall)
    * [func entersyscall_sysmon()](#entersyscall_sysmon)
    * [func entersyscall_gcwait()](#entersyscall_gcwait)
    * [func entersyscallblock()](#entersyscallblock)
    * [func entersyscallblock_handoff()](#entersyscallblock_handoff)
    * [func exitsyscall()](#exitsyscall)
    * [func exitsyscallfast(oldp *p) bool](#exitsyscallfast)
    * [func exitsyscallfast_reacquired()](#exitsyscallfast_reacquired)
    * [func exitsyscallfast_pidle() bool](#exitsyscallfast_pidle)
    * [func exitsyscall0(gp *g)](#exitsyscall0)
    * [func beforefork()](#beforefork)
    * [func syscall_runtime_BeforeFork()](#syscall_runtime_BeforeFork)
    * [func afterfork()](#afterfork)
    * [func syscall_runtime_AfterFork()](#syscall_runtime_AfterFork)
    * [func syscall_runtime_AfterForkInChild()](#syscall_runtime_AfterForkInChild)
    * [func syscall_runtime_BeforeExec()](#syscall_runtime_BeforeExec)
    * [func syscall_runtime_AfterExec()](#syscall_runtime_AfterExec)
    * [func newproc(siz int32, fn *funcval)](#newproc)
    * [func saveAncestors(callergp *g) *[]ancestorInfo](#saveAncestors)
    * [func gfput(_p_ *p, gp *g)](#gfput)
    * [func gfpurge(_p_ *p)](#gfpurge)
    * [func Breakpoint()](#Breakpoint)
    * [func dolockOSThread()](#dolockOSThread)
    * [func LockOSThread()](#LockOSThread)
    * [func lockOSThread()](#lockOSThread)
    * [func dounlockOSThread()](#dounlockOSThread)
    * [func UnlockOSThread()](#UnlockOSThread)
    * [func unlockOSThread()](#unlockOSThread)
    * [func badunlockosthread()](#badunlockosthread)
    * [func gcount() int32](#gcount)
    * [func mcount() int32](#mcount)
    * [func _System()](#_System)
    * [func _ExternalCode()](#_ExternalCode)
    * [func _LostExternalCode()](#_LostExternalCode)
    * [func _GC()](#_GC)
    * [func _LostSIGPROFDuringAtomic64()](#_LostSIGPROFDuringAtomic64)
    * [func _VDSO()](#_VDSO)
    * [func sigprof(pc, sp, lr uintptr, gp *g, mp *m)](#sigprof)
    * [func sigprofNonGo()](#sigprofNonGo)
    * [func sigprofNonGoPC(pc uintptr)](#sigprofNonGoPC)
    * [func setcpuprofilerate(hz int32)](#setcpuprofilerate)
    * [func acquirep(_p_ *p)](#acquirep)
    * [func wirep(_p_ *p)](#wirep)
    * [func incidlelocked(v int32)](#incidlelocked)
    * [func checkdead()](#checkdead)
    * [func sysmon()](#sysmon)
    * [func retake(now int64) uint32](#retake)
    * [func preemptall() bool](#preemptall)
    * [func preemptone(_p_ *p) bool](#preemptone)
    * [func schedtrace(detailed bool)](#schedtrace)
    * [func schedEnableUser(enable bool)](#schedEnableUser)
    * [func schedEnabled(gp *g) bool](#schedEnabled)
    * [func mput(mp *m)](#mput)
    * [func globrunqput(gp *g)](#globrunqput)
    * [func globrunqputhead(gp *g)](#globrunqputhead)
    * [func globrunqputbatch(batch *gQueue, n int32)](#globrunqputbatch)
    * [func updateTimerPMask(pp *p)](#updateTimerPMask)
    * [func pidleput(_p_ *p)](#pidleput)
    * [func runqempty(_p_ *p) bool](#runqempty)
    * [func runqput(_p_ *p, gp *g, next bool)](#runqput)
    * [func runqputslow(_p_ *p, gp *g, h, t uint32) bool](#runqputslow)
    * [func runqputbatch(pp *p, q *gQueue, qsize int)](#runqputbatch)
    * [func runqgrab(_p_ *p, batch *[256]guintptr, batchHead uint32, stealRunNextG bool) uint32](#runqgrab)
    * [func setMaxThreads(in int) (out int)](#setMaxThreads)
    * [func procPin() int](#procPin)
    * [func procUnpin()](#procUnpin)
    * [func sync_runtime_procPin() int](#sync_runtime_procPin)
    * [func sync_runtime_procUnpin()](#sync_runtime_procUnpin)
    * [func sync_atomic_runtime_procPin() int](#sync_atomic_runtime_procPin)
    * [func sync_atomic_runtime_procUnpin()](#sync_atomic_runtime_procUnpin)
    * [func sync_runtime_canSpin(i int) bool](#sync_runtime_canSpin)
    * [func sync_runtime_doSpin()](#sync_runtime_doSpin)
    * [func gcd(a, b uint32) uint32](#gcd)
    * [func doInit(t *initTask)](#doInit)
    * [func countSub(x, y uint32) int](#countSub)
    * [func runtime_setProfLabel(labels unsafe.Pointer)](#runtime_setProfLabel)
    * [func runtime_getProfLabel() unsafe.Pointer](#runtime_getProfLabel)
    * [func raceReadObjectPC(t *_type, addr unsafe.Pointer, callerpc, pc uintptr)](#raceReadObjectPC)
    * [func raceWriteObjectPC(t *_type, addr unsafe.Pointer, callerpc, pc uintptr)](#raceWriteObjectPC)
    * [func raceinit() (uintptr, uintptr)](#raceinit)
    * [func racefini()](#racefini)
    * [func raceproccreate() uintptr](#raceproccreate)
    * [func raceprocdestroy(ctx uintptr)](#raceprocdestroy)
    * [func racemapshadow(addr unsafe.Pointer, size uintptr)](#racemapshadow)
    * [func racewritepc(addr unsafe.Pointer, callerpc, pc uintptr)](#racewritepc)
    * [func racereadpc(addr unsafe.Pointer, callerpc, pc uintptr)](#racereadpc)
    * [func racereadrangepc(addr unsafe.Pointer, sz, callerpc, pc uintptr)](#racereadrangepc)
    * [func racewriterangepc(addr unsafe.Pointer, sz, callerpc, pc uintptr)](#racewriterangepc)
    * [func raceacquire(addr unsafe.Pointer)](#raceacquire)
    * [func raceacquireg(gp *g, addr unsafe.Pointer)](#raceacquireg)
    * [func raceacquirectx(racectx uintptr, addr unsafe.Pointer)](#raceacquirectx)
    * [func racerelease(addr unsafe.Pointer)](#racerelease)
    * [func racereleaseg(gp *g, addr unsafe.Pointer)](#racereleaseg)
    * [func racereleaseacquire(addr unsafe.Pointer)](#racereleaseacquire)
    * [func racereleaseacquireg(gp *g, addr unsafe.Pointer)](#racereleaseacquireg)
    * [func racereleasemerge(addr unsafe.Pointer)](#racereleasemerge)
    * [func racereleasemergeg(gp *g, addr unsafe.Pointer)](#racereleasemergeg)
    * [func racefingo()](#racefingo)
    * [func racemalloc(p unsafe.Pointer, sz uintptr)](#racemalloc)
    * [func racefree(p unsafe.Pointer, sz uintptr)](#racefree)
    * [func racegostart(pc uintptr) uintptr](#racegostart)
    * [func racegoend()](#racegoend)
    * [func racectxend(racectx uintptr)](#racectxend)
    * [func setMaxStack(in int) (out int)](#setMaxStack)
    * [func setPanicOnFault(new bool) (old bool)](#setPanicOnFault)
    * [func osRelax(relax bool)](#osRelax)
    * [func tickspersecond() int64](#tickspersecond)
    * [func syscall_runtime_envs() []string](#syscall_runtime_envs)
    * [func syscall_Getpagesize() int](#syscall_Getpagesize)
    * [func os_runtime_args() []string](#os_runtime_args)
    * [func syscall_Exit(code int)](#syscall_Exit)
    * [func gotraceback() (level int32, all, crash bool)](#gotraceback)
    * [func argv_index(argv **byte, i int32) *byte](#argv_index)
    * [func args(c int32, v **byte)](#args)
    * [func goargs()](#goargs)
    * [func goenvs_unix()](#goenvs_unix)
    * [func environ() []string](#environ)
    * [func testAtomic64()](#testAtomic64)
    * [func check()](#check)
    * [func parsedebugvars()](#parsedebugvars)
    * [func setTraceback(level string)](#setTraceback)
    * [func timediv(v int64, div int32, rem *int32) int32](#timediv)
    * [func releasem(mp *m)](#releasem)
    * [func reflect_typelinks() ([]unsafe.Pointer, [][]int32)](#reflect_typelinks)
    * [func reflect_resolveNameOff(ptrInModule unsafe.Pointer, off int32) unsafe.Pointer](#reflect_resolveNameOff)
    * [func reflect_resolveTypeOff(rtype unsafe.Pointer, off int32) unsafe.Pointer](#reflect_resolveTypeOff)
    * [func reflect_resolveTextOff(rtype unsafe.Pointer, off int32) unsafe.Pointer](#reflect_resolveTextOff)
    * [func reflectlite_resolveNameOff(ptrInModule unsafe.Pointer, off int32) unsafe.Pointer](#reflectlite_resolveNameOff)
    * [func reflectlite_resolveTypeOff(rtype unsafe.Pointer, off int32) unsafe.Pointer](#reflectlite_resolveTypeOff)
    * [func reflect_addReflectOff(ptr unsafe.Pointer) int32](#reflect_addReflectOff)
    * [func setGNoWB(gp **g, new *g)](#setGNoWB)
    * [func setMNoWB(mp **m, new *m)](#setMNoWB)
    * [func extendRandom(r []byte, n int)](#extendRandom)
    * [func selectsetpc(pc *uintptr)](#selectsetpc)
    * [func sellock(scases []scase, lockorder []uint16)](#sellock)
    * [func selunlock(scases []scase, lockorder []uint16)](#selunlock)
    * [func selparkcommit(gp *g, _ unsafe.Pointer) bool](#selparkcommit)
    * [func block()](#block)
    * [func selectgo(cas0 *scase, order0 *uint16, pc0 *uintptr, nsends, nrecvs int, block bool) (int, bool)](#selectgo)
    * [func reflect_rselect(cases []runtimeSelect) (int, bool)](#reflect_rselect)
    * [func sync_runtime_Semacquire(addr *uint32)](#sync_runtime_Semacquire)
    * [func poll_runtime_Semacquire(addr *uint32)](#poll_runtime_Semacquire)
    * [func sync_runtime_Semrelease(addr *uint32, handoff bool, skipframes int)](#sync_runtime_Semrelease)
    * [func sync_runtime_SemacquireMutex(addr *uint32, lifo bool, skipframes int)](#sync_runtime_SemacquireMutex)
    * [func poll_runtime_Semrelease(addr *uint32)](#poll_runtime_Semrelease)
    * [func readyWithTime(s *sudog, traceskip int)](#readyWithTime)
    * [func semacquire(addr *uint32)](#semacquire)
    * [func semacquire1(addr *uint32, lifo bool, profile semaProfileFlags, skipframes int)](#semacquire1)
    * [func semrelease(addr *uint32)](#semrelease)
    * [func semrelease1(addr *uint32, handoff bool, skipframes int)](#semrelease1)
    * [func cansemacquire(addr *uint32) bool](#cansemacquire)
    * [func less(a, b uint32) bool](#less)
    * [func notifyListAdd(l *notifyList) uint32](#notifyListAdd)
    * [func notifyListWait(l *notifyList, t uint32)](#notifyListWait)
    * [func notifyListNotifyAll(l *notifyList)](#notifyListNotifyAll)
    * [func notifyListNotifyOne(l *notifyList)](#notifyListNotifyOne)
    * [func notifyListCheck(sz uintptr)](#notifyListCheck)
    * [func sync_nanotime() int64](#sync_nanotime)
    * [func dumpregs(c *sigctxt)](#dumpregs)
    * [func os_sigpipe()](#os_sigpipe)
    * [func signame(sig uint32) string](#signame)
    * [func init()](#init)
    * [func initsig(preinit bool)](#initsig)
    * [func sigInstallGoHandler(sig uint32) bool](#sigInstallGoHandler)
    * [func sigenable(sig uint32)](#sigenable)
    * [func sigdisable(sig uint32)](#sigdisable)
    * [func sigignore(sig uint32)](#sigignore)
    * [func clearSignalHandlers()](#clearSignalHandlers)
    * [func setProcessCPUProfiler(hz int32)](#setProcessCPUProfiler)
    * [func setThreadCPUProfiler(hz int32)](#setThreadCPUProfiler)
    * [func sigpipe()](#sigpipe)
    * [func doSigPreempt(gp *g, ctxt *sigctxt)](#doSigPreempt)
    * [func preemptM(mp *m)](#preemptM)
    * [func sigtrampgo(sig uint32, info *siginfo, ctx unsafe.Pointer)](#sigtrampgo)
    * [func adjustSignalStack(sig uint32, mp *m, gsigStack *gsignalStack) bool](#adjustSignalStack)
    * [func sighandler(sig uint32, info *siginfo, ctxt unsafe.Pointer, gp *g)](#sighandler)
    * [func sigpanic()](#sigpanic)
    * [func dieFromSignal(sig uint32)](#dieFromSignal)
    * [func raisebadsignal(sig uint32, c *sigctxt)](#raisebadsignal)
    * [func crash()](#crash)
    * [func ensureSigM()](#ensureSigM)
    * [func noSignalStack(sig uint32)](#noSignalStack)
    * [func sigNotOnStack(sig uint32)](#sigNotOnStack)
    * [func signalDuringFork(sig uint32)](#signalDuringFork)
    * [func badsignal(sig uintptr, c *sigctxt)](#badsignal)
    * [func sigfwd(fn uintptr, sig uint32, info *siginfo, ctx unsafe.Pointer)](#sigfwd)
    * [func sigfwdgo(sig uint32, info *siginfo, ctx unsafe.Pointer) bool](#sigfwdgo)
    * [func sigsave(p *sigset)](#sigsave)
    * [func msigrestore(sigmask sigset)](#msigrestore)
    * [func sigblock(exiting bool)](#sigblock)
    * [func unblocksig(sig uint32)](#unblocksig)
    * [func minitSignals()](#minitSignals)
    * [func minitSignalStack()](#minitSignalStack)
    * [func minitSignalMask()](#minitSignalMask)
    * [func unminitSignals()](#unminitSignals)
    * [func blockableSig(sig uint32) bool](#blockableSig)
    * [func setGsignalStack(st *stackt, old *gsignalStack)](#setGsignalStack)
    * [func restoreGsignalStack(st *gsignalStack)](#restoreGsignalStack)
    * [func signalstack(s *stack)](#signalstack)
    * [func setsigsegv(pc uintptr)](#setsigsegv)
    * [func sigsend(s uint32) bool](#sigsend)
    * [func sigRecvPrepareForFixup()](#sigRecvPrepareForFixup)
    * [func signal_recv() uint32](#signal_recv)
    * [func signalWaitUntilIdle()](#signalWaitUntilIdle)
    * [func signal_enable(s uint32)](#signal_enable)
    * [func signal_disable(s uint32)](#signal_disable)
    * [func signal_ignore(s uint32)](#signal_ignore)
    * [func sigInitIgnored(s uint32)](#sigInitIgnored)
    * [func signal_ignored(s uint32) bool](#signal_ignored)
    * [func panicmakeslicelen()](#panicmakeslicelen)
    * [func panicmakeslicecap()](#panicmakeslicecap)
    * [func makeslicecopy(et *_type, tolen int, fromlen int, from unsafe.Pointer) unsafe.Pointer](#makeslicecopy)
    * [func makeslice(et *_type, len, cap int) unsafe.Pointer](#makeslice)
    * [func makeslice64(et *_type, len64, cap64 int64) unsafe.Pointer](#makeslice64)
    * [func unsafeslice(et *_type, len int)](#unsafeslice)
    * [func unsafeslice64(et *_type, len64 int64)](#unsafeslice64)
    * [func panicunsafeslicelen()](#panicunsafeslicelen)
    * [func isPowerOfTwo(x uintptr) bool](#isPowerOfTwo)
    * [func slicecopy(toPtr unsafe.Pointer, toLen int, fromPtr unsafe.Pointer, fromLen int, width uintptr) int](#slicecopy)
    * [func funpack64(f uint64) (sign, mant uint64, exp int, inf, nan bool)](#funpack64)
    * [func funpack32(f uint32) (sign, mant uint32, exp int, inf, nan bool)](#funpack32)
    * [func fpack64(sign, mant uint64, exp int, trunc uint64) uint64](#fpack64)
    * [func fpack32(sign, mant uint32, exp int, trunc uint32) uint32](#fpack32)
    * [func fadd64(f, g uint64) uint64](#fadd64)
    * [func fsub64(f, g uint64) uint64](#fsub64)
    * [func fneg64(f uint64) uint64](#fneg64)
    * [func fmul64(f, g uint64) uint64](#fmul64)
    * [func fdiv64(f, g uint64) uint64](#fdiv64)
    * [func f64to32(f uint64) uint32](#f64to32)
    * [func f32to64(f uint32) uint64](#f32to64)
    * [func fcmp64(f, g uint64) (cmp int32, isnan bool)](#fcmp64)
    * [func f64toint(f uint64) (val int64, ok bool)](#f64toint)
    * [func fintto64(val int64) (f uint64)](#fintto64)
    * [func mullu(u, v uint64) (lo, hi uint64)](#mullu)
    * [func divlu(u1, u0, v uint64) (q, r uint64)](#divlu)
    * [func fadd32(x, y uint32) uint32](#fadd32)
    * [func fmul32(x, y uint32) uint32](#fmul32)
    * [func fdiv32(x, y uint32) uint32](#fdiv32)
    * [func feq32(x, y uint32) bool](#feq32)
    * [func fgt32(x, y uint32) bool](#fgt32)
    * [func fge32(x, y uint32) bool](#fge32)
    * [func feq64(x, y uint64) bool](#feq64)
    * [func fgt64(x, y uint64) bool](#fgt64)
    * [func fge64(x, y uint64) bool](#fge64)
    * [func fint32to32(x int32) uint32](#fint32to32)
    * [func fint32to64(x int32) uint64](#fint32to64)
    * [func fint64to32(x int64) uint32](#fint64to32)
    * [func fint64to64(x int64) uint64](#fint64to64)
    * [func f32toint32(x uint32) int32](#f32toint32)
    * [func f32toint64(x uint32) int64](#f32toint64)
    * [func f64toint32(x uint64) int32](#f64toint32)
    * [func f64toint64(x uint64) int64](#f64toint64)
    * [func f64touint64(x float64) uint64](#f64touint64)
    * [func f32touint64(x float32) uint64](#f32touint64)
    * [func fuint64to64(x uint64) float64](#fuint64to64)
    * [func fuint64to32(x uint64) float32](#fuint64to32)
    * [func stackinit()](#stackinit)
    * [func stacklog2(n uintptr) int](#stacklog2)
    * [func stackpoolfree(x gclinkptr, order uint8)](#stackpoolfree)
    * [func stackcacherefill(c *mcache, order uint8)](#stackcacherefill)
    * [func stackcacherelease(c *mcache, order uint8)](#stackcacherelease)
    * [func stackcache_clear(c *mcache)](#stackcache_clear)
    * [func stackfree(stk stack)](#stackfree)
    * [func adjustpointer(adjinfo *adjustinfo, vpp unsafe.Pointer)](#adjustpointer)
    * [func adjustpointers(scanp unsafe.Pointer, bv *bitvector, adjinfo *adjustinfo, f funcInfo)](#adjustpointers)
    * [func adjustframe(frame *stkframe, arg unsafe.Pointer) bool](#adjustframe)
    * [func adjustctxt(gp *g, adjinfo *adjustinfo)](#adjustctxt)
    * [func adjustdefers(gp *g, adjinfo *adjustinfo)](#adjustdefers)
    * [func adjustpanics(gp *g, adjinfo *adjustinfo)](#adjustpanics)
    * [func adjustsudogs(gp *g, adjinfo *adjustinfo)](#adjustsudogs)
    * [func fillstack(stk stack, b byte)](#fillstack)
    * [func findsghi(gp *g, stk stack) uintptr](#findsghi)
    * [func syncadjustsudogs(gp *g, used uintptr, adjinfo *adjustinfo) uintptr](#syncadjustsudogs)
    * [func copystack(gp *g, newsize uintptr)](#copystack)
    * [func round2(x int32) int32](#round2)
    * [func newstack()](#newstack)
    * [func nilfunc()](#nilfunc)
    * [func gostartcallfn(gobuf *gobuf, fv *funcval)](#gostartcallfn)
    * [func isShrinkStackSafe(gp *g) bool](#isShrinkStackSafe)
    * [func shrinkstack(gp *g)](#shrinkstack)
    * [func freeStackSpans()](#freeStackSpans)
    * [func init()](#init)
    * [func morestackc()](#morestackc)
    * [func concatstrings(buf *tmpBuf, a []string) string](#concatstrings)
    * [func concatstring2(buf *tmpBuf, a0, a1 string) string](#concatstring2)
    * [func concatstring3(buf *tmpBuf, a0, a1, a2 string) string](#concatstring3)
    * [func concatstring4(buf *tmpBuf, a0, a1, a2, a3 string) string](#concatstring4)
    * [func concatstring5(buf *tmpBuf, a0, a1, a2, a3, a4 string) string](#concatstring5)
    * [func slicebytetostring(buf *tmpBuf, ptr *byte, n int) (str string)](#slicebytetostring)
    * [func stringDataOnStack(s string) bool](#stringDataOnStack)
    * [func rawstringtmp(buf *tmpBuf, l int) (s string, b []byte)](#rawstringtmp)
    * [func slicebytetostringtmp(ptr *byte, n int) (str string)](#slicebytetostringtmp)
    * [func stringtoslicebyte(buf *tmpBuf, s string) []byte](#stringtoslicebyte)
    * [func stringtoslicerune(buf *[tmpStringBufSize]rune, s string) []rune](#stringtoslicerune)
    * [func slicerunetostring(buf *tmpBuf, a []rune) string](#slicerunetostring)
    * [func intstring(buf *[4]byte, v int64) (s string)](#intstring)
    * [func rawstring(size int) (s string, b []byte)](#rawstring)
    * [func rawbyteslice(size int) (b []byte)](#rawbyteslice)
    * [func rawruneslice(size int) (b []rune)](#rawruneslice)
    * [func gobytes(p *byte, n int) (b []byte)](#gobytes)
    * [func gostring(p *byte) string](#gostring)
    * [func gostringn(p *byte, l int) string](#gostringn)
    * [func hasPrefix(s, prefix string) bool](#hasPrefix)
    * [func atoi(s string) (int, bool)](#atoi)
    * [func atoi32(s string) (int32, bool)](#atoi32)
    * [func findnull(s *byte) int](#findnull)
    * [func findnullw(s *uint16) int](#findnullw)
    * [func gostringnocopy(str *byte) string](#gostringnocopy)
    * [func gostringw(strw *uint16) string](#gostringw)
    * [func add(p unsafe.Pointer, x uintptr) unsafe.Pointer](#add)
    * [func mcall(fn func(*g))](#mcall)
    * [func systemstack(fn func())](#systemstack)
    * [func badsystemstack()](#badsystemstack)
    * [func memclrNoHeapPointers(ptr unsafe.Pointer, n uintptr)](#memclrNoHeapPointers)
    * [func reflect_memclrNoHeapPointers(ptr unsafe.Pointer, n uintptr)](#reflect_memclrNoHeapPointers)
    * [func memmove(to, from unsafe.Pointer, n uintptr)](#memmove)
    * [func reflect_memmove(to, from unsafe.Pointer, n uintptr)](#reflect_memmove)
    * [func fastrand() uint32](#fastrand)
    * [func fastrandn(n uint32) uint32](#fastrandn)
    * [func sync_fastrand() uint32](#sync_fastrand)
    * [func net_fastrand() uint32](#net_fastrand)
    * [func os_fastrand() uint32](#os_fastrand)
    * [func memequal(a, b unsafe.Pointer, size uintptr) bool](#memequal)
    * [func noescape(p unsafe.Pointer) unsafe.Pointer](#noescape)
    * [func cgocallback(fn, frame, ctxt uintptr)](#cgocallback)
    * [func gogo(buf *gobuf)](#gogo)
    * [func jmpdefer(fv *funcval, argp uintptr)](#jmpdefer)
    * [func asminit()](#asminit)
    * [func setg(gg *g)](#setg)
    * [func breakpoint()](#breakpoint)
    * [func reflectcall(stackArgsType *_type, fn, stackArgs unsafe.Pointer, stackArgsSize, stackRetOffset, frameSize uint32, regArgs *abi.RegArgs)](#reflectcall)
    * [func procyield(cycles uint32)](#procyield)
    * [func goexit(neverCallThisFunction)](#goexit)
    * [func publicationBarrier()](#publicationBarrier)
    * [func getcallerpc() uintptr](#getcallerpc)
    * [func getcallersp() uintptr](#getcallersp)
    * [func getclosureptr() uintptr](#getclosureptr)
    * [func asmcgocall(fn, arg unsafe.Pointer) int32](#asmcgocall)
    * [func morestack()](#morestack)
    * [func morestack_noctxt()](#morestack_noctxt)
    * [func rt0_go()](#rt0_go)
    * [func return0()](#return0)
    * [func call16(typ, fn, stackArgs unsafe.Pointer, stackArgsSize, stackRetOffset, frameSize uint32, regArgs *abi.RegArgs)](#call16)
    * [func call32(typ, fn, stackArgs unsafe.Pointer, stackArgsSize, stackRetOffset, frameSize uint32, regArgs *abi.RegArgs)](#call32)
    * [func call64(typ, fn, stackArgs unsafe.Pointer, stackArgsSize, stackRetOffset, frameSize uint32, regArgs *abi.RegArgs)](#call64)
    * [func call128(typ, fn, stackArgs unsafe.Pointer, stackArgsSize, stackRetOffset, frameSize uint32, regArgs *abi.RegArgs)](#call128)
    * [func call256(typ, fn, stackArgs unsafe.Pointer, stackArgsSize, stackRetOffset, frameSize uint32, regArgs *abi.RegArgs)](#call256)
    * [func call512(typ, fn, stackArgs unsafe.Pointer, stackArgsSize, stackRetOffset, frameSize uint32, regArgs *abi.RegArgs)](#call512)
    * [func call1024(typ, fn, stackArgs unsafe.Pointer, stackArgsSize, stackRetOffset, frameSize uint32, regArgs *abi.RegArgs)](#call1024)
    * [func call2048(typ, fn, stackArgs unsafe.Pointer, stackArgsSize, stackRetOffset, frameSize uint32, regArgs *abi.RegArgs)](#call2048)
    * [func call4096(typ, fn, stackArgs unsafe.Pointer, stackArgsSize, stackRetOffset, frameSize uint32, regArgs *abi.RegArgs)](#call4096)
    * [func call8192(typ, fn, stackArgs unsafe.Pointer, stackArgsSize, stackRetOffset, frameSize uint32, regArgs *abi.RegArgs)](#call8192)
    * [func call16384(typ, fn, stackArgs unsafe.Pointer, stackArgsSize, stackRetOffset, frameSize uint32, regArgs *abi.RegArgs)](#call16384)
    * [func call32768(typ, fn, stackArgs unsafe.Pointer, stackArgsSize, stackRetOffset, frameSize uint32, regArgs *abi.RegArgs)](#call32768)
    * [func call65536(typ, fn, stackArgs unsafe.Pointer, stackArgsSize, stackRetOffset, frameSize uint32, regArgs *abi.RegArgs)](#call65536)
    * [func call131072(typ, fn, stackArgs unsafe.Pointer, stackArgsSize, stackRetOffset, frameSize uint32, regArgs *abi.RegArgs)](#call131072)
    * [func call262144(typ, fn, stackArgs unsafe.Pointer, stackArgsSize, stackRetOffset, frameSize uint32, regArgs *abi.RegArgs)](#call262144)
    * [func call524288(typ, fn, stackArgs unsafe.Pointer, stackArgsSize, stackRetOffset, frameSize uint32, regArgs *abi.RegArgs)](#call524288)
    * [func call1048576(typ, fn, stackArgs unsafe.Pointer, stackArgsSize, stackRetOffset, frameSize uint32, regArgs *abi.RegArgs)](#call1048576)
    * [func call2097152(typ, fn, stackArgs unsafe.Pointer, stackArgsSize, stackRetOffset, frameSize uint32, regArgs *abi.RegArgs)](#call2097152)
    * [func call4194304(typ, fn, stackArgs unsafe.Pointer, stackArgsSize, stackRetOffset, frameSize uint32, regArgs *abi.RegArgs)](#call4194304)
    * [func call8388608(typ, fn, stackArgs unsafe.Pointer, stackArgsSize, stackRetOffset, frameSize uint32, regArgs *abi.RegArgs)](#call8388608)
    * [func call16777216(typ, fn, stackArgs unsafe.Pointer, stackArgsSize, stackRetOffset, frameSize uint32, regArgs *abi.RegArgs)](#call16777216)
    * [func call33554432(typ, fn, stackArgs unsafe.Pointer, stackArgsSize, stackRetOffset, frameSize uint32, regArgs *abi.RegArgs)](#call33554432)
    * [func call67108864(typ, fn, stackArgs unsafe.Pointer, stackArgsSize, stackRetOffset, frameSize uint32, regArgs *abi.RegArgs)](#call67108864)
    * [func call134217728(typ, fn, stackArgs unsafe.Pointer, stackArgsSize, stackRetOffset, frameSize uint32, regArgs *abi.RegArgs)](#call134217728)
    * [func call268435456(typ, fn, stackArgs unsafe.Pointer, stackArgsSize, stackRetOffset, frameSize uint32, regArgs *abi.RegArgs)](#call268435456)
    * [func call536870912(typ, fn, stackArgs unsafe.Pointer, stackArgsSize, stackRetOffset, frameSize uint32, regArgs *abi.RegArgs)](#call536870912)
    * [func call1073741824(typ, fn, stackArgs unsafe.Pointer, stackArgsSize, stackRetOffset, frameSize uint32, regArgs *abi.RegArgs)](#call1073741824)
    * [func systemstack_switch()](#systemstack_switch)
    * [func alignUp(n, a uintptr) uintptr](#alignUp)
    * [func alignDown(n, a uintptr) uintptr](#alignDown)
    * [func divRoundUp(n, a uintptr) uintptr](#divRoundUp)
    * [func checkASM() bool](#checkASM)
    * [func memequal_varlen(a, b unsafe.Pointer) bool](#memequal_varlen)
    * [func bool2int(x bool) int](#bool2int)
    * [func abort()](#abort)
    * [func gcWriteBarrier()](#gcWriteBarrier)
    * [func duffzero()](#duffzero)
    * [func duffcopy()](#duffcopy)
    * [func addmoduledata()](#addmoduledata)
    * [func sigpanic0()](#sigpanic0)
    * [func gcWriteBarrierCX()](#gcWriteBarrierCX)
    * [func gcWriteBarrierDX()](#gcWriteBarrierDX)
    * [func gcWriteBarrierBX()](#gcWriteBarrierBX)
    * [func gcWriteBarrierBP()](#gcWriteBarrierBP)
    * [func gcWriteBarrierSI()](#gcWriteBarrierSI)
    * [func gcWriteBarrierR8()](#gcWriteBarrierR8)
    * [func gcWriteBarrierR9()](#gcWriteBarrierR9)
    * [func stackcheck()](#stackcheck)
    * [func settls()](#settls)
    * [func retpolineAX()](#retpolineAX)
    * [func retpolineCX()](#retpolineCX)
    * [func retpolineDX()](#retpolineDX)
    * [func retpolineBX()](#retpolineBX)
    * [func retpolineBP()](#retpolineBP)
    * [func retpolineSI()](#retpolineSI)
    * [func retpolineDI()](#retpolineDI)
    * [func retpolineR8()](#retpolineR8)
    * [func retpolineR9()](#retpolineR9)
    * [func retpolineR10()](#retpolineR10)
    * [func retpolineR11()](#retpolineR11)
    * [func retpolineR12()](#retpolineR12)
    * [func retpolineR13()](#retpolineR13)
    * [func retpolineR14()](#retpolineR14)
    * [func retpolineR15()](#retpolineR15)
    * [func asmcgocall_no_g(fn, arg unsafe.Pointer)](#asmcgocall_no_g)
    * [func spillArgs()](#spillArgs)
    * [func unspillArgs()](#unspillArgs)
    * [func sbrk0() uintptr](#sbrk0)
    * [func runtime_expandFinalInlineFrame(stk []uintptr) []uintptr](#runtime_expandFinalInlineFrame)
    * [func expandCgoFrames(pc uintptr) []Frame](#expandCgoFrames)
    * [func activeModules() []*moduledata](#activeModules)
    * [func modulesinit()](#modulesinit)
    * [func moduledataverify()](#moduledataverify)
    * [func moduledataverify1(datap *moduledata)](#moduledataverify1)
    * [func pcvalueCacheKey(targetpc uintptr) uintptr](#pcvalueCacheKey)
    * [func pcvalue(f funcInfo, off uint32, targetpc uintptr, cache *pcvalueCache, strict bool) (int32, uintptr)](#pcvalue)
    * [func cfuncname(f funcInfo) *byte](#cfuncname)
    * [func funcname(f funcInfo) string](#funcname)
    * [func funcpkgpath(f funcInfo) string](#funcpkgpath)
    * [func cfuncnameFromNameoff(f funcInfo, nameoff int32) *byte](#cfuncnameFromNameoff)
    * [func funcnameFromNameoff(f funcInfo, nameoff int32) string](#funcnameFromNameoff)
    * [func funcfile(f funcInfo, fileno int32) string](#funcfile)
    * [func funcline1(f funcInfo, targetpc uintptr, strict bool) (file string, line int32)](#funcline1)
    * [func funcline(f funcInfo, targetpc uintptr) (file string, line int32)](#funcline)
    * [func funcspdelta(f funcInfo, targetpc uintptr, cache *pcvalueCache) int32](#funcspdelta)
    * [func funcMaxSPDelta(f funcInfo) int32](#funcMaxSPDelta)
    * [func pcdatastart(f funcInfo, table uint32) uint32](#pcdatastart)
    * [func pcdatavalue(f funcInfo, table uint32, targetpc uintptr, cache *pcvalueCache) int32](#pcdatavalue)
    * [func pcdatavalue1(f funcInfo, table uint32, targetpc uintptr, cache *pcvalueCache, strict bool) int32](#pcdatavalue1)
    * [func pcdatavalue2(f funcInfo, table uint32, targetpc uintptr) (int32, uintptr)](#pcdatavalue2)
    * [func funcdata(f funcInfo, i uint8) unsafe.Pointer](#funcdata)
    * [func step(p []byte, pc *uintptr, val *int32, first bool) (newp []byte, ok bool)](#step)
    * [func readvarint(p []byte) (read uint32, val uint32)](#readvarint)
    * [func syscall_syscall(fn, a1, a2, a3 uintptr) (r1, r2, err uintptr)](#syscall_syscall)
    * [func syscall()](#syscall)
    * [func syscall_syscallX(fn, a1, a2, a3 uintptr) (r1, r2, err uintptr)](#syscall_syscallX)
    * [func syscallX()](#syscallX)
    * [func syscall_syscall6(fn, a1, a2, a3, a4, a5, a6 uintptr) (r1, r2, err uintptr)](#syscall_syscall6)
    * [func syscall6()](#syscall6)
    * [func syscall_syscall6X(fn, a1, a2, a3, a4, a5, a6 uintptr) (r1, r2, err uintptr)](#syscall_syscall6X)
    * [func syscall6X()](#syscall6X)
    * [func syscall_syscallPtr(fn, a1, a2, a3 uintptr) (r1, r2, err uintptr)](#syscall_syscallPtr)
    * [func syscallPtr()](#syscallPtr)
    * [func syscall_rawSyscall(fn, a1, a2, a3 uintptr) (r1, r2, err uintptr)](#syscall_rawSyscall)
    * [func syscall_rawSyscall6(fn, a1, a2, a3, a4, a5, a6 uintptr) (r1, r2, err uintptr)](#syscall_rawSyscall6)
    * [func crypto_x509_syscall(fn, a1, a2, a3, a4, a5, a6 uintptr) (r1 uintptr)](#crypto_x509_syscall)
    * [func syscallNoErr()](#syscallNoErr)
    * [func pthread_attr_init(attr *pthreadattr) int32](#pthread_attr_init)
    * [func pthread_attr_init_trampoline()](#pthread_attr_init_trampoline)
    * [func pthread_attr_getstacksize(attr *pthreadattr, size *uintptr) int32](#pthread_attr_getstacksize)
    * [func pthread_attr_getstacksize_trampoline()](#pthread_attr_getstacksize_trampoline)
    * [func pthread_attr_setdetachstate(attr *pthreadattr, state int) int32](#pthread_attr_setdetachstate)
    * [func pthread_attr_setdetachstate_trampoline()](#pthread_attr_setdetachstate_trampoline)
    * [func pthread_create(attr *pthreadattr, start uintptr, arg unsafe.Pointer) int32](#pthread_create)
    * [func pthread_create_trampoline()](#pthread_create_trampoline)
    * [func raise(sig uint32)](#raise)
    * [func raise_trampoline()](#raise_trampoline)
    * [func pthread_self_trampoline()](#pthread_self_trampoline)
    * [func pthread_kill(t pthread, sig uint32)](#pthread_kill)
    * [func pthread_kill_trampoline()](#pthread_kill_trampoline)
    * [func mmap(addr unsafe.Pointer, n uintptr, prot, flags, fd int32, off uint32) (unsafe.Pointer, int)](#mmap)
    * [func mmap_trampoline()](#mmap_trampoline)
    * [func munmap(addr unsafe.Pointer, n uintptr)](#munmap)
    * [func munmap_trampoline()](#munmap_trampoline)
    * [func madvise(addr unsafe.Pointer, n uintptr, flags int32)](#madvise)
    * [func madvise_trampoline()](#madvise_trampoline)
    * [func mlock(addr unsafe.Pointer, n uintptr)](#mlock)
    * [func mlock_trampoline()](#mlock_trampoline)
    * [func read(fd int32, p unsafe.Pointer, n int32) int32](#read)
    * [func read_trampoline()](#read_trampoline)
    * [func pipe() (r, w int32, errno int32)](#pipe)
    * [func pipe_trampoline()](#pipe_trampoline)
    * [func closefd(fd int32) int32](#closefd)
    * [func close_trampoline()](#close_trampoline)
    * [func exit(code int32)](#exit)
    * [func exit_trampoline()](#exit_trampoline)
    * [func usleep(usec uint32)](#usleep)
    * [func usleep_trampoline()](#usleep_trampoline)
    * [func usleep_no_g(usec uint32)](#usleep_no_g)
    * [func write1(fd uintptr, p unsafe.Pointer, n int32) int32](#write1)
    * [func write_trampoline()](#write_trampoline)
    * [func open(name *byte, mode, perm int32) (ret int32)](#open)
    * [func open_trampoline()](#open_trampoline)
    * [func nanotime1() int64](#nanotime1)
    * [func nanotime_trampoline()](#nanotime_trampoline)
    * [func walltime() (int64, int32)](#walltime)
    * [func walltime_trampoline()](#walltime_trampoline)
    * [func sigaction(sig uint32, new *usigactiont, old *usigactiont)](#sigaction)
    * [func sigaction_trampoline()](#sigaction_trampoline)
    * [func sigprocmask(how uint32, new *sigset, old *sigset)](#sigprocmask)
    * [func sigprocmask_trampoline()](#sigprocmask_trampoline)
    * [func sigaltstack(new *stackt, old *stackt)](#sigaltstack)
    * [func sigaltstack_trampoline()](#sigaltstack_trampoline)
    * [func raiseproc(sig uint32)](#raiseproc)
    * [func raiseproc_trampoline()](#raiseproc_trampoline)
    * [func setitimer(mode int32, new, old *itimerval)](#setitimer)
    * [func setitimer_trampoline()](#setitimer_trampoline)
    * [func sysctl(mib *uint32, miblen uint32, oldp *byte, oldlenp *uintptr, newp *byte, newlen uintptr) int32](#sysctl)
    * [func sysctl_trampoline()](#sysctl_trampoline)
    * [func sysctlbyname(name *byte, oldp *byte, oldlenp *uintptr, newp *byte, newlen uintptr) int32](#sysctlbyname)
    * [func sysctlbyname_trampoline()](#sysctlbyname_trampoline)
    * [func fcntl(fd, cmd, arg int32) int32](#fcntl)
    * [func fcntl_trampoline()](#fcntl_trampoline)
    * [func kqueue() int32](#kqueue)
    * [func kqueue_trampoline()](#kqueue_trampoline)
    * [func kevent(kq int32, ch *keventt, nch int32, ev *keventt, nev int32, ts *timespec) int32](#kevent)
    * [func kevent_trampoline()](#kevent_trampoline)
    * [func pthread_mutex_init(m *pthreadmutex, attr *pthreadmutexattr) int32](#pthread_mutex_init)
    * [func pthread_mutex_init_trampoline()](#pthread_mutex_init_trampoline)
    * [func pthread_mutex_lock(m *pthreadmutex) int32](#pthread_mutex_lock)
    * [func pthread_mutex_lock_trampoline()](#pthread_mutex_lock_trampoline)
    * [func pthread_mutex_unlock(m *pthreadmutex) int32](#pthread_mutex_unlock)
    * [func pthread_mutex_unlock_trampoline()](#pthread_mutex_unlock_trampoline)
    * [func pthread_cond_init(c *pthreadcond, attr *pthreadcondattr) int32](#pthread_cond_init)
    * [func pthread_cond_init_trampoline()](#pthread_cond_init_trampoline)
    * [func pthread_cond_wait(c *pthreadcond, m *pthreadmutex) int32](#pthread_cond_wait)
    * [func pthread_cond_wait_trampoline()](#pthread_cond_wait_trampoline)
    * [func pthread_cond_timedwait_relative_np(c *pthreadcond, m *pthreadmutex, t *timespec) int32](#pthread_cond_timedwait_relative_np)
    * [func pthread_cond_timedwait_relative_np_trampoline()](#pthread_cond_timedwait_relative_np_trampoline)
    * [func pthread_cond_signal(c *pthreadcond) int32](#pthread_cond_signal)
    * [func pthread_cond_signal_trampoline()](#pthread_cond_signal_trampoline)
    * [func exitThread(wait *uint32)](#exitThread)
    * [func closeonexec(fd int32)](#closeonexec)
    * [func setNonblock(fd int32)](#setNonblock)
    * [func libcCall(fn, arg unsafe.Pointer) int32](#libcCall)
    * [func prepGoExitFrame(sp uintptr)](#prepGoExitFrame)
    * [func gostartcall(buf *gobuf, fn, ctxt unsafe.Pointer)](#gostartcall)
    * [func timeSleep(ns int64)](#timeSleep)
    * [func resetForSleep(gp *g, ut unsafe.Pointer) bool](#resetForSleep)
    * [func startTimer(t *timer)](#startTimer)
    * [func stopTimer(t *timer) bool](#stopTimer)
    * [func resetTimer(t *timer, when int64) bool](#resetTimer)
    * [func modTimer(t *timer, when, period int64, f func(interface{}, uintptr), arg interface{}, seq uintptr)](#modTimer)
    * [func goroutineReady(arg interface{}, seq uintptr)](#goroutineReady)
    * [func addtimer(t *timer)](#addtimer)
    * [func doaddtimer(pp *p, t *timer)](#doaddtimer)
    * [func deltimer(t *timer) bool](#deltimer)
    * [func dodeltimer(pp *p, i int)](#dodeltimer)
    * [func dodeltimer0(pp *p)](#dodeltimer0)
    * [func modtimer(t *timer, when, period int64, f func(interface{}, uintptr), arg interface{}, seq uintptr) bool](#modtimer)
    * [func resettimer(t *timer, when int64) bool](#resettimer)
    * [func cleantimers(pp *p)](#cleantimers)
    * [func moveTimers(pp *p, timers []*timer)](#moveTimers)
    * [func adjusttimers(pp *p, now int64)](#adjusttimers)
    * [func addAdjustedTimers(pp *p, moved []*timer)](#addAdjustedTimers)
    * [func nobarrierWakeTime(pp *p) int64](#nobarrierWakeTime)
    * [func runtimer(pp *p, now int64) int64](#runtimer)
    * [func runOneTimer(pp *p, t *timer, now int64)](#runOneTimer)
    * [func clearDeletedTimers(pp *p)](#clearDeletedTimers)
    * [func verifyTimerHeap(pp *p)](#verifyTimerHeap)
    * [func updateTimer0When(pp *p)](#updateTimer0When)
    * [func updateTimerModifiedEarliest(pp *p, nextwhen int64)](#updateTimerModifiedEarliest)
    * [func siftupTimer(t []*timer, i int)](#siftupTimer)
    * [func siftdownTimer(t []*timer, i int)](#siftdownTimer)
    * [func badTimer()](#badTimer)
    * [func nanotime() int64](#nanotime)
    * [func write(fd uintptr, p unsafe.Pointer, n int32) int32](#write)
    * [func time_now() (sec int64, nsec int32, mono int64)](#time_now)
    * [func osSetupTLS(mp *m)](#osSetupTLS)
    * [func StartTrace() error](#StartTrace)
    * [func StopTrace()](#StopTrace)
    * [func ReadTrace() []byte](#ReadTrace)
    * [func traceProcFree(pp *p)](#traceProcFree)
    * [func traceFullQueue(buf traceBufPtr)](#traceFullQueue)
    * [func traceEvent(ev byte, skip int, args ...uint64)](#traceEvent)
    * [func traceEventLocked(extraBytes int, mp *m, pid int32, bufp *traceBufPtr, ev byte, skip int, args ...uint64)](#traceEventLocked)
    * [func traceStackID(mp *m, buf []uintptr, skip int) uint64](#traceStackID)
    * [func traceReleaseBuffer(pid int32)](#traceReleaseBuffer)
    * [func traceAppend(buf []byte, v uint64) []byte](#traceAppend)
    * [func allFrames(pcs []uintptr) []Frame](#allFrames)
    * [func traceGomaxprocs(procs int32)](#traceGomaxprocs)
    * [func traceProcStart()](#traceProcStart)
    * [func traceProcStop(pp *p)](#traceProcStop)
    * [func traceGCStart()](#traceGCStart)
    * [func traceGCDone()](#traceGCDone)
    * [func traceGCSTWStart(kind int)](#traceGCSTWStart)
    * [func traceGCSTWDone()](#traceGCSTWDone)
    * [func traceGCSweepStart()](#traceGCSweepStart)
    * [func traceGCSweepSpan(bytesSwept uintptr)](#traceGCSweepSpan)
    * [func traceGCSweepDone()](#traceGCSweepDone)
    * [func traceGCMarkAssistStart()](#traceGCMarkAssistStart)
    * [func traceGCMarkAssistDone()](#traceGCMarkAssistDone)
    * [func traceGoCreate(newg *g, pc uintptr)](#traceGoCreate)
    * [func traceGoStart()](#traceGoStart)
    * [func traceGoEnd()](#traceGoEnd)
    * [func traceGoSched()](#traceGoSched)
    * [func traceGoPreempt()](#traceGoPreempt)
    * [func traceGoPark(traceEv byte, skip int)](#traceGoPark)
    * [func traceGoUnpark(gp *g, skip int)](#traceGoUnpark)
    * [func traceGoSysCall()](#traceGoSysCall)
    * [func traceGoSysExit(ts int64)](#traceGoSysExit)
    * [func traceGoSysBlock(pp *p)](#traceGoSysBlock)
    * [func traceHeapAlloc()](#traceHeapAlloc)
    * [func traceHeapGoal()](#traceHeapGoal)
    * [func trace_userTaskCreate(id, parentID uint64, taskType string)](#trace_userTaskCreate)
    * [func trace_userTaskEnd(id uint64)](#trace_userTaskEnd)
    * [func trace_userRegion(id, mode uint64, name string)](#trace_userRegion)
    * [func trace_userLog(id uint64, category, message string)](#trace_userLog)
    * [func tracebackdefers(gp *g, callback func(*stkframe, unsafe.Pointer) bool, v unsafe.Pointer)](#tracebackdefers)
    * [func gentraceback(pc0, sp0, lr0 uintptr, gp *g, skip int, pcbuf *uintptr, max int, callback func(*stkframe, unsafe.Pointer) bool, v unsafe.Pointer, flags uint) int](#gentraceback)
    * [func printArgs(f funcInfo, argp unsafe.Pointer)](#printArgs)
    * [func tracebackCgoContext(pcbuf *uintptr, printing bool, ctxt uintptr, n, max int) int](#tracebackCgoContext)
    * [func printcreatedby(gp *g)](#printcreatedby)
    * [func printcreatedby1(f funcInfo, pc uintptr)](#printcreatedby1)
    * [func traceback(pc, sp, lr uintptr, gp *g)](#traceback)
    * [func tracebacktrap(pc, sp, lr uintptr, gp *g)](#tracebacktrap)
    * [func traceback1(pc, sp, lr uintptr, gp *g, flags uint)](#traceback1)
    * [func printAncestorTraceback(ancestor ancestorInfo)](#printAncestorTraceback)
    * [func printAncestorTracebackFuncInfo(f funcInfo, pc uintptr)](#printAncestorTracebackFuncInfo)
    * [func callers(skip int, pcbuf []uintptr) int](#callers)
    * [func gcallers(gp *g, skip int, pcbuf []uintptr) int](#gcallers)
    * [func showframe(f funcInfo, gp *g, firstFrame bool, funcID, childID funcID) bool](#showframe)
    * [func showfuncinfo(f funcInfo, firstFrame bool, funcID, childID funcID) bool](#showfuncinfo)
    * [func isExportedRuntime(name string) bool](#isExportedRuntime)
    * [func elideWrapperCalling(id funcID) bool](#elideWrapperCalling)
    * [func goroutineheader(gp *g)](#goroutineheader)
    * [func tracebackothers(me *g)](#tracebackothers)
    * [func tracebackHexdump(stk stack, frame *stkframe, bad uintptr)](#tracebackHexdump)
    * [func isSystemGoroutine(gp *g, fixed bool) bool](#isSystemGoroutine)
    * [func SetCgoTraceback(version int, traceback, context, symbolizer unsafe.Pointer)](#SetCgoTraceback)
    * [func printCgoTraceback(callers *cgoCallers)](#printCgoTraceback)
    * [func printOneCgoTraceback(pc uintptr, max int, arg *cgoSymbolizerArg) int](#printOneCgoTraceback)
    * [func callCgoSymbolizer(arg *cgoSymbolizerArg)](#callCgoSymbolizer)
    * [func cgoContextPCs(ctxt uintptr, buf []uintptr)](#cgoContextPCs)
    * [func reflectOffsLock()](#reflectOffsLock)
    * [func reflectOffsUnlock()](#reflectOffsUnlock)
    * [func typelinksinit()](#typelinksinit)
    * [func typesEqual(t, v *_type, seen map[_typePair]struct{}) bool](#typesEqual)
    * [func isDirectIface(t *_type) bool](#isDirectIface)
    * [func countrunes(s string) int](#countrunes)
    * [func decoderune(s string, k int) (r rune, pos int)](#decoderune)
    * [func encoderune(p []byte, r rune) int](#encoderune)
    * [func inVDSOPage(pc uintptr) bool](#inVDSOPage)
    * [func writeErr(b []byte)](#writeErr)
    * [func Fcntl(fd, cmd, arg uintptr) (uintptr, uintptr)](#Fcntl)
    * [func DumpDebugLog() string](#DumpDebugLog)
    * [func ResetDebugLog()](#ResetDebugLog)
    * [func GetPhysPageSize() uintptr](#GetPhysPageSize)
    * [func LFStackPush(head *uint64, node *LFNode)](#LFStackPush)
    * [func Netpoll(delta int64)](#Netpoll)
    * [func GCMask(x interface{}) (ret []byte)](#GCMask)
    * [func RunSchedLocalQueueTest()](#RunSchedLocalQueueTest)
    * [func RunSchedLocalQueueStealTest()](#RunSchedLocalQueueStealTest)
    * [func RunSchedLocalQueueEmptyTest(iters int)](#RunSchedLocalQueueEmptyTest)
    * [func MemclrBytes(b []byte)](#MemclrBytes)
    * [func GostringW(w []uint16) (s string)](#GostringW)
    * [func Envs() []string](#Envs)
    * [func SetEnvs(e []string)](#SetEnvs)
    * [func BenchSetType(n int, x interface{})](#BenchSetType)
    * [func SetTracebackEnv(level string)](#SetTracebackEnv)
    * [func CountPagesInUse() (pagesInUse, counted uintptr)](#CountPagesInUse)
    * [func Fastrand() uint32](#Fastrand)
    * [func Fastrandn(n uint32) uint32](#Fastrandn)
    * [func ReadMetricsSlow(memStats *MemStats, samplesp unsafe.Pointer, len, cap int)](#ReadMetricsSlow)
    * [func BlockOnSystemStack()](#BlockOnSystemStack)
    * [func blockOnSystemStackInternal()](#blockOnSystemStackInternal)
    * [func MapBucketsCount(m map[int]int) int](#MapBucketsCount)
    * [func MapBucketsPointerIsNil(m map[int]int) bool](#MapBucketsPointerIsNil)
    * [func LockOSCounts() (external, internal uint32)](#LockOSCounts)
    * [func TracebackSystemstack(stk []uintptr, i int) int](#TracebackSystemstack)
    * [func KeepNArenaHints(n int)](#KeepNArenaHints)
    * [func MapNextArenaHint() (start, end uintptr)](#MapNextArenaHint)
    * [func GetNextArenaHint() uintptr](#GetNextArenaHint)
    * [func PanicForTesting(b []byte, i int) byte](#PanicForTesting)
    * [func unexportedPanicForTesting(b []byte, i int) byte](#unexportedPanicForTesting)
    * [func G0StackOverflow()](#G0StackOverflow)
    * [func stackOverflow(x *byte)](#stackOverflow)
    * [func MapTombstoneCheck(m map[int]int)](#MapTombstoneCheck)
    * [func RunGetgThreadSwitchTest()](#RunGetgThreadSwitchTest)
    * [func FindBitRange64(c uint64, n uint) uint](#FindBitRange64)
    * [func DiffPallocBits(a, b *PallocBits) []BitRange](#DiffPallocBits)
    * [func StringifyPallocBits(b *PallocBits, r BitRange) string](#StringifyPallocBits)
    * [func FillAligned(x uint64, m uint) uint64](#FillAligned)
    * [func FreePageAlloc(pp *PageAlloc)](#FreePageAlloc)
    * [func PageBase(c ChunkIdx, pageIdx uint) uintptr](#PageBase)
    * [func CheckScavengedBitsCleared(mismatches []BitsMismatch) (n int, ok bool)](#CheckScavengedBitsCleared)
    * [func PageCachePagesLeaked() (leaked uintptr)](#PageCachePagesLeaked)
    * [func SemNwait(addr *uint32) uint32](#SemNwait)
    * [func FreeMSpan(s *MSpan)](#FreeMSpan)
    * [func MSpanCountAlloc(ms *MSpan, bits []byte) int](#MSpanCountAlloc)
    * [func SetIntArgRegs(a int) int](#SetIntArgRegs)
    * [func FinalizerGAsleep() bool](#FinalizerGAsleep)
    * [func GCTestIsReachable(ptrs ...unsafe.Pointer) (mask uint64)](#GCTestIsReachable)
    * [func GCTestPointerClass(p unsafe.Pointer) string](#GCTestPointerClass)
    * [func sigismember(mask *sigset, i int) bool](#sigismember)
    * [func Sigisblocked(i int) bool](#Sigisblocked)
    * [func WaitForSigusr1(r, w int32, ready func(mp *M)) (int64, int64)](#WaitForSigusr1)
    * [func waitForSigusr1Callback(gp *g) bool](#waitForSigusr1Callback)
    * [func SendSigusr1(mp *M)](#SendSigusr1)
    * [func RunStealOrderTest()](#RunStealOrderTest)


## <a id="const" href="#const">Constants</a>

```
tags: [exported]
```

### <a id="c0" href="#c0">const c0</a>

```
searchKey: runtime.c0
```

```Go
const c0 = uintptr((8-sys.PtrSize)/4*2860486313 + (sys.PtrSize-4)/4*33054211828000289)
```

### <a id="c1" href="#c1">const c1</a>

```
searchKey: runtime.c1
```

```Go
const c1 = uintptr((8-sys.PtrSize)/4*3267000013 + (sys.PtrSize-4)/4*23344194077549503)
```

### <a id="hashRandomBytes" href="#hashRandomBytes">const hashRandomBytes</a>

```
searchKey: runtime.hashRandomBytes
```

```Go
const hashRandomBytes = sys.PtrSize / 4 * 64
```

### <a id="cgoCheckPointerFail" href="#cgoCheckPointerFail">const cgoCheckPointerFail</a>

```
searchKey: runtime.cgoCheckPointerFail
```

```Go
const cgoCheckPointerFail = "cgo argument has Go pointer to Go pointer"
```

### <a id="cgoResultFail" href="#cgoResultFail">const cgoResultFail</a>

```
searchKey: runtime.cgoResultFail
```

```Go
const cgoResultFail = "cgo result has Go pointer"
```

### <a id="cgoWriteBarrierFail" href="#cgoWriteBarrierFail">const cgoWriteBarrierFail</a>

```
searchKey: runtime.cgoWriteBarrierFail
```

```Go
const cgoWriteBarrierFail = "Go pointer stored into non-Go memory"
```

### <a id="maxAlign" href="#maxAlign">const maxAlign</a>

```
searchKey: runtime.maxAlign
```

```Go
const maxAlign = 8
```

### <a id="hchanSize" href="#hchanSize">const hchanSize</a>

```
searchKey: runtime.hchanSize
```

```Go
const hchanSize = unsafe.Sizeof(hchan{}) + uintptr(-int(unsafe.Sizeof(hchan{}))&(maxAlign-1))
```

### <a id="debugChan" href="#debugChan">const debugChan</a>

```
searchKey: runtime.debugChan
```

```Go
const debugChan = false
```

### <a id="Compiler" href="#Compiler">const Compiler</a>

```
searchKey: runtime.Compiler
tags: [exported]
```

```Go
const Compiler = "gc"
```

Compiler is the name of the compiler toolchain that built the running binary. Known toolchains are: 

```
gc      Also known as cmd/compile.
gccgo   The gccgo front end, part of the GCC compiler suite.

```
### <a id="offsetX86HasAVX" href="#offsetX86HasAVX">const offsetX86HasAVX</a>

```
searchKey: runtime.offsetX86HasAVX
```

```Go
const offsetX86HasAVX = unsafe.Offsetof(cpu.X86.HasAVX)
```

Offsets into internal/cpu records for use in assembly. 

### <a id="offsetX86HasAVX2" href="#offsetX86HasAVX2">const offsetX86HasAVX2</a>

```
searchKey: runtime.offsetX86HasAVX2
```

```Go
const offsetX86HasAVX2 = unsafe.Offsetof(cpu.X86.HasAVX2)
```

Offsets into internal/cpu records for use in assembly. 

### <a id="offsetX86HasERMS" href="#offsetX86HasERMS">const offsetX86HasERMS</a>

```
searchKey: runtime.offsetX86HasERMS
```

```Go
const offsetX86HasERMS = unsafe.Offsetof(cpu.X86.HasERMS)
```

Offsets into internal/cpu records for use in assembly. 

### <a id="offsetX86HasSSE2" href="#offsetX86HasSSE2">const offsetX86HasSSE2</a>

```
searchKey: runtime.offsetX86HasSSE2
```

```Go
const offsetX86HasSSE2 = unsafe.Offsetof(cpu.X86.HasSSE2)
```

Offsets into internal/cpu records for use in assembly. 

### <a id="offsetARMHasIDIVA" href="#offsetARMHasIDIVA">const offsetARMHasIDIVA</a>

```
searchKey: runtime.offsetARMHasIDIVA
```

```Go
const offsetARMHasIDIVA = unsafe.Offsetof(cpu.ARM.HasIDIVA)
```

Offsets into internal/cpu records for use in assembly. 

### <a id="offsetMIPS64XHasMSA" href="#offsetMIPS64XHasMSA">const offsetMIPS64XHasMSA</a>

```
searchKey: runtime.offsetMIPS64XHasMSA
```

```Go
const offsetMIPS64XHasMSA = unsafe.Offsetof(cpu.MIPS64X.HasMSA)
```

Offsets into internal/cpu records for use in assembly. 

### <a id="maxCPUProfStack" href="#maxCPUProfStack">const maxCPUProfStack</a>

```
searchKey: runtime.maxCPUProfStack
```

```Go
const maxCPUProfStack = 64
```

### <a id="debugCallSystemStack" href="#debugCallSystemStack">const debugCallSystemStack</a>

```
searchKey: runtime.debugCallSystemStack
```

```Go
const debugCallSystemStack = "executing on Go runtime stack"
```

### <a id="debugCallUnknownFunc" href="#debugCallUnknownFunc">const debugCallUnknownFunc</a>

```
searchKey: runtime.debugCallUnknownFunc
```

```Go
const debugCallUnknownFunc = "call from unknown function"
```

### <a id="debugCallRuntime" href="#debugCallRuntime">const debugCallRuntime</a>

```
searchKey: runtime.debugCallRuntime
```

```Go
const debugCallRuntime = "call from within the Go runtime"
```

### <a id="debugCallUnsafePoint" href="#debugCallUnsafePoint">const debugCallUnsafePoint</a>

```
searchKey: runtime.debugCallUnsafePoint
```

```Go
const debugCallUnsafePoint = "call not at safe point"
```

### <a id="debugLogBytes" href="#debugLogBytes">const debugLogBytes</a>

```
searchKey: runtime.debugLogBytes
```

```Go
const debugLogBytes = 16 << 10
```

debugLogBytes is the size of each per-M ring buffer. This is allocated off-heap to avoid blowing up the M and hence the GC'd heap size. 

### <a id="debugLogStringLimit" href="#debugLogStringLimit">const debugLogStringLimit</a>

```
searchKey: runtime.debugLogStringLimit
```

```Go
const debugLogStringLimit = debugLogBytes / 8
```

debugLogStringLimit is the maximum number of bytes in a string. Above this, the string will be truncated with "..(n more bytes).." 

### <a id="debugLogUnknown" href="#debugLogUnknown">const debugLogUnknown</a>

```
searchKey: runtime.debugLogUnknown
```

```Go
const debugLogUnknown = 1 + iota
```

### <a id="debugLogBoolTrue" href="#debugLogBoolTrue">const debugLogBoolTrue</a>

```
searchKey: runtime.debugLogBoolTrue
```

```Go
const debugLogBoolTrue
```

### <a id="debugLogBoolFalse" href="#debugLogBoolFalse">const debugLogBoolFalse</a>

```
searchKey: runtime.debugLogBoolFalse
```

```Go
const debugLogBoolFalse
```

### <a id="debugLogInt" href="#debugLogInt">const debugLogInt</a>

```
searchKey: runtime.debugLogInt
```

```Go
const debugLogInt
```

### <a id="debugLogUint" href="#debugLogUint">const debugLogUint</a>

```
searchKey: runtime.debugLogUint
```

```Go
const debugLogUint
```

### <a id="debugLogHex" href="#debugLogHex">const debugLogHex</a>

```
searchKey: runtime.debugLogHex
```

```Go
const debugLogHex
```

### <a id="debugLogPtr" href="#debugLogPtr">const debugLogPtr</a>

```
searchKey: runtime.debugLogPtr
```

```Go
const debugLogPtr
```

### <a id="debugLogString" href="#debugLogString">const debugLogString</a>

```
searchKey: runtime.debugLogString
```

```Go
const debugLogString
```

### <a id="debugLogConstString" href="#debugLogConstString">const debugLogConstString</a>

```
searchKey: runtime.debugLogConstString
```

```Go
const debugLogConstString
```

### <a id="debugLogStringOverflow" href="#debugLogStringOverflow">const debugLogStringOverflow</a>

```
searchKey: runtime.debugLogStringOverflow
```

```Go
const debugLogStringOverflow
```

### <a id="debugLogPC" href="#debugLogPC">const debugLogPC</a>

```
searchKey: runtime.debugLogPC
```

```Go
const debugLogPC
```

### <a id="debugLogTraceback" href="#debugLogTraceback">const debugLogTraceback</a>

```
searchKey: runtime.debugLogTraceback
```

```Go
const debugLogTraceback
```

### <a id="debugLogHeaderSize" href="#debugLogHeaderSize">const debugLogHeaderSize</a>

```
searchKey: runtime.debugLogHeaderSize
```

```Go
const debugLogHeaderSize = 2
```

debugLogHeaderSize is the number of bytes in the framing header of every dlog record. 

### <a id="debugLogSyncSize" href="#debugLogSyncSize">const debugLogSyncSize</a>

```
searchKey: runtime.debugLogSyncSize
```

```Go
const debugLogSyncSize = debugLogHeaderSize + 2*8
```

debugLogSyncSize is the number of bytes in a sync record. 

### <a id="dlogEnabled" href="#dlogEnabled">const dlogEnabled</a>

```
searchKey: runtime.dlogEnabled
```

```Go
const dlogEnabled = false
```

### <a id="_EINTR" href="#_EINTR">const _EINTR</a>

```
searchKey: runtime._EINTR
```

```Go
const _EINTR = 0x4
```

### <a id="_EFAULT" href="#_EFAULT">const _EFAULT</a>

```
searchKey: runtime._EFAULT
```

```Go
const _EFAULT = 0xe
```

### <a id="_EAGAIN" href="#_EAGAIN">const _EAGAIN</a>

```
searchKey: runtime._EAGAIN
```

```Go
const _EAGAIN = 0x23
```

### <a id="_ETIMEDOUT" href="#_ETIMEDOUT">const _ETIMEDOUT</a>

```
searchKey: runtime._ETIMEDOUT
```

```Go
const _ETIMEDOUT = 0x3c
```

### <a id="_PROT_NONE" href="#_PROT_NONE">const _PROT_NONE</a>

```
searchKey: runtime._PROT_NONE
```

```Go
const _PROT_NONE = 0x0
```

### <a id="_PROT_READ" href="#_PROT_READ">const _PROT_READ</a>

```
searchKey: runtime._PROT_READ
```

```Go
const _PROT_READ = 0x1
```

### <a id="_PROT_WRITE" href="#_PROT_WRITE">const _PROT_WRITE</a>

```
searchKey: runtime._PROT_WRITE
```

```Go
const _PROT_WRITE = 0x2
```

### <a id="_PROT_EXEC" href="#_PROT_EXEC">const _PROT_EXEC</a>

```
searchKey: runtime._PROT_EXEC
```

```Go
const _PROT_EXEC = 0x4
```

### <a id="_MAP_ANON" href="#_MAP_ANON">const _MAP_ANON</a>

```
searchKey: runtime._MAP_ANON
```

```Go
const _MAP_ANON = 0x1000
```

### <a id="_MAP_PRIVATE" href="#_MAP_PRIVATE">const _MAP_PRIVATE</a>

```
searchKey: runtime._MAP_PRIVATE
```

```Go
const _MAP_PRIVATE = 0x2
```

### <a id="_MAP_FIXED" href="#_MAP_FIXED">const _MAP_FIXED</a>

```
searchKey: runtime._MAP_FIXED
```

```Go
const _MAP_FIXED = 0x10
```

### <a id="_MADV_DONTNEED" href="#_MADV_DONTNEED">const _MADV_DONTNEED</a>

```
searchKey: runtime._MADV_DONTNEED
```

```Go
const _MADV_DONTNEED = 0x4
```

### <a id="_MADV_FREE" href="#_MADV_FREE">const _MADV_FREE</a>

```
searchKey: runtime._MADV_FREE
```

```Go
const _MADV_FREE = 0x5
```

### <a id="_MADV_FREE_REUSABLE" href="#_MADV_FREE_REUSABLE">const _MADV_FREE_REUSABLE</a>

```
searchKey: runtime._MADV_FREE_REUSABLE
```

```Go
const _MADV_FREE_REUSABLE = 0x7
```

### <a id="_MADV_FREE_REUSE" href="#_MADV_FREE_REUSE">const _MADV_FREE_REUSE</a>

```
searchKey: runtime._MADV_FREE_REUSE
```

```Go
const _MADV_FREE_REUSE = 0x8
```

### <a id="_SA_SIGINFO" href="#_SA_SIGINFO">const _SA_SIGINFO</a>

```
searchKey: runtime._SA_SIGINFO
```

```Go
const _SA_SIGINFO = 0x40
```

### <a id="_SA_RESTART" href="#_SA_RESTART">const _SA_RESTART</a>

```
searchKey: runtime._SA_RESTART
```

```Go
const _SA_RESTART = 0x2
```

### <a id="_SA_ONSTACK" href="#_SA_ONSTACK">const _SA_ONSTACK</a>

```
searchKey: runtime._SA_ONSTACK
```

```Go
const _SA_ONSTACK = 0x1
```

### <a id="_SA_USERTRAMP" href="#_SA_USERTRAMP">const _SA_USERTRAMP</a>

```
searchKey: runtime._SA_USERTRAMP
```

```Go
const _SA_USERTRAMP = 0x100
```

### <a id="_SA_64REGSET" href="#_SA_64REGSET">const _SA_64REGSET</a>

```
searchKey: runtime._SA_64REGSET
```

```Go
const _SA_64REGSET = 0x200
```

### <a id="_SIGHUP" href="#_SIGHUP">const _SIGHUP</a>

```
searchKey: runtime._SIGHUP
```

```Go
const _SIGHUP = 0x1
```

### <a id="_SIGINT" href="#_SIGINT">const _SIGINT</a>

```
searchKey: runtime._SIGINT
```

```Go
const _SIGINT = 0x2
```

### <a id="_SIGQUIT" href="#_SIGQUIT">const _SIGQUIT</a>

```
searchKey: runtime._SIGQUIT
```

```Go
const _SIGQUIT = 0x3
```

### <a id="_SIGILL" href="#_SIGILL">const _SIGILL</a>

```
searchKey: runtime._SIGILL
```

```Go
const _SIGILL = 0x4
```

### <a id="_SIGTRAP" href="#_SIGTRAP">const _SIGTRAP</a>

```
searchKey: runtime._SIGTRAP
```

```Go
const _SIGTRAP = 0x5
```

### <a id="_SIGABRT" href="#_SIGABRT">const _SIGABRT</a>

```
searchKey: runtime._SIGABRT
```

```Go
const _SIGABRT = 0x6
```

### <a id="_SIGEMT" href="#_SIGEMT">const _SIGEMT</a>

```
searchKey: runtime._SIGEMT
```

```Go
const _SIGEMT = 0x7
```

### <a id="_SIGFPE" href="#_SIGFPE">const _SIGFPE</a>

```
searchKey: runtime._SIGFPE
```

```Go
const _SIGFPE = 0x8
```

### <a id="_SIGKILL" href="#_SIGKILL">const _SIGKILL</a>

```
searchKey: runtime._SIGKILL
```

```Go
const _SIGKILL = 0x9
```

### <a id="_SIGBUS" href="#_SIGBUS">const _SIGBUS</a>

```
searchKey: runtime._SIGBUS
```

```Go
const _SIGBUS = 0xa
```

### <a id="_SIGSEGV" href="#_SIGSEGV">const _SIGSEGV</a>

```
searchKey: runtime._SIGSEGV
```

```Go
const _SIGSEGV = 0xb
```

### <a id="_SIGSYS" href="#_SIGSYS">const _SIGSYS</a>

```
searchKey: runtime._SIGSYS
```

```Go
const _SIGSYS = 0xc
```

### <a id="_SIGPIPE" href="#_SIGPIPE">const _SIGPIPE</a>

```
searchKey: runtime._SIGPIPE
```

```Go
const _SIGPIPE = 0xd
```

### <a id="_SIGALRM" href="#_SIGALRM">const _SIGALRM</a>

```
searchKey: runtime._SIGALRM
```

```Go
const _SIGALRM = 0xe
```

### <a id="_SIGTERM" href="#_SIGTERM">const _SIGTERM</a>

```
searchKey: runtime._SIGTERM
```

```Go
const _SIGTERM = 0xf
```

### <a id="_SIGURG" href="#_SIGURG">const _SIGURG</a>

```
searchKey: runtime._SIGURG
```

```Go
const _SIGURG = 0x10
```

### <a id="_SIGSTOP" href="#_SIGSTOP">const _SIGSTOP</a>

```
searchKey: runtime._SIGSTOP
```

```Go
const _SIGSTOP = 0x11
```

### <a id="_SIGTSTP" href="#_SIGTSTP">const _SIGTSTP</a>

```
searchKey: runtime._SIGTSTP
```

```Go
const _SIGTSTP = 0x12
```

### <a id="_SIGCONT" href="#_SIGCONT">const _SIGCONT</a>

```
searchKey: runtime._SIGCONT
```

```Go
const _SIGCONT = 0x13
```

### <a id="_SIGCHLD" href="#_SIGCHLD">const _SIGCHLD</a>

```
searchKey: runtime._SIGCHLD
```

```Go
const _SIGCHLD = 0x14
```

### <a id="_SIGTTIN" href="#_SIGTTIN">const _SIGTTIN</a>

```
searchKey: runtime._SIGTTIN
```

```Go
const _SIGTTIN = 0x15
```

### <a id="_SIGTTOU" href="#_SIGTTOU">const _SIGTTOU</a>

```
searchKey: runtime._SIGTTOU
```

```Go
const _SIGTTOU = 0x16
```

### <a id="_SIGIO" href="#_SIGIO">const _SIGIO</a>

```
searchKey: runtime._SIGIO
```

```Go
const _SIGIO = 0x17
```

### <a id="_SIGXCPU" href="#_SIGXCPU">const _SIGXCPU</a>

```
searchKey: runtime._SIGXCPU
```

```Go
const _SIGXCPU = 0x18
```

### <a id="_SIGXFSZ" href="#_SIGXFSZ">const _SIGXFSZ</a>

```
searchKey: runtime._SIGXFSZ
```

```Go
const _SIGXFSZ = 0x19
```

### <a id="_SIGVTALRM" href="#_SIGVTALRM">const _SIGVTALRM</a>

```
searchKey: runtime._SIGVTALRM
```

```Go
const _SIGVTALRM = 0x1a
```

### <a id="_SIGPROF" href="#_SIGPROF">const _SIGPROF</a>

```
searchKey: runtime._SIGPROF
```

```Go
const _SIGPROF = 0x1b
```

### <a id="_SIGWINCH" href="#_SIGWINCH">const _SIGWINCH</a>

```
searchKey: runtime._SIGWINCH
```

```Go
const _SIGWINCH = 0x1c
```

### <a id="_SIGINFO" href="#_SIGINFO">const _SIGINFO</a>

```
searchKey: runtime._SIGINFO
```

```Go
const _SIGINFO = 0x1d
```

### <a id="_SIGUSR1" href="#_SIGUSR1">const _SIGUSR1</a>

```
searchKey: runtime._SIGUSR1
```

```Go
const _SIGUSR1 = 0x1e
```

### <a id="_SIGUSR2" href="#_SIGUSR2">const _SIGUSR2</a>

```
searchKey: runtime._SIGUSR2
```

```Go
const _SIGUSR2 = 0x1f
```

### <a id="_FPE_INTDIV" href="#_FPE_INTDIV">const _FPE_INTDIV</a>

```
searchKey: runtime._FPE_INTDIV
```

```Go
const _FPE_INTDIV = 0x7
```

### <a id="_FPE_INTOVF" href="#_FPE_INTOVF">const _FPE_INTOVF</a>

```
searchKey: runtime._FPE_INTOVF
```

```Go
const _FPE_INTOVF = 0x8
```

### <a id="_FPE_FLTDIV" href="#_FPE_FLTDIV">const _FPE_FLTDIV</a>

```
searchKey: runtime._FPE_FLTDIV
```

```Go
const _FPE_FLTDIV = 0x1
```

### <a id="_FPE_FLTOVF" href="#_FPE_FLTOVF">const _FPE_FLTOVF</a>

```
searchKey: runtime._FPE_FLTOVF
```

```Go
const _FPE_FLTOVF = 0x2
```

### <a id="_FPE_FLTUND" href="#_FPE_FLTUND">const _FPE_FLTUND</a>

```
searchKey: runtime._FPE_FLTUND
```

```Go
const _FPE_FLTUND = 0x3
```

### <a id="_FPE_FLTRES" href="#_FPE_FLTRES">const _FPE_FLTRES</a>

```
searchKey: runtime._FPE_FLTRES
```

```Go
const _FPE_FLTRES = 0x4
```

### <a id="_FPE_FLTINV" href="#_FPE_FLTINV">const _FPE_FLTINV</a>

```
searchKey: runtime._FPE_FLTINV
```

```Go
const _FPE_FLTINV = 0x5
```

### <a id="_FPE_FLTSUB" href="#_FPE_FLTSUB">const _FPE_FLTSUB</a>

```
searchKey: runtime._FPE_FLTSUB
```

```Go
const _FPE_FLTSUB = 0x6
```

### <a id="_BUS_ADRALN" href="#_BUS_ADRALN">const _BUS_ADRALN</a>

```
searchKey: runtime._BUS_ADRALN
```

```Go
const _BUS_ADRALN = 0x1
```

### <a id="_BUS_ADRERR" href="#_BUS_ADRERR">const _BUS_ADRERR</a>

```
searchKey: runtime._BUS_ADRERR
```

```Go
const _BUS_ADRERR = 0x2
```

### <a id="_BUS_OBJERR" href="#_BUS_OBJERR">const _BUS_OBJERR</a>

```
searchKey: runtime._BUS_OBJERR
```

```Go
const _BUS_OBJERR = 0x3
```

### <a id="_SEGV_MAPERR" href="#_SEGV_MAPERR">const _SEGV_MAPERR</a>

```
searchKey: runtime._SEGV_MAPERR
```

```Go
const _SEGV_MAPERR = 0x1
```

### <a id="_SEGV_ACCERR" href="#_SEGV_ACCERR">const _SEGV_ACCERR</a>

```
searchKey: runtime._SEGV_ACCERR
```

```Go
const _SEGV_ACCERR = 0x2
```

### <a id="_ITIMER_REAL" href="#_ITIMER_REAL">const _ITIMER_REAL</a>

```
searchKey: runtime._ITIMER_REAL
```

```Go
const _ITIMER_REAL = 0x0
```

### <a id="_ITIMER_VIRTUAL" href="#_ITIMER_VIRTUAL">const _ITIMER_VIRTUAL</a>

```
searchKey: runtime._ITIMER_VIRTUAL
```

```Go
const _ITIMER_VIRTUAL = 0x1
```

### <a id="_ITIMER_PROF" href="#_ITIMER_PROF">const _ITIMER_PROF</a>

```
searchKey: runtime._ITIMER_PROF
```

```Go
const _ITIMER_PROF = 0x2
```

### <a id="_EV_ADD" href="#_EV_ADD">const _EV_ADD</a>

```
searchKey: runtime._EV_ADD
```

```Go
const _EV_ADD = 0x1
```

### <a id="_EV_DELETE" href="#_EV_DELETE">const _EV_DELETE</a>

```
searchKey: runtime._EV_DELETE
```

```Go
const _EV_DELETE = 0x2
```

### <a id="_EV_CLEAR" href="#_EV_CLEAR">const _EV_CLEAR</a>

```
searchKey: runtime._EV_CLEAR
```

```Go
const _EV_CLEAR = 0x20
```

### <a id="_EV_RECEIPT" href="#_EV_RECEIPT">const _EV_RECEIPT</a>

```
searchKey: runtime._EV_RECEIPT
```

```Go
const _EV_RECEIPT = 0x40
```

### <a id="_EV_ERROR" href="#_EV_ERROR">const _EV_ERROR</a>

```
searchKey: runtime._EV_ERROR
```

```Go
const _EV_ERROR = 0x4000
```

### <a id="_EV_EOF" href="#_EV_EOF">const _EV_EOF</a>

```
searchKey: runtime._EV_EOF
```

```Go
const _EV_EOF = 0x8000
```

### <a id="_EVFILT_READ" href="#_EVFILT_READ">const _EVFILT_READ</a>

```
searchKey: runtime._EVFILT_READ
```

```Go
const _EVFILT_READ = -0x1
```

### <a id="_EVFILT_WRITE" href="#_EVFILT_WRITE">const _EVFILT_WRITE</a>

```
searchKey: runtime._EVFILT_WRITE
```

```Go
const _EVFILT_WRITE = -0x2
```

### <a id="_PTHREAD_CREATE_DETACHED" href="#_PTHREAD_CREATE_DETACHED">const _PTHREAD_CREATE_DETACHED</a>

```
searchKey: runtime._PTHREAD_CREATE_DETACHED
```

```Go
const _PTHREAD_CREATE_DETACHED = 0x2
```

### <a id="_F_SETFD" href="#_F_SETFD">const _F_SETFD</a>

```
searchKey: runtime._F_SETFD
```

```Go
const _F_SETFD = 0x2
```

### <a id="_F_GETFL" href="#_F_GETFL">const _F_GETFL</a>

```
searchKey: runtime._F_GETFL
```

```Go
const _F_GETFL = 0x3
```

### <a id="_F_SETFL" href="#_F_SETFL">const _F_SETFL</a>

```
searchKey: runtime._F_SETFL
```

```Go
const _F_SETFL = 0x4
```

### <a id="_FD_CLOEXEC" href="#_FD_CLOEXEC">const _FD_CLOEXEC</a>

```
searchKey: runtime._FD_CLOEXEC
```

```Go
const _FD_CLOEXEC = 0x1
```

### <a id="_O_NONBLOCK" href="#_O_NONBLOCK">const _O_NONBLOCK</a>

```
searchKey: runtime._O_NONBLOCK
```

```Go
const _O_NONBLOCK = 4
```

### <a id="boundsIndex" href="#boundsIndex">const boundsIndex</a>

```
searchKey: runtime.boundsIndex
```

```Go
const boundsIndex boundsErrorCode = iota // s[x], 0 <= x < len(s) failed

```

### <a id="boundsSliceAlen" href="#boundsSliceAlen">const boundsSliceAlen</a>

```
searchKey: runtime.boundsSliceAlen
```

```Go
const boundsSliceAlen // s[?:x], 0 <= x <= len(s) failed

```

### <a id="boundsSliceAcap" href="#boundsSliceAcap">const boundsSliceAcap</a>

```
searchKey: runtime.boundsSliceAcap
```

```Go
const boundsSliceAcap // s[?:x], 0 <= x <= cap(s) failed

```

### <a id="boundsSliceB" href="#boundsSliceB">const boundsSliceB</a>

```
searchKey: runtime.boundsSliceB
```

```Go
const boundsSliceB // s[x:y], 0 <= x <= y failed (but boundsSliceA didn't happen)

```

### <a id="boundsSlice3Alen" href="#boundsSlice3Alen">const boundsSlice3Alen</a>

```
searchKey: runtime.boundsSlice3Alen
```

```Go
const boundsSlice3Alen // s[?:?:x], 0 <= x <= len(s) failed

```

### <a id="boundsSlice3Acap" href="#boundsSlice3Acap">const boundsSlice3Acap</a>

```
searchKey: runtime.boundsSlice3Acap
```

```Go
const boundsSlice3Acap // s[?:?:x], 0 <= x <= cap(s) failed

```

### <a id="boundsSlice3B" href="#boundsSlice3B">const boundsSlice3B</a>

```
searchKey: runtime.boundsSlice3B
```

```Go
const boundsSlice3B // s[?:x:y], 0 <= x <= y failed (but boundsSlice3A didn't happen)

```

### <a id="boundsSlice3C" href="#boundsSlice3C">const boundsSlice3C</a>

```
searchKey: runtime.boundsSlice3C
```

```Go
const boundsSlice3C // s[x:y:?], 0 <= x <= y failed (but boundsSlice3A/B didn't happen)

```

### <a id="boundsConvert" href="#boundsConvert">const boundsConvert</a>

```
searchKey: runtime.boundsConvert
```

```Go
const boundsConvert // (*[x]T)(s), 0 <= x <= len(s) failed

```

### <a id="GOOS" href="#GOOS">const GOOS</a>

```
searchKey: runtime.GOOS
tags: [exported]
```

```Go
const GOOS string = sys.GOOS
```

GOOS is the running program's operating system target: one of darwin, freebsd, linux, and so on. To view possible combinations of GOOS and GOARCH, run "go tool dist list". 

### <a id="GOARCH" href="#GOARCH">const GOARCH</a>

```
searchKey: runtime.GOARCH
tags: [exported]
```

```Go
const GOARCH string = sys.GOARCH
```

GOARCH is the running program's architecture target: one of 386, amd64, arm, s390x, and so on. 

### <a id="fastlogNumBits" href="#fastlogNumBits">const fastlogNumBits</a>

```
searchKey: runtime.fastlogNumBits
```

```Go
const fastlogNumBits = 5
```

### <a id="m1" href="#m1">const m1</a>

```
searchKey: runtime.m1
```

```Go
const m1 = 0xa0761d6478bd642f
```

### <a id="m2" href="#m2">const m2</a>

```
searchKey: runtime.m2
```

```Go
const m2 = 0xe7037ed1a0b428db
```

### <a id="m3" href="#m3">const m3</a>

```
searchKey: runtime.m3
```

```Go
const m3 = 0x8ebc6af09c88c6e3
```

### <a id="m4" href="#m4">const m4</a>

```
searchKey: runtime.m4
```

```Go
const m4 = 0x589965cc75374cc3
```

### <a id="m5" href="#m5">const m5</a>

```
searchKey: runtime.m5
```

```Go
const m5 = 0x1d8e4e27c47d124f
```

### <a id="fieldKindEol" href="#fieldKindEol">const fieldKindEol</a>

```
searchKey: runtime.fieldKindEol
```

```Go
const fieldKindEol = 0
```

### <a id="fieldKindPtr" href="#fieldKindPtr">const fieldKindPtr</a>

```
searchKey: runtime.fieldKindPtr
```

```Go
const fieldKindPtr = 1
```

### <a id="fieldKindIface" href="#fieldKindIface">const fieldKindIface</a>

```
searchKey: runtime.fieldKindIface
```

```Go
const fieldKindIface = 2
```

### <a id="fieldKindEface" href="#fieldKindEface">const fieldKindEface</a>

```
searchKey: runtime.fieldKindEface
```

```Go
const fieldKindEface = 3
```

### <a id="tagEOF" href="#tagEOF">const tagEOF</a>

```
searchKey: runtime.tagEOF
```

```Go
const tagEOF = 0
```

### <a id="tagObject" href="#tagObject">const tagObject</a>

```
searchKey: runtime.tagObject
```

```Go
const tagObject = 1
```

### <a id="tagOtherRoot" href="#tagOtherRoot">const tagOtherRoot</a>

```
searchKey: runtime.tagOtherRoot
```

```Go
const tagOtherRoot = 2
```

### <a id="tagType" href="#tagType">const tagType</a>

```
searchKey: runtime.tagType
```

```Go
const tagType = 3
```

### <a id="tagGoroutine" href="#tagGoroutine">const tagGoroutine</a>

```
searchKey: runtime.tagGoroutine
```

```Go
const tagGoroutine = 4
```

### <a id="tagStackFrame" href="#tagStackFrame">const tagStackFrame</a>

```
searchKey: runtime.tagStackFrame
```

```Go
const tagStackFrame = 5
```

### <a id="tagParams" href="#tagParams">const tagParams</a>

```
searchKey: runtime.tagParams
```

```Go
const tagParams = 6
```

### <a id="tagFinalizer" href="#tagFinalizer">const tagFinalizer</a>

```
searchKey: runtime.tagFinalizer
```

```Go
const tagFinalizer = 7
```

### <a id="tagItab" href="#tagItab">const tagItab</a>

```
searchKey: runtime.tagItab
```

```Go
const tagItab = 8
```

### <a id="tagOSThread" href="#tagOSThread">const tagOSThread</a>

```
searchKey: runtime.tagOSThread
```

```Go
const tagOSThread = 9
```

### <a id="tagMemStats" href="#tagMemStats">const tagMemStats</a>

```
searchKey: runtime.tagMemStats
```

```Go
const tagMemStats = 10
```

### <a id="tagQueuedFinalizer" href="#tagQueuedFinalizer">const tagQueuedFinalizer</a>

```
searchKey: runtime.tagQueuedFinalizer
```

```Go
const tagQueuedFinalizer = 11
```

### <a id="tagData" href="#tagData">const tagData</a>

```
searchKey: runtime.tagData
```

```Go
const tagData = 12
```

### <a id="tagBSS" href="#tagBSS">const tagBSS</a>

```
searchKey: runtime.tagBSS
```

```Go
const tagBSS = 13
```

### <a id="tagDefer" href="#tagDefer">const tagDefer</a>

```
searchKey: runtime.tagDefer
```

```Go
const tagDefer = 14
```

### <a id="tagPanic" href="#tagPanic">const tagPanic</a>

```
searchKey: runtime.tagPanic
```

```Go
const tagPanic = 15
```

### <a id="tagMemProf" href="#tagMemProf">const tagMemProf</a>

```
searchKey: runtime.tagMemProf
```

```Go
const tagMemProf = 16
```

### <a id="tagAllocSample" href="#tagAllocSample">const tagAllocSample</a>

```
searchKey: runtime.tagAllocSample
```

```Go
const tagAllocSample = 17
```

### <a id="bufSize" href="#bufSize">const bufSize</a>

```
searchKey: runtime.bufSize
```

```Go
const bufSize = 4096
```

buffer of pending write data 

### <a id="typeCacheBuckets" href="#typeCacheBuckets">const typeCacheBuckets</a>

```
searchKey: runtime.typeCacheBuckets
```

```Go
const typeCacheBuckets = 256
```

Cache of types that have been serialized already. We use a type's hash field to pick a bucket. Inside a bucket, we keep a list of types that have been serialized so far, most recently used first. Note: when a bucket overflows we may end up serializing a type more than once. That's ok. 

### <a id="typeCacheAssoc" href="#typeCacheAssoc">const typeCacheAssoc</a>

```
searchKey: runtime.typeCacheAssoc
```

```Go
const typeCacheAssoc = 4
```

Cache of types that have been serialized already. We use a type's hash field to pick a bucket. Inside a bucket, we keep a list of types that have been serialized so far, most recently used first. Note: when a bucket overflows we may end up serializing a type more than once. That's ok. 

### <a id="timeHistSubBucketBits" href="#timeHistSubBucketBits">const timeHistSubBucketBits</a>

```
searchKey: runtime.timeHistSubBucketBits
```

```Go
const timeHistSubBucketBits = 4
```

For the time histogram type, we use an HDR histogram. Values are placed in super-buckets based solely on the most significant set bit. Thus, super-buckets are power-of-2 sized. Values are then placed into sub-buckets based on the value of the next timeHistSubBucketBits most significant bits. Thus, sub-buckets are linear within a super-bucket. 

Therefore, the number of sub-buckets (timeHistNumSubBuckets) defines the error. This error may be computed as 1/timeHistNumSubBuckets*100%. For example, for 16 sub-buckets per super-bucket the error is approximately 6%. 

The number of super-buckets (timeHistNumSuperBuckets), on the other hand, defines the range. To reserve room for sub-buckets, bit timeHistSubBucketBits is the first bit considered for super-buckets, so super-bucket indices are adjusted accordingly. 

As an example, consider 45 super-buckets with 16 sub-buckets. 

```
00110
^----
│  ^
│  └---- Lowest 4 bits -> sub-bucket 6
└------- Bit 4 unset -> super-bucket 0

10110
^----
│  ^
│  └---- Next 4 bits -> sub-bucket 6
└------- Bit 4 set -> super-bucket 1
100010
^----^
│  ^ └-- Lower bits ignored
│  └---- Next 4 bits -> sub-bucket 1
└------- Bit 5 set -> super-bucket 2

```
Following this pattern, bucket 45 will have the bit 48 set. We don't have any buckets for higher values, so the highest sub-bucket will contain values of 2^48-1 nanoseconds or approx. 3 days. This range is more than enough to handle durations produced by the runtime. 

### <a id="timeHistNumSubBuckets" href="#timeHistNumSubBuckets">const timeHistNumSubBuckets</a>

```
searchKey: runtime.timeHistNumSubBuckets
```

```Go
const timeHistNumSubBuckets = 1 << timeHistSubBucketBits
```

### <a id="timeHistNumSuperBuckets" href="#timeHistNumSuperBuckets">const timeHistNumSuperBuckets</a>

```
searchKey: runtime.timeHistNumSuperBuckets
```

```Go
const timeHistNumSuperBuckets = 45
```

### <a id="timeHistTotalBuckets" href="#timeHistTotalBuckets">const timeHistTotalBuckets</a>

```
searchKey: runtime.timeHistTotalBuckets
```

```Go
const timeHistTotalBuckets = timeHistNumSuperBuckets*timeHistNumSubBuckets + 1
```

### <a id="fInf" href="#fInf">const fInf</a>

```
searchKey: runtime.fInf
```

```Go
const fInf = 0x7FF0000000000000
```

### <a id="fNegInf" href="#fNegInf">const fNegInf</a>

```
searchKey: runtime.fNegInf
```

```Go
const fNegInf = 0xFFF0000000000000
```

### <a id="itabInitSize" href="#itabInitSize">const itabInitSize</a>

```
searchKey: runtime.itabInitSize
```

```Go
const itabInitSize = 512
```

### <a id="addrBits" href="#addrBits">const addrBits</a>

```
searchKey: runtime.addrBits
```

```Go
const addrBits = 48
```

addrBits is the number of bits needed to represent a virtual address. 

See heapAddrBits for a table of address space sizes on various architectures. 48 bits is enough for all architectures except s390x. 

On AMD64, virtual addresses are 48-bit (or 57-bit) numbers sign extended to 64. We shift the address left 16 to eliminate the sign extended part and make room in the bottom for the count. 

On s390x, virtual addresses are 64-bit. There's not much we can do about this, so we just hope that the kernel doesn't get to really high addresses and panic if it does. 

### <a id="cntBits" href="#cntBits">const cntBits</a>

```
searchKey: runtime.cntBits
```

```Go
const cntBits = 64 - addrBits + 3
```

In addition to the 16 bits taken from the top, we can take 3 from the bottom, because node must be pointer-aligned, giving a total of 19 bits of count. 

### <a id="aixAddrBits" href="#aixAddrBits">const aixAddrBits</a>

```
searchKey: runtime.aixAddrBits
```

```Go
const aixAddrBits = 57
```

On AIX, 64-bit addresses are split into 36-bit segment number and 28-bit offset in segment.  Segment numbers in the range 0x0A0000000-0x0AFFFFFFF(LSA) are available for mmap. We assume all lfnode addresses are from memory allocated with mmap. We use one bit to distinguish between the two ranges. 

### <a id="aixCntBits" href="#aixCntBits">const aixCntBits</a>

```
searchKey: runtime.aixCntBits
```

```Go
const aixCntBits = 64 - aixAddrBits + 3
```

### <a id="locked" href="#locked">const locked</a>

```
searchKey: runtime.locked
```

```Go
const locked uintptr = 1
```

This implementation depends on OS-specific implementations of 

```
func semacreate(mp *m)
	Create a semaphore for mp, if it does not already have one.

func semasleep(ns int64) int32
	If ns < 0, acquire m's semaphore and return 0.
	If ns >= 0, try to acquire m's semaphore for at most ns nanoseconds.
	Return 0 if the semaphore was acquired, -1 if interrupted or timed out.

func semawakeup(mp *m)
	Wake up mp, which is or will soon be sleeping on its semaphore.

```
### <a id="active_spin" href="#active_spin">const active_spin</a>

```
searchKey: runtime.active_spin
```

```Go
const active_spin = 4
```

This implementation depends on OS-specific implementations of 

```
func semacreate(mp *m)
	Create a semaphore for mp, if it does not already have one.

func semasleep(ns int64) int32
	If ns < 0, acquire m's semaphore and return 0.
	If ns >= 0, try to acquire m's semaphore for at most ns nanoseconds.
	Return 0 if the semaphore was acquired, -1 if interrupted or timed out.

func semawakeup(mp *m)
	Wake up mp, which is or will soon be sleeping on its semaphore.

```
### <a id="active_spin_cnt" href="#active_spin_cnt">const active_spin_cnt</a>

```
searchKey: runtime.active_spin_cnt
```

```Go
const active_spin_cnt = 30
```

This implementation depends on OS-specific implementations of 

```
func semacreate(mp *m)
	Create a semaphore for mp, if it does not already have one.

func semasleep(ns int64) int32
	If ns < 0, acquire m's semaphore and return 0.
	If ns >= 0, try to acquire m's semaphore for at most ns nanoseconds.
	Return 0 if the semaphore was acquired, -1 if interrupted or timed out.

func semawakeup(mp *m)
	Wake up mp, which is or will soon be sleeping on its semaphore.

```
### <a id="passive_spin" href="#passive_spin">const passive_spin</a>

```
searchKey: runtime.passive_spin
```

```Go
const passive_spin = 1
```

This implementation depends on OS-specific implementations of 

```
func semacreate(mp *m)
	Create a semaphore for mp, if it does not already have one.

func semasleep(ns int64) int32
	If ns < 0, acquire m's semaphore and return 0.
	If ns >= 0, try to acquire m's semaphore for at most ns nanoseconds.
	Return 0 if the semaphore was acquired, -1 if interrupted or timed out.

func semawakeup(mp *m)
	Wake up mp, which is or will soon be sleeping on its semaphore.

```
### <a id="lockRankDummy" href="#lockRankDummy">const lockRankDummy</a>

```
searchKey: runtime.lockRankDummy
```

```Go
const lockRankDummy lockRank = iota
```

Constants representing the lock rank of the architecture-independent locks in the runtime. Locks with lower rank must be taken before locks with higher rank. 

### <a id="lockRankSysmon" href="#lockRankSysmon">const lockRankSysmon</a>

```
searchKey: runtime.lockRankSysmon
```

```Go
const lockRankSysmon
```

Constants representing the lock rank of the architecture-independent locks in the runtime. Locks with lower rank must be taken before locks with higher rank. 

Locks held above sched 

### <a id="lockRankScavenge" href="#lockRankScavenge">const lockRankScavenge</a>

```
searchKey: runtime.lockRankScavenge
```

```Go
const lockRankScavenge
```

Constants representing the lock rank of the architecture-independent locks in the runtime. Locks with lower rank must be taken before locks with higher rank. 

### <a id="lockRankForcegc" href="#lockRankForcegc">const lockRankForcegc</a>

```
searchKey: runtime.lockRankForcegc
```

```Go
const lockRankForcegc
```

Constants representing the lock rank of the architecture-independent locks in the runtime. Locks with lower rank must be taken before locks with higher rank. 

### <a id="lockRankSweepWaiters" href="#lockRankSweepWaiters">const lockRankSweepWaiters</a>

```
searchKey: runtime.lockRankSweepWaiters
```

```Go
const lockRankSweepWaiters
```

Constants representing the lock rank of the architecture-independent locks in the runtime. Locks with lower rank must be taken before locks with higher rank. 

### <a id="lockRankAssistQueue" href="#lockRankAssistQueue">const lockRankAssistQueue</a>

```
searchKey: runtime.lockRankAssistQueue
```

```Go
const lockRankAssistQueue
```

Constants representing the lock rank of the architecture-independent locks in the runtime. Locks with lower rank must be taken before locks with higher rank. 

### <a id="lockRankCpuprof" href="#lockRankCpuprof">const lockRankCpuprof</a>

```
searchKey: runtime.lockRankCpuprof
```

```Go
const lockRankCpuprof
```

Constants representing the lock rank of the architecture-independent locks in the runtime. Locks with lower rank must be taken before locks with higher rank. 

### <a id="lockRankSweep" href="#lockRankSweep">const lockRankSweep</a>

```
searchKey: runtime.lockRankSweep
```

```Go
const lockRankSweep
```

Constants representing the lock rank of the architecture-independent locks in the runtime. Locks with lower rank must be taken before locks with higher rank. 

### <a id="lockRankPollDesc" href="#lockRankPollDesc">const lockRankPollDesc</a>

```
searchKey: runtime.lockRankPollDesc
```

```Go
const lockRankPollDesc
```

Constants representing the lock rank of the architecture-independent locks in the runtime. Locks with lower rank must be taken before locks with higher rank. 

### <a id="lockRankSched" href="#lockRankSched">const lockRankSched</a>

```
searchKey: runtime.lockRankSched
```

```Go
const lockRankSched
```

Constants representing the lock rank of the architecture-independent locks in the runtime. Locks with lower rank must be taken before locks with higher rank. 

### <a id="lockRankDeadlock" href="#lockRankDeadlock">const lockRankDeadlock</a>

```
searchKey: runtime.lockRankDeadlock
```

```Go
const lockRankDeadlock
```

Constants representing the lock rank of the architecture-independent locks in the runtime. Locks with lower rank must be taken before locks with higher rank. 

### <a id="lockRankAllg" href="#lockRankAllg">const lockRankAllg</a>

```
searchKey: runtime.lockRankAllg
```

```Go
const lockRankAllg
```

Constants representing the lock rank of the architecture-independent locks in the runtime. Locks with lower rank must be taken before locks with higher rank. 

### <a id="lockRankAllp" href="#lockRankAllp">const lockRankAllp</a>

```
searchKey: runtime.lockRankAllp
```

```Go
const lockRankAllp
```

Constants representing the lock rank of the architecture-independent locks in the runtime. Locks with lower rank must be taken before locks with higher rank. 

### <a id="lockRankTimers" href="#lockRankTimers">const lockRankTimers</a>

```
searchKey: runtime.lockRankTimers
```

```Go
const lockRankTimers // Multiple timers locked simultaneously in destroy()

```

Constants representing the lock rank of the architecture-independent locks in the runtime. Locks with lower rank must be taken before locks with higher rank. 

### <a id="lockRankItab" href="#lockRankItab">const lockRankItab</a>

```
searchKey: runtime.lockRankItab
```

```Go
const lockRankItab
```

Constants representing the lock rank of the architecture-independent locks in the runtime. Locks with lower rank must be taken before locks with higher rank. 

### <a id="lockRankReflectOffs" href="#lockRankReflectOffs">const lockRankReflectOffs</a>

```
searchKey: runtime.lockRankReflectOffs
```

```Go
const lockRankReflectOffs
```

Constants representing the lock rank of the architecture-independent locks in the runtime. Locks with lower rank must be taken before locks with higher rank. 

### <a id="lockRankHchan" href="#lockRankHchan">const lockRankHchan</a>

```
searchKey: runtime.lockRankHchan
```

```Go
const lockRankHchan // Multiple hchans acquired in lock order in syncadjustsudogs()

```

Constants representing the lock rank of the architecture-independent locks in the runtime. Locks with lower rank must be taken before locks with higher rank. 

### <a id="lockRankFin" href="#lockRankFin">const lockRankFin</a>

```
searchKey: runtime.lockRankFin
```

```Go
const lockRankFin
```

Constants representing the lock rank of the architecture-independent locks in the runtime. Locks with lower rank must be taken before locks with higher rank. 

### <a id="lockRankNotifyList" href="#lockRankNotifyList">const lockRankNotifyList</a>

```
searchKey: runtime.lockRankNotifyList
```

```Go
const lockRankNotifyList
```

Constants representing the lock rank of the architecture-independent locks in the runtime. Locks with lower rank must be taken before locks with higher rank. 

### <a id="lockRankTraceBuf" href="#lockRankTraceBuf">const lockRankTraceBuf</a>

```
searchKey: runtime.lockRankTraceBuf
```

```Go
const lockRankTraceBuf
```

Constants representing the lock rank of the architecture-independent locks in the runtime. Locks with lower rank must be taken before locks with higher rank. 

### <a id="lockRankTraceStrings" href="#lockRankTraceStrings">const lockRankTraceStrings</a>

```
searchKey: runtime.lockRankTraceStrings
```

```Go
const lockRankTraceStrings
```

Constants representing the lock rank of the architecture-independent locks in the runtime. Locks with lower rank must be taken before locks with higher rank. 

### <a id="lockRankMspanSpecial" href="#lockRankMspanSpecial">const lockRankMspanSpecial</a>

```
searchKey: runtime.lockRankMspanSpecial
```

```Go
const lockRankMspanSpecial
```

Constants representing the lock rank of the architecture-independent locks in the runtime. Locks with lower rank must be taken before locks with higher rank. 

### <a id="lockRankProf" href="#lockRankProf">const lockRankProf</a>

```
searchKey: runtime.lockRankProf
```

```Go
const lockRankProf
```

Constants representing the lock rank of the architecture-independent locks in the runtime. Locks with lower rank must be taken before locks with higher rank. 

### <a id="lockRankGcBitsArenas" href="#lockRankGcBitsArenas">const lockRankGcBitsArenas</a>

```
searchKey: runtime.lockRankGcBitsArenas
```

```Go
const lockRankGcBitsArenas
```

Constants representing the lock rank of the architecture-independent locks in the runtime. Locks with lower rank must be taken before locks with higher rank. 

### <a id="lockRankRoot" href="#lockRankRoot">const lockRankRoot</a>

```
searchKey: runtime.lockRankRoot
```

```Go
const lockRankRoot
```

Constants representing the lock rank of the architecture-independent locks in the runtime. Locks with lower rank must be taken before locks with higher rank. 

### <a id="lockRankTrace" href="#lockRankTrace">const lockRankTrace</a>

```
searchKey: runtime.lockRankTrace
```

```Go
const lockRankTrace
```

Constants representing the lock rank of the architecture-independent locks in the runtime. Locks with lower rank must be taken before locks with higher rank. 

### <a id="lockRankTraceStackTab" href="#lockRankTraceStackTab">const lockRankTraceStackTab</a>

```
searchKey: runtime.lockRankTraceStackTab
```

```Go
const lockRankTraceStackTab
```

Constants representing the lock rank of the architecture-independent locks in the runtime. Locks with lower rank must be taken before locks with higher rank. 

### <a id="lockRankNetpollInit" href="#lockRankNetpollInit">const lockRankNetpollInit</a>

```
searchKey: runtime.lockRankNetpollInit
```

```Go
const lockRankNetpollInit
```

Constants representing the lock rank of the architecture-independent locks in the runtime. Locks with lower rank must be taken before locks with higher rank. 

### <a id="lockRankRwmutexW" href="#lockRankRwmutexW">const lockRankRwmutexW</a>

```
searchKey: runtime.lockRankRwmutexW
```

```Go
const lockRankRwmutexW
```

Constants representing the lock rank of the architecture-independent locks in the runtime. Locks with lower rank must be taken before locks with higher rank. 

### <a id="lockRankRwmutexR" href="#lockRankRwmutexR">const lockRankRwmutexR</a>

```
searchKey: runtime.lockRankRwmutexR
```

```Go
const lockRankRwmutexR
```

Constants representing the lock rank of the architecture-independent locks in the runtime. Locks with lower rank must be taken before locks with higher rank. 

### <a id="lockRankSpanSetSpine" href="#lockRankSpanSetSpine">const lockRankSpanSetSpine</a>

```
searchKey: runtime.lockRankSpanSetSpine
```

```Go
const lockRankSpanSetSpine
```

Constants representing the lock rank of the architecture-independent locks in the runtime. Locks with lower rank must be taken before locks with higher rank. 

### <a id="lockRankGscan" href="#lockRankGscan">const lockRankGscan</a>

```
searchKey: runtime.lockRankGscan
```

```Go
const lockRankGscan
```

Constants representing the lock rank of the architecture-independent locks in the runtime. Locks with lower rank must be taken before locks with higher rank. 

### <a id="lockRankStackpool" href="#lockRankStackpool">const lockRankStackpool</a>

```
searchKey: runtime.lockRankStackpool
```

```Go
const lockRankStackpool
```

Constants representing the lock rank of the architecture-independent locks in the runtime. Locks with lower rank must be taken before locks with higher rank. 

### <a id="lockRankStackLarge" href="#lockRankStackLarge">const lockRankStackLarge</a>

```
searchKey: runtime.lockRankStackLarge
```

```Go
const lockRankStackLarge
```

Constants representing the lock rank of the architecture-independent locks in the runtime. Locks with lower rank must be taken before locks with higher rank. 

### <a id="lockRankDefer" href="#lockRankDefer">const lockRankDefer</a>

```
searchKey: runtime.lockRankDefer
```

```Go
const lockRankDefer
```

Constants representing the lock rank of the architecture-independent locks in the runtime. Locks with lower rank must be taken before locks with higher rank. 

### <a id="lockRankSudog" href="#lockRankSudog">const lockRankSudog</a>

```
searchKey: runtime.lockRankSudog
```

```Go
const lockRankSudog
```

Constants representing the lock rank of the architecture-independent locks in the runtime. Locks with lower rank must be taken before locks with higher rank. 

### <a id="lockRankWbufSpans" href="#lockRankWbufSpans">const lockRankWbufSpans</a>

```
searchKey: runtime.lockRankWbufSpans
```

```Go
const lockRankWbufSpans
```

Constants representing the lock rank of the architecture-independent locks in the runtime. Locks with lower rank must be taken before locks with higher rank. 

Memory-related non-leaf locks 

### <a id="lockRankMheap" href="#lockRankMheap">const lockRankMheap</a>

```
searchKey: runtime.lockRankMheap
```

```Go
const lockRankMheap
```

Constants representing the lock rank of the architecture-independent locks in the runtime. Locks with lower rank must be taken before locks with higher rank. 

### <a id="lockRankMheapSpecial" href="#lockRankMheapSpecial">const lockRankMheapSpecial</a>

```
searchKey: runtime.lockRankMheapSpecial
```

```Go
const lockRankMheapSpecial
```

Constants representing the lock rank of the architecture-independent locks in the runtime. Locks with lower rank must be taken before locks with higher rank. 

### <a id="lockRankGlobalAlloc" href="#lockRankGlobalAlloc">const lockRankGlobalAlloc</a>

```
searchKey: runtime.lockRankGlobalAlloc
```

```Go
const lockRankGlobalAlloc
```

Constants representing the lock rank of the architecture-independent locks in the runtime. Locks with lower rank must be taken before locks with higher rank. 

Memory-related leaf locks 

### <a id="lockRankGFree" href="#lockRankGFree">const lockRankGFree</a>

```
searchKey: runtime.lockRankGFree
```

```Go
const lockRankGFree
```

Constants representing the lock rank of the architecture-independent locks in the runtime. Locks with lower rank must be taken before locks with higher rank. 

Other leaf locks 

### <a id="lockRankHchanLeaf" href="#lockRankHchanLeaf">const lockRankHchanLeaf</a>

```
searchKey: runtime.lockRankHchanLeaf
```

```Go
const lockRankHchanLeaf
```

Constants representing the lock rank of the architecture-independent locks in the runtime. Locks with lower rank must be taken before locks with higher rank. 

Generally, hchan must be acquired before gscan. But in one specific case (in syncadjustsudogs from markroot after the g has been suspended by suspendG), we allow gscan to be acquired, and then an hchan lock. To allow this case, we get this lockRankHchanLeaf rank in syncadjustsudogs(), rather than lockRankHchan. By using this special rank, we don't allow any further locks to be acquired other than more hchan locks. 

### <a id="lockRankPanic" href="#lockRankPanic">const lockRankPanic</a>

```
searchKey: runtime.lockRankPanic
```

```Go
const lockRankPanic
```

Constants representing the lock rank of the architecture-independent locks in the runtime. Locks with lower rank must be taken before locks with higher rank. 

### <a id="lockRankNewmHandoff" href="#lockRankNewmHandoff">const lockRankNewmHandoff</a>

```
searchKey: runtime.lockRankNewmHandoff
```

```Go
const lockRankNewmHandoff
```

Constants representing the lock rank of the architecture-independent locks in the runtime. Locks with lower rank must be taken before locks with higher rank. 

Leaf locks with no dependencies, so these constants are not actually used anywhere. There are other architecture-dependent leaf locks as well. 

### <a id="lockRankDebugPtrmask" href="#lockRankDebugPtrmask">const lockRankDebugPtrmask</a>

```
searchKey: runtime.lockRankDebugPtrmask
```

```Go
const lockRankDebugPtrmask
```

Constants representing the lock rank of the architecture-independent locks in the runtime. Locks with lower rank must be taken before locks with higher rank. 

### <a id="lockRankFaketimeState" href="#lockRankFaketimeState">const lockRankFaketimeState</a>

```
searchKey: runtime.lockRankFaketimeState
```

```Go
const lockRankFaketimeState
```

Constants representing the lock rank of the architecture-independent locks in the runtime. Locks with lower rank must be taken before locks with higher rank. 

### <a id="lockRankTicks" href="#lockRankTicks">const lockRankTicks</a>

```
searchKey: runtime.lockRankTicks
```

```Go
const lockRankTicks
```

Constants representing the lock rank of the architecture-independent locks in the runtime. Locks with lower rank must be taken before locks with higher rank. 

### <a id="lockRankRaceFini" href="#lockRankRaceFini">const lockRankRaceFini</a>

```
searchKey: runtime.lockRankRaceFini
```

```Go
const lockRankRaceFini
```

Constants representing the lock rank of the architecture-independent locks in the runtime. Locks with lower rank must be taken before locks with higher rank. 

### <a id="lockRankPollCache" href="#lockRankPollCache">const lockRankPollCache</a>

```
searchKey: runtime.lockRankPollCache
```

```Go
const lockRankPollCache
```

Constants representing the lock rank of the architecture-independent locks in the runtime. Locks with lower rank must be taken before locks with higher rank. 

### <a id="lockRankDebug" href="#lockRankDebug">const lockRankDebug</a>

```
searchKey: runtime.lockRankDebug
```

```Go
const lockRankDebug
```

Constants representing the lock rank of the architecture-independent locks in the runtime. Locks with lower rank must be taken before locks with higher rank. 

### <a id="lockRankLeafRank" href="#lockRankLeafRank">const lockRankLeafRank</a>

```
searchKey: runtime.lockRankLeafRank
```

```Go
const lockRankLeafRank lockRank = 1000
```

lockRankLeafRank is the rank of lock that does not have a declared rank, and hence is a leaf lock. 

### <a id="debugMalloc" href="#debugMalloc">const debugMalloc</a>

```
searchKey: runtime.debugMalloc
```

```Go
const debugMalloc = false
```

### <a id="maxTinySize" href="#maxTinySize">const maxTinySize</a>

```
searchKey: runtime.maxTinySize
```

```Go
const maxTinySize = _TinySize
```

### <a id="tinySizeClass" href="#tinySizeClass">const tinySizeClass</a>

```
searchKey: runtime.tinySizeClass
```

```Go
const tinySizeClass = _TinySizeClass
```

### <a id="maxSmallSize" href="#maxSmallSize">const maxSmallSize</a>

```
searchKey: runtime.maxSmallSize
```

```Go
const maxSmallSize = _MaxSmallSize
```

### <a id="pageShift" href="#pageShift">const pageShift</a>

```
searchKey: runtime.pageShift
```

```Go
const pageShift = _PageShift
```

### <a id="pageSize" href="#pageSize">const pageSize</a>

```
searchKey: runtime.pageSize
```

```Go
const pageSize = _PageSize
```

### <a id="pageMask" href="#pageMask">const pageMask</a>

```
searchKey: runtime.pageMask
```

```Go
const pageMask = _PageMask
```

### <a id="maxObjsPerSpan" href="#maxObjsPerSpan">const maxObjsPerSpan</a>

```
searchKey: runtime.maxObjsPerSpan
```

```Go
const maxObjsPerSpan = pageSize / 8
```

By construction, single page spans of the smallest object class have the most objects per span. 

### <a id="concurrentSweep" href="#concurrentSweep">const concurrentSweep</a>

```
searchKey: runtime.concurrentSweep
```

```Go
const concurrentSweep = _ConcurrentSweep
```

### <a id="_PageSize" href="#_PageSize">const _PageSize</a>

```
searchKey: runtime._PageSize
```

```Go
const _PageSize = 1 << _PageShift
```

### <a id="_PageMask" href="#_PageMask">const _PageMask</a>

```
searchKey: runtime._PageMask
```

```Go
const _PageMask = _PageSize - 1
```

### <a id="_64bit" href="#_64bit">const _64bit</a>

```
searchKey: runtime._64bit
```

```Go
const _64bit = 1 << (^uintptr(0) >> 63) / 2
```

_64bit = 1 on 64-bit systems, 0 on 32-bit systems 

### <a id="_TinySize" href="#_TinySize">const _TinySize</a>

```
searchKey: runtime._TinySize
```

```Go
const _TinySize = 16
```

Tiny allocator parameters, see "Tiny allocator" comment in malloc.go. 

### <a id="_TinySizeClass" href="#_TinySizeClass">const _TinySizeClass</a>

```
searchKey: runtime._TinySizeClass
```

```Go
const _TinySizeClass = int8(2)
```

### <a id="_FixAllocChunk" href="#_FixAllocChunk">const _FixAllocChunk</a>

```
searchKey: runtime._FixAllocChunk
```

```Go
const _FixAllocChunk = 16 << 10 // Chunk size for FixAlloc

```

### <a id="_StackCacheSize" href="#_StackCacheSize">const _StackCacheSize</a>

```
searchKey: runtime._StackCacheSize
```

```Go
const _StackCacheSize = 32 * 1024
```

Per-P, per order stack segment cache size. 

### <a id="_NumStackOrders" href="#_NumStackOrders">const _NumStackOrders</a>

```
searchKey: runtime._NumStackOrders
```

```Go
const _NumStackOrders = 4 - sys.PtrSize/4*sys.GoosWindows - 1*sys.GoosPlan9
```

Number of orders that get caching. Order 0 is FixedStack and each successive order is twice as large. We want to cache 2KB, 4KB, 8KB, and 16KB stacks. Larger stacks will be allocated directly. Since FixedStack is different on different systems, we must vary NumStackOrders to keep the same maximum cached size. 

```
OS               | FixedStack | NumStackOrders
-----------------+------------+---------------
linux/darwin/bsd | 2KB        | 4
windows/32       | 4KB        | 3
windows/64       | 8KB        | 2
plan9            | 4KB        | 3

```
### <a id="heapAddrBits" href="#heapAddrBits">const heapAddrBits</a>

```
searchKey: runtime.heapAddrBits
```

```Go
const heapAddrBits = ...
```

heapAddrBits is the number of bits in a heap address. On amd64, addresses are sign-extended beyond heapAddrBits. On other arches, they are zero-extended. 

On most 64-bit platforms, we limit this to 48 bits based on a combination of hardware and OS limitations. 

amd64 hardware limits addresses to 48 bits, sign-extended to 64 bits. Addresses where the top 16 bits are not either all 0 or all 1 are "non-canonical" and invalid. Because of these "negative" addresses, we offset addresses by 1<<47 (arenaBaseOffset) on amd64 before computing indexes into the heap arenas index. In 2017, amd64 hardware added support for 57 bit addresses; however, currently only Linux supports this extension and the kernel will never choose an address above 1<<47 unless mmap is called with a hint address above 1<<47 (which we never do). 

arm64 hardware (as of ARMv8) limits user addresses to 48 bits, in the range [0, 1<<48). 

ppc64, mips64, and s390x support arbitrary 64 bit addresses in hardware. On Linux, Go leans on stricter OS limits. Based on Linux's processor.h, the user address space is limited as follows on 64-bit architectures: 

Architecture  Name              Maximum Value (exclusive) --------------------------------------------------------------------- amd64         TASK_SIZE_MAX     0x007ffffffff000 (47 bit addresses) arm64         TASK_SIZE_64      0x01000000000000 (48 bit addresses) ppc64{,le}    TASK_SIZE_USER64  0x00400000000000 (46 bit addresses) mips64{,le}   TASK_SIZE64       0x00010000000000 (40 bit addresses) s390x         TASK_SIZE         1<<64 (64 bit addresses) 

These limits may increase over time, but are currently at most 48 bits except on s390x. On all architectures, Linux starts placing mmap'd regions at addresses that are significantly below 48 bits, so even if it's possible to exceed Go's 48 bit limit, it's extremely unlikely in practice. 

On 32-bit platforms, we accept the full 32-bit address space because doing so is cheap. mips32 only has access to the low 2GB of virtual memory, so we further limit it to 31 bits. 

On ios/arm64, although 64-bit pointers are presumably available, pointers are truncated to 33 bits. Furthermore, only the top 4 GiB of the address space are actually available to the application, but we allow the whole 33 bits anyway for simplicity. TODO(mknyszek): Consider limiting it to 32 bits and using arenaBaseOffset to offset into the top 4 GiB. 

WebAssembly currently has a limit of 4GB linear memory. 

### <a id="maxAlloc" href="#maxAlloc">const maxAlloc</a>

```
searchKey: runtime.maxAlloc
```

```Go
const maxAlloc = (1 << heapAddrBits) - (1-_64bit)*1
```

maxAlloc is the maximum size of an allocation. On 64-bit, it's theoretically possible to allocate 1<<heapAddrBits bytes. On 32-bit, however, this is one less than 1<<32 because the number of bytes in the address space doesn't actually fit in a uintptr. 

### <a id="heapArenaBytes" href="#heapArenaBytes">const heapArenaBytes</a>

```
searchKey: runtime.heapArenaBytes
```

```Go
const heapArenaBytes = 1 << logHeapArenaBytes
```

heapArenaBytes is the size of a heap arena. The heap consists of mappings of size heapArenaBytes, aligned to heapArenaBytes. The initial heap mapping is one arena. 

This is currently 64MB on 64-bit non-Windows and 4MB on 32-bit and on Windows. We use smaller arenas on Windows because all committed memory is charged to the process, even if it's not touched. Hence, for processes with small heaps, the mapped arena space needs to be commensurate. This is particularly important with the race detector, since it significantly amplifies the cost of committed memory. 

### <a id="logHeapArenaBytes" href="#logHeapArenaBytes">const logHeapArenaBytes</a>

```
searchKey: runtime.logHeapArenaBytes
```

```Go
const logHeapArenaBytes = ...
```

logHeapArenaBytes is log_2 of heapArenaBytes. For clarity, prefer using heapArenaBytes where possible (we need the constant to compute some other constants). 

### <a id="heapArenaBitmapBytes" href="#heapArenaBitmapBytes">const heapArenaBitmapBytes</a>

```
searchKey: runtime.heapArenaBitmapBytes
```

```Go
const heapArenaBitmapBytes = heapArenaBytes / (sys.PtrSize * 8 / 2)
```

heapArenaBitmapBytes is the size of each heap arena's bitmap. 

### <a id="pagesPerArena" href="#pagesPerArena">const pagesPerArena</a>

```
searchKey: runtime.pagesPerArena
```

```Go
const pagesPerArena = heapArenaBytes / pageSize
```

### <a id="arenaL1Bits" href="#arenaL1Bits">const arenaL1Bits</a>

```
searchKey: runtime.arenaL1Bits
```

```Go
const arenaL1Bits = 6 * (_64bit * sys.GoosWindows)
```

arenaL1Bits is the number of bits of the arena number covered by the first level arena map. 

This number should be small, since the first level arena map requires PtrSize*(1<<arenaL1Bits) of space in the binary's BSS. It can be zero, in which case the first level index is effectively unused. There is a performance benefit to this, since the generated code can be more efficient, but comes at the cost of having a large L2 mapping. 

We use the L1 map on 64-bit Windows because the arena size is small, but the address space is still 48 bits, and there's a high cost to having a large L2. 

### <a id="arenaL2Bits" href="#arenaL2Bits">const arenaL2Bits</a>

```
searchKey: runtime.arenaL2Bits
```

```Go
const arenaL2Bits = heapAddrBits - logHeapArenaBytes - arenaL1Bits
```

arenaL2Bits is the number of bits of the arena number covered by the second level arena index. 

The size of each arena map allocation is proportional to 1<<arenaL2Bits, so it's important that this not be too large. 48 bits leads to 32MB arena index allocations, which is about the practical threshold. 

### <a id="arenaL1Shift" href="#arenaL1Shift">const arenaL1Shift</a>

```
searchKey: runtime.arenaL1Shift
```

```Go
const arenaL1Shift = arenaL2Bits
```

arenaL1Shift is the number of bits to shift an arena frame number by to compute an index into the first level arena map. 

### <a id="arenaBits" href="#arenaBits">const arenaBits</a>

```
searchKey: runtime.arenaBits
```

```Go
const arenaBits = arenaL1Bits + arenaL2Bits
```

arenaBits is the total bits in a combined arena map index. This is split between the index into the L1 arena map and the L2 arena map. 

### <a id="arenaBaseOffset" href="#arenaBaseOffset">const arenaBaseOffset</a>

```
searchKey: runtime.arenaBaseOffset
```

```Go
const arenaBaseOffset = 0xffff800000000000*sys.GoarchAmd64 + 0x0a00000000000000*sys.GoosAix
```

arenaBaseOffset is the pointer value that corresponds to index 0 in the heap arena map. 

On amd64, the address space is 48 bits, sign extended to 64 bits. This offset lets us handle "negative" addresses (or high addresses if viewed as unsigned). 

On aix/ppc64, this offset allows to keep the heapAddrBits to 48. Otherwise, it would be 60 in order to handle mmap addresses (in range 0x0a00000000000000 - 0x0afffffffffffff). But in this case, the memory reserved in (s *pageAlloc).init for chunks is causing important slowdowns. 

On other platforms, the user address space is contiguous and starts at 0, so no offset is necessary. 

### <a id="arenaBaseOffsetUintptr" href="#arenaBaseOffsetUintptr">const arenaBaseOffsetUintptr</a>

```
searchKey: runtime.arenaBaseOffsetUintptr
```

```Go
const arenaBaseOffsetUintptr = uintptr(arenaBaseOffset)
```

A typed version of this constant that will make it into DWARF (for viewcore). 

### <a id="_MaxGcproc" href="#_MaxGcproc">const _MaxGcproc</a>

```
searchKey: runtime._MaxGcproc
```

```Go
const _MaxGcproc = 32
```

Max number of threads to run garbage collection. 2, 3, and 4 are all plausible maximums depending on the hardware details of the machine. The garbage collector scales well to 32 cpus. 

### <a id="minLegalPointer" href="#minLegalPointer">const minLegalPointer</a>

```
searchKey: runtime.minLegalPointer
```

```Go
const minLegalPointer uintptr = 4096
```

minLegalPointer is the smallest possible legal pointer. This is the smallest possible architectural page size, since we assume that the first page is never mapped. 

This should agree with minZeroPage in the compiler. 

### <a id="persistentChunkSize" href="#persistentChunkSize">const persistentChunkSize</a>

```
searchKey: runtime.persistentChunkSize
```

```Go
const persistentChunkSize = 256 << 10
```

persistentChunkSize is the number of bytes we allocate when we grow a persistentAlloc. 

### <a id="bucketCntBits" href="#bucketCntBits">const bucketCntBits</a>

```
searchKey: runtime.bucketCntBits
```

```Go
const bucketCntBits = 3
```

Maximum number of key/elem pairs a bucket can hold. 

### <a id="bucketCnt" href="#bucketCnt">const bucketCnt</a>

```
searchKey: runtime.bucketCnt
```

```Go
const bucketCnt = 1 << bucketCntBits
```

### <a id="loadFactorNum" href="#loadFactorNum">const loadFactorNum</a>

```
searchKey: runtime.loadFactorNum
```

```Go
const loadFactorNum = 13
```

Maximum average load of a bucket that triggers growth is 6.5. Represent as loadFactorNum/loadFactorDen, to allow integer math. 

### <a id="loadFactorDen" href="#loadFactorDen">const loadFactorDen</a>

```
searchKey: runtime.loadFactorDen
```

```Go
const loadFactorDen = 2
```

### <a id="maxKeySize" href="#maxKeySize">const maxKeySize</a>

```
searchKey: runtime.maxKeySize
```

```Go
const maxKeySize = 128
```

Maximum key or elem size to keep inline (instead of mallocing per element). Must fit in a uint8. Fast versions cannot handle big elems - the cutoff size for fast versions in cmd/compile/internal/gc/walk.go must be at most this elem. 

### <a id="maxElemSize" href="#maxElemSize">const maxElemSize</a>

```
searchKey: runtime.maxElemSize
```

```Go
const maxElemSize = 128
```

### <a id="dataOffset" href="#dataOffset">const dataOffset</a>

```
searchKey: runtime.dataOffset
```

```Go
const dataOffset = unsafe.Offsetof(struct {
	b bmap
	v int64
}{}.v)
```

data offset should be the size of the bmap struct, but needs to be aligned correctly. For amd64p32 this means 64-bit alignment even though pointers are 32 bit. 

### <a id="emptyRest" href="#emptyRest">const emptyRest</a>

```
searchKey: runtime.emptyRest
```

```Go
const emptyRest // this cell is empty, and there are no more non-empty cells at higher indexes or overflows.
 = ...
```

Possible tophash values. We reserve a few possibilities for special marks. Each bucket (including its overflow buckets, if any) will have either all or none of its entries in the evacuated* states (except during the evacuate() method, which only happens during map writes and thus no one else can observe the map during that time). 

### <a id="emptyOne" href="#emptyOne">const emptyOne</a>

```
searchKey: runtime.emptyOne
```

```Go
const emptyOne = 1 // this cell is empty

```

### <a id="evacuatedX" href="#evacuatedX">const evacuatedX</a>

```
searchKey: runtime.evacuatedX
```

```Go
const evacuatedX = 2 // key/elem is valid.  Entry has been evacuated to first half of larger table.

```

### <a id="evacuatedY" href="#evacuatedY">const evacuatedY</a>

```
searchKey: runtime.evacuatedY
```

```Go
const evacuatedY = 3 // same as above, but evacuated to second half of larger table.

```

### <a id="evacuatedEmpty" href="#evacuatedEmpty">const evacuatedEmpty</a>

```
searchKey: runtime.evacuatedEmpty
```

```Go
const evacuatedEmpty = 4 // cell is empty, bucket is evacuated.

```

### <a id="minTopHash" href="#minTopHash">const minTopHash</a>

```
searchKey: runtime.minTopHash
```

```Go
const minTopHash = 5 // minimum tophash for a normal filled cell.

```

### <a id="iterator" href="#iterator">const iterator</a>

```
searchKey: runtime.iterator
```

```Go
const iterator = 1 // there may be an iterator using buckets

```

flags 

### <a id="oldIterator" href="#oldIterator">const oldIterator</a>

```
searchKey: runtime.oldIterator
```

```Go
const oldIterator = 2 // there may be an iterator using oldbuckets

```

### <a id="hashWriting" href="#hashWriting">const hashWriting</a>

```
searchKey: runtime.hashWriting
```

```Go
const hashWriting = 4 // a goroutine is writing to the map

```

### <a id="sameSizeGrow" href="#sameSizeGrow">const sameSizeGrow</a>

```
searchKey: runtime.sameSizeGrow
```

```Go
const sameSizeGrow = 8 // the current map growth is to a new map of the same size

```

### <a id="noCheck" href="#noCheck">const noCheck</a>

```
searchKey: runtime.noCheck
```

```Go
const noCheck = 1<<(8*sys.PtrSize) - 1
```

sentinel bucket ID for iterator checks 

### <a id="maxZero" href="#maxZero">const maxZero</a>

```
searchKey: runtime.maxZero
```

```Go
const maxZero // must match value in reflect/value.go:maxZero cmd/compile/internal/gc/walk.go:zeroValSize
 = ...
```

### <a id="bitPointer" href="#bitPointer">const bitPointer</a>

```
searchKey: runtime.bitPointer
```

```Go
const bitPointer = 1 << 0
```

### <a id="bitScan" href="#bitScan">const bitScan</a>

```
searchKey: runtime.bitScan
```

```Go
const bitScan = 1 << 4
```

### <a id="heapBitsShift" href="#heapBitsShift">const heapBitsShift</a>

```
searchKey: runtime.heapBitsShift
```

```Go
const heapBitsShift = 1 // shift offset between successive bitPointer or bitScan entries

```

### <a id="wordsPerBitmapByte" href="#wordsPerBitmapByte">const wordsPerBitmapByte</a>

```
searchKey: runtime.wordsPerBitmapByte
```

```Go
const wordsPerBitmapByte = 8 / 2 // heap words described by one bitmap byte

```

### <a id="bitScanAll" href="#bitScanAll">const bitScanAll</a>

```
searchKey: runtime.bitScanAll
```

```Go
const bitScanAll = ...
```

all scan/pointer bits in a byte 

### <a id="bitPointerAll" href="#bitPointerAll">const bitPointerAll</a>

```
searchKey: runtime.bitPointerAll
```

```Go
const bitPointerAll = ...
```

### <a id="clobberdeadPtr" href="#clobberdeadPtr">const clobberdeadPtr</a>

```
searchKey: runtime.clobberdeadPtr
```

```Go
const clobberdeadPtr = uintptr(0xdeaddead | 0xdeaddead<<((^uintptr(0)>>63)*32))
```

clobberdeadPtr is a special value that is used by the compiler to clobber dead stack slots, when -clobberdead flag is set. 

### <a id="_ENOMEM" href="#_ENOMEM">const _ENOMEM</a>

```
searchKey: runtime._ENOMEM
```

```Go
const _ENOMEM = 12
```

### <a id="heapStatsDep" href="#heapStatsDep">const heapStatsDep</a>

```
searchKey: runtime.heapStatsDep
```

```Go
const heapStatsDep statDep = iota // corresponds to heapStatsAggregate

```

### <a id="sysStatsDep" href="#sysStatsDep">const sysStatsDep</a>

```
searchKey: runtime.sysStatsDep
```

```Go
const sysStatsDep // corresponds to sysStatsAggregate

```

### <a id="numStatsDeps" href="#numStatsDeps">const numStatsDeps</a>

```
searchKey: runtime.numStatsDeps
```

```Go
const numStatsDeps
```

### <a id="metricKindBad" href="#metricKindBad">const metricKindBad</a>

```
searchKey: runtime.metricKindBad
```

```Go
const metricKindBad metricKind = iota
```

These values must be kept identical to their corresponding Kind* values in the runtime/metrics package. 

### <a id="metricKindUint64" href="#metricKindUint64">const metricKindUint64</a>

```
searchKey: runtime.metricKindUint64
```

```Go
const metricKindUint64
```

### <a id="metricKindFloat64" href="#metricKindFloat64">const metricKindFloat64</a>

```
searchKey: runtime.metricKindFloat64
```

```Go
const metricKindFloat64
```

### <a id="metricKindFloat64Histogram" href="#metricKindFloat64Histogram">const metricKindFloat64Histogram</a>

```
searchKey: runtime.metricKindFloat64Histogram
```

```Go
const metricKindFloat64Histogram
```

### <a id="_DebugGC" href="#_DebugGC">const _DebugGC</a>

```
searchKey: runtime._DebugGC
```

```Go
const _DebugGC = 0
```

### <a id="_ConcurrentSweep" href="#_ConcurrentSweep">const _ConcurrentSweep</a>

```
searchKey: runtime._ConcurrentSweep
```

```Go
const _ConcurrentSweep = true
```

### <a id="_FinBlockSize" href="#_FinBlockSize">const _FinBlockSize</a>

```
searchKey: runtime._FinBlockSize
```

```Go
const _FinBlockSize = 4 * 1024
```

### <a id="debugScanConservative" href="#debugScanConservative">const debugScanConservative</a>

```
searchKey: runtime.debugScanConservative
```

```Go
const debugScanConservative = false
```

debugScanConservative enables debug logging for stack frames that are scanned conservatively. 

### <a id="sweepMinHeapDistance" href="#sweepMinHeapDistance">const sweepMinHeapDistance</a>

```
searchKey: runtime.sweepMinHeapDistance
```

```Go
const sweepMinHeapDistance = 1024 * 1024
```

sweepMinHeapDistance is a lower bound on the heap distance (in bytes) reserved for concurrent sweeping between GC cycles. 

### <a id="_GCoff" href="#_GCoff">const _GCoff</a>

```
searchKey: runtime._GCoff
```

```Go
const _GCoff = iota // GC not running; sweeping in background, write barrier disabled

```

### <a id="_GCmark" href="#_GCmark">const _GCmark</a>

```
searchKey: runtime._GCmark
```

```Go
const _GCmark // GC marking roots and workbufs: allocate black, write barrier ENABLED

```

### <a id="_GCmarktermination" href="#_GCmarktermination">const _GCmarktermination</a>

```
searchKey: runtime._GCmarktermination
```

```Go
const _GCmarktermination // GC mark termination: allocate black, P's help GC, write barrier ENABLED

```

### <a id="gcMarkWorkerNotWorker" href="#gcMarkWorkerNotWorker">const gcMarkWorkerNotWorker</a>

```
searchKey: runtime.gcMarkWorkerNotWorker
```

```Go
const gcMarkWorkerNotWorker gcMarkWorkerMode = iota
```

gcMarkWorkerNotWorker indicates that the next scheduled G is not starting work and the mode should be ignored. 

### <a id="gcMarkWorkerDedicatedMode" href="#gcMarkWorkerDedicatedMode">const gcMarkWorkerDedicatedMode</a>

```
searchKey: runtime.gcMarkWorkerDedicatedMode
```

```Go
const gcMarkWorkerDedicatedMode
```

gcMarkWorkerDedicatedMode indicates that the P of a mark worker is dedicated to running that mark worker. The mark worker should run without preemption. 

### <a id="gcMarkWorkerFractionalMode" href="#gcMarkWorkerFractionalMode">const gcMarkWorkerFractionalMode</a>

```
searchKey: runtime.gcMarkWorkerFractionalMode
```

```Go
const gcMarkWorkerFractionalMode
```

gcMarkWorkerFractionalMode indicates that a P is currently running the "fractional" mark worker. The fractional worker is necessary when GOMAXPROCS*gcBackgroundUtilization is not an integer and using only dedicated workers would result in utilization too far from the target of gcBackgroundUtilization. The fractional worker should run until it is preempted and will be scheduled to pick up the fractional part of GOMAXPROCS*gcBackgroundUtilization. 

### <a id="gcMarkWorkerIdleMode" href="#gcMarkWorkerIdleMode">const gcMarkWorkerIdleMode</a>

```
searchKey: runtime.gcMarkWorkerIdleMode
```

```Go
const gcMarkWorkerIdleMode
```

gcMarkWorkerIdleMode indicates that a P is running the mark worker because it has nothing else to do. The idle worker should run until it is preempted and account its time against gcController.idleMarkTime. 

### <a id="gcBackgroundMode" href="#gcBackgroundMode">const gcBackgroundMode</a>

```
searchKey: runtime.gcBackgroundMode
```

```Go
const gcBackgroundMode gcMode = iota // concurrent GC and sweep

```

### <a id="gcForceMode" href="#gcForceMode">const gcForceMode</a>

```
searchKey: runtime.gcForceMode
```

```Go
const gcForceMode // stop-the-world GC now, concurrent sweep

```

### <a id="gcForceBlockMode" href="#gcForceBlockMode">const gcForceBlockMode</a>

```
searchKey: runtime.gcForceBlockMode
```

```Go
const gcForceBlockMode // stop-the-world GC now and STW sweep (forced by user)

```

### <a id="gcTriggerHeap" href="#gcTriggerHeap">const gcTriggerHeap</a>

```
searchKey: runtime.gcTriggerHeap
```

```Go
const gcTriggerHeap gcTriggerKind = iota
```

gcTriggerHeap indicates that a cycle should be started when the heap size reaches the trigger heap size computed by the controller. 

### <a id="gcTriggerTime" href="#gcTriggerTime">const gcTriggerTime</a>

```
searchKey: runtime.gcTriggerTime
```

```Go
const gcTriggerTime
```

gcTriggerTime indicates that a cycle should be started when it's been more than forcegcperiod nanoseconds since the previous GC cycle. 

### <a id="gcTriggerCycle" href="#gcTriggerCycle">const gcTriggerCycle</a>

```
searchKey: runtime.gcTriggerCycle
```

```Go
const gcTriggerCycle
```

gcTriggerCycle indicates that a cycle should be started if we have not yet started cycle number gcTrigger.n (relative to work.cycles). 

### <a id="fixedRootFinalizers" href="#fixedRootFinalizers">const fixedRootFinalizers</a>

```
searchKey: runtime.fixedRootFinalizers
```

```Go
const fixedRootFinalizers = iota
```

### <a id="fixedRootFreeGStacks" href="#fixedRootFreeGStacks">const fixedRootFreeGStacks</a>

```
searchKey: runtime.fixedRootFreeGStacks
```

```Go
const fixedRootFreeGStacks
```

### <a id="fixedRootCount" href="#fixedRootCount">const fixedRootCount</a>

```
searchKey: runtime.fixedRootCount
```

```Go
const fixedRootCount
```

### <a id="rootBlockBytes" href="#rootBlockBytes">const rootBlockBytes</a>

```
searchKey: runtime.rootBlockBytes
```

```Go
const rootBlockBytes = 256 << 10
```

rootBlockBytes is the number of bytes to scan per data or BSS root. 

### <a id="maxObletBytes" href="#maxObletBytes">const maxObletBytes</a>

```
searchKey: runtime.maxObletBytes
```

```Go
const maxObletBytes = 128 << 10
```

maxObletBytes is the maximum bytes of an object to scan at once. Larger objects will be split up into "oblets" of at most this size. Since we can scan 1–2 MB/ms, 128 KB bounds scan preemption at ~100 µs. 

This must be > _MaxSmallSize so that the object base is the span base. 

### <a id="drainCheckThreshold" href="#drainCheckThreshold">const drainCheckThreshold</a>

```
searchKey: runtime.drainCheckThreshold
```

```Go
const drainCheckThreshold = 100000
```

drainCheckThreshold specifies how many units of work to do between self-preemption checks in gcDrain. Assuming a scan rate of 1 MB/ms, this is ~100 µs. Lower values have higher overhead in the scan loop (the scheduler check may perform a syscall, so its overhead is nontrivial). Higher values make the system less responsive to incoming work. 

### <a id="pagesPerSpanRoot" href="#pagesPerSpanRoot">const pagesPerSpanRoot</a>

```
searchKey: runtime.pagesPerSpanRoot
```

```Go
const pagesPerSpanRoot = 512
```

pagesPerSpanRoot indicates how many pages to scan from a span root at a time. Used by special root marking. 

Higher values improve throughput by increasing locality, but increase the minimum latency of a marking operation. 

Must be a multiple of the pageInUse bitmap element size and must also evenly divide pagesPerArena. 

### <a id="gcDrainUntilPreempt" href="#gcDrainUntilPreempt">const gcDrainUntilPreempt</a>

```
searchKey: runtime.gcDrainUntilPreempt
```

```Go
const gcDrainUntilPreempt gcDrainFlags = 1 << iota
```

### <a id="gcDrainFlushBgCredit" href="#gcDrainFlushBgCredit">const gcDrainFlushBgCredit</a>

```
searchKey: runtime.gcDrainFlushBgCredit
```

```Go
const gcDrainFlushBgCredit
```

### <a id="gcDrainIdle" href="#gcDrainIdle">const gcDrainIdle</a>

```
searchKey: runtime.gcDrainIdle
```

```Go
const gcDrainIdle
```

### <a id="gcDrainFractional" href="#gcDrainFractional">const gcDrainFractional</a>

```
searchKey: runtime.gcDrainFractional
```

```Go
const gcDrainFractional
```

### <a id="gcGoalUtilization" href="#gcGoalUtilization">const gcGoalUtilization</a>

```
searchKey: runtime.gcGoalUtilization
```

```Go
const gcGoalUtilization = 0.30
```

gcGoalUtilization is the goal CPU utilization for marking as a fraction of GOMAXPROCS. 

### <a id="gcBackgroundUtilization" href="#gcBackgroundUtilization">const gcBackgroundUtilization</a>

```
searchKey: runtime.gcBackgroundUtilization
```

```Go
const gcBackgroundUtilization = 0.25
```

gcBackgroundUtilization is the fixed CPU utilization for background marking. It must be <= gcGoalUtilization. The difference between gcGoalUtilization and gcBackgroundUtilization will be made up by mark assists. The scheduler will aim to use within 50% of this goal. 

Setting this to < gcGoalUtilization avoids saturating the trigger feedback controller when there are no assists, which allows it to better control CPU and heap growth. However, the larger the gap, the more mutator assists are expected to happen, which impact mutator latency. 

### <a id="gcCreditSlack" href="#gcCreditSlack">const gcCreditSlack</a>

```
searchKey: runtime.gcCreditSlack
```

```Go
const gcCreditSlack = 2000
```

gcCreditSlack is the amount of scan work credit that can accumulate locally before updating gcController.scanWork and, optionally, gcController.bgScanCredit. Lower values give a more accurate assist ratio and make it more likely that assists will successfully steal background credit. Higher values reduce memory contention. 

### <a id="gcAssistTimeSlack" href="#gcAssistTimeSlack">const gcAssistTimeSlack</a>

```
searchKey: runtime.gcAssistTimeSlack
```

```Go
const gcAssistTimeSlack = 5000
```

gcAssistTimeSlack is the nanoseconds of mutator assist time that can accumulate on a P before updating gcController.assistTime. 

### <a id="gcOverAssistWork" href="#gcOverAssistWork">const gcOverAssistWork</a>

```
searchKey: runtime.gcOverAssistWork
```

```Go
const gcOverAssistWork = 64 << 10
```

gcOverAssistWork determines how many extra units of scan work a GC assist does when an assist happens. This amortizes the cost of an assist by pre-paying for this many bytes of future allocations. 

### <a id="defaultHeapMinimum" href="#defaultHeapMinimum">const defaultHeapMinimum</a>

```
searchKey: runtime.defaultHeapMinimum
```

```Go
const defaultHeapMinimum = 4 << 20
```

defaultHeapMinimum is the value of heapMinimum for GOGC==100. 

### <a id="scavengePercent" href="#scavengePercent">const scavengePercent</a>

```
searchKey: runtime.scavengePercent
```

```Go
const scavengePercent = 1 // 1%

```

The background scavenger is paced according to these parameters. 

scavengePercent represents the portion of mutator time we're willing to spend on scavenging in percent. 

### <a id="retainExtraPercent" href="#retainExtraPercent">const retainExtraPercent</a>

```
searchKey: runtime.retainExtraPercent
```

```Go
const retainExtraPercent = 10
```

retainExtraPercent represents the amount of memory over the heap goal that the scavenger should keep as a buffer space for the allocator. 

The purpose of maintaining this overhead is to have a greater pool of unscavenged memory available for allocation (since using scavenged memory incurs an additional cost), to account for heap fragmentation and the ever-changing layout of the heap. 

### <a id="maxPagesPerPhysPage" href="#maxPagesPerPhysPage">const maxPagesPerPhysPage</a>

```
searchKey: runtime.maxPagesPerPhysPage
```

```Go
const maxPagesPerPhysPage = maxPhysPageSize / pageSize
```

maxPagesPerPhysPage is the maximum number of supported runtime pages per physical page, based on maxPhysPageSize. 

### <a id="scavengeCostRatio" href="#scavengeCostRatio">const scavengeCostRatio</a>

```
searchKey: runtime.scavengeCostRatio
```

```Go
const scavengeCostRatio = 0.7 * (sys.GoosDarwin + sys.GoosIos)
```

scavengeCostRatio is the approximate ratio between the costs of using previously scavenged memory and scavenging memory. 

For most systems the cost of scavenging greatly outweighs the costs associated with using scavenged memory, making this constant 0. On other systems (especially ones where "sysUsed" is not just a no-op) this cost is non-trivial. 

This ratio is used as part of multiplicative factor to help the scavenger account for the additional costs of using scavenged memory in its pacing. 

### <a id="scavengeReservationShards" href="#scavengeReservationShards">const scavengeReservationShards</a>

```
searchKey: runtime.scavengeReservationShards
```

```Go
const scavengeReservationShards = 64
```

scavengeReservationShards determines the amount of memory the scavenger should reserve for scavenging at a time. Specifically, the amount of memory reserved is (heap size in bytes) / scavengeReservationShards. 

### <a id="stackTraceDebug" href="#stackTraceDebug">const stackTraceDebug</a>

```
searchKey: runtime.stackTraceDebug
```

```Go
const stackTraceDebug = false
```

### <a id="numSweepClasses" href="#numSweepClasses">const numSweepClasses</a>

```
searchKey: runtime.numSweepClasses
```

```Go
const numSweepClasses = numSpanClasses * 2
```

### <a id="sweepClassDone" href="#sweepClassDone">const sweepClassDone</a>

```
searchKey: runtime.sweepClassDone
```

```Go
const sweepClassDone sweepClass = sweepClass(^uint32(0))
```

### <a id="_WorkbufSize" href="#_WorkbufSize">const _WorkbufSize</a>

```
searchKey: runtime._WorkbufSize
```

```Go
const _WorkbufSize = 2048 // in bytes; larger values result in less contention

```

### <a id="workbufAlloc" href="#workbufAlloc">const workbufAlloc</a>

```
searchKey: runtime.workbufAlloc
```

```Go
const workbufAlloc = 32 << 10
```

workbufAlloc is the number of bytes to allocate at a time for new workbufs. This must be a multiple of pageSize and should be a multiple of _WorkbufSize. 

Larger values reduce workbuf allocation overhead. Smaller values reduce heap fragmentation. 

### <a id="minPhysPageSize" href="#minPhysPageSize">const minPhysPageSize</a>

```
searchKey: runtime.minPhysPageSize
```

```Go
const minPhysPageSize = 4096
```

minPhysPageSize is a lower-bound on the physical page size. The true physical page size may be larger than this. In contrast, sys.PhysPageSize is an upper-bound on the physical page size. 

### <a id="maxPhysPageSize" href="#maxPhysPageSize">const maxPhysPageSize</a>

```
searchKey: runtime.maxPhysPageSize
```

```Go
const maxPhysPageSize = 512 << 10
```

maxPhysPageSize is the maximum page size the runtime supports. 

### <a id="maxPhysHugePageSize" href="#maxPhysHugePageSize">const maxPhysHugePageSize</a>

```
searchKey: runtime.maxPhysHugePageSize
```

```Go
const maxPhysHugePageSize = pallocChunkBytes
```

maxPhysHugePageSize sets an upper-bound on the maximum huge page size that the runtime supports. 

### <a id="pagesPerReclaimerChunk" href="#pagesPerReclaimerChunk">const pagesPerReclaimerChunk</a>

```
searchKey: runtime.pagesPerReclaimerChunk
```

```Go
const pagesPerReclaimerChunk = 512
```

pagesPerReclaimerChunk indicates how many pages to scan from the pageInUse bitmap at a time. Used by the page reclaimer. 

Higher values reduce contention on scanning indexes (such as h.reclaimIndex), but increase the minimum latency of the operation. 

The time required to scan this many pages can vary a lot depending on how many spans are actually freed. Experimentally, it can scan for pages at ~300 GB/ms on a 2.6GHz Core i7, but can only free spans at ~32 MB/ms. Using 512 pages bounds this at roughly 100µs. 

Must be a multiple of the pageInUse bitmap element size and must also evenly divide pagesPerArena. 

### <a id="physPageAlignedStacks" href="#physPageAlignedStacks">const physPageAlignedStacks</a>

```
searchKey: runtime.physPageAlignedStacks
```

```Go
const physPageAlignedStacks = GOOS == "openbsd"
```

physPageAlignedStacks indicates whether stack allocations must be physical page aligned. This is a requirement for MAP_STACK on OpenBSD. 

### <a id="mSpanDead" href="#mSpanDead">const mSpanDead</a>

```
searchKey: runtime.mSpanDead
```

```Go
const mSpanDead mSpanState = iota
```

### <a id="mSpanInUse" href="#mSpanInUse">const mSpanInUse</a>

```
searchKey: runtime.mSpanInUse
```

```Go
const mSpanInUse // allocated for garbage collected heap

```

### <a id="mSpanManual" href="#mSpanManual">const mSpanManual</a>

```
searchKey: runtime.mSpanManual
```

```Go
const mSpanManual // allocated for manual management (e.g., stack allocator)

```

### <a id="numSpanClasses" href="#numSpanClasses">const numSpanClasses</a>

```
searchKey: runtime.numSpanClasses
```

```Go
const numSpanClasses = _NumSizeClasses << 1
```

### <a id="tinySpanClass" href="#tinySpanClass">const tinySpanClass</a>

```
searchKey: runtime.tinySpanClass
```

```Go
const tinySpanClass = spanClass(tinySizeClass<<1 | 1)
```

### <a id="spanAllocHeap" href="#spanAllocHeap">const spanAllocHeap</a>

```
searchKey: runtime.spanAllocHeap
```

```Go
const spanAllocHeap spanAllocType = iota // heap span

```

### <a id="spanAllocStack" href="#spanAllocStack">const spanAllocStack</a>

```
searchKey: runtime.spanAllocStack
```

```Go
const spanAllocStack // stack span

```

### <a id="spanAllocPtrScalarBits" href="#spanAllocPtrScalarBits">const spanAllocPtrScalarBits</a>

```
searchKey: runtime.spanAllocPtrScalarBits
```

```Go
const spanAllocPtrScalarBits // unrolled GC prog bitmap span

```

### <a id="spanAllocWorkBuf" href="#spanAllocWorkBuf">const spanAllocWorkBuf</a>

```
searchKey: runtime.spanAllocWorkBuf
```

```Go
const spanAllocWorkBuf // work buf span

```

### <a id="_KindSpecialFinalizer" href="#_KindSpecialFinalizer">const _KindSpecialFinalizer</a>

```
searchKey: runtime._KindSpecialFinalizer
```

```Go
const _KindSpecialFinalizer = 1
```

### <a id="_KindSpecialProfile" href="#_KindSpecialProfile">const _KindSpecialProfile</a>

```
searchKey: runtime._KindSpecialProfile
```

```Go
const _KindSpecialProfile = 2
```

### <a id="_KindSpecialReachable" href="#_KindSpecialReachable">const _KindSpecialReachable</a>

```
searchKey: runtime._KindSpecialReachable
```

```Go
const _KindSpecialReachable = 3
```

_KindSpecialReachable is a special used for tracking reachability during testing. 

### <a id="gcBitsChunkBytes" href="#gcBitsChunkBytes">const gcBitsChunkBytes</a>

```
searchKey: runtime.gcBitsChunkBytes
```

```Go
const gcBitsChunkBytes = uintptr(64 << 10)
```

### <a id="gcBitsHeaderBytes" href="#gcBitsHeaderBytes">const gcBitsHeaderBytes</a>

```
searchKey: runtime.gcBitsHeaderBytes
```

```Go
const gcBitsHeaderBytes = unsafe.Sizeof(gcBitsHeader{})
```

### <a id="pallocChunkPages" href="#pallocChunkPages">const pallocChunkPages</a>

```
searchKey: runtime.pallocChunkPages
```

```Go
const pallocChunkPages = 1 << logPallocChunkPages
```

The size of a bitmap chunk, i.e. the amount of bits (that is, pages) to consider in the bitmap at once. 

### <a id="pallocChunkBytes" href="#pallocChunkBytes">const pallocChunkBytes</a>

```
searchKey: runtime.pallocChunkBytes
```

```Go
const pallocChunkBytes = pallocChunkPages * pageSize
```

### <a id="logPallocChunkPages" href="#logPallocChunkPages">const logPallocChunkPages</a>

```
searchKey: runtime.logPallocChunkPages
```

```Go
const logPallocChunkPages = 9
```

### <a id="logPallocChunkBytes" href="#logPallocChunkBytes">const logPallocChunkBytes</a>

```
searchKey: runtime.logPallocChunkBytes
```

```Go
const logPallocChunkBytes = logPallocChunkPages + pageShift
```

### <a id="summaryLevelBits" href="#summaryLevelBits">const summaryLevelBits</a>

```
searchKey: runtime.summaryLevelBits
```

```Go
const summaryLevelBits = 3
```

The number of radix bits for each level. 

The value of 3 is chosen such that the block of summaries we need to scan at each level fits in 64 bytes (2^3 summaries * 8 bytes per summary), which is close to the L1 cache line width on many systems. Also, a value of 3 fits 4 tree levels perfectly into the 21-bit pallocBits summary field at the root level. 

The following equation explains how each of the constants relate: summaryL0Bits + (summaryLevels-1)*summaryLevelBits + logPallocChunkBytes = heapAddrBits 

summaryLevels is an architecture-dependent value defined in mpagealloc_*.go. 

### <a id="summaryL0Bits" href="#summaryL0Bits">const summaryL0Bits</a>

```
searchKey: runtime.summaryL0Bits
```

```Go
const summaryL0Bits = heapAddrBits - logPallocChunkBytes - (summaryLevels-1)*summaryLevelBits
```

### <a id="pallocChunksL2Bits" href="#pallocChunksL2Bits">const pallocChunksL2Bits</a>

```
searchKey: runtime.pallocChunksL2Bits
```

```Go
const pallocChunksL2Bits = heapAddrBits - logPallocChunkBytes - pallocChunksL1Bits
```

pallocChunksL2Bits is the number of bits of the chunk index number covered by the second level of the chunks map. 

See (*pageAlloc).chunks for more details. Update the documentation there should this change. 

### <a id="pallocChunksL1Shift" href="#pallocChunksL1Shift">const pallocChunksL1Shift</a>

```
searchKey: runtime.pallocChunksL1Shift
```

```Go
const pallocChunksL1Shift = pallocChunksL2Bits
```

### <a id="pallocSumBytes" href="#pallocSumBytes">const pallocSumBytes</a>

```
searchKey: runtime.pallocSumBytes
```

```Go
const pallocSumBytes = unsafe.Sizeof(pallocSum(0))
```

### <a id="maxPackedValue" href="#maxPackedValue">const maxPackedValue</a>

```
searchKey: runtime.maxPackedValue
```

```Go
const maxPackedValue = 1 << logMaxPackedValue
```

maxPackedValue is the maximum value that any of the three fields in the pallocSum may take on. 

### <a id="logMaxPackedValue" href="#logMaxPackedValue">const logMaxPackedValue</a>

```
searchKey: runtime.logMaxPackedValue
```

```Go
const logMaxPackedValue = logPallocChunkPages + (summaryLevels-1)*summaryLevelBits
```

### <a id="freeChunkSum" href="#freeChunkSum">const freeChunkSum</a>

```
searchKey: runtime.freeChunkSum
```

```Go
const freeChunkSum = ...
```

### <a id="summaryLevels" href="#summaryLevels">const summaryLevels</a>

```
searchKey: runtime.summaryLevels
```

```Go
const summaryLevels = 5
```

The number of levels in the radix tree. 

### <a id="pageAlloc32Bit" href="#pageAlloc32Bit">const pageAlloc32Bit</a>

```
searchKey: runtime.pageAlloc32Bit
```

```Go
const pageAlloc32Bit = 0
```

Constants for testing. 

### <a id="pageAlloc64Bit" href="#pageAlloc64Bit">const pageAlloc64Bit</a>

```
searchKey: runtime.pageAlloc64Bit
```

```Go
const pageAlloc64Bit = 1
```

### <a id="pallocChunksL1Bits" href="#pallocChunksL1Bits">const pallocChunksL1Bits</a>

```
searchKey: runtime.pallocChunksL1Bits
```

```Go
const pallocChunksL1Bits = 13
```

Number of bits needed to represent all indices into the L1 of the chunks map. 

See (*pageAlloc).chunks for more details. Update the documentation there should this number change. 

### <a id="pageCachePages" href="#pageCachePages">const pageCachePages</a>

```
searchKey: runtime.pageCachePages
```

```Go
const pageCachePages = 8 * unsafe.Sizeof(pageCache{}.cache)
```

### <a id="memProfile" href="#memProfile">const memProfile</a>

```
searchKey: runtime.memProfile
```

```Go
const memProfile bucketType = 1 + iota
```

profile types 

### <a id="blockProfile" href="#blockProfile">const blockProfile</a>

```
searchKey: runtime.blockProfile
```

```Go
const blockProfile
```

### <a id="mutexProfile" href="#mutexProfile">const mutexProfile</a>

```
searchKey: runtime.mutexProfile
```

```Go
const mutexProfile
```

### <a id="buckHashSize" href="#buckHashSize">const buckHashSize</a>

```
searchKey: runtime.buckHashSize
```

```Go
const buckHashSize = 179999
```

size of bucket hash table 

### <a id="maxStack" href="#maxStack">const maxStack</a>

```
searchKey: runtime.maxStack
```

```Go
const maxStack = 32
```

max depth of stack to record in bucket 

### <a id="mProfCycleWrap" href="#mProfCycleWrap">const mProfCycleWrap</a>

```
searchKey: runtime.mProfCycleWrap
```

```Go
const mProfCycleWrap = uint32(len(memRecord{}.future)) * (2 << 24)
```

### <a id="msanenabled" href="#msanenabled">const msanenabled</a>

```
searchKey: runtime.msanenabled
```

```Go
const msanenabled = false
```

### <a id="spanSetBlockEntries" href="#spanSetBlockEntries">const spanSetBlockEntries</a>

```
searchKey: runtime.spanSetBlockEntries
```

```Go
const spanSetBlockEntries = 512 // 4KB on 64-bit

```

### <a id="spanSetInitSpineCap" href="#spanSetInitSpineCap">const spanSetInitSpineCap</a>

```
searchKey: runtime.spanSetInitSpineCap
```

```Go
const spanSetInitSpineCap = 256 // Enough for 1GB heap on 64-bit

```

### <a id="testSmallBuf" href="#testSmallBuf">const testSmallBuf</a>

```
searchKey: runtime.testSmallBuf
```

```Go
const testSmallBuf = false
```

testSmallBuf forces a small write barrier buffer to stress write barrier flushing. 

### <a id="wbBufEntries" href="#wbBufEntries">const wbBufEntries</a>

```
searchKey: runtime.wbBufEntries
```

```Go
const wbBufEntries = 256
```

wbBufEntries is the number of write barriers between flushes of the write barrier buffer. 

This trades latency for throughput amortization. Higher values amortize flushing overhead more, but increase the latency of flushing. Higher values also increase the cache footprint of the buffer. 

TODO: What is the latency cost of this? Tune this value. 

### <a id="wbBufEntryPointers" href="#wbBufEntryPointers">const wbBufEntryPointers</a>

```
searchKey: runtime.wbBufEntryPointers
```

```Go
const wbBufEntryPointers = 2
```

wbBufEntryPointers is the number of pointers added to the buffer by each write barrier. 

### <a id="pollNoError" href="#pollNoError">const pollNoError</a>

```
searchKey: runtime.pollNoError
```

```Go
const pollNoError = 0 // no error

```

Error codes returned by runtime_pollReset and runtime_pollWait. These must match the values in internal/poll/fd_poll_runtime.go. 

### <a id="pollErrClosing" href="#pollErrClosing">const pollErrClosing</a>

```
searchKey: runtime.pollErrClosing
```

```Go
const pollErrClosing = 1 // descriptor is closed

```

Error codes returned by runtime_pollReset and runtime_pollWait. These must match the values in internal/poll/fd_poll_runtime.go. 

### <a id="pollErrTimeout" href="#pollErrTimeout">const pollErrTimeout</a>

```
searchKey: runtime.pollErrTimeout
```

```Go
const pollErrTimeout = 2 // I/O timeout

```

Error codes returned by runtime_pollReset and runtime_pollWait. These must match the values in internal/poll/fd_poll_runtime.go. 

### <a id="pollErrNotPollable" href="#pollErrNotPollable">const pollErrNotPollable</a>

```
searchKey: runtime.pollErrNotPollable
```

```Go
const pollErrNotPollable = 3 // general error polling descriptor

```

Error codes returned by runtime_pollReset and runtime_pollWait. These must match the values in internal/poll/fd_poll_runtime.go. 

### <a id="pdReady" href="#pdReady">const pdReady</a>

```
searchKey: runtime.pdReady
```

```Go
const pdReady uintptr = 1
```

pollDesc contains 2 binary semaphores, rg and wg, to park reader and writer goroutines respectively. The semaphore can be in the following states: pdReady - io readiness notification is pending; 

```
a goroutine consumes the notification by changing the state to nil.

```
pdWait - a goroutine prepares to park on the semaphore, but not yet parked; 

```
the goroutine commits to park by changing the state to G pointer,
or, alternatively, concurrent io notification changes the state to pdReady,
or, alternatively, concurrent timeout/close changes the state to nil.

```
G pointer - the goroutine is blocked on the semaphore; 

```
io notification or timeout/close changes the state to pdReady or nil respectively
and unparks the goroutine.

```
nil - none of the above. 

### <a id="pdWait" href="#pdWait">const pdWait</a>

```
searchKey: runtime.pdWait
```

```Go
const pdWait uintptr = 2
```

pollDesc contains 2 binary semaphores, rg and wg, to park reader and writer goroutines respectively. The semaphore can be in the following states: pdReady - io readiness notification is pending; 

```
a goroutine consumes the notification by changing the state to nil.

```
pdWait - a goroutine prepares to park on the semaphore, but not yet parked; 

```
the goroutine commits to park by changing the state to G pointer,
or, alternatively, concurrent io notification changes the state to pdReady,
or, alternatively, concurrent timeout/close changes the state to nil.

```
G pointer - the goroutine is blocked on the semaphore; 

```
io notification or timeout/close changes the state to pdReady or nil respectively
and unparks the goroutine.

```
nil - none of the above. 

### <a id="pollBlockSize" href="#pollBlockSize">const pollBlockSize</a>

```
searchKey: runtime.pollBlockSize
```

```Go
const pollBlockSize = 4 * 1024
```

### <a id="_CTL_HW" href="#_CTL_HW">const _CTL_HW</a>

```
searchKey: runtime._CTL_HW
```

```Go
const _CTL_HW = 6
```

### <a id="_HW_NCPU" href="#_HW_NCPU">const _HW_NCPU</a>

```
searchKey: runtime._HW_NCPU
```

```Go
const _HW_NCPU = 3
```

### <a id="_HW_PAGESIZE" href="#_HW_PAGESIZE">const _HW_PAGESIZE</a>

```
searchKey: runtime._HW_PAGESIZE
```

```Go
const _HW_PAGESIZE = 7
```

### <a id="_NSIG" href="#_NSIG">const _NSIG</a>

```
searchKey: runtime._NSIG
```

```Go
const _NSIG = 32
```

### <a id="_SI_USER" href="#_SI_USER">const _SI_USER</a>

```
searchKey: runtime._SI_USER
```

```Go
const _SI_USER = 0 /* empirically true, but not what headers say */

```

### <a id="_SIG_BLOCK" href="#_SIG_BLOCK">const _SIG_BLOCK</a>

```
searchKey: runtime._SIG_BLOCK
```

```Go
const _SIG_BLOCK = 1
```

### <a id="_SIG_UNBLOCK" href="#_SIG_UNBLOCK">const _SIG_UNBLOCK</a>

```
searchKey: runtime._SIG_UNBLOCK
```

```Go
const _SIG_UNBLOCK = 2
```

### <a id="_SIG_SETMASK" href="#_SIG_SETMASK">const _SIG_SETMASK</a>

```
searchKey: runtime._SIG_SETMASK
```

```Go
const _SIG_SETMASK = 3
```

### <a id="_SS_DISABLE" href="#_SS_DISABLE">const _SS_DISABLE</a>

```
searchKey: runtime._SS_DISABLE
```

```Go
const _SS_DISABLE = 4
```

### <a id="deferHeaderSize" href="#deferHeaderSize">const deferHeaderSize</a>

```
searchKey: runtime.deferHeaderSize
```

```Go
const deferHeaderSize = unsafe.Sizeof(_defer{})
```

### <a id="minDeferAlloc" href="#minDeferAlloc">const minDeferAlloc</a>

```
searchKey: runtime.minDeferAlloc
```

```Go
const minDeferAlloc = (deferHeaderSize + 15) &^ 15
```

### <a id="minDeferArgs" href="#minDeferArgs">const minDeferArgs</a>

```
searchKey: runtime.minDeferArgs
```

```Go
const minDeferArgs = minDeferAlloc - deferHeaderSize
```

### <a id="_GoidCacheBatch" href="#_GoidCacheBatch">const _GoidCacheBatch</a>

```
searchKey: runtime._GoidCacheBatch
```

```Go
const _GoidCacheBatch = 16
```

Number of goroutine ids to grab from sched.goidgen to local per-P cache at once. 16 seems to provide enough amortization, but other than that it's mostly arbitrary number. 

### <a id="freezeStopWait" href="#freezeStopWait">const freezeStopWait</a>

```
searchKey: runtime.freezeStopWait
```

```Go
const freezeStopWait = 0x7fffffff
```

freezeStopWait is a large value that freezetheworld sets sched.stopwait to in order to request that all Gs permanently stop. 

### <a id="forcePreemptNS" href="#forcePreemptNS">const forcePreemptNS</a>

```
searchKey: runtime.forcePreemptNS
```

```Go
const forcePreemptNS = 10 * 1000 * 1000 // 10ms

```

forcePreemptNS is the time slice given to a G before it is preempted. 

### <a id="randomizeScheduler" href="#randomizeScheduler">const randomizeScheduler</a>

```
searchKey: runtime.randomizeScheduler
```

```Go
const randomizeScheduler = raceenabled
```

To shake out latent assumptions about scheduling order, we introduce some randomness into scheduling decisions when running with the race detector. The need for this was made obvious by changing the (deterministic) scheduling order in Go 1.5 and breaking many poorly-written tests. With the randomness here, as long as the tests pass consistently with -race, they shouldn't have latent scheduling assumptions. 

### <a id="profReaderSleeping" href="#profReaderSleeping">const profReaderSleeping</a>

```
searchKey: runtime.profReaderSleeping
```

```Go
const profReaderSleeping profIndex = 1 << 32 // reader is sleeping and must be woken up

```

### <a id="profWriteExtra" href="#profWriteExtra">const profWriteExtra</a>

```
searchKey: runtime.profWriteExtra
```

```Go
const profWriteExtra profIndex = 1 << 33 // overflow or eof waiting

```

### <a id="profBufBlocking" href="#profBufBlocking">const profBufBlocking</a>

```
searchKey: runtime.profBufBlocking
```

```Go
const profBufBlocking profBufReadMode = iota
```

### <a id="profBufNonBlocking" href="#profBufNonBlocking">const profBufNonBlocking</a>

```
searchKey: runtime.profBufNonBlocking
```

```Go
const profBufNonBlocking
```

### <a id="raceenabled" href="#raceenabled">const raceenabled</a>

```
searchKey: runtime.raceenabled
```

```Go
const raceenabled = false
```

### <a id="osRelaxMinNS" href="#osRelaxMinNS">const osRelaxMinNS</a>

```
searchKey: runtime.osRelaxMinNS
```

```Go
const osRelaxMinNS = 0
```

osRelaxMinNS is the number of nanoseconds of idleness to tolerate without performing an osRelax. Since osRelax may reduce the precision of timers, this should be enough larger than the relaxed timer precision to keep the timer error acceptable. 

### <a id="tracebackCrash" href="#tracebackCrash">const tracebackCrash</a>

```
searchKey: runtime.tracebackCrash
```

```Go
const tracebackCrash = 1 << iota
```

Keep a cached value to make gotraceback fast, since we call it on every call to gentraceback. The cached value is a uint32 in which the low bits are the "crash" and "all" settings and the remaining bits are the traceback value (0 off, 1 on, 2 include system). 

### <a id="tracebackAll" href="#tracebackAll">const tracebackAll</a>

```
searchKey: runtime.tracebackAll
```

```Go
const tracebackAll
```

Keep a cached value to make gotraceback fast, since we call it on every call to gentraceback. The cached value is a uint32 in which the low bits are the "crash" and "all" settings and the remaining bits are the traceback value (0 off, 1 on, 2 include system). 

### <a id="tracebackShift" href="#tracebackShift">const tracebackShift</a>

```
searchKey: runtime.tracebackShift
```

```Go
const tracebackShift = iota
```

Keep a cached value to make gotraceback fast, since we call it on every call to gentraceback. The cached value is a uint32 in which the low bits are the "crash" and "all" settings and the remaining bits are the traceback value (0 off, 1 on, 2 include system). 

### <a id="_Gidle" href="#_Gidle">const _Gidle</a>

```
searchKey: runtime._Gidle
```

```Go
const _Gidle = iota // 0

```

defined constants 

_Gidle means this goroutine was just allocated and has not yet been initialized. 

### <a id="_Grunnable" href="#_Grunnable">const _Grunnable</a>

```
searchKey: runtime._Grunnable
```

```Go
const _Grunnable // 1

```

defined constants 

_Grunnable means this goroutine is on a run queue. It is not currently executing user code. The stack is not owned. 

### <a id="_Grunning" href="#_Grunning">const _Grunning</a>

```
searchKey: runtime._Grunning
```

```Go
const _Grunning // 2

```

defined constants 

_Grunning means this goroutine may execute user code. The stack is owned by this goroutine. It is not on a run queue. It is assigned an M and a P (g.m and g.m.p are valid). 

### <a id="_Gsyscall" href="#_Gsyscall">const _Gsyscall</a>

```
searchKey: runtime._Gsyscall
```

```Go
const _Gsyscall // 3

```

defined constants 

_Gsyscall means this goroutine is executing a system call. It is not executing user code. The stack is owned by this goroutine. It is not on a run queue. It is assigned an M. 

### <a id="_Gwaiting" href="#_Gwaiting">const _Gwaiting</a>

```
searchKey: runtime._Gwaiting
```

```Go
const _Gwaiting // 4

```

defined constants 

_Gwaiting means this goroutine is blocked in the runtime. It is not executing user code. It is not on a run queue, but should be recorded somewhere (e.g., a channel wait queue) so it can be ready()d when necessary. The stack is not owned *except* that a channel operation may read or write parts of the stack under the appropriate channel lock. Otherwise, it is not safe to access the stack after a goroutine enters _Gwaiting (e.g., it may get moved). 

### <a id="_Gmoribund_unused" href="#_Gmoribund_unused">const _Gmoribund_unused</a>

```
searchKey: runtime._Gmoribund_unused
```

```Go
const _Gmoribund_unused // 5

```

defined constants 

_Gmoribund_unused is currently unused, but hardcoded in gdb scripts. 

### <a id="_Gdead" href="#_Gdead">const _Gdead</a>

```
searchKey: runtime._Gdead
```

```Go
const _Gdead // 6

```

defined constants 

_Gdead means this goroutine is currently unused. It may be just exited, on a free list, or just being initialized. It is not executing user code. It may or may not have a stack allocated. The G and its stack (if any) are owned by the M that is exiting the G or that obtained the G from the free list. 

### <a id="_Genqueue_unused" href="#_Genqueue_unused">const _Genqueue_unused</a>

```
searchKey: runtime._Genqueue_unused
```

```Go
const _Genqueue_unused // 7

```

defined constants 

_Genqueue_unused is currently unused. 

### <a id="_Gcopystack" href="#_Gcopystack">const _Gcopystack</a>

```
searchKey: runtime._Gcopystack
```

```Go
const _Gcopystack // 8

```

defined constants 

_Gcopystack means this goroutine's stack is being moved. It is not executing user code and is not on a run queue. The stack is owned by the goroutine that put it in _Gcopystack. 

### <a id="_Gpreempted" href="#_Gpreempted">const _Gpreempted</a>

```
searchKey: runtime._Gpreempted
```

```Go
const _Gpreempted // 9

```

defined constants 

_Gpreempted means this goroutine stopped itself for a suspendG preemption. It is like _Gwaiting, but nothing is yet responsible for ready()ing it. Some suspendG must CAS the status to _Gwaiting to take responsibility for ready()ing this G. 

### <a id="_Gscan" href="#_Gscan">const _Gscan</a>

```
searchKey: runtime._Gscan
```

```Go
const _Gscan = 0x1000
```

defined constants 

_Gscan combined with one of the above states other than _Grunning indicates that GC is scanning the stack. The goroutine is not executing user code and the stack is owned by the goroutine that set the _Gscan bit. 

_Gscanrunning is different: it is used to briefly block state transitions while GC signals the G to scan its own stack. This is otherwise like _Grunning. 

atomicstatus&~Gscan gives the state the goroutine will return to when the scan completes. 

### <a id="_Gscanrunnable" href="#_Gscanrunnable">const _Gscanrunnable</a>

```
searchKey: runtime._Gscanrunnable
```

```Go
const _Gscanrunnable = _Gscan + _Grunnable // 0x1001

```

defined constants 

### <a id="_Gscanrunning" href="#_Gscanrunning">const _Gscanrunning</a>

```
searchKey: runtime._Gscanrunning
```

```Go
const _Gscanrunning = _Gscan + _Grunning // 0x1002

```

defined constants 

### <a id="_Gscansyscall" href="#_Gscansyscall">const _Gscansyscall</a>

```
searchKey: runtime._Gscansyscall
```

```Go
const _Gscansyscall = _Gscan + _Gsyscall // 0x1003

```

defined constants 

### <a id="_Gscanwaiting" href="#_Gscanwaiting">const _Gscanwaiting</a>

```
searchKey: runtime._Gscanwaiting
```

```Go
const _Gscanwaiting = _Gscan + _Gwaiting // 0x1004

```

defined constants 

### <a id="_Gscanpreempted" href="#_Gscanpreempted">const _Gscanpreempted</a>

```
searchKey: runtime._Gscanpreempted
```

```Go
const _Gscanpreempted = _Gscan + _Gpreempted // 0x1009

```

defined constants 

### <a id="_Pidle" href="#_Pidle">const _Pidle</a>

```
searchKey: runtime._Pidle
```

```Go
const _Pidle = iota
```

_Pidle means a P is not being used to run user code or the scheduler. Typically, it's on the idle P list and available to the scheduler, but it may just be transitioning between other states. 

The P is owned by the idle list or by whatever is transitioning its state. Its run queue is empty. 

### <a id="_Prunning" href="#_Prunning">const _Prunning</a>

```
searchKey: runtime._Prunning
```

```Go
const _Prunning
```

_Prunning means a P is owned by an M and is being used to run user code or the scheduler. Only the M that owns this P is allowed to change the P's status from _Prunning. The M may transition the P to _Pidle (if it has no more work to do), _Psyscall (when entering a syscall), or _Pgcstop (to halt for the GC). The M may also hand ownership of the P off directly to another M (e.g., to schedule a locked G). 

### <a id="_Psyscall" href="#_Psyscall">const _Psyscall</a>

```
searchKey: runtime._Psyscall
```

```Go
const _Psyscall
```

_Psyscall means a P is not running user code. It has affinity to an M in a syscall but is not owned by it and may be stolen by another M. This is similar to _Pidle but uses lightweight transitions and maintains M affinity. 

Leaving _Psyscall must be done with a CAS, either to steal or retake the P. Note that there's an ABA hazard: even if an M successfully CASes its original P back to _Prunning after a syscall, it must understand the P may have been used by another M in the interim. 

### <a id="_Pgcstop" href="#_Pgcstop">const _Pgcstop</a>

```
searchKey: runtime._Pgcstop
```

```Go
const _Pgcstop
```

_Pgcstop means a P is halted for STW and owned by the M that stopped the world. The M that stopped the world continues to use its P, even in _Pgcstop. Transitioning from _Prunning to _Pgcstop causes an M to release its P and park. 

The P retains its run queue and startTheWorld will restart the scheduler on Ps with non-empty run queues. 

### <a id="_Pdead" href="#_Pdead">const _Pdead</a>

```
searchKey: runtime._Pdead
```

```Go
const _Pdead
```

_Pdead means a P is no longer used (GOMAXPROCS shrank). We reuse Ps if GOMAXPROCS increases. A dead P is mostly stripped of its resources, though a few things remain (e.g., trace buffers). 

### <a id="gTrackingPeriod" href="#gTrackingPeriod">const gTrackingPeriod</a>

```
searchKey: runtime.gTrackingPeriod
```

```Go
const gTrackingPeriod = 8
```

gTrackingPeriod is the number of transitions out of _Grunning between latency tracking runs. 

### <a id="tlsSlots" href="#tlsSlots">const tlsSlots</a>

```
searchKey: runtime.tlsSlots
```

```Go
const tlsSlots = 6
```

tlsSlots is the number of pointer-sized slots reserved for TLS on some platforms, like Windows. 

### <a id="tlsSize" href="#tlsSize">const tlsSize</a>

```
searchKey: runtime.tlsSize
```

```Go
const tlsSize = tlsSlots * sys.PtrSize
```

### <a id="_SigNotify" href="#_SigNotify">const _SigNotify</a>

```
searchKey: runtime._SigNotify
```

```Go
const _SigNotify = 1 << iota // let signal.Notify have signal, even if from kernel

```

Values for the flags field of a sigTabT. 

### <a id="_SigKill" href="#_SigKill">const _SigKill</a>

```
searchKey: runtime._SigKill
```

```Go
const _SigKill // if signal.Notify doesn't take it, exit quietly

```

Values for the flags field of a sigTabT. 

### <a id="_SigThrow" href="#_SigThrow">const _SigThrow</a>

```
searchKey: runtime._SigThrow
```

```Go
const _SigThrow // if signal.Notify doesn't take it, exit loudly

```

Values for the flags field of a sigTabT. 

### <a id="_SigPanic" href="#_SigPanic">const _SigPanic</a>

```
searchKey: runtime._SigPanic
```

```Go
const _SigPanic // if the signal is from the kernel, panic

```

Values for the flags field of a sigTabT. 

### <a id="_SigDefault" href="#_SigDefault">const _SigDefault</a>

```
searchKey: runtime._SigDefault
```

```Go
const _SigDefault // if the signal isn't explicitly requested, don't monitor it

```

Values for the flags field of a sigTabT. 

### <a id="_SigGoExit" href="#_SigGoExit">const _SigGoExit</a>

```
searchKey: runtime._SigGoExit
```

```Go
const _SigGoExit // cause all runtime procs to exit (only used on Plan 9).

```

Values for the flags field of a sigTabT. 

### <a id="_SigSetStack" href="#_SigSetStack">const _SigSetStack</a>

```
searchKey: runtime._SigSetStack
```

```Go
const _SigSetStack // add SA_ONSTACK to libc handler

```

Values for the flags field of a sigTabT. 

### <a id="_SigUnblock" href="#_SigUnblock">const _SigUnblock</a>

```
searchKey: runtime._SigUnblock
```

```Go
const _SigUnblock // always unblock; see blockableSig

```

Values for the flags field of a sigTabT. 

### <a id="_SigIgn" href="#_SigIgn">const _SigIgn</a>

```
searchKey: runtime._SigIgn
```

```Go
const _SigIgn // _SIG_DFL action is to ignore the signal

```

Values for the flags field of a sigTabT. 

### <a id="_TraceRuntimeFrames" href="#_TraceRuntimeFrames">const _TraceRuntimeFrames</a>

```
searchKey: runtime._TraceRuntimeFrames
```

```Go
const _TraceRuntimeFrames = 1 << iota // include frames for internal runtime functions.

```

### <a id="_TraceTrap" href="#_TraceTrap">const _TraceTrap</a>

```
searchKey: runtime._TraceTrap
```

```Go
const _TraceTrap // the initial PC, SP are from a trap, not a return PC from a call

```

### <a id="_TraceJumpStack" href="#_TraceJumpStack">const _TraceJumpStack</a>

```
searchKey: runtime._TraceJumpStack
```

```Go
const _TraceJumpStack // if traceback is on a systemstack, resume trace at g that called into it

```

### <a id="_TracebackMaxFrames" href="#_TracebackMaxFrames">const _TracebackMaxFrames</a>

```
searchKey: runtime._TracebackMaxFrames
```

```Go
const _TracebackMaxFrames = 100
```

The maximum number of frames we print for a traceback 

### <a id="waitReasonZero" href="#waitReasonZero">const waitReasonZero</a>

```
searchKey: runtime.waitReasonZero
```

```Go
const waitReasonZero waitReason = iota // ""

```

### <a id="waitReasonGCAssistMarking" href="#waitReasonGCAssistMarking">const waitReasonGCAssistMarking</a>

```
searchKey: runtime.waitReasonGCAssistMarking
```

```Go
const waitReasonGCAssistMarking // "GC assist marking"

```

### <a id="waitReasonIOWait" href="#waitReasonIOWait">const waitReasonIOWait</a>

```
searchKey: runtime.waitReasonIOWait
```

```Go
const waitReasonIOWait // "IO wait"

```

### <a id="waitReasonChanReceiveNilChan" href="#waitReasonChanReceiveNilChan">const waitReasonChanReceiveNilChan</a>

```
searchKey: runtime.waitReasonChanReceiveNilChan
```

```Go
const waitReasonChanReceiveNilChan // "chan receive (nil chan)"

```

### <a id="waitReasonChanSendNilChan" href="#waitReasonChanSendNilChan">const waitReasonChanSendNilChan</a>

```
searchKey: runtime.waitReasonChanSendNilChan
```

```Go
const waitReasonChanSendNilChan // "chan send (nil chan)"

```

### <a id="waitReasonDumpingHeap" href="#waitReasonDumpingHeap">const waitReasonDumpingHeap</a>

```
searchKey: runtime.waitReasonDumpingHeap
```

```Go
const waitReasonDumpingHeap // "dumping heap"

```

### <a id="waitReasonGarbageCollection" href="#waitReasonGarbageCollection">const waitReasonGarbageCollection</a>

```
searchKey: runtime.waitReasonGarbageCollection
```

```Go
const waitReasonGarbageCollection // "garbage collection"

```

### <a id="waitReasonGarbageCollectionScan" href="#waitReasonGarbageCollectionScan">const waitReasonGarbageCollectionScan</a>

```
searchKey: runtime.waitReasonGarbageCollectionScan
```

```Go
const waitReasonGarbageCollectionScan // "garbage collection scan"

```

### <a id="waitReasonPanicWait" href="#waitReasonPanicWait">const waitReasonPanicWait</a>

```
searchKey: runtime.waitReasonPanicWait
```

```Go
const waitReasonPanicWait // "panicwait"

```

### <a id="waitReasonSelect" href="#waitReasonSelect">const waitReasonSelect</a>

```
searchKey: runtime.waitReasonSelect
```

```Go
const waitReasonSelect // "select"

```

### <a id="waitReasonSelectNoCases" href="#waitReasonSelectNoCases">const waitReasonSelectNoCases</a>

```
searchKey: runtime.waitReasonSelectNoCases
```

```Go
const waitReasonSelectNoCases // "select (no cases)"

```

### <a id="waitReasonGCAssistWait" href="#waitReasonGCAssistWait">const waitReasonGCAssistWait</a>

```
searchKey: runtime.waitReasonGCAssistWait
```

```Go
const waitReasonGCAssistWait // "GC assist wait"

```

### <a id="waitReasonGCSweepWait" href="#waitReasonGCSweepWait">const waitReasonGCSweepWait</a>

```
searchKey: runtime.waitReasonGCSweepWait
```

```Go
const waitReasonGCSweepWait // "GC sweep wait"

```

### <a id="waitReasonGCScavengeWait" href="#waitReasonGCScavengeWait">const waitReasonGCScavengeWait</a>

```
searchKey: runtime.waitReasonGCScavengeWait
```

```Go
const waitReasonGCScavengeWait // "GC scavenge wait"

```

### <a id="waitReasonChanReceive" href="#waitReasonChanReceive">const waitReasonChanReceive</a>

```
searchKey: runtime.waitReasonChanReceive
```

```Go
const waitReasonChanReceive // "chan receive"

```

### <a id="waitReasonChanSend" href="#waitReasonChanSend">const waitReasonChanSend</a>

```
searchKey: runtime.waitReasonChanSend
```

```Go
const waitReasonChanSend // "chan send"

```

### <a id="waitReasonFinalizerWait" href="#waitReasonFinalizerWait">const waitReasonFinalizerWait</a>

```
searchKey: runtime.waitReasonFinalizerWait
```

```Go
const waitReasonFinalizerWait // "finalizer wait"

```

### <a id="waitReasonForceGCIdle" href="#waitReasonForceGCIdle">const waitReasonForceGCIdle</a>

```
searchKey: runtime.waitReasonForceGCIdle
```

```Go
const waitReasonForceGCIdle // "force gc (idle)"

```

### <a id="waitReasonSemacquire" href="#waitReasonSemacquire">const waitReasonSemacquire</a>

```
searchKey: runtime.waitReasonSemacquire
```

```Go
const waitReasonSemacquire // "semacquire"

```

### <a id="waitReasonSleep" href="#waitReasonSleep">const waitReasonSleep</a>

```
searchKey: runtime.waitReasonSleep
```

```Go
const waitReasonSleep // "sleep"

```

### <a id="waitReasonSyncCondWait" href="#waitReasonSyncCondWait">const waitReasonSyncCondWait</a>

```
searchKey: runtime.waitReasonSyncCondWait
```

```Go
const waitReasonSyncCondWait // "sync.Cond.Wait"

```

### <a id="waitReasonTimerGoroutineIdle" href="#waitReasonTimerGoroutineIdle">const waitReasonTimerGoroutineIdle</a>

```
searchKey: runtime.waitReasonTimerGoroutineIdle
```

```Go
const waitReasonTimerGoroutineIdle // "timer goroutine (idle)"

```

### <a id="waitReasonTraceReaderBlocked" href="#waitReasonTraceReaderBlocked">const waitReasonTraceReaderBlocked</a>

```
searchKey: runtime.waitReasonTraceReaderBlocked
```

```Go
const waitReasonTraceReaderBlocked // "trace reader (blocked)"

```

### <a id="waitReasonWaitForGCCycle" href="#waitReasonWaitForGCCycle">const waitReasonWaitForGCCycle</a>

```
searchKey: runtime.waitReasonWaitForGCCycle
```

```Go
const waitReasonWaitForGCCycle // "wait for GC cycle"

```

### <a id="waitReasonGCWorkerIdle" href="#waitReasonGCWorkerIdle">const waitReasonGCWorkerIdle</a>

```
searchKey: runtime.waitReasonGCWorkerIdle
```

```Go
const waitReasonGCWorkerIdle // "GC worker (idle)"

```

### <a id="waitReasonPreempted" href="#waitReasonPreempted">const waitReasonPreempted</a>

```
searchKey: runtime.waitReasonPreempted
```

```Go
const waitReasonPreempted // "preempted"

```

### <a id="waitReasonDebugCall" href="#waitReasonDebugCall">const waitReasonDebugCall</a>

```
searchKey: runtime.waitReasonDebugCall
```

```Go
const waitReasonDebugCall // "debug call"

```

### <a id="framepointer_enabled" href="#framepointer_enabled">const framepointer_enabled</a>

```
searchKey: runtime.framepointer_enabled
```

```Go
const framepointer_enabled = GOARCH == "amd64" || GOARCH == "arm64"
```

Must agree with internal/buildcfg.Experiment.FramePointer. 

### <a id="rwmutexMaxReaders" href="#rwmutexMaxReaders">const rwmutexMaxReaders</a>

```
searchKey: runtime.rwmutexMaxReaders
```

```Go
const rwmutexMaxReaders = 1 << 30
```

### <a id="debugSelect" href="#debugSelect">const debugSelect</a>

```
searchKey: runtime.debugSelect
```

```Go
const debugSelect = false
```

### <a id="selectSend" href="#selectSend">const selectSend</a>

```
searchKey: runtime.selectSend
```

```Go
const selectSend // case Chan <- Send

```

### <a id="selectRecv" href="#selectRecv">const selectRecv</a>

```
searchKey: runtime.selectRecv
```

```Go
const selectRecv // case <-Chan:

```

### <a id="selectDefault" href="#selectDefault">const selectDefault</a>

```
searchKey: runtime.selectDefault
```

```Go
const selectDefault // default

```

### <a id="semTabSize" href="#semTabSize">const semTabSize</a>

```
searchKey: runtime.semTabSize
```

```Go
const semTabSize = 251
```

Prime to not correlate with any user patterns. 

### <a id="semaBlockProfile" href="#semaBlockProfile">const semaBlockProfile</a>

```
searchKey: runtime.semaBlockProfile
```

```Go
const semaBlockProfile semaProfileFlags = 1 << iota
```

### <a id="semaMutexProfile" href="#semaMutexProfile">const semaMutexProfile</a>

```
searchKey: runtime.semaMutexProfile
```

```Go
const semaMutexProfile
```

### <a id="_SIG_DFL" href="#_SIG_DFL">const _SIG_DFL</a>

```
searchKey: runtime._SIG_DFL
```

```Go
const _SIG_DFL uintptr = 0
```

### <a id="_SIG_IGN" href="#_SIG_IGN">const _SIG_IGN</a>

```
searchKey: runtime._SIG_IGN
```

```Go
const _SIG_IGN uintptr = 1
```

### <a id="sigPreempt" href="#sigPreempt">const sigPreempt</a>

```
searchKey: runtime.sigPreempt
```

```Go
const sigPreempt = _SIGURG
```

sigPreempt is the signal used for non-cooperative preemption. 

There's no good way to choose this signal, but there are some heuristics: 

1. It should be a signal that's passed-through by debuggers by default. On Linux, this is SIGALRM, SIGURG, SIGCHLD, SIGIO, SIGVTALRM, SIGPROF, and SIGWINCH, plus some glibc-internal signals. 

2. It shouldn't be used internally by libc in mixed Go/C binaries because libc may assume it's the only thing that can handle these signals. For example SIGCANCEL or SIGSETXID. 

3. It should be a signal that can happen spuriously without consequences. For example, SIGALRM is a bad choice because the signal handler can't tell if it was caused by the real process alarm or not (arguably this means the signal is broken, but I digress). SIGUSR1 and SIGUSR2 are also bad because those are often used in meaningful ways by applications. 

4. We need to deal with platforms without real-time signals (like macOS), so those are out. 

We use SIGURG because it meets all of these criteria, is extremely unlikely to be used by an application for its "real" meaning (both because out-of-band data is basically unused and because SIGURG doesn't report which socket has the condition, making it pretty useless), and even if it is, the application has to be ready for spurious SIGURG. SIGIO wouldn't be a bad choice either, but is more likely to be used for real. 

### <a id="preemptMSupported" href="#preemptMSupported">const preemptMSupported</a>

```
searchKey: runtime.preemptMSupported
```

```Go
const preemptMSupported = true
```

### <a id="sigIdle" href="#sigIdle">const sigIdle</a>

```
searchKey: runtime.sigIdle
```

```Go
const sigIdle = iota
```

### <a id="sigReceiving" href="#sigReceiving">const sigReceiving</a>

```
searchKey: runtime.sigReceiving
```

```Go
const sigReceiving
```

### <a id="sigSending" href="#sigSending">const sigSending</a>

```
searchKey: runtime.sigSending
```

```Go
const sigSending
```

### <a id="sigFixup" href="#sigFixup">const sigFixup</a>

```
searchKey: runtime.sigFixup
```

```Go
const sigFixup
```

### <a id="_MaxSmallSize" href="#_MaxSmallSize">const _MaxSmallSize</a>

```
searchKey: runtime._MaxSmallSize
```

```Go
const _MaxSmallSize = 32768
```

### <a id="smallSizeDiv" href="#smallSizeDiv">const smallSizeDiv</a>

```
searchKey: runtime.smallSizeDiv
```

```Go
const smallSizeDiv = 8
```

### <a id="smallSizeMax" href="#smallSizeMax">const smallSizeMax</a>

```
searchKey: runtime.smallSizeMax
```

```Go
const smallSizeMax = 1024
```

### <a id="largeSizeDiv" href="#largeSizeDiv">const largeSizeDiv</a>

```
searchKey: runtime.largeSizeDiv
```

```Go
const largeSizeDiv = 128
```

### <a id="_NumSizeClasses" href="#_NumSizeClasses">const _NumSizeClasses</a>

```
searchKey: runtime._NumSizeClasses
```

```Go
const _NumSizeClasses = 68
```

### <a id="_PageShift" href="#_PageShift">const _PageShift</a>

```
searchKey: runtime._PageShift
```

```Go
const _PageShift = 13
```

### <a id="mantbits64" href="#mantbits64">const mantbits64</a>

```
searchKey: runtime.mantbits64
```

```Go
const mantbits64 uint = 52
```

### <a id="expbits64" href="#expbits64">const expbits64</a>

```
searchKey: runtime.expbits64
```

```Go
const expbits64 uint = 11
```

### <a id="bias64" href="#bias64">const bias64</a>

```
searchKey: runtime.bias64
```

```Go
const bias64 = -1<<(expbits64-1) + 1
```

### <a id="nan64" href="#nan64">const nan64</a>

```
searchKey: runtime.nan64
```

```Go
const nan64 uint64 = (1<<expbits64-1)<<mantbits64 + 1<<(mantbits64-1) // quiet NaN, 0 payload

```

### <a id="inf64" href="#inf64">const inf64</a>

```
searchKey: runtime.inf64
```

```Go
const inf64 uint64 = (1<<expbits64 - 1) << mantbits64
```

### <a id="neg64" href="#neg64">const neg64</a>

```
searchKey: runtime.neg64
```

```Go
const neg64 uint64 = 1 << (expbits64 + mantbits64)
```

### <a id="mantbits32" href="#mantbits32">const mantbits32</a>

```
searchKey: runtime.mantbits32
```

```Go
const mantbits32 uint = 23
```

### <a id="expbits32" href="#expbits32">const expbits32</a>

```
searchKey: runtime.expbits32
```

```Go
const expbits32 uint = 8
```

### <a id="bias32" href="#bias32">const bias32</a>

```
searchKey: runtime.bias32
```

```Go
const bias32 = -1<<(expbits32-1) + 1
```

### <a id="nan32" href="#nan32">const nan32</a>

```
searchKey: runtime.nan32
```

```Go
const nan32 uint32 = (1<<expbits32-1)<<mantbits32 + 1<<(mantbits32-1) // quiet NaN, 0 payload

```

### <a id="inf32" href="#inf32">const inf32</a>

```
searchKey: runtime.inf32
```

```Go
const inf32 uint32 = (1<<expbits32 - 1) << mantbits32
```

### <a id="neg32" href="#neg32">const neg32</a>

```
searchKey: runtime.neg32
```

```Go
const neg32 uint32 = 1 << (expbits32 + mantbits32)
```

### <a id="_StackSystem" href="#_StackSystem">const _StackSystem</a>

```
searchKey: runtime._StackSystem
```

```Go
const _StackSystem = ...
```

StackSystem is a number of additional bytes to add to each stack below the usual guard area for OS-specific purposes like signal handling. Used on Windows, Plan 9, and iOS because they do not use a separate stack. 

### <a id="_StackMin" href="#_StackMin">const _StackMin</a>

```
searchKey: runtime._StackMin
```

```Go
const _StackMin = 2048
```

The minimum size of stack used by Go code 

### <a id="_FixedStack0" href="#_FixedStack0">const _FixedStack0</a>

```
searchKey: runtime._FixedStack0
```

```Go
const _FixedStack0 = _StackMin + _StackSystem
```

The minimum stack size to allocate. The hackery here rounds FixedStack0 up to a power of 2. 

### <a id="_FixedStack1" href="#_FixedStack1">const _FixedStack1</a>

```
searchKey: runtime._FixedStack1
```

```Go
const _FixedStack1 = _FixedStack0 - 1
```

### <a id="_FixedStack2" href="#_FixedStack2">const _FixedStack2</a>

```
searchKey: runtime._FixedStack2
```

```Go
const _FixedStack2 = _FixedStack1 | (_FixedStack1 >> 1)
```

### <a id="_FixedStack3" href="#_FixedStack3">const _FixedStack3</a>

```
searchKey: runtime._FixedStack3
```

```Go
const _FixedStack3 = _FixedStack2 | (_FixedStack2 >> 2)
```

### <a id="_FixedStack4" href="#_FixedStack4">const _FixedStack4</a>

```
searchKey: runtime._FixedStack4
```

```Go
const _FixedStack4 = _FixedStack3 | (_FixedStack3 >> 4)
```

### <a id="_FixedStack5" href="#_FixedStack5">const _FixedStack5</a>

```
searchKey: runtime._FixedStack5
```

```Go
const _FixedStack5 = _FixedStack4 | (_FixedStack4 >> 8)
```

### <a id="_FixedStack6" href="#_FixedStack6">const _FixedStack6</a>

```
searchKey: runtime._FixedStack6
```

```Go
const _FixedStack6 = _FixedStack5 | (_FixedStack5 >> 16)
```

### <a id="_FixedStack" href="#_FixedStack">const _FixedStack</a>

```
searchKey: runtime._FixedStack
```

```Go
const _FixedStack = _FixedStack6 + 1
```

### <a id="_StackBig" href="#_StackBig">const _StackBig</a>

```
searchKey: runtime._StackBig
```

```Go
const _StackBig = 4096
```

Functions that need frames bigger than this use an extra instruction to do the stack split check, to avoid overflow in case SP - framesize wraps below zero. This value can be no bigger than the size of the unmapped space at zero. 

### <a id="_StackGuard" href="#_StackGuard">const _StackGuard</a>

```
searchKey: runtime._StackGuard
```

```Go
const _StackGuard = 928*sys.StackGuardMultiplier + _StackSystem
```

The stack guard is a pointer this many bytes above the bottom of the stack. 

The guard leaves enough room for one _StackSmall frame plus a _StackLimit chain of NOSPLIT calls plus _StackSystem bytes for the OS. 

### <a id="_StackSmall" href="#_StackSmall">const _StackSmall</a>

```
searchKey: runtime._StackSmall
```

```Go
const _StackSmall = 128
```

After a stack split check the SP is allowed to be this many bytes below the stack guard. This saves an instruction in the checking sequence for tiny frames. 

### <a id="_StackLimit" href="#_StackLimit">const _StackLimit</a>

```
searchKey: runtime._StackLimit
```

```Go
const _StackLimit = _StackGuard - _StackSystem - _StackSmall
```

The maximum number of bytes that a chain of NOSPLIT functions can use. 

### <a id="stackDebug" href="#stackDebug">const stackDebug</a>

```
searchKey: runtime.stackDebug
```

```Go
const stackDebug = 0
```

stackDebug == 0: no logging 

```
== 1: logging of per-stack operations
== 2: logging of per-frame operations
== 3: logging of per-word updates
== 4: logging of per-word reads

```
### <a id="stackFromSystem" href="#stackFromSystem">const stackFromSystem</a>

```
searchKey: runtime.stackFromSystem
```

```Go
const stackFromSystem = 0 // allocate stacks from system memory instead of the heap

```

### <a id="stackFaultOnFree" href="#stackFaultOnFree">const stackFaultOnFree</a>

```
searchKey: runtime.stackFaultOnFree
```

```Go
const stackFaultOnFree = 0 // old stacks are mapped noaccess to detect use after free

```

### <a id="stackPoisonCopy" href="#stackPoisonCopy">const stackPoisonCopy</a>

```
searchKey: runtime.stackPoisonCopy
```

```Go
const stackPoisonCopy // fill stack that should not be accessed with garbage, to detect bad dereferences during copy
 = ...
```

### <a id="stackNoCache" href="#stackNoCache">const stackNoCache</a>

```
searchKey: runtime.stackNoCache
```

```Go
const stackNoCache = 0 // disable per-P small stack caches

```

### <a id="debugCheckBP" href="#debugCheckBP">const debugCheckBP</a>

```
searchKey: runtime.debugCheckBP
```

```Go
const debugCheckBP = false
```

check the BP links during traceback. 

### <a id="uintptrMask" href="#uintptrMask">const uintptrMask</a>

```
searchKey: runtime.uintptrMask
```

```Go
const uintptrMask = 1<<(8*sys.PtrSize) - 1
```

### <a id="stackPreempt" href="#stackPreempt">const stackPreempt</a>

```
searchKey: runtime.stackPreempt
```

```Go
const stackPreempt = uintptrMask & -1314
```

Goroutine preemption request. 0xfffffade in hex. 

### <a id="stackFork" href="#stackFork">const stackFork</a>

```
searchKey: runtime.stackFork
```

```Go
const stackFork = uintptrMask & -1234
```

Thread is forking. Causes a split stack check failure. 0xfffffb2e in hex. 

### <a id="stackForceMove" href="#stackForceMove">const stackForceMove</a>

```
searchKey: runtime.stackForceMove
```

```Go
const stackForceMove = uintptrMask & -275
```

Force a stack movement. Used for debugging. 0xfffffeed in hex. 

### <a id="tmpStringBufSize" href="#tmpStringBufSize">const tmpStringBufSize</a>

```
searchKey: runtime.tmpStringBufSize
```

```Go
const tmpStringBufSize = 32
```

The constant is known to the compiler. There is no fundamental theory behind this number. 

### <a id="maxUint" href="#maxUint">const maxUint</a>

```
searchKey: runtime.maxUint
```

```Go
const maxUint = ^uint(0)
```

### <a id="maxInt" href="#maxInt">const maxInt</a>

```
searchKey: runtime.maxInt
```

```Go
const maxInt = int(maxUint >> 1)
```

### <a id="_PCDATA_UnsafePoint" href="#_PCDATA_UnsafePoint">const _PCDATA_UnsafePoint</a>

```
searchKey: runtime._PCDATA_UnsafePoint
```

```Go
const _PCDATA_UnsafePoint = 0
```

PCDATA and FUNCDATA table indexes. 

See funcdata.h and ../cmd/internal/objabi/funcdata.go. 

### <a id="_PCDATA_StackMapIndex" href="#_PCDATA_StackMapIndex">const _PCDATA_StackMapIndex</a>

```
searchKey: runtime._PCDATA_StackMapIndex
```

```Go
const _PCDATA_StackMapIndex = 1
```

PCDATA and FUNCDATA table indexes. 

See funcdata.h and ../cmd/internal/objabi/funcdata.go. 

### <a id="_PCDATA_InlTreeIndex" href="#_PCDATA_InlTreeIndex">const _PCDATA_InlTreeIndex</a>

```
searchKey: runtime._PCDATA_InlTreeIndex
```

```Go
const _PCDATA_InlTreeIndex = 2
```

PCDATA and FUNCDATA table indexes. 

See funcdata.h and ../cmd/internal/objabi/funcdata.go. 

### <a id="_FUNCDATA_ArgsPointerMaps" href="#_FUNCDATA_ArgsPointerMaps">const _FUNCDATA_ArgsPointerMaps</a>

```
searchKey: runtime._FUNCDATA_ArgsPointerMaps
```

```Go
const _FUNCDATA_ArgsPointerMaps = 0
```

PCDATA and FUNCDATA table indexes. 

See funcdata.h and ../cmd/internal/objabi/funcdata.go. 

### <a id="_FUNCDATA_LocalsPointerMaps" href="#_FUNCDATA_LocalsPointerMaps">const _FUNCDATA_LocalsPointerMaps</a>

```
searchKey: runtime._FUNCDATA_LocalsPointerMaps
```

```Go
const _FUNCDATA_LocalsPointerMaps = 1
```

PCDATA and FUNCDATA table indexes. 

See funcdata.h and ../cmd/internal/objabi/funcdata.go. 

### <a id="_FUNCDATA_StackObjects" href="#_FUNCDATA_StackObjects">const _FUNCDATA_StackObjects</a>

```
searchKey: runtime._FUNCDATA_StackObjects
```

```Go
const _FUNCDATA_StackObjects = 2
```

PCDATA and FUNCDATA table indexes. 

See funcdata.h and ../cmd/internal/objabi/funcdata.go. 

### <a id="_FUNCDATA_InlTree" href="#_FUNCDATA_InlTree">const _FUNCDATA_InlTree</a>

```
searchKey: runtime._FUNCDATA_InlTree
```

```Go
const _FUNCDATA_InlTree = 3
```

PCDATA and FUNCDATA table indexes. 

See funcdata.h and ../cmd/internal/objabi/funcdata.go. 

### <a id="_FUNCDATA_OpenCodedDeferInfo" href="#_FUNCDATA_OpenCodedDeferInfo">const _FUNCDATA_OpenCodedDeferInfo</a>

```
searchKey: runtime._FUNCDATA_OpenCodedDeferInfo
```

```Go
const _FUNCDATA_OpenCodedDeferInfo = 4
```

PCDATA and FUNCDATA table indexes. 

See funcdata.h and ../cmd/internal/objabi/funcdata.go. 

### <a id="_FUNCDATA_ArgInfo" href="#_FUNCDATA_ArgInfo">const _FUNCDATA_ArgInfo</a>

```
searchKey: runtime._FUNCDATA_ArgInfo
```

```Go
const _FUNCDATA_ArgInfo = 5
```

PCDATA and FUNCDATA table indexes. 

See funcdata.h and ../cmd/internal/objabi/funcdata.go. 

### <a id="_ArgsSizeUnknown" href="#_ArgsSizeUnknown">const _ArgsSizeUnknown</a>

```
searchKey: runtime._ArgsSizeUnknown
```

```Go
const _ArgsSizeUnknown = -0x80000000
```

PCDATA and FUNCDATA table indexes. 

See funcdata.h and ../cmd/internal/objabi/funcdata.go. 

### <a id="_PCDATA_UnsafePointSafe" href="#_PCDATA_UnsafePointSafe">const _PCDATA_UnsafePointSafe</a>

```
searchKey: runtime._PCDATA_UnsafePointSafe
```

```Go
const _PCDATA_UnsafePointSafe = -1 // Safe for async preemption

```

PCDATA_UnsafePoint values. 

### <a id="_PCDATA_UnsafePointUnsafe" href="#_PCDATA_UnsafePointUnsafe">const _PCDATA_UnsafePointUnsafe</a>

```
searchKey: runtime._PCDATA_UnsafePointUnsafe
```

```Go
const _PCDATA_UnsafePointUnsafe = -2 // Unsafe for async preemption

```

### <a id="_PCDATA_Restart1" href="#_PCDATA_Restart1">const _PCDATA_Restart1</a>

```
searchKey: runtime._PCDATA_Restart1
```

```Go
const _PCDATA_Restart1 = -3
```

_PCDATA_Restart1(2) apply on a sequence of instructions, within which if an async preemption happens, we should back off the PC to the start of the sequence when resume. We need two so we can distinguish the start/end of the sequence in case that two sequences are next to each other. 

### <a id="_PCDATA_Restart2" href="#_PCDATA_Restart2">const _PCDATA_Restart2</a>

```
searchKey: runtime._PCDATA_Restart2
```

```Go
const _PCDATA_Restart2 = -4
```

### <a id="_PCDATA_RestartAtEntry" href="#_PCDATA_RestartAtEntry">const _PCDATA_RestartAtEntry</a>

```
searchKey: runtime._PCDATA_RestartAtEntry
```

```Go
const _PCDATA_RestartAtEntry = -5
```

Like _PCDATA_RestartAtEntry, but back to function entry if async preempted. 

### <a id="funcID_normal" href="#funcID_normal">const funcID_normal</a>

```
searchKey: runtime.funcID_normal
```

```Go
const funcID_normal funcID = iota // not a special function

```

### <a id="funcID_abort" href="#funcID_abort">const funcID_abort</a>

```
searchKey: runtime.funcID_abort
```

```Go
const funcID_abort
```

### <a id="funcID_asmcgocall" href="#funcID_asmcgocall">const funcID_asmcgocall</a>

```
searchKey: runtime.funcID_asmcgocall
```

```Go
const funcID_asmcgocall
```

### <a id="funcID_asyncPreempt" href="#funcID_asyncPreempt">const funcID_asyncPreempt</a>

```
searchKey: runtime.funcID_asyncPreempt
```

```Go
const funcID_asyncPreempt
```

### <a id="funcID_cgocallback" href="#funcID_cgocallback">const funcID_cgocallback</a>

```
searchKey: runtime.funcID_cgocallback
```

```Go
const funcID_cgocallback
```

### <a id="funcID_debugCallV2" href="#funcID_debugCallV2">const funcID_debugCallV2</a>

```
searchKey: runtime.funcID_debugCallV2
```

```Go
const funcID_debugCallV2
```

### <a id="funcID_gcBgMarkWorker" href="#funcID_gcBgMarkWorker">const funcID_gcBgMarkWorker</a>

```
searchKey: runtime.funcID_gcBgMarkWorker
```

```Go
const funcID_gcBgMarkWorker
```

### <a id="funcID_goexit" href="#funcID_goexit">const funcID_goexit</a>

```
searchKey: runtime.funcID_goexit
```

```Go
const funcID_goexit
```

### <a id="funcID_gogo" href="#funcID_gogo">const funcID_gogo</a>

```
searchKey: runtime.funcID_gogo
```

```Go
const funcID_gogo
```

### <a id="funcID_gopanic" href="#funcID_gopanic">const funcID_gopanic</a>

```
searchKey: runtime.funcID_gopanic
```

```Go
const funcID_gopanic
```

### <a id="funcID_handleAsyncEvent" href="#funcID_handleAsyncEvent">const funcID_handleAsyncEvent</a>

```
searchKey: runtime.funcID_handleAsyncEvent
```

```Go
const funcID_handleAsyncEvent
```

### <a id="funcID_jmpdefer" href="#funcID_jmpdefer">const funcID_jmpdefer</a>

```
searchKey: runtime.funcID_jmpdefer
```

```Go
const funcID_jmpdefer
```

### <a id="funcID_mcall" href="#funcID_mcall">const funcID_mcall</a>

```
searchKey: runtime.funcID_mcall
```

```Go
const funcID_mcall
```

### <a id="funcID_morestack" href="#funcID_morestack">const funcID_morestack</a>

```
searchKey: runtime.funcID_morestack
```

```Go
const funcID_morestack
```

### <a id="funcID_mstart" href="#funcID_mstart">const funcID_mstart</a>

```
searchKey: runtime.funcID_mstart
```

```Go
const funcID_mstart
```

### <a id="funcID_panicwrap" href="#funcID_panicwrap">const funcID_panicwrap</a>

```
searchKey: runtime.funcID_panicwrap
```

```Go
const funcID_panicwrap
```

### <a id="funcID_rt0_go" href="#funcID_rt0_go">const funcID_rt0_go</a>

```
searchKey: runtime.funcID_rt0_go
```

```Go
const funcID_rt0_go
```

### <a id="funcID_runfinq" href="#funcID_runfinq">const funcID_runfinq</a>

```
searchKey: runtime.funcID_runfinq
```

```Go
const funcID_runfinq
```

### <a id="funcID_runtime_main" href="#funcID_runtime_main">const funcID_runtime_main</a>

```
searchKey: runtime.funcID_runtime_main
```

```Go
const funcID_runtime_main
```

### <a id="funcID_sigpanic" href="#funcID_sigpanic">const funcID_sigpanic</a>

```
searchKey: runtime.funcID_sigpanic
```

```Go
const funcID_sigpanic
```

### <a id="funcID_systemstack" href="#funcID_systemstack">const funcID_systemstack</a>

```
searchKey: runtime.funcID_systemstack
```

```Go
const funcID_systemstack
```

### <a id="funcID_systemstack_switch" href="#funcID_systemstack_switch">const funcID_systemstack_switch</a>

```
searchKey: runtime.funcID_systemstack_switch
```

```Go
const funcID_systemstack_switch
```

### <a id="funcID_wrapper" href="#funcID_wrapper">const funcID_wrapper</a>

```
searchKey: runtime.funcID_wrapper
```

```Go
const funcID_wrapper // any autogenerated code (hash/eq algorithms, method wrappers, etc.)

```

### <a id="funcFlag_TOPFRAME" href="#funcFlag_TOPFRAME">const funcFlag_TOPFRAME</a>

```
searchKey: runtime.funcFlag_TOPFRAME
```

```Go
const funcFlag_TOPFRAME funcFlag = 1 << iota
```

TOPFRAME indicates a function that appears at the top of its stack. The traceback routine stop at such a function and consider that a successful, complete traversal of the stack. Examples of TOPFRAME functions include goexit, which appears at the top of a user goroutine stack, and mstart, which appears at the top of a system goroutine stack. 

### <a id="funcFlag_SPWRITE" href="#funcFlag_SPWRITE">const funcFlag_SPWRITE</a>

```
searchKey: runtime.funcFlag_SPWRITE
```

```Go
const funcFlag_SPWRITE
```

SPWRITE indicates a function that writes an arbitrary value to SP (any write other than adding or subtracting a constant amount). The traceback routines cannot encode such changes into the pcsp tables, so the function traceback cannot safely unwind past SPWRITE functions. Stopping at an SPWRITE function is considered to be an incomplete unwinding of the stack. In certain contexts (in particular garbage collector stack scans) that is a fatal error. 

### <a id="minfunc" href="#minfunc">const minfunc</a>

```
searchKey: runtime.minfunc
```

```Go
const minfunc = 16 // minimum function size

```

### <a id="pcbucketsize" href="#pcbucketsize">const pcbucketsize</a>

```
searchKey: runtime.pcbucketsize
```

```Go
const pcbucketsize = 256 * minfunc // size of bucket in the pc->func lookup table

```

### <a id="debugPcln" href="#debugPcln">const debugPcln</a>

```
searchKey: runtime.debugPcln
```

```Go
const debugPcln = false
```

### <a id="timerNoStatus" href="#timerNoStatus">const timerNoStatus</a>

```
searchKey: runtime.timerNoStatus
```

```Go
const timerNoStatus = iota
```

Values for the timer status field. 

Timer has no status set yet. 

### <a id="timerWaiting" href="#timerWaiting">const timerWaiting</a>

```
searchKey: runtime.timerWaiting
```

```Go
const timerWaiting
```

Values for the timer status field. 

Waiting for timer to fire. The timer is in some P's heap. 

### <a id="timerRunning" href="#timerRunning">const timerRunning</a>

```
searchKey: runtime.timerRunning
```

```Go
const timerRunning
```

Values for the timer status field. 

Running the timer function. A timer will only have this status briefly. 

### <a id="timerDeleted" href="#timerDeleted">const timerDeleted</a>

```
searchKey: runtime.timerDeleted
```

```Go
const timerDeleted
```

Values for the timer status field. 

The timer is deleted and should be removed. It should not be run, but it is still in some P's heap. 

### <a id="timerRemoving" href="#timerRemoving">const timerRemoving</a>

```
searchKey: runtime.timerRemoving
```

```Go
const timerRemoving
```

Values for the timer status field. 

The timer is being removed. The timer will only have this status briefly. 

### <a id="timerRemoved" href="#timerRemoved">const timerRemoved</a>

```
searchKey: runtime.timerRemoved
```

```Go
const timerRemoved
```

Values for the timer status field. 

The timer has been stopped. It is not in any P's heap. 

### <a id="timerModifying" href="#timerModifying">const timerModifying</a>

```
searchKey: runtime.timerModifying
```

```Go
const timerModifying
```

Values for the timer status field. 

The timer is being modified. The timer will only have this status briefly. 

### <a id="timerModifiedEarlier" href="#timerModifiedEarlier">const timerModifiedEarlier</a>

```
searchKey: runtime.timerModifiedEarlier
```

```Go
const timerModifiedEarlier
```

Values for the timer status field. 

The timer has been modified to an earlier time. The new when value is in the nextwhen field. The timer is in some P's heap, possibly in the wrong place. 

### <a id="timerModifiedLater" href="#timerModifiedLater">const timerModifiedLater</a>

```
searchKey: runtime.timerModifiedLater
```

```Go
const timerModifiedLater
```

Values for the timer status field. 

The timer has been modified to the same or a later time. The new when value is in the nextwhen field. The timer is in some P's heap, possibly in the wrong place. 

### <a id="timerMoving" href="#timerMoving">const timerMoving</a>

```
searchKey: runtime.timerMoving
```

```Go
const timerMoving
```

Values for the timer status field. 

The timer has been modified and is being moved. The timer will only have this status briefly. 

### <a id="maxWhen" href="#maxWhen">const maxWhen</a>

```
searchKey: runtime.maxWhen
```

```Go
const maxWhen = 1<<63 - 1
```

maxWhen is the maximum value for timer's when field. 

### <a id="verifyTimers" href="#verifyTimers">const verifyTimers</a>

```
searchKey: runtime.verifyTimers
```

```Go
const verifyTimers = false
```

verifyTimers can be set to true to add debugging checks that the timer heaps are valid. 

### <a id="traceEvNone" href="#traceEvNone">const traceEvNone</a>

```
searchKey: runtime.traceEvNone
```

```Go
const traceEvNone = 0 // unused

```

Event types in the trace, args are given in square brackets. 

### <a id="traceEvBatch" href="#traceEvBatch">const traceEvBatch</a>

```
searchKey: runtime.traceEvBatch
```

```Go
const traceEvBatch = 1 // start of per-P batch of events [pid, timestamp]

```

Event types in the trace, args are given in square brackets. 

### <a id="traceEvFrequency" href="#traceEvFrequency">const traceEvFrequency</a>

```
searchKey: runtime.traceEvFrequency
```

```Go
const traceEvFrequency = 2 // contains tracer timer frequency [frequency (ticks per second)]

```

Event types in the trace, args are given in square brackets. 

### <a id="traceEvStack" href="#traceEvStack">const traceEvStack</a>

```
searchKey: runtime.traceEvStack
```

```Go
const traceEvStack // stack [stack id, number of PCs, array of {PC, func string ID, file string ID, line}]
 = ...
```

Event types in the trace, args are given in square brackets. 

### <a id="traceEvGomaxprocs" href="#traceEvGomaxprocs">const traceEvGomaxprocs</a>

```
searchKey: runtime.traceEvGomaxprocs
```

```Go
const traceEvGomaxprocs = 4 // current value of GOMAXPROCS [timestamp, GOMAXPROCS, stack id]

```

Event types in the trace, args are given in square brackets. 

### <a id="traceEvProcStart" href="#traceEvProcStart">const traceEvProcStart</a>

```
searchKey: runtime.traceEvProcStart
```

```Go
const traceEvProcStart = 5 // start of P [timestamp, thread id]

```

Event types in the trace, args are given in square brackets. 

### <a id="traceEvProcStop" href="#traceEvProcStop">const traceEvProcStop</a>

```
searchKey: runtime.traceEvProcStop
```

```Go
const traceEvProcStop = 6 // stop of P [timestamp]

```

Event types in the trace, args are given in square brackets. 

### <a id="traceEvGCStart" href="#traceEvGCStart">const traceEvGCStart</a>

```
searchKey: runtime.traceEvGCStart
```

```Go
const traceEvGCStart = 7 // GC start [timestamp, seq, stack id]

```

Event types in the trace, args are given in square brackets. 

### <a id="traceEvGCDone" href="#traceEvGCDone">const traceEvGCDone</a>

```
searchKey: runtime.traceEvGCDone
```

```Go
const traceEvGCDone = 8 // GC done [timestamp]

```

Event types in the trace, args are given in square brackets. 

### <a id="traceEvGCSTWStart" href="#traceEvGCSTWStart">const traceEvGCSTWStart</a>

```
searchKey: runtime.traceEvGCSTWStart
```

```Go
const traceEvGCSTWStart = 9 // GC STW start [timestamp, kind]

```

Event types in the trace, args are given in square brackets. 

### <a id="traceEvGCSTWDone" href="#traceEvGCSTWDone">const traceEvGCSTWDone</a>

```
searchKey: runtime.traceEvGCSTWDone
```

```Go
const traceEvGCSTWDone = 10 // GC STW done [timestamp]

```

Event types in the trace, args are given in square brackets. 

### <a id="traceEvGCSweepStart" href="#traceEvGCSweepStart">const traceEvGCSweepStart</a>

```
searchKey: runtime.traceEvGCSweepStart
```

```Go
const traceEvGCSweepStart = 11 // GC sweep start [timestamp, stack id]

```

Event types in the trace, args are given in square brackets. 

### <a id="traceEvGCSweepDone" href="#traceEvGCSweepDone">const traceEvGCSweepDone</a>

```
searchKey: runtime.traceEvGCSweepDone
```

```Go
const traceEvGCSweepDone = 12 // GC sweep done [timestamp, swept, reclaimed]

```

Event types in the trace, args are given in square brackets. 

### <a id="traceEvGoCreate" href="#traceEvGoCreate">const traceEvGoCreate</a>

```
searchKey: runtime.traceEvGoCreate
```

```Go
const traceEvGoCreate // goroutine creation [timestamp, new goroutine id, new stack id, stack id]
 = ...
```

Event types in the trace, args are given in square brackets. 

### <a id="traceEvGoStart" href="#traceEvGoStart">const traceEvGoStart</a>

```
searchKey: runtime.traceEvGoStart
```

```Go
const traceEvGoStart = 14 // goroutine starts running [timestamp, goroutine id, seq]

```

Event types in the trace, args are given in square brackets. 

### <a id="traceEvGoEnd" href="#traceEvGoEnd">const traceEvGoEnd</a>

```
searchKey: runtime.traceEvGoEnd
```

```Go
const traceEvGoEnd = 15 // goroutine ends [timestamp]

```

Event types in the trace, args are given in square brackets. 

### <a id="traceEvGoStop" href="#traceEvGoStop">const traceEvGoStop</a>

```
searchKey: runtime.traceEvGoStop
```

```Go
const traceEvGoStop = 16 // goroutine stops (like in select{}) [timestamp, stack]

```

Event types in the trace, args are given in square brackets. 

### <a id="traceEvGoSched" href="#traceEvGoSched">const traceEvGoSched</a>

```
searchKey: runtime.traceEvGoSched
```

```Go
const traceEvGoSched = 17 // goroutine calls Gosched [timestamp, stack]

```

Event types in the trace, args are given in square brackets. 

### <a id="traceEvGoPreempt" href="#traceEvGoPreempt">const traceEvGoPreempt</a>

```
searchKey: runtime.traceEvGoPreempt
```

```Go
const traceEvGoPreempt = 18 // goroutine is preempted [timestamp, stack]

```

Event types in the trace, args are given in square brackets. 

### <a id="traceEvGoSleep" href="#traceEvGoSleep">const traceEvGoSleep</a>

```
searchKey: runtime.traceEvGoSleep
```

```Go
const traceEvGoSleep = 19 // goroutine calls Sleep [timestamp, stack]

```

Event types in the trace, args are given in square brackets. 

### <a id="traceEvGoBlock" href="#traceEvGoBlock">const traceEvGoBlock</a>

```
searchKey: runtime.traceEvGoBlock
```

```Go
const traceEvGoBlock = 20 // goroutine blocks [timestamp, stack]

```

Event types in the trace, args are given in square brackets. 

### <a id="traceEvGoUnblock" href="#traceEvGoUnblock">const traceEvGoUnblock</a>

```
searchKey: runtime.traceEvGoUnblock
```

```Go
const traceEvGoUnblock = 21 // goroutine is unblocked [timestamp, goroutine id, seq, stack]

```

Event types in the trace, args are given in square brackets. 

### <a id="traceEvGoBlockSend" href="#traceEvGoBlockSend">const traceEvGoBlockSend</a>

```
searchKey: runtime.traceEvGoBlockSend
```

```Go
const traceEvGoBlockSend = 22 // goroutine blocks on chan send [timestamp, stack]

```

Event types in the trace, args are given in square brackets. 

### <a id="traceEvGoBlockRecv" href="#traceEvGoBlockRecv">const traceEvGoBlockRecv</a>

```
searchKey: runtime.traceEvGoBlockRecv
```

```Go
const traceEvGoBlockRecv = 23 // goroutine blocks on chan recv [timestamp, stack]

```

Event types in the trace, args are given in square brackets. 

### <a id="traceEvGoBlockSelect" href="#traceEvGoBlockSelect">const traceEvGoBlockSelect</a>

```
searchKey: runtime.traceEvGoBlockSelect
```

```Go
const traceEvGoBlockSelect = 24 // goroutine blocks on select [timestamp, stack]

```

Event types in the trace, args are given in square brackets. 

### <a id="traceEvGoBlockSync" href="#traceEvGoBlockSync">const traceEvGoBlockSync</a>

```
searchKey: runtime.traceEvGoBlockSync
```

```Go
const traceEvGoBlockSync = 25 // goroutine blocks on Mutex/RWMutex [timestamp, stack]

```

Event types in the trace, args are given in square brackets. 

### <a id="traceEvGoBlockCond" href="#traceEvGoBlockCond">const traceEvGoBlockCond</a>

```
searchKey: runtime.traceEvGoBlockCond
```

```Go
const traceEvGoBlockCond = 26 // goroutine blocks on Cond [timestamp, stack]

```

Event types in the trace, args are given in square brackets. 

### <a id="traceEvGoBlockNet" href="#traceEvGoBlockNet">const traceEvGoBlockNet</a>

```
searchKey: runtime.traceEvGoBlockNet
```

```Go
const traceEvGoBlockNet = 27 // goroutine blocks on network [timestamp, stack]

```

Event types in the trace, args are given in square brackets. 

### <a id="traceEvGoSysCall" href="#traceEvGoSysCall">const traceEvGoSysCall</a>

```
searchKey: runtime.traceEvGoSysCall
```

```Go
const traceEvGoSysCall = 28 // syscall enter [timestamp, stack]

```

Event types in the trace, args are given in square brackets. 

### <a id="traceEvGoSysExit" href="#traceEvGoSysExit">const traceEvGoSysExit</a>

```
searchKey: runtime.traceEvGoSysExit
```

```Go
const traceEvGoSysExit = 29 // syscall exit [timestamp, goroutine id, seq, real timestamp]

```

Event types in the trace, args are given in square brackets. 

### <a id="traceEvGoSysBlock" href="#traceEvGoSysBlock">const traceEvGoSysBlock</a>

```
searchKey: runtime.traceEvGoSysBlock
```

```Go
const traceEvGoSysBlock = 30 // syscall blocks [timestamp]

```

Event types in the trace, args are given in square brackets. 

### <a id="traceEvGoWaiting" href="#traceEvGoWaiting">const traceEvGoWaiting</a>

```
searchKey: runtime.traceEvGoWaiting
```

```Go
const traceEvGoWaiting // denotes that goroutine is blocked when tracing starts [timestamp, goroutine id]
 = ...
```

Event types in the trace, args are given in square brackets. 

### <a id="traceEvGoInSyscall" href="#traceEvGoInSyscall">const traceEvGoInSyscall</a>

```
searchKey: runtime.traceEvGoInSyscall
```

```Go
const traceEvGoInSyscall // denotes that goroutine is in syscall when tracing starts [timestamp, goroutine id]
 = ...
```

Event types in the trace, args are given in square brackets. 

### <a id="traceEvHeapAlloc" href="#traceEvHeapAlloc">const traceEvHeapAlloc</a>

```
searchKey: runtime.traceEvHeapAlloc
```

```Go
const traceEvHeapAlloc = 33 // gcController.heapLive change [timestamp, heap_alloc]

```

Event types in the trace, args are given in square brackets. 

### <a id="traceEvHeapGoal" href="#traceEvHeapGoal">const traceEvHeapGoal</a>

```
searchKey: runtime.traceEvHeapGoal
```

```Go
const traceEvHeapGoal // gcController.heapGoal (formerly next_gc) change [timestamp, heap goal in bytes]
 = ...
```

Event types in the trace, args are given in square brackets. 

### <a id="traceEvTimerGoroutine" href="#traceEvTimerGoroutine">const traceEvTimerGoroutine</a>

```
searchKey: runtime.traceEvTimerGoroutine
```

```Go
const traceEvTimerGoroutine // not currently used; previously denoted timer goroutine [timer goroutine id]
 = ...
```

Event types in the trace, args are given in square brackets. 

### <a id="traceEvFutileWakeup" href="#traceEvFutileWakeup">const traceEvFutileWakeup</a>

```
searchKey: runtime.traceEvFutileWakeup
```

```Go
const traceEvFutileWakeup // denotes that the previous wakeup of this goroutine was futile [timestamp]
 = ...
```

Event types in the trace, args are given in square brackets. 

### <a id="traceEvString" href="#traceEvString">const traceEvString</a>

```
searchKey: runtime.traceEvString
```

```Go
const traceEvString = 37 // string dictionary entry [ID, length, string]

```

Event types in the trace, args are given in square brackets. 

### <a id="traceEvGoStartLocal" href="#traceEvGoStartLocal">const traceEvGoStartLocal</a>

```
searchKey: runtime.traceEvGoStartLocal
```

```Go
const traceEvGoStartLocal // goroutine starts running on the same P as the last event [timestamp, goroutine id]
 = ...
```

Event types in the trace, args are given in square brackets. 

### <a id="traceEvGoUnblockLocal" href="#traceEvGoUnblockLocal">const traceEvGoUnblockLocal</a>

```
searchKey: runtime.traceEvGoUnblockLocal
```

```Go
const traceEvGoUnblockLocal // goroutine is unblocked on the same P as the last event [timestamp, goroutine id, stack]
 = ...
```

Event types in the trace, args are given in square brackets. 

### <a id="traceEvGoSysExitLocal" href="#traceEvGoSysExitLocal">const traceEvGoSysExitLocal</a>

```
searchKey: runtime.traceEvGoSysExitLocal
```

```Go
const traceEvGoSysExitLocal // syscall exit on the same P as the last event [timestamp, goroutine id, real timestamp]
 = ...
```

Event types in the trace, args are given in square brackets. 

### <a id="traceEvGoStartLabel" href="#traceEvGoStartLabel">const traceEvGoStartLabel</a>

```
searchKey: runtime.traceEvGoStartLabel
```

```Go
const traceEvGoStartLabel // goroutine starts running with label [timestamp, goroutine id, seq, label string id]
 = ...
```

Event types in the trace, args are given in square brackets. 

### <a id="traceEvGoBlockGC" href="#traceEvGoBlockGC">const traceEvGoBlockGC</a>

```
searchKey: runtime.traceEvGoBlockGC
```

```Go
const traceEvGoBlockGC = 42 // goroutine blocks on GC assist [timestamp, stack]

```

Event types in the trace, args are given in square brackets. 

### <a id="traceEvGCMarkAssistStart" href="#traceEvGCMarkAssistStart">const traceEvGCMarkAssistStart</a>

```
searchKey: runtime.traceEvGCMarkAssistStart
```

```Go
const traceEvGCMarkAssistStart = 43 // GC mark assist start [timestamp, stack]

```

Event types in the trace, args are given in square brackets. 

### <a id="traceEvGCMarkAssistDone" href="#traceEvGCMarkAssistDone">const traceEvGCMarkAssistDone</a>

```
searchKey: runtime.traceEvGCMarkAssistDone
```

```Go
const traceEvGCMarkAssistDone = 44 // GC mark assist done [timestamp]

```

Event types in the trace, args are given in square brackets. 

### <a id="traceEvUserTaskCreate" href="#traceEvUserTaskCreate">const traceEvUserTaskCreate</a>

```
searchKey: runtime.traceEvUserTaskCreate
```

```Go
const traceEvUserTaskCreate // trace.NewContext [timestamp, internal task id, internal parent task id, stack, name string]
 = ...
```

Event types in the trace, args are given in square brackets. 

### <a id="traceEvUserTaskEnd" href="#traceEvUserTaskEnd">const traceEvUserTaskEnd</a>

```
searchKey: runtime.traceEvUserTaskEnd
```

```Go
const traceEvUserTaskEnd = 46 // end of a task [timestamp, internal task id, stack]

```

Event types in the trace, args are given in square brackets. 

### <a id="traceEvUserRegion" href="#traceEvUserRegion">const traceEvUserRegion</a>

```
searchKey: runtime.traceEvUserRegion
```

```Go
const traceEvUserRegion // trace.WithRegion [timestamp, internal task id, mode(0:start, 1:end), stack, name string]
 = ...
```

Event types in the trace, args are given in square brackets. 

### <a id="traceEvUserLog" href="#traceEvUserLog">const traceEvUserLog</a>

```
searchKey: runtime.traceEvUserLog
```

```Go
const traceEvUserLog // trace.Log [timestamp, internal task id, key string id, stack, value string]
 = ...
```

Event types in the trace, args are given in square brackets. 

### <a id="traceEvCount" href="#traceEvCount">const traceEvCount</a>

```
searchKey: runtime.traceEvCount
```

```Go
const traceEvCount = 49
```

Event types in the trace, args are given in square brackets. 

### <a id="traceTickDiv" href="#traceTickDiv">const traceTickDiv</a>

```
searchKey: runtime.traceTickDiv
```

```Go
const traceTickDiv = 16 + 48*(sys.Goarch386|sys.GoarchAmd64)
```

Timestamps in trace are cputicks/traceTickDiv. This makes absolute values of timestamp diffs smaller, and so they are encoded in less number of bytes. 64 on x86 is somewhat arbitrary (one tick is ~20ns on a 3GHz machine). The suggested increment frequency for PowerPC's time base register is 512 MHz according to Power ISA v2.07 section 6.2, so we use 16 on ppc64 and ppc64le. Tracing won't work reliably for architectures where cputicks is emulated by nanotime, so the value doesn't matter for those architectures. 

### <a id="traceStackSize" href="#traceStackSize">const traceStackSize</a>

```
searchKey: runtime.traceStackSize
```

```Go
const traceStackSize = 128
```

Maximum number of PCs in a single stack trace. Since events contain only stack id rather than whole stack trace, we can allow quite large values here. 

### <a id="traceGlobProc" href="#traceGlobProc">const traceGlobProc</a>

```
searchKey: runtime.traceGlobProc
```

```Go
const traceGlobProc = -1
```

Identifier of a fake P that is used when we trace without a real P. 

### <a id="traceBytesPerNumber" href="#traceBytesPerNumber">const traceBytesPerNumber</a>

```
searchKey: runtime.traceBytesPerNumber
```

```Go
const traceBytesPerNumber = 10
```

Maximum number of bytes to encode uint64 in base-128. 

### <a id="traceArgCountShift" href="#traceArgCountShift">const traceArgCountShift</a>

```
searchKey: runtime.traceArgCountShift
```

```Go
const traceArgCountShift = 6
```

Shift of the number of arguments in the first event byte. 

### <a id="traceFutileWakeup" href="#traceFutileWakeup">const traceFutileWakeup</a>

```
searchKey: runtime.traceFutileWakeup
```

```Go
const traceFutileWakeup byte = 128
```

Flag passed to traceGoPark to denote that the previous wakeup of this goroutine was futile. For example, a goroutine was unblocked on a mutex, but another goroutine got ahead and acquired the mutex before the first goroutine is scheduled, so the first goroutine has to block again. Such wakeups happen on buffered channels and sync.Mutex, but are generally not interesting for end user. 

### <a id="usesLR" href="#usesLR">const usesLR</a>

```
searchKey: runtime.usesLR
```

```Go
const usesLR = sys.MinFrameSize > 0
```

### <a id="tflagUncommon" href="#tflagUncommon">const tflagUncommon</a>

```
searchKey: runtime.tflagUncommon
```

```Go
const tflagUncommon tflag = 1 << 0
```

### <a id="tflagExtraStar" href="#tflagExtraStar">const tflagExtraStar</a>

```
searchKey: runtime.tflagExtraStar
```

```Go
const tflagExtraStar tflag = 1 << 1
```

### <a id="tflagNamed" href="#tflagNamed">const tflagNamed</a>

```
searchKey: runtime.tflagNamed
```

```Go
const tflagNamed tflag = 1 << 2
```

### <a id="tflagRegularMemory" href="#tflagRegularMemory">const tflagRegularMemory</a>

```
searchKey: runtime.tflagRegularMemory
```

```Go
const tflagRegularMemory tflag // equal and hash can treat values of this type as a single region of t.size bytes
 = ...
```

### <a id="kindBool" href="#kindBool">const kindBool</a>

```
searchKey: runtime.kindBool
```

```Go
const kindBool = 1 + iota
```

### <a id="kindInt" href="#kindInt">const kindInt</a>

```
searchKey: runtime.kindInt
```

```Go
const kindInt
```

### <a id="kindInt8" href="#kindInt8">const kindInt8</a>

```
searchKey: runtime.kindInt8
```

```Go
const kindInt8
```

### <a id="kindInt16" href="#kindInt16">const kindInt16</a>

```
searchKey: runtime.kindInt16
```

```Go
const kindInt16
```

### <a id="kindInt32" href="#kindInt32">const kindInt32</a>

```
searchKey: runtime.kindInt32
```

```Go
const kindInt32
```

### <a id="kindInt64" href="#kindInt64">const kindInt64</a>

```
searchKey: runtime.kindInt64
```

```Go
const kindInt64
```

### <a id="kindUint" href="#kindUint">const kindUint</a>

```
searchKey: runtime.kindUint
```

```Go
const kindUint
```

### <a id="kindUint8" href="#kindUint8">const kindUint8</a>

```
searchKey: runtime.kindUint8
```

```Go
const kindUint8
```

### <a id="kindUint16" href="#kindUint16">const kindUint16</a>

```
searchKey: runtime.kindUint16
```

```Go
const kindUint16
```

### <a id="kindUint32" href="#kindUint32">const kindUint32</a>

```
searchKey: runtime.kindUint32
```

```Go
const kindUint32
```

### <a id="kindUint64" href="#kindUint64">const kindUint64</a>

```
searchKey: runtime.kindUint64
```

```Go
const kindUint64
```

### <a id="kindUintptr" href="#kindUintptr">const kindUintptr</a>

```
searchKey: runtime.kindUintptr
```

```Go
const kindUintptr
```

### <a id="kindFloat32" href="#kindFloat32">const kindFloat32</a>

```
searchKey: runtime.kindFloat32
```

```Go
const kindFloat32
```

### <a id="kindFloat64" href="#kindFloat64">const kindFloat64</a>

```
searchKey: runtime.kindFloat64
```

```Go
const kindFloat64
```

### <a id="kindComplex64" href="#kindComplex64">const kindComplex64</a>

```
searchKey: runtime.kindComplex64
```

```Go
const kindComplex64
```

### <a id="kindComplex128" href="#kindComplex128">const kindComplex128</a>

```
searchKey: runtime.kindComplex128
```

```Go
const kindComplex128
```

### <a id="kindArray" href="#kindArray">const kindArray</a>

```
searchKey: runtime.kindArray
```

```Go
const kindArray
```

### <a id="kindChan" href="#kindChan">const kindChan</a>

```
searchKey: runtime.kindChan
```

```Go
const kindChan
```

### <a id="kindFunc" href="#kindFunc">const kindFunc</a>

```
searchKey: runtime.kindFunc
```

```Go
const kindFunc
```

### <a id="kindInterface" href="#kindInterface">const kindInterface</a>

```
searchKey: runtime.kindInterface
```

```Go
const kindInterface
```

### <a id="kindMap" href="#kindMap">const kindMap</a>

```
searchKey: runtime.kindMap
```

```Go
const kindMap
```

### <a id="kindPtr" href="#kindPtr">const kindPtr</a>

```
searchKey: runtime.kindPtr
```

```Go
const kindPtr
```

### <a id="kindSlice" href="#kindSlice">const kindSlice</a>

```
searchKey: runtime.kindSlice
```

```Go
const kindSlice
```

### <a id="kindString" href="#kindString">const kindString</a>

```
searchKey: runtime.kindString
```

```Go
const kindString
```

### <a id="kindStruct" href="#kindStruct">const kindStruct</a>

```
searchKey: runtime.kindStruct
```

```Go
const kindStruct
```

### <a id="kindUnsafePointer" href="#kindUnsafePointer">const kindUnsafePointer</a>

```
searchKey: runtime.kindUnsafePointer
```

```Go
const kindUnsafePointer
```

### <a id="kindDirectIface" href="#kindDirectIface">const kindDirectIface</a>

```
searchKey: runtime.kindDirectIface
```

```Go
const kindDirectIface = 1 << 5
```

### <a id="kindGCProg" href="#kindGCProg">const kindGCProg</a>

```
searchKey: runtime.kindGCProg
```

```Go
const kindGCProg = 1 << 6
```

### <a id="kindMask" href="#kindMask">const kindMask</a>

```
searchKey: runtime.kindMask
```

```Go
const kindMask = (1 << 5) - 1
```

### <a id="runeError" href="#runeError">const runeError</a>

```
searchKey: runtime.runeError
```

```Go
const runeError = '\uFFFD' // the "error" Rune or "Unicode replacement character"

```

Numbers fundamental to the encoding. 

### <a id="runeSelf" href="#runeSelf">const runeSelf</a>

```
searchKey: runtime.runeSelf
```

```Go
const runeSelf = 0x80 // characters below runeSelf are represented as themselves in a single byte.

```

Numbers fundamental to the encoding. 

### <a id="maxRune" href="#maxRune">const maxRune</a>

```
searchKey: runtime.maxRune
```

```Go
const maxRune = '\U0010FFFF' // Maximum valid Unicode code point.

```

Numbers fundamental to the encoding. 

### <a id="surrogateMin" href="#surrogateMin">const surrogateMin</a>

```
searchKey: runtime.surrogateMin
```

```Go
const surrogateMin = 0xD800
```

Code points in the surrogate range are not valid for UTF-8. 

### <a id="surrogateMax" href="#surrogateMax">const surrogateMax</a>

```
searchKey: runtime.surrogateMax
```

```Go
const surrogateMax = 0xDFFF
```

Code points in the surrogate range are not valid for UTF-8. 

### <a id="t1" href="#t1">const t1</a>

```
searchKey: runtime.t1
```

```Go
const t1 = 0x00 // 0000 0000

```

### <a id="tx" href="#tx">const tx</a>

```
searchKey: runtime.tx
```

```Go
const tx = 0x80 // 1000 0000

```

### <a id="t2" href="#t2">const t2</a>

```
searchKey: runtime.t2
```

```Go
const t2 = 0xC0 // 1100 0000

```

### <a id="t3" href="#t3">const t3</a>

```
searchKey: runtime.t3
```

```Go
const t3 = 0xE0 // 1110 0000

```

### <a id="t4" href="#t4">const t4</a>

```
searchKey: runtime.t4
```

```Go
const t4 = 0xF0 // 1111 0000

```

### <a id="t5" href="#t5">const t5</a>

```
searchKey: runtime.t5
```

```Go
const t5 = 0xF8 // 1111 1000

```

### <a id="maskx" href="#maskx">const maskx</a>

```
searchKey: runtime.maskx
```

```Go
const maskx = 0x3F // 0011 1111

```

### <a id="mask2" href="#mask2">const mask2</a>

```
searchKey: runtime.mask2
```

```Go
const mask2 = 0x1F // 0001 1111

```

### <a id="mask3" href="#mask3">const mask3</a>

```
searchKey: runtime.mask3
```

```Go
const mask3 = 0x0F // 0000 1111

```

### <a id="mask4" href="#mask4">const mask4</a>

```
searchKey: runtime.mask4
```

```Go
const mask4 = 0x07 // 0000 0111

```

### <a id="rune1Max" href="#rune1Max">const rune1Max</a>

```
searchKey: runtime.rune1Max
```

```Go
const rune1Max = 1<<7 - 1
```

### <a id="rune2Max" href="#rune2Max">const rune2Max</a>

```
searchKey: runtime.rune2Max
```

```Go
const rune2Max = 1<<11 - 1
```

### <a id="rune3Max" href="#rune3Max">const rune3Max</a>

```
searchKey: runtime.rune3Max
```

```Go
const rune3Max = 1<<16 - 1
```

### <a id="locb" href="#locb">const locb</a>

```
searchKey: runtime.locb
```

```Go
const locb = 0x80 // 1000 0000

```

The default lowest and highest continuation byte. 

### <a id="hicb" href="#hicb">const hicb</a>

```
searchKey: runtime.hicb
```

```Go
const hicb = 0xBF // 1011 1111

```

### <a id="DlogEnabled" href="#DlogEnabled">const DlogEnabled</a>

```
searchKey: runtime.DlogEnabled
```

```Go
const DlogEnabled = dlogEnabled
```

### <a id="DebugLogBytes" href="#DebugLogBytes">const DebugLogBytes</a>

```
searchKey: runtime.DebugLogBytes
```

```Go
const DebugLogBytes = debugLogBytes
```

### <a id="DebugLogStringLimit" href="#DebugLogStringLimit">const DebugLogStringLimit</a>

```
searchKey: runtime.DebugLogStringLimit
```

```Go
const DebugLogStringLimit = debugLogStringLimit
```

### <a id="ENOMEM" href="#ENOMEM">const ENOMEM</a>

```
searchKey: runtime.ENOMEM
```

```Go
const ENOMEM = _ENOMEM
```

### <a id="MAP_ANON" href="#MAP_ANON">const MAP_ANON</a>

```
searchKey: runtime.MAP_ANON
```

```Go
const MAP_ANON = _MAP_ANON
```

### <a id="MAP_PRIVATE" href="#MAP_PRIVATE">const MAP_PRIVATE</a>

```
searchKey: runtime.MAP_PRIVATE
```

```Go
const MAP_PRIVATE = _MAP_PRIVATE
```

### <a id="MAP_FIXED" href="#MAP_FIXED">const MAP_FIXED</a>

```
searchKey: runtime.MAP_FIXED
```

```Go
const MAP_FIXED = _MAP_FIXED
```

### <a id="PreemptMSupported" href="#PreemptMSupported">const PreemptMSupported</a>

```
searchKey: runtime.PreemptMSupported
```

```Go
const PreemptMSupported = preemptMSupported
```

### <a id="PtrSize" href="#PtrSize">const PtrSize</a>

```
searchKey: runtime.PtrSize
```

```Go
const PtrSize = sys.PtrSize
```

### <a id="ProfBufBlocking" href="#ProfBufBlocking">const ProfBufBlocking</a>

```
searchKey: runtime.ProfBufBlocking
```

```Go
const ProfBufBlocking = profBufBlocking
```

### <a id="ProfBufNonBlocking" href="#ProfBufNonBlocking">const ProfBufNonBlocking</a>

```
searchKey: runtime.ProfBufNonBlocking
```

```Go
const ProfBufNonBlocking = profBufNonBlocking
```

### <a id="RuntimeHmapSize" href="#RuntimeHmapSize">const RuntimeHmapSize</a>

```
searchKey: runtime.RuntimeHmapSize
```

```Go
const RuntimeHmapSize = unsafe.Sizeof(hmap{})
```

### <a id="PageSize" href="#PageSize">const PageSize</a>

```
searchKey: runtime.PageSize
```

```Go
const PageSize = pageSize
```

### <a id="PallocChunkPages" href="#PallocChunkPages">const PallocChunkPages</a>

```
searchKey: runtime.PallocChunkPages
```

```Go
const PallocChunkPages = pallocChunkPages
```

### <a id="PageAlloc64Bit" href="#PageAlloc64Bit">const PageAlloc64Bit</a>

```
searchKey: runtime.PageAlloc64Bit
```

```Go
const PageAlloc64Bit = pageAlloc64Bit
```

### <a id="PallocSumBytes" href="#PallocSumBytes">const PallocSumBytes</a>

```
searchKey: runtime.PallocSumBytes
```

```Go
const PallocSumBytes = pallocSumBytes
```

### <a id="PageCachePages" href="#PageCachePages">const PageCachePages</a>

```
searchKey: runtime.PageCachePages
```

```Go
const PageCachePages = pageCachePages
```

### <a id="TimeHistSubBucketBits" href="#TimeHistSubBucketBits">const TimeHistSubBucketBits</a>

```
searchKey: runtime.TimeHistSubBucketBits
```

```Go
const TimeHistSubBucketBits = timeHistSubBucketBits
```

### <a id="TimeHistNumSubBuckets" href="#TimeHistNumSubBuckets">const TimeHistNumSubBuckets</a>

```
searchKey: runtime.TimeHistNumSubBuckets
```

```Go
const TimeHistNumSubBuckets = timeHistNumSubBuckets
```

### <a id="TimeHistNumSuperBuckets" href="#TimeHistNumSuperBuckets">const TimeHistNumSuperBuckets</a>

```
searchKey: runtime.TimeHistNumSuperBuckets
```

```Go
const TimeHistNumSuperBuckets = timeHistNumSuperBuckets
```

### <a id="Raceenabled" href="#Raceenabled">const Raceenabled</a>

```
searchKey: runtime.Raceenabled
```

```Go
const Raceenabled = raceenabled
```

## <a id="var" href="#var">Variables</a>

```
tags: [exported]
```

### <a id="useAeshash" href="#useAeshash">var useAeshash</a>

```
searchKey: runtime.useAeshash
```

```Go
var useAeshash bool
```

runtime variable to check if the processor we're running on actually supports the instructions used by the AES-based hash implementation. 

### <a id="aeskeysched" href="#aeskeysched">var aeskeysched</a>

```
searchKey: runtime.aeskeysched
```

```Go
var aeskeysched [hashRandomBytes]byte
```

used in asm_{386,amd64,arm64}.s to seed the hash function 

### <a id="hashkey" href="#hashkey">var hashkey</a>

```
searchKey: runtime.hashkey
```

```Go
var hashkey [4]uintptr
```

used in hash{32,64}.go to seed the hash function 

### <a id="_cgo_init" href="#_cgo_init">var _cgo_init</a>

```
searchKey: runtime._cgo_init
```

```Go
var _cgo_init unsafe.Pointer
```

### <a id="_cgo_thread_start" href="#_cgo_thread_start">var _cgo_thread_start</a>

```
searchKey: runtime._cgo_thread_start
```

```Go
var _cgo_thread_start unsafe.Pointer
```

### <a id="_cgo_sys_thread_create" href="#_cgo_sys_thread_create">var _cgo_sys_thread_create</a>

```
searchKey: runtime._cgo_sys_thread_create
```

```Go
var _cgo_sys_thread_create unsafe.Pointer
```

### <a id="_cgo_notify_runtime_init_done" href="#_cgo_notify_runtime_init_done">var _cgo_notify_runtime_init_done</a>

```
searchKey: runtime._cgo_notify_runtime_init_done
```

```Go
var _cgo_notify_runtime_init_done unsafe.Pointer
```

### <a id="_cgo_callers" href="#_cgo_callers">var _cgo_callers</a>

```
searchKey: runtime._cgo_callers
```

```Go
var _cgo_callers unsafe.Pointer
```

### <a id="_cgo_set_context_function" href="#_cgo_set_context_function">var _cgo_set_context_function</a>

```
searchKey: runtime._cgo_set_context_function
```

```Go
var _cgo_set_context_function unsafe.Pointer
```

### <a id="_cgo_yield" href="#_cgo_yield">var _cgo_yield</a>

```
searchKey: runtime._cgo_yield
```

```Go
var _cgo_yield unsafe.Pointer
```

### <a id="iscgo" href="#iscgo">var iscgo</a>

```
searchKey: runtime.iscgo
```

```Go
var iscgo bool
```

iscgo is set to true by the runtime/cgo package 

### <a id="cgoHasExtraM" href="#cgoHasExtraM">var cgoHasExtraM</a>

```
searchKey: runtime.cgoHasExtraM
```

```Go
var cgoHasExtraM bool
```

cgoHasExtraM is set on startup when an extra M is created for cgo. The extra M must be created before any C/C++ code calls cgocallback. 

### <a id="cgoAlwaysFalse" href="#cgoAlwaysFalse">var cgoAlwaysFalse</a>

```
searchKey: runtime.cgoAlwaysFalse
```

```Go
var cgoAlwaysFalse bool
```

cgoAlwaysFalse is a boolean value that is always false. The cgo-generated code says if cgoAlwaysFalse { cgoUse(p) }. The compiler cannot see that cgoAlwaysFalse is always false, so it emits the test and keeps the call, giving the desired escape analysis result. The test is cheaper than the call. 

### <a id="cgo_yield" href="#cgo_yield">var cgo_yield</a>

```
searchKey: runtime.cgo_yield
```

```Go
var cgo_yield = &_cgo_yield
```

### <a id="racecgosync" href="#racecgosync">var racecgosync</a>

```
searchKey: runtime.racecgosync
```

```Go
var racecgosync uint64 // represents possible synchronization in C code

```

### <a id="x86HasPOPCNT" href="#x86HasPOPCNT">var x86HasPOPCNT</a>

```
searchKey: runtime.x86HasPOPCNT
```

```Go
var x86HasPOPCNT bool
```

Set in runtime.cpuinit. TODO: deprecate these; use internal/cpu directly. 

### <a id="x86HasSSE41" href="#x86HasSSE41">var x86HasSSE41</a>

```
searchKey: runtime.x86HasSSE41
```

```Go
var x86HasSSE41 bool
```

### <a id="x86HasFMA" href="#x86HasFMA">var x86HasFMA</a>

```
searchKey: runtime.x86HasFMA
```

```Go
var x86HasFMA bool
```

### <a id="armHasVFPv4" href="#armHasVFPv4">var armHasVFPv4</a>

```
searchKey: runtime.armHasVFPv4
```

```Go
var armHasVFPv4 bool
```

### <a id="arm64HasATOMICS" href="#arm64HasATOMICS">var arm64HasATOMICS</a>

```
searchKey: runtime.arm64HasATOMICS
```

```Go
var arm64HasATOMICS bool
```

### <a id="useAVXmemmove" href="#useAVXmemmove">var useAVXmemmove</a>

```
searchKey: runtime.useAVXmemmove
```

```Go
var useAVXmemmove bool
```

### <a id="cpuprof" href="#cpuprof">var cpuprof</a>

```
searchKey: runtime.cpuprof
```

```Go
var cpuprof cpuProfile
```

### <a id="allDloggers" href="#allDloggers">var allDloggers</a>

```
searchKey: runtime.allDloggers
```

```Go
var allDloggers *dlogger
```

allDloggers is a list of all dloggers, linked through dlogger.allLink. This is accessed atomically. This is prepend only, so it doesn't need to protect against ABA races. 

### <a id="_cgo_setenv" href="#_cgo_setenv">var _cgo_setenv</a>

```
searchKey: runtime._cgo_setenv
```

```Go
var _cgo_setenv unsafe.Pointer // pointer to C function

```

### <a id="_cgo_unsetenv" href="#_cgo_unsetenv">var _cgo_unsetenv</a>

```
searchKey: runtime._cgo_unsetenv
```

```Go
var _cgo_unsetenv unsafe.Pointer // pointer to C function

```

### <a id="boundsErrorFmts" href="#boundsErrorFmts">var boundsErrorFmts</a>

```
searchKey: runtime.boundsErrorFmts
```

```Go
var boundsErrorFmts = ...
```

boundsErrorFmts provide error text for various out-of-bounds panics. Note: if you change these strings, you should adjust the size of the buffer in boundsError.Error below as well. 

### <a id="boundsNegErrorFmts" href="#boundsNegErrorFmts">var boundsNegErrorFmts</a>

```
searchKey: runtime.boundsNegErrorFmts
```

```Go
var boundsNegErrorFmts = ...
```

boundsNegErrorFmts are overriding formats if x is negative. In this case there's no need to report y. 

### <a id="defaultGOROOT" href="#defaultGOROOT">var defaultGOROOT</a>

```
searchKey: runtime.defaultGOROOT
```

```Go
var defaultGOROOT string // set by cmd/link

```

### <a id="buildVersion" href="#buildVersion">var buildVersion</a>

```
searchKey: runtime.buildVersion
```

```Go
var buildVersion string
```

buildVersion is the Go tree's version string at build time. 

If any GOEXPERIMENTs are set to non-default values, it will include "X:<GOEXPERIMENT>". 

This is set by the linker. 

This is accessed by "go version <binary>". 

### <a id="fastlog2Table" href="#fastlog2Table">var fastlog2Table</a>

```
searchKey: runtime.fastlog2Table
```

```Go
var fastlog2Table = ...
```

### <a id="inf" href="#inf">var inf</a>

```
searchKey: runtime.inf
```

```Go
var inf = float64frombits(0x7FF0000000000000)
```

### <a id="dumpfd" href="#dumpfd">var dumpfd</a>

```
searchKey: runtime.dumpfd
```

```Go
var dumpfd uintptr // fd to write the dump to.

```

### <a id="tmpbuf" href="#tmpbuf">var tmpbuf</a>

```
searchKey: runtime.tmpbuf
```

```Go
var tmpbuf []byte
```

### <a id="buf" href="#buf">var buf</a>

```
searchKey: runtime.buf
```

```Go
var buf [bufSize]byte
```

### <a id="nbuf" href="#nbuf">var nbuf</a>

```
searchKey: runtime.nbuf
```

```Go
var nbuf uintptr
```

### <a id="typecache" href="#typecache">var typecache</a>

```
searchKey: runtime.typecache
```

```Go
var typecache [typeCacheBuckets]typeCacheBucket
```

### <a id="freemark" href="#freemark">var freemark</a>

```
searchKey: runtime.freemark
```

```Go
var freemark [_PageSize / 8]bool
```

Bit vector of free marks. Needs to be as big as the largest number of objects per span. 

### <a id="dumphdr" href="#dumphdr">var dumphdr</a>

```
searchKey: runtime.dumphdr
```

```Go
var dumphdr = []byte("go1.7 heap dump\n")
```

### <a id="itabLock" href="#itabLock">var itabLock</a>

```
searchKey: runtime.itabLock
```

```Go
var itabLock mutex // lock for accessing itab table

```

### <a id="itabTable" href="#itabTable">var itabTable</a>

```
searchKey: runtime.itabTable
```

```Go
var itabTable = &itabTableInit // pointer to current table

```

### <a id="itabTableInit" href="#itabTableInit">var itabTableInit</a>

```
searchKey: runtime.itabTableInit
```

```Go
var itabTableInit = itabTableType{size: itabInitSize} // starter table

```

### <a id="uint16Eface" href="#uint16Eface">var uint16Eface</a>

```
searchKey: runtime.uint16Eface
```

```Go
var uint16Eface interface{} = uint16InterfacePtr(0)
```

### <a id="uint32Eface" href="#uint32Eface">var uint32Eface</a>

```
searchKey: runtime.uint32Eface
```

```Go
var uint32Eface interface{} = uint32InterfacePtr(0)
```

### <a id="uint64Eface" href="#uint64Eface">var uint64Eface</a>

```
searchKey: runtime.uint64Eface
```

```Go
var uint64Eface interface{} = uint64InterfacePtr(0)
```

### <a id="stringEface" href="#stringEface">var stringEface</a>

```
searchKey: runtime.stringEface
```

```Go
var stringEface interface{} = stringInterfacePtr("")
```

### <a id="sliceEface" href="#sliceEface">var sliceEface</a>

```
searchKey: runtime.sliceEface
```

```Go
var sliceEface interface{} = sliceInterfacePtr(nil)
```

### <a id="uint16Type" href="#uint16Type">var uint16Type</a>

```
searchKey: runtime.uint16Type
```

```Go
var uint16Type *_type = efaceOf(&uint16Eface)._type
```

### <a id="uint32Type" href="#uint32Type">var uint32Type</a>

```
searchKey: runtime.uint32Type
```

```Go
var uint32Type *_type = efaceOf(&uint32Eface)._type
```

### <a id="uint64Type" href="#uint64Type">var uint64Type</a>

```
searchKey: runtime.uint64Type
```

```Go
var uint64Type *_type = efaceOf(&uint64Eface)._type
```

### <a id="stringType" href="#stringType">var stringType</a>

```
searchKey: runtime.stringType
```

```Go
var stringType *_type = efaceOf(&stringEface)._type
```

### <a id="sliceType" href="#sliceType">var sliceType</a>

```
searchKey: runtime.sliceType
```

```Go
var sliceType *_type = efaceOf(&sliceEface)._type
```

### <a id="staticuint64s" href="#staticuint64s">var staticuint64s</a>

```
searchKey: runtime.staticuint64s
```

```Go
var staticuint64s = ...
```

staticuint64s is used to avoid allocating in convTx for small integer values. 

### <a id="lockNames" href="#lockNames">var lockNames</a>

```
searchKey: runtime.lockNames
```

```Go
var lockNames = ...
```

lockNames gives the names associated with each of the above ranks 

### <a id="lockPartialOrder" href="#lockPartialOrder">var lockPartialOrder</a>

```
searchKey: runtime.lockPartialOrder
```

```Go
var lockPartialOrder [][]lockRank = ...
```

lockPartialOrder is a partial order among the various lock types, listing the immediate ordering that has actually been observed in the runtime. Each entry (which corresponds to a particular lock rank) specifies the list of locks that can already be held immediately "above" it. 

So, for example, the lockRankSched entry shows that all the locks preceding it in rank can actually be held. The allp lock shows that only the sysmon or sched lock can be held immediately above it when it is acquired. 

### <a id="physPageSize" href="#physPageSize">var physPageSize</a>

```
searchKey: runtime.physPageSize
```

```Go
var physPageSize uintptr
```

physPageSize is the size in bytes of the OS's physical pages. Mapping and unmapping operations must be done at multiples of physPageSize. 

This must be set by the OS init code (typically in osinit) before mallocinit. 

### <a id="physHugePageSize" href="#physHugePageSize">var physHugePageSize</a>

```
searchKey: runtime.physHugePageSize
```

```Go
var physHugePageSize uintptr
```

physHugePageSize is the size in bytes of the OS's default physical huge page size whose allocation is opaque to the application. It is assumed and verified to be a power of two. 

If set, this must be set by the OS init code (typically in osinit) before mallocinit. However, setting it at all is optional, and leaving the default value is always safe (though potentially less efficient). 

Since physHugePageSize is always assumed to be a power of two, physHugePageShift is defined as physHugePageSize == 1 << physHugePageShift. The purpose of physHugePageShift is to avoid doing divisions in performance critical functions. 

### <a id="physHugePageShift" href="#physHugePageShift">var physHugePageShift</a>

```
searchKey: runtime.physHugePageShift
```

```Go
var physHugePageShift uint
```

physHugePageSize is the size in bytes of the OS's default physical huge page size whose allocation is opaque to the application. It is assumed and verified to be a power of two. 

If set, this must be set by the OS init code (typically in osinit) before mallocinit. However, setting it at all is optional, and leaving the default value is always safe (though potentially less efficient). 

Since physHugePageSize is always assumed to be a power of two, physHugePageShift is defined as physHugePageSize == 1 << physHugePageShift. The purpose of physHugePageShift is to avoid doing divisions in performance critical functions. 

### <a id="zerobase" href="#zerobase">var zerobase</a>

```
searchKey: runtime.zerobase
```

```Go
var zerobase uintptr
```

base address for all 0-byte allocations 

### <a id="globalAlloc" href="#globalAlloc">var globalAlloc</a>

```
searchKey: runtime.globalAlloc
```

```Go
var globalAlloc struct {
	mutex
	persistentAlloc
}
```

### <a id="persistentChunks" href="#persistentChunks">var persistentChunks</a>

```
searchKey: runtime.persistentChunks
```

```Go
var persistentChunks *notInHeap
```

persistentChunks is a list of all the persistent chunks we have allocated. The list is maintained through the first word in the persistent chunk. This is updated atomically. 

### <a id="zeroVal" href="#zeroVal">var zeroVal</a>

```
searchKey: runtime.zeroVal
```

```Go
var zeroVal [maxZero]byte
```

### <a id="debugPtrmask" href="#debugPtrmask">var debugPtrmask</a>

```
searchKey: runtime.debugPtrmask
```

```Go
var debugPtrmask struct {
	lock mutex
	data *byte
}
```

### <a id="emptymspan" href="#emptymspan">var emptymspan</a>

```
searchKey: runtime.emptymspan
```

```Go
var emptymspan mspan
```

dummy mspan that contains no free objects. 

### <a id="useCheckmark" href="#useCheckmark">var useCheckmark</a>

```
searchKey: runtime.useCheckmark
```

```Go
var useCheckmark = false
```

If useCheckmark is true, marking of an object uses the checkmark bits instead of the standard mark bits. 

### <a id="metricsSema" href="#metricsSema">var metricsSema</a>

```
searchKey: runtime.metricsSema
```

```Go
var metricsSema uint32 = 1
```

metrics is a map of runtime/metrics keys to data used by the runtime to sample each metric's value. 

### <a id="metricsInit" href="#metricsInit">var metricsInit</a>

```
searchKey: runtime.metricsInit
```

```Go
var metricsInit bool
```

### <a id="metrics" href="#metrics">var metrics</a>

```
searchKey: runtime.metrics
```

```Go
var metrics map[string]metricData
```

### <a id="sizeClassBuckets" href="#sizeClassBuckets">var sizeClassBuckets</a>

```
searchKey: runtime.sizeClassBuckets
```

```Go
var sizeClassBuckets []float64
```

### <a id="timeHistBuckets" href="#timeHistBuckets">var timeHistBuckets</a>

```
searchKey: runtime.timeHistBuckets
```

```Go
var timeHistBuckets []float64
```

### <a id="agg" href="#agg">var agg</a>

```
searchKey: runtime.agg
```

```Go
var agg statAggregate
```

agg is used by readMetrics, and is protected by metricsSema. 

Managed as a global variable because its pointer will be an argument to a dynamically-defined function, and we'd like to avoid it escaping to the heap. 

### <a id="finlock" href="#finlock">var finlock</a>

```
searchKey: runtime.finlock
```

```Go
var finlock mutex // protects the following variables

```

### <a id="fing" href="#fing">var fing</a>

```
searchKey: runtime.fing
```

```Go
var fing *g // goroutine that runs finalizers

```

### <a id="finq" href="#finq">var finq</a>

```
searchKey: runtime.finq
```

```Go
var finq *finblock // list of finalizers that are to be executed

```

### <a id="finc" href="#finc">var finc</a>

```
searchKey: runtime.finc
```

```Go
var finc *finblock // cache of free blocks

```

### <a id="finptrmask" href="#finptrmask">var finptrmask</a>

```
searchKey: runtime.finptrmask
```

```Go
var finptrmask [_FinBlockSize / sys.PtrSize / 8]byte
```

### <a id="fingwait" href="#fingwait">var fingwait</a>

```
searchKey: runtime.fingwait
```

```Go
var fingwait bool
```

### <a id="fingwake" href="#fingwake">var fingwake</a>

```
searchKey: runtime.fingwake
```

```Go
var fingwake bool
```

### <a id="allfin" href="#allfin">var allfin</a>

```
searchKey: runtime.allfin
```

```Go
var allfin *finblock // list of all blocks

```

### <a id="finalizer1" href="#finalizer1">var finalizer1</a>

```
searchKey: runtime.finalizer1
```

```Go
var finalizer1 = ...
```

### <a id="fingCreate" href="#fingCreate">var fingCreate</a>

```
searchKey: runtime.fingCreate
```

```Go
var fingCreate uint32
```

### <a id="fingRunning" href="#fingRunning">var fingRunning</a>

```
searchKey: runtime.fingRunning
```

```Go
var fingRunning bool
```

### <a id="gcenable_setup" href="#gcenable_setup">var gcenable_setup</a>

```
searchKey: runtime.gcenable_setup
```

```Go
var gcenable_setup chan int
```

Temporary in order to enable register ABI work. TODO(register args): convert back to local chan in gcenabled, passed to "go" stmts. 

### <a id="gcphase" href="#gcphase">var gcphase</a>

```
searchKey: runtime.gcphase
```

```Go
var gcphase uint32
```

Garbage collector phase. Indicates to write barrier and synchronization task to perform. 

### <a id="writeBarrier" href="#writeBarrier">var writeBarrier</a>

```
searchKey: runtime.writeBarrier
```

```Go
var writeBarrier struct {
	enabled bool    // compiler emits a check of this before calling write barrier
	pad     [3]byte // compiler uses 32-bit load for "enabled" field
	needed  bool    // whether we need a write barrier for current GC phase
	cgo     bool    // whether we need a write barrier for a cgo check
	alignme uint64  // guarantee alignment so that compiler can use a 32 or 64-bit load
} = ...
```

The compiler knows about this variable. If you change it, you must change builtin/runtime.go, too. If you change the first four bytes, you must also change the write barrier insertion code. 

### <a id="gcBlackenEnabled" href="#gcBlackenEnabled">var gcBlackenEnabled</a>

```
searchKey: runtime.gcBlackenEnabled
```

```Go
var gcBlackenEnabled uint32
```

gcBlackenEnabled is 1 if mutator assists and background mark workers are allowed to blacken objects. This must only be set when gcphase == _GCmark. 

### <a id="gcMarkWorkerModeStrings" href="#gcMarkWorkerModeStrings">var gcMarkWorkerModeStrings</a>

```
searchKey: runtime.gcMarkWorkerModeStrings
```

```Go
var gcMarkWorkerModeStrings = ...
```

gcMarkWorkerModeStrings are the strings labels of gcMarkWorkerModes to use in execution traces. 

### <a id="work" href="#work">var work</a>

```
searchKey: runtime.work
```

```Go
var work struct {
	full  lfstack          // lock-free list of full blocks workbuf
	empty lfstack          // lock-free list of empty blocks workbuf
	pad0  cpu.CacheLinePad // prevents false-sharing between full/empty and nproc/nwait

	wbufSpans struct {
		lock mutex
		// free is a list of spans dedicated to workbufs, but
		// that don't currently contain any workbufs.
		free mSpanList
		// busy is a list of all spans containing workbufs on
		// one of the workbuf lists.
		busy mSpanList
	}

	// Restore 64-bit alignment on 32-bit.
	_ uint32

	// bytesMarked is the number of bytes marked this cycle. This
	// includes bytes blackened in scanned objects, noscan objects
	// that go straight to black, and permagrey objects scanned by
	// markroot during the concurrent scan phase. This is updated
	// atomically during the cycle. Updates may be batched
	// arbitrarily, since the value is only read at the end of the
	// cycle.
	//
	// Because of benign races during marking, this number may not
	// be the exact number of marked bytes, but it should be very
	// close.
	//
	// Put this field here because it needs 64-bit atomic access
	// (and thus 8-byte alignment even on 32-bit architectures).
	bytesMarked uint64

	markrootNext uint32 // next markroot job
	markrootJobs uint32 // number of markroot jobs

	nproc  uint32
	tstart int64
	nwait  uint32

	// Number of roots of various root types. Set by gcMarkRootPrepare.
	nDataRoots, nBSSRoots, nSpanRoots, nStackRoots int

	// Base indexes of each root type. Set by gcMarkRootPrepare.
	baseData, baseBSS, baseSpans, baseStacks, baseEnd uint32

	// Each type of GC state transition is protected by a lock.
	// Since multiple threads can simultaneously detect the state
	// transition condition, any thread that detects a transition
	// condition must acquire the appropriate transition lock,
	// re-check the transition condition and return if it no
	// longer holds or perform the transition if it does.
	// Likewise, any transition must invalidate the transition
	// condition before releasing the lock. This ensures that each
	// transition is performed by exactly one thread and threads
	// that need the transition to happen block until it has
	// happened.
	//
	// startSema protects the transition from "off" to mark or
	// mark termination.
	startSema uint32
	// markDoneSema protects transitions from mark to mark termination.
	markDoneSema uint32

	bgMarkReady note   // signal background mark worker has started
	bgMarkDone  uint32 // cas to 1 when at a background mark completion point

	// mode is the concurrency mode of the current GC cycle.
	mode gcMode

	// userForced indicates the current GC cycle was forced by an
	// explicit user call.
	userForced bool

	// totaltime is the CPU nanoseconds spent in GC since the
	// program started if debug.gctrace > 0.
	totaltime int64

	// initialHeapLive is the value of gcController.heapLive at the
	// beginning of this GC cycle.
	initialHeapLive uint64

	// assistQueue is a queue of assists that are blocked because
	// there was neither enough credit to steal or enough work to
	// do.
	assistQueue struct {
		lock mutex
		q    gQueue
	}

	// sweepWaiters is a list of blocked goroutines to wake when
	// we transition from mark termination to sweep.
	sweepWaiters struct {
		lock mutex
		list gList
	}

	// cycles is the number of completed GC cycles, where a GC
	// cycle is sweep termination, mark, mark termination, and
	// sweep. This differs from memstats.numgc, which is
	// incremented at mark termination.
	cycles uint32

	// Timing/utilization stats for this cycle.
	stwprocs, maxprocs                 int32
	tSweepTerm, tMark, tMarkTerm, tEnd int64 // nanotime() of phase start

	pauseNS    int64 // total STW time this cycle
	pauseStart int64 // nanotime() of last STW

	// debug.gctrace heap sizes for this cycle.
	heap0, heap1, heap2, heapGoal uint64
} = ...
```

### <a id="gcMarkDoneFlushed" href="#gcMarkDoneFlushed">var gcMarkDoneFlushed</a>

```
searchKey: runtime.gcMarkDoneFlushed
```

```Go
var gcMarkDoneFlushed uint32
```

gcMarkDoneFlushed counts the number of P's with flushed work. 

Ideally this would be a captured local in gcMarkDone, but forEachP escapes its callback closure, so it can't capture anything. 

This is protected by markDoneSema. 

### <a id="poolcleanup" href="#poolcleanup">var poolcleanup</a>

```
searchKey: runtime.poolcleanup
```

```Go
var poolcleanup func()
```

### <a id="oneptrmask" href="#oneptrmask">var oneptrmask</a>

```
searchKey: runtime.oneptrmask
```

```Go
var oneptrmask = [...]uint8{1}
```

ptrmask for an allocation containing a single pointer. 

### <a id="gcController" href="#gcController">var gcController</a>

```
searchKey: runtime.gcController
```

```Go
var gcController gcControllerState
```

gcController implements the GC pacing controller that determines when to trigger concurrent garbage collection and how much marking work to do in mutator assists and background marking. 

It uses a feedback control algorithm to adjust the gcController.trigger trigger based on the heap growth and GC CPU utilization each cycle. This algorithm optimizes for heap growth to match GOGC and for CPU utilization between assist and background marking to be 25% of GOMAXPROCS. The high-level design of this algorithm is documented at [https://golang.org/s/go15gcpacing](https://golang.org/s/go15gcpacing). 

All fields of gcController are used only during a single mark cycle. 

### <a id="scavenge" href="#scavenge">var scavenge</a>

```
searchKey: runtime.scavenge
```

```Go
var scavenge struct {
	lock       mutex
	g          *g
	parked     bool
	timer      *timer
	sysmonWake uint32 // Set atomically.
} = ...
```

Sleep/wait state of the background scavenger. 

### <a id="sweep" href="#sweep">var sweep</a>

```
searchKey: runtime.sweep
```

```Go
var sweep sweepdata
```

### <a id="mheap_" href="#mheap_">var mheap_</a>

```
searchKey: runtime.mheap_
```

```Go
var mheap_ mheap
```

### <a id="mSpanStateNames" href="#mSpanStateNames">var mSpanStateNames</a>

```
searchKey: runtime.mSpanStateNames
```

```Go
var mSpanStateNames = []string{
	"mSpanDead",
	"mSpanInUse",
	"mSpanManual",
	"mSpanFree",
}
```

mSpanStateNames are the names of the span states, indexed by mSpanState. 

### <a id="gcBitsArenas" href="#gcBitsArenas">var gcBitsArenas</a>

```
searchKey: runtime.gcBitsArenas
```

```Go
var gcBitsArenas struct {
	lock     mutex
	free     *gcBitsArena
	next     *gcBitsArena // Read atomically. Write atomically under lock.
	current  *gcBitsArena
	previous *gcBitsArena
} = ...
```

### <a id="maxSearchAddr" href="#maxSearchAddr">var maxSearchAddr</a>

```
searchKey: runtime.maxSearchAddr
```

```Go
var maxSearchAddr = maxOffAddr
```

Maximum searchAddr value, which indicates that the heap has no free space. 

We alias maxOffAddr just to make it clear that this is the maximum address for the page allocator's search space. See maxOffAddr for details. 

### <a id="levelBits" href="#levelBits">var levelBits</a>

```
searchKey: runtime.levelBits
```

```Go
var levelBits = ...
```

levelBits is the number of bits in the radix for a given level in the super summary structure. 

The sum of all the entries of levelBits should equal heapAddrBits. 

### <a id="levelShift" href="#levelShift">var levelShift</a>

```
searchKey: runtime.levelShift
```

```Go
var levelShift = ...
```

levelShift is the number of bits to shift to acquire the radix for a given level in the super summary structure. 

With levelShift, one can compute the index of the summary at level l related to a pointer p by doing: 

```
p >> levelShift[l]

```
### <a id="levelLogPages" href="#levelLogPages">var levelLogPages</a>

```
searchKey: runtime.levelLogPages
```

```Go
var levelLogPages = ...
```

levelLogPages is log2 the maximum number of runtime pages in the address space a summary in the given level represents. 

The leaf level always represents exactly log2 of 1 chunk's worth of pages. 

### <a id="proflock" href="#proflock">var proflock</a>

```
searchKey: runtime.proflock
```

```Go
var proflock mutex
```

NOTE(rsc): Everything here could use cas if contention became an issue. 

### <a id="mbuckets" href="#mbuckets">var mbuckets</a>

```
searchKey: runtime.mbuckets
```

```Go
var mbuckets *bucket // memory profile buckets

```

### <a id="bbuckets" href="#bbuckets">var bbuckets</a>

```
searchKey: runtime.bbuckets
```

```Go
var bbuckets *bucket // blocking profile buckets

```

### <a id="xbuckets" href="#xbuckets">var xbuckets</a>

```
searchKey: runtime.xbuckets
```

```Go
var xbuckets *bucket // mutex profile buckets

```

### <a id="buckhash" href="#buckhash">var buckhash</a>

```
searchKey: runtime.buckhash
```

```Go
var buckhash *[179999]*bucket
```

### <a id="bucketmem" href="#bucketmem">var bucketmem</a>

```
searchKey: runtime.bucketmem
```

```Go
var bucketmem uintptr
```

### <a id="mProf" href="#mProf">var mProf</a>

```
searchKey: runtime.mProf
```

```Go
var mProf struct {

	// cycle is the global heap profile cycle. This wraps
	// at mProfCycleWrap.
	cycle uint32
	// flushed indicates that future[cycle] in all buckets
	// has been flushed to the active profile.
	flushed bool
} = ...
```

### <a id="blockprofilerate" href="#blockprofilerate">var blockprofilerate</a>

```
searchKey: runtime.blockprofilerate
```

```Go
var blockprofilerate uint64 // in CPU ticks

```

### <a id="mutexprofilerate" href="#mutexprofilerate">var mutexprofilerate</a>

```
searchKey: runtime.mutexprofilerate
```

```Go
var mutexprofilerate uint64 // fraction sampled

```

### <a id="MemProfileRate" href="#MemProfileRate">var MemProfileRate</a>

```
searchKey: runtime.MemProfileRate
tags: [exported]
```

```Go
var MemProfileRate int = defaultMemProfileRate(512 * 1024)
```

MemProfileRate controls the fraction of memory allocations that are recorded and reported in the memory profile. The profiler aims to sample an average of one allocation per MemProfileRate bytes allocated. 

To include every allocated block in the profile, set MemProfileRate to 1. To turn off profiling entirely, set MemProfileRate to 0. 

The tools that process the memory profiles assume that the profile rate is constant across the lifetime of the program and equal to the current value. Programs that change the memory profiling rate should do so just once, as early as possible in the execution of the program (for example, at the beginning of main). 

### <a id="disableMemoryProfiling" href="#disableMemoryProfiling">var disableMemoryProfiling</a>

```
searchKey: runtime.disableMemoryProfiling
```

```Go
var disableMemoryProfiling bool
```

disableMemoryProfiling is set by the linker if runtime.MemProfile is not used and the link type guarantees nobody else could use it elsewhere. 

### <a id="tracelock" href="#tracelock">var tracelock</a>

```
searchKey: runtime.tracelock
```

```Go
var tracelock mutex
```

### <a id="minOffAddr" href="#minOffAddr">var minOffAddr</a>

```
searchKey: runtime.minOffAddr
```

```Go
var minOffAddr = offAddr{arenaBaseOffset}
```

minOffAddr is the minimum address in the offset space, and it corresponds to the virtual address arenaBaseOffset. 

### <a id="maxOffAddr" href="#maxOffAddr">var maxOffAddr</a>

```
searchKey: runtime.maxOffAddr
```

```Go
var maxOffAddr = offAddr{(((1 << heapAddrBits) - 1) + arenaBaseOffset) & uintptrMask}
```

maxOffAddr is the maximum address in the offset address space. It corresponds to the highest virtual address representable by the page alloc chunk and heap arena maps. 

### <a id="spanSetBlockPool" href="#spanSetBlockPool">var spanSetBlockPool</a>

```
searchKey: runtime.spanSetBlockPool
```

```Go
var spanSetBlockPool spanSetBlockAlloc
```

spanSetBlockPool is a global pool of spanSetBlocks. 

### <a id="memstats" href="#memstats">var memstats</a>

```
searchKey: runtime.memstats
```

```Go
var memstats mstats
```

### <a id="netpollInitLock" href="#netpollInitLock">var netpollInitLock</a>

```
searchKey: runtime.netpollInitLock
```

```Go
var netpollInitLock mutex
```

### <a id="netpollInited" href="#netpollInited">var netpollInited</a>

```
searchKey: runtime.netpollInited
```

```Go
var netpollInited uint32
```

### <a id="pollcache" href="#pollcache">var pollcache</a>

```
searchKey: runtime.pollcache
```

```Go
var pollcache pollCache
```

### <a id="netpollWaiters" href="#netpollWaiters">var netpollWaiters</a>

```
searchKey: runtime.netpollWaiters
```

```Go
var netpollWaiters uint32
```

### <a id="pdEface" href="#pdEface">var pdEface</a>

```
searchKey: runtime.pdEface
```

```Go
var pdEface interface{} = (*pollDesc)(nil)
```

### <a id="pdType" href="#pdType">var pdType</a>

```
searchKey: runtime.pdType
```

```Go
var pdType *_type = efaceOf(&pdEface)._type
```

### <a id="kq" href="#kq">var kq</a>

```
searchKey: runtime.kq
```

```Go
var kq int32 = -1
```

### <a id="netpollBreakRd" href="#netpollBreakRd">var netpollBreakRd</a>

```
searchKey: runtime.netpollBreakRd
```

```Go
var netpollBreakRd, netpollBreakWr uintptr // for netpollBreak

```

### <a id="netpollBreakWr" href="#netpollBreakWr">var netpollBreakWr</a>

```
searchKey: runtime.netpollBreakWr
```

```Go
var netpollBreakRd, netpollBreakWr uintptr // for netpollBreak

```

### <a id="netpollWakeSig" href="#netpollWakeSig">var netpollWakeSig</a>

```
searchKey: runtime.netpollWakeSig
```

```Go
var netpollWakeSig uint32 // used to avoid duplicate calls of netpollBreak

```

### <a id="sigNoteRead" href="#sigNoteRead">var sigNoteRead</a>

```
searchKey: runtime.sigNoteRead
```

```Go
var sigNoteRead, sigNoteWrite int32
```

The read and write file descriptors used by the sigNote functions. 

### <a id="sigNoteWrite" href="#sigNoteWrite">var sigNoteWrite</a>

```
searchKey: runtime.sigNoteWrite
```

```Go
var sigNoteRead, sigNoteWrite int32
```

The read and write file descriptors used by the sigNote functions. 

### <a id="urandom_dev" href="#urandom_dev">var urandom_dev</a>

```
searchKey: runtime.urandom_dev
```

```Go
var urandom_dev = []byte("/dev/urandom\x00")
```

### <a id="failallocatestack" href="#failallocatestack">var failallocatestack</a>

```
searchKey: runtime.failallocatestack
```

```Go
var failallocatestack = []byte("runtime: failed to allocate stack for the new OS thread\n")
```

### <a id="failthreadcreate" href="#failthreadcreate">var failthreadcreate</a>

```
searchKey: runtime.failthreadcreate
```

```Go
var failthreadcreate = []byte("runtime: failed to create new OS thread\n")
```

### <a id="sigset_all" href="#sigset_all">var sigset_all</a>

```
searchKey: runtime.sigset_all
```

```Go
var sigset_all = ^sigset(0)
```

### <a id="executablePath" href="#executablePath">var executablePath</a>

```
searchKey: runtime.executablePath
```

```Go
var executablePath string
```

### <a id="shiftError" href="#shiftError">var shiftError</a>

```
searchKey: runtime.shiftError
```

```Go
var shiftError = error(errorString("negative shift amount"))
```

### <a id="divideError" href="#divideError">var divideError</a>

```
searchKey: runtime.divideError
```

```Go
var divideError = error(errorString("integer divide by zero"))
```

### <a id="overflowError" href="#overflowError">var overflowError</a>

```
searchKey: runtime.overflowError
```

```Go
var overflowError = error(errorString("integer overflow"))
```

### <a id="floatError" href="#floatError">var floatError</a>

```
searchKey: runtime.floatError
```

```Go
var floatError = error(errorString("floating point error"))
```

### <a id="memoryError" href="#memoryError">var memoryError</a>

```
searchKey: runtime.memoryError
```

```Go
var memoryError = error(errorString("invalid memory address or nil pointer dereference"))
```

### <a id="deferType" href="#deferType">var deferType</a>

```
searchKey: runtime.deferType
```

```Go
var deferType *_type // type of _defer struct

```

### <a id="runningPanicDefers" href="#runningPanicDefers">var runningPanicDefers</a>

```
searchKey: runtime.runningPanicDefers
```

```Go
var runningPanicDefers uint32
```

runningPanicDefers is non-zero while running deferred functions for panic. runningPanicDefers is incremented and decremented atomically. This is used to try hard to get a panic stack trace out when exiting. 

### <a id="panicking" href="#panicking">var panicking</a>

```
searchKey: runtime.panicking
```

```Go
var panicking uint32
```

panicking is non-zero when crashing the program for an unrecovered panic. panicking is incremented and decremented atomically. 

### <a id="paniclk" href="#paniclk">var paniclk</a>

```
searchKey: runtime.paniclk
```

```Go
var paniclk mutex
```

paniclk is held while printing the panic information and stack trace, so that two concurrent panics don't overlap their output. 

### <a id="didothers" href="#didothers">var didothers</a>

```
searchKey: runtime.didothers
```

```Go
var didothers bool
```

### <a id="deadlock" href="#deadlock">var deadlock</a>

```
searchKey: runtime.deadlock
```

```Go
var deadlock mutex
```

### <a id="asyncPreemptStack" href="#asyncPreemptStack">var asyncPreemptStack</a>

```
searchKey: runtime.asyncPreemptStack
```

```Go
var asyncPreemptStack = ^uintptr(0)
```

asyncPreemptStack is the bytes of stack space required to inject an asyncPreempt call. 

### <a id="no_pointers_stackmap" href="#no_pointers_stackmap">var no_pointers_stackmap</a>

```
searchKey: runtime.no_pointers_stackmap
```

```Go
var no_pointers_stackmap uint64 // defined in assembly, for NO_LOCAL_POINTERS macro

```

### <a id="printBacklog" href="#printBacklog">var printBacklog</a>

```
searchKey: runtime.printBacklog
```

```Go
var printBacklog [512]byte
```

printBacklog is a circular buffer of messages written with the builtin print* functions, for use in postmortem analysis of core dumps. 

### <a id="printBacklogIndex" href="#printBacklogIndex">var printBacklogIndex</a>

```
searchKey: runtime.printBacklogIndex
```

```Go
var printBacklogIndex int
```

### <a id="debuglock" href="#debuglock">var debuglock</a>

```
searchKey: runtime.debuglock
```

```Go
var debuglock mutex
```

### <a id="minhexdigits" href="#minhexdigits">var minhexdigits</a>

```
searchKey: runtime.minhexdigits
```

```Go
var minhexdigits = 0 // protected by printlock

```

### <a id="modinfo" href="#modinfo">var modinfo</a>

```
searchKey: runtime.modinfo
```

```Go
var modinfo string
```

set using cmd/go/internal/modload.ModInfoProg 

### <a id="m0" href="#m0">var m0</a>

```
searchKey: runtime.m0
```

```Go
var m0 m
```

### <a id="g0" href="#g0">var g0</a>

```
searchKey: runtime.g0
```

```Go
var g0 g
```

### <a id="mcache0" href="#mcache0">var mcache0</a>

```
searchKey: runtime.mcache0
```

```Go
var mcache0 *mcache
```

### <a id="raceprocctx0" href="#raceprocctx0">var raceprocctx0</a>

```
searchKey: runtime.raceprocctx0
```

```Go
var raceprocctx0 uintptr
```

### <a id="runtime_inittask" href="#runtime_inittask">var runtime_inittask</a>

```
searchKey: runtime.runtime_inittask
```

```Go
var runtime_inittask initTask
```

### <a id="main_inittask" href="#main_inittask">var main_inittask</a>

```
searchKey: runtime.main_inittask
```

```Go
var main_inittask initTask
```

### <a id="main_init_done" href="#main_init_done">var main_init_done</a>

```
searchKey: runtime.main_init_done
```

```Go
var main_init_done chan bool
```

main_init_done is a signal used by cgocallbackg that initialization has been completed. It is made before _cgo_notify_runtime_init_done, so all cgo calls can rely on it existing. When main_init is complete, it is closed, meaning cgocallbackg can reliably receive from it. 

### <a id="mainStarted" href="#mainStarted">var mainStarted</a>

```
searchKey: runtime.mainStarted
```

```Go
var mainStarted bool
```

mainStarted indicates that the main M has started. 

### <a id="runtimeInitTime" href="#runtimeInitTime">var runtimeInitTime</a>

```
searchKey: runtime.runtimeInitTime
```

```Go
var runtimeInitTime int64
```

runtimeInitTime is the nanotime() at which the runtime started. 

### <a id="initSigmask" href="#initSigmask">var initSigmask</a>

```
searchKey: runtime.initSigmask
```

```Go
var initSigmask sigset
```

Value to use for signal mask for newly created M's. 

### <a id="badmorestackg0Msg" href="#badmorestackg0Msg">var badmorestackg0Msg</a>

```
searchKey: runtime.badmorestackg0Msg
```

```Go
var badmorestackg0Msg = "fatal: morestack on g0\n"
```

### <a id="badmorestackgsignalMsg" href="#badmorestackgsignalMsg">var badmorestackgsignalMsg</a>

```
searchKey: runtime.badmorestackgsignalMsg
```

```Go
var badmorestackgsignalMsg = "fatal: morestack on gsignal\n"
```

### <a id="allglock" href="#allglock">var allglock</a>

```
searchKey: runtime.allglock
```

```Go
var allglock mutex
```

allgs contains all Gs ever created (including dead Gs), and thus never shrinks. 

Access via the slice is protected by allglock or stop-the-world. Readers that cannot take the lock may (carefully!) use the atomic variables below. 

### <a id="allgs" href="#allgs">var allgs</a>

```
searchKey: runtime.allgs
```

```Go
var allgs []*g
```

### <a id="allglen" href="#allglen">var allglen</a>

```
searchKey: runtime.allglen
```

```Go
var allglen uintptr
```

allglen and allgptr are atomic variables that contain len(allgs) and &allgs[0] respectively. Proper ordering depends on totally-ordered loads and stores. Writes are protected by allglock. 

allgptr is updated before allglen. Readers should read allglen before allgptr to ensure that allglen is always <= len(allgptr). New Gs appended during the race can be missed. For a consistent view of all Gs, allglock must be held. 

allgptr copies should always be stored as a concrete type or unsafe.Pointer, not uintptr, to ensure that GC can still reach it even if it points to a stale array. 

### <a id="allgptr" href="#allgptr">var allgptr</a>

```
searchKey: runtime.allgptr
```

```Go
var allgptr **g
```

### <a id="fastrandseed" href="#fastrandseed">var fastrandseed</a>

```
searchKey: runtime.fastrandseed
```

```Go
var fastrandseed uintptr
```

### <a id="freezing" href="#freezing">var freezing</a>

```
searchKey: runtime.freezing
```

```Go
var freezing uint32
```

freezing is set to non-zero if the runtime is trying to freeze the world. 

### <a id="worldsema" href="#worldsema">var worldsema</a>

```
searchKey: runtime.worldsema
```

```Go
var worldsema uint32 = 1
```

Holding worldsema grants an M the right to try to stop the world. 

### <a id="gcsema" href="#gcsema">var gcsema</a>

```
searchKey: runtime.gcsema
```

```Go
var gcsema uint32 = 1
```

Holding gcsema grants the M the right to block a GC, and blocks until the current GC is done. In particular, it prevents gomaxprocs from changing concurrently. 

TODO(mknyszek): Once gomaxprocs and the execution tracer can handle being changed/enabled during a GC, remove this. 

### <a id="cgoThreadStart" href="#cgoThreadStart">var cgoThreadStart</a>

```
searchKey: runtime.cgoThreadStart
```

```Go
var cgoThreadStart unsafe.Pointer
```

When running with cgo, we call _cgo_thread_start to start threads for us so that we can play nicely with foreign code. 

### <a id="earlycgocallback" href="#earlycgocallback">var earlycgocallback</a>

```
searchKey: runtime.earlycgocallback
```

```Go
var earlycgocallback = []byte("fatal error: cgo callback before cgo call\n")
```

### <a id="extram" href="#extram">var extram</a>

```
searchKey: runtime.extram
```

```Go
var extram uintptr
```

### <a id="extraMCount" href="#extraMCount">var extraMCount</a>

```
searchKey: runtime.extraMCount
```

```Go
var extraMCount uint32 // Protected by lockextra

```

### <a id="extraMWaiters" href="#extraMWaiters">var extraMWaiters</a>

```
searchKey: runtime.extraMWaiters
```

```Go
var extraMWaiters uint32
```

### <a id="execLock" href="#execLock">var execLock</a>

```
searchKey: runtime.execLock
```

```Go
var execLock rwmutex
```

execLock serializes exec and clone to avoid bugs or unspecified behaviour around exec'ing while creating/destroying threads.  See issue #19546. 

### <a id="newmHandoff" href="#newmHandoff">var newmHandoff</a>

```
searchKey: runtime.newmHandoff
```

```Go
var newmHandoff struct {
	lock mutex

	// newm points to a list of M structures that need new OS
	// threads. The list is linked through m.schedlink.
	newm muintptr

	// waiting indicates that wake needs to be notified when an m
	// is put on the list.
	waiting bool
	wake    note

	// haveTemplateThread indicates that the templateThread has
	// been started. This is not protected by lock. Use cas to set
	// to 1.
	haveTemplateThread uint32
} = ...
```

newmHandoff contains a list of m structures that need new OS threads. This is used by newm in situations where newm itself can't safely start an OS thread. 

### <a id="mFixupRace" href="#mFixupRace">var mFixupRace</a>

```
searchKey: runtime.mFixupRace
```

```Go
var mFixupRace struct {
	lock mutex
	ctx  uintptr
}
```

mFixupRace is used to temporarily borrow the race context from the coordinating m during a syscall_runtime_doAllThreadsSyscall and loan it out to each of the m's of the runtime so they can execute a mFixup.fn in that context. 

### <a id="inForkedChild" href="#inForkedChild">var inForkedChild</a>

```
searchKey: runtime.inForkedChild
```

```Go
var inForkedChild bool
```

inForkedChild is true while manipulating signals in the child process. This is used to avoid calling libc functions in case we are using vfork. 

### <a id="pendingPreemptSignals" href="#pendingPreemptSignals">var pendingPreemptSignals</a>

```
searchKey: runtime.pendingPreemptSignals
```

```Go
var pendingPreemptSignals uint32
```

pendingPreemptSignals is the number of preemption signals that have been sent but not received. This is only used on Darwin. For #41702. 

### <a id="prof" href="#prof">var prof</a>

```
searchKey: runtime.prof
```

```Go
var prof struct {
	signalLock uint32
	hz         int32
}
```

### <a id="sigprofCallers" href="#sigprofCallers">var sigprofCallers</a>

```
searchKey: runtime.sigprofCallers
```

```Go
var sigprofCallers cgoCallers
```

If the signal handler receives a SIGPROF signal on a non-Go thread, it tries to collect a traceback into sigprofCallers. sigprofCallersUse is set to non-zero while sigprofCallers holds a traceback. 

### <a id="sigprofCallersUse" href="#sigprofCallersUse">var sigprofCallersUse</a>

```
searchKey: runtime.sigprofCallersUse
```

```Go
var sigprofCallersUse uint32
```

### <a id="forcegcperiod" href="#forcegcperiod">var forcegcperiod</a>

```
searchKey: runtime.forcegcperiod
```

```Go
var forcegcperiod int64 = 2 * 60 * 1e9
```

forcegcperiod is the maximum time in nanoseconds between garbage collections. If we go this long without a garbage collection, one is forced to run. 

This is a variable for testing purposes. It normally doesn't change. 

### <a id="starttime" href="#starttime">var starttime</a>

```
searchKey: runtime.starttime
```

```Go
var starttime int64
```

### <a id="stealOrder" href="#stealOrder">var stealOrder</a>

```
searchKey: runtime.stealOrder
```

```Go
var stealOrder randomOrder
```

### <a id="inittrace" href="#inittrace">var inittrace</a>

```
searchKey: runtime.inittrace
```

```Go
var inittrace tracestat
```

inittrace stores statistics for init functions which are updated by malloc and newproc when active is true. 

### <a id="overflowTag" href="#overflowTag">var overflowTag</a>

```
searchKey: runtime.overflowTag
```

```Go
var overflowTag [1]unsafe.Pointer // always nil

```

### <a id="labelSync" href="#labelSync">var labelSync</a>

```
searchKey: runtime.labelSync
```

```Go
var labelSync uintptr
```

### <a id="ticks" href="#ticks">var ticks</a>

```
searchKey: runtime.ticks
```

```Go
var ticks struct {
	lock mutex
	pad  uint32 // ensure 8-byte alignment of val on 386
	val  uint64
}
```

### <a id="envs" href="#envs">var envs</a>

```
searchKey: runtime.envs
```

```Go
var envs []string
```

### <a id="argslice" href="#argslice">var argslice</a>

```
searchKey: runtime.argslice
```

```Go
var argslice []string
```

### <a id="traceback_cache" href="#traceback_cache">var traceback_cache</a>

```
searchKey: runtime.traceback_cache
```

```Go
var traceback_cache uint32 = 2 << tracebackShift
```

### <a id="traceback_env" href="#traceback_env">var traceback_env</a>

```
searchKey: runtime.traceback_env
```

```Go
var traceback_env uint32
```

### <a id="argc" href="#argc">var argc</a>

```
searchKey: runtime.argc
```

```Go
var argc int32
```

### <a id="argv" href="#argv">var argv</a>

```
searchKey: runtime.argv
```

```Go
var argv **byte
```

### <a id="test_z64" href="#test_z64">var test_z64</a>

```
searchKey: runtime.test_z64
```

```Go
var test_z64, test_x64 uint64
```

TODO: These should be locals in testAtomic64, but we don't 8-byte align stack variables on 386. 

### <a id="test_x64" href="#test_x64">var test_x64</a>

```
searchKey: runtime.test_x64
```

```Go
var test_z64, test_x64 uint64
```

TODO: These should be locals in testAtomic64, but we don't 8-byte align stack variables on 386. 

### <a id="debug" href="#debug">var debug</a>

```
searchKey: runtime.debug
```

```Go
var debug struct {
	cgocheck           int32
	clobberfree        int32
	efence             int32
	gccheckmark        int32
	gcpacertrace       int32
	gcshrinkstackoff   int32
	gcstoptheworld     int32
	gctrace            int32
	invalidptr         int32
	madvdontneed       int32 // for Linux; issue 28466
	scavtrace          int32
	scheddetail        int32
	schedtrace         int32
	tracebackancestors int32
	asyncpreemptoff    int32

	// debug.malloc is used as a combined debug check
	// in the malloc function and should be set
	// if any of the below debug options is != 0.
	malloc         bool
	allocfreetrace int32
	inittrace      int32
	sbrk           int32
} = ...
```

Holds variables parsed from GODEBUG env var, except for "memprofilerate" since there is an existing int var for that value, which may already have an initial value. 

### <a id="dbgvars" href="#dbgvars">var dbgvars</a>

```
searchKey: runtime.dbgvars
```

```Go
var dbgvars = ...
```

### <a id="waitReasonStrings" href="#waitReasonStrings">var waitReasonStrings</a>

```
searchKey: runtime.waitReasonStrings
```

```Go
var waitReasonStrings = ...
```

### <a id="allm" href="#allm">var allm</a>

```
searchKey: runtime.allm
```

```Go
var allm *m
```

### <a id="gomaxprocs" href="#gomaxprocs">var gomaxprocs</a>

```
searchKey: runtime.gomaxprocs
```

```Go
var gomaxprocs int32
```

### <a id="ncpu" href="#ncpu">var ncpu</a>

```
searchKey: runtime.ncpu
```

```Go
var ncpu int32
```

### <a id="forcegc" href="#forcegc">var forcegc</a>

```
searchKey: runtime.forcegc
```

```Go
var forcegc forcegcstate
```

### <a id="sched" href="#sched">var sched</a>

```
searchKey: runtime.sched
```

```Go
var sched schedt
```

### <a id="newprocs" href="#newprocs">var newprocs</a>

```
searchKey: runtime.newprocs
```

```Go
var newprocs int32
```

### <a id="allpLock" href="#allpLock">var allpLock</a>

```
searchKey: runtime.allpLock
```

```Go
var allpLock mutex
```

allpLock protects P-less reads and size changes of allp, idlepMask, and timerpMask, and all writes to allp. 

### <a id="allp" href="#allp">var allp</a>

```
searchKey: runtime.allp
```

```Go
var allp []*p
```

len(allp) == gomaxprocs; may change at safe points, otherwise immutable. 

### <a id="idlepMask" href="#idlepMask">var idlepMask</a>

```
searchKey: runtime.idlepMask
```

```Go
var idlepMask pMask
```

Bitmask of Ps in _Pidle list, one bit per P. Reads and writes must be atomic. Length may change at safe points. 

Each P must update only its own bit. In order to maintain consistency, a P going idle must the idle mask simultaneously with updates to the idle P list under the sched.lock, otherwise a racing pidleget may clear the mask before pidleput sets the mask, corrupting the bitmap. 

N.B., procresize takes ownership of all Ps in stopTheWorldWithSema. 

### <a id="timerpMask" href="#timerpMask">var timerpMask</a>

```
searchKey: runtime.timerpMask
```

```Go
var timerpMask pMask
```

Bitmask of Ps that may have a timer, one bit per P. Reads and writes must be atomic. Length may change at safe points. 

### <a id="gcBgMarkWorkerPool" href="#gcBgMarkWorkerPool">var gcBgMarkWorkerPool</a>

```
searchKey: runtime.gcBgMarkWorkerPool
```

```Go
var gcBgMarkWorkerPool lfstack
```

Pool of GC parked background workers. Entries are type *gcBgMarkWorkerNode. 

### <a id="gcBgMarkWorkerCount" href="#gcBgMarkWorkerCount">var gcBgMarkWorkerCount</a>

```
searchKey: runtime.gcBgMarkWorkerCount
```

```Go
var gcBgMarkWorkerCount int32
```

Total number of gcBgMarkWorker goroutines. Protected by worldsema. 

### <a id="processorVersionInfo" href="#processorVersionInfo">var processorVersionInfo</a>

```
searchKey: runtime.processorVersionInfo
```

```Go
var processorVersionInfo uint32
```

Information about what cpu features are available. Packages outside the runtime should not use these as they are not an external api. Set on startup in asm_{386,amd64}.s 

### <a id="isIntel" href="#isIntel">var isIntel</a>

```
searchKey: runtime.isIntel
```

```Go
var isIntel bool
```

### <a id="lfenceBeforeRdtsc" href="#lfenceBeforeRdtsc">var lfenceBeforeRdtsc</a>

```
searchKey: runtime.lfenceBeforeRdtsc
```

```Go
var lfenceBeforeRdtsc bool
```

### <a id="goarm" href="#goarm">var goarm</a>

```
searchKey: runtime.goarm
```

```Go
var goarm uint8 // set by cmd/link on arm systems

```

### <a id="islibrary" href="#islibrary">var islibrary</a>

```
searchKey: runtime.islibrary
```

```Go
var islibrary bool // -buildmode=c-shared

```

Set by the linker so the runtime can determine the buildmode. 

### <a id="isarchive" href="#isarchive">var isarchive</a>

```
searchKey: runtime.isarchive
```

```Go
var isarchive bool // -buildmode=c-archive

```

Set by the linker so the runtime can determine the buildmode. 

### <a id="chansendpc" href="#chansendpc">var chansendpc</a>

```
searchKey: runtime.chansendpc
```

```Go
var chansendpc = funcPC(chansend)
```

### <a id="chanrecvpc" href="#chanrecvpc">var chanrecvpc</a>

```
searchKey: runtime.chanrecvpc
```

```Go
var chanrecvpc = funcPC(chanrecv)
```

### <a id="semtable" href="#semtable">var semtable</a>

```
searchKey: runtime.semtable
```

```Go
var semtable [semTabSize]struct {
	root semaRoot
	pad  [cpu.CacheLinePadSize - unsafe.Sizeof(semaRoot{})]byte
} = ...
```

### <a id="sigtable" href="#sigtable">var sigtable</a>

```
searchKey: runtime.sigtable
```

```Go
var sigtable = ...
```

### <a id="fwdSig" href="#fwdSig">var fwdSig</a>

```
searchKey: runtime.fwdSig
```

```Go
var fwdSig [_NSIG]uintptr
```

Stores the signal handlers registered before Go installed its own. These signal handlers will be invoked in cases where Go doesn't want to handle a particular signal (e.g., signal occurred on a non-Go thread). See sigfwdgo for more information on when the signals are forwarded. 

This is read by the signal handler; accesses should use atomic.Loaduintptr and atomic.Storeuintptr. 

### <a id="handlingSig" href="#handlingSig">var handlingSig</a>

```
searchKey: runtime.handlingSig
```

```Go
var handlingSig [_NSIG]uint32
```

handlingSig is indexed by signal number and is non-zero if we are currently handling the signal. Or, to put it another way, whether the signal handler is currently set to the Go signal handler or not. This is uint32 rather than bool so that we can use atomic instructions. 

### <a id="disableSigChan" href="#disableSigChan">var disableSigChan</a>

```
searchKey: runtime.disableSigChan
```

```Go
var disableSigChan chan uint32
```

channels for synchronizing signal mask updates with the signal mask thread 

### <a id="enableSigChan" href="#enableSigChan">var enableSigChan</a>

```
searchKey: runtime.enableSigChan
```

```Go
var enableSigChan chan uint32
```

channels for synchronizing signal mask updates with the signal mask thread 

### <a id="maskUpdatedChan" href="#maskUpdatedChan">var maskUpdatedChan</a>

```
searchKey: runtime.maskUpdatedChan
```

```Go
var maskUpdatedChan chan struct{}
```

channels for synchronizing signal mask updates with the signal mask thread 

### <a id="signalsOK" href="#signalsOK">var signalsOK</a>

```
searchKey: runtime.signalsOK
```

```Go
var signalsOK bool
```

### <a id="crashing" href="#crashing">var crashing</a>

```
searchKey: runtime.crashing
```

```Go
var crashing int32
```

crashing is the number of m's we have waited for when implementing GOTRACEBACK=crash when a signal is received. 

### <a id="testSigtrap" href="#testSigtrap">var testSigtrap</a>

```
searchKey: runtime.testSigtrap
```

```Go
var testSigtrap func(info *siginfo, ctxt *sigctxt, gp *g) bool
```

testSigtrap and testSigusr1 are used by the runtime tests. If non-nil, it is called on SIGTRAP/SIGUSR1. If it returns true, the normal behavior on this signal is suppressed. 

### <a id="testSigusr1" href="#testSigusr1">var testSigusr1</a>

```
searchKey: runtime.testSigusr1
```

```Go
var testSigusr1 func(gp *g) bool
```

### <a id="badginsignalMsg" href="#badginsignalMsg">var badginsignalMsg</a>

```
searchKey: runtime.badginsignalMsg
```

```Go
var badginsignalMsg = "fatal: bad g in signal handler\n"
```

### <a id="sigsetAllExiting" href="#sigsetAllExiting">var sigsetAllExiting</a>

```
searchKey: runtime.sigsetAllExiting
```

```Go
var sigsetAllExiting = sigset_all
```

sigsetAllExiting is used by sigblock(true) when a thread is exiting. sigset_all is defined in OS specific code, and per GOOS behavior may override this default for sigsetAllExiting: see osinit(). 

### <a id="sig" href="#sig">var sig</a>

```
searchKey: runtime.sig
```

```Go
var sig struct {
	note       note
	mask       [(_NSIG + 31) / 32]uint32
	wanted     [(_NSIG + 31) / 32]uint32
	ignored    [(_NSIG + 31) / 32]uint32
	recv       [(_NSIG + 31) / 32]uint32
	state      uint32
	delivering uint32
	inuse      bool
} = ...
```

sig handles communication between the signal handler and os/signal. Other than the inuse and recv fields, the fields are accessed atomically. 

The wanted and ignored fields are only written by one goroutine at a time; access is controlled by the handlers Mutex in os/signal. The fields are only read by that one goroutine and by the signal handler. We access them atomically to minimize the race between setting them in the goroutine calling os/signal and the signal handler, which may be running in a different thread. That race is unavoidable, as there is no connection between handling a signal and receiving one, but atomic instructions should minimize it. 

### <a id="class_to_size" href="#class_to_size">var class_to_size</a>

```
searchKey: runtime.class_to_size
```

```Go
var class_to_size = ...
```

### <a id="class_to_allocnpages" href="#class_to_allocnpages">var class_to_allocnpages</a>

```
searchKey: runtime.class_to_allocnpages
```

```Go
var class_to_allocnpages = ...
```

### <a id="class_to_divmagic" href="#class_to_divmagic">var class_to_divmagic</a>

```
searchKey: runtime.class_to_divmagic
```

```Go
var class_to_divmagic = ...
```

### <a id="size_to_class8" href="#size_to_class8">var size_to_class8</a>

```
searchKey: runtime.size_to_class8
```

```Go
var size_to_class8 = ...
```

### <a id="size_to_class128" href="#size_to_class128">var size_to_class128</a>

```
searchKey: runtime.size_to_class128
```

```Go
var size_to_class128 = ...
```

### <a id="stackpool" href="#stackpool">var stackpool</a>

```
searchKey: runtime.stackpool
```

```Go
var stackpool [_NumStackOrders]struct {
	item stackpoolItem
	_    [cpu.CacheLinePadSize - unsafe.Sizeof(stackpoolItem{})%cpu.CacheLinePadSize]byte
} = ...
```

Global pool of spans that have free stacks. Stacks are assigned an order according to size. 

```
order = log_2(size/FixedStack)

```
There is a free list for each order. 

### <a id="stackLarge" href="#stackLarge">var stackLarge</a>

```
searchKey: runtime.stackLarge
```

```Go
var stackLarge struct {
	lock mutex
	free [heapAddrBits - pageShift]mSpanList // free lists by log_2(s.npages)
} = ...
```

Global pool of large stack spans. 

### <a id="maxstacksize" href="#maxstacksize">var maxstacksize</a>

```
searchKey: runtime.maxstacksize
```

```Go
var maxstacksize uintptr = 1 << 20 // enough until runtime.main sets it for real

```

### <a id="maxstackceiling" href="#maxstackceiling">var maxstackceiling</a>

```
searchKey: runtime.maxstackceiling
```

```Go
var maxstackceiling = maxstacksize
```

### <a id="ptrnames" href="#ptrnames">var ptrnames</a>

```
searchKey: runtime.ptrnames
```

```Go
var ptrnames = []string{
	0: "scalar",
	1: "ptr",
}
```

### <a id="abiRegArgsEface" href="#abiRegArgsEface">var abiRegArgsEface</a>

```
searchKey: runtime.abiRegArgsEface
```

```Go
var abiRegArgsEface interface{} = abi.RegArgs{}
```

### <a id="abiRegArgsType" href="#abiRegArgsType">var abiRegArgsType</a>

```
searchKey: runtime.abiRegArgsType
```

```Go
var abiRegArgsType *_type = efaceOf(&abiRegArgsEface)._type
```

### <a id="methodValueCallFrameObjs" href="#methodValueCallFrameObjs">var methodValueCallFrameObjs</a>

```
searchKey: runtime.methodValueCallFrameObjs
```

```Go
var methodValueCallFrameObjs = ...
```

### <a id="badsystemstackMsg" href="#badsystemstackMsg">var badsystemstackMsg</a>

```
searchKey: runtime.badsystemstackMsg
```

```Go
var badsystemstackMsg = "fatal: systemstack called from unexpected goroutine"
```

### <a id="hashLoad" href="#hashLoad">var hashLoad</a>

```
searchKey: runtime.hashLoad
```

```Go
var hashLoad = float32(loadFactorNum) / float32(loadFactorDen)
```

exported value for testing 

### <a id="intArgRegs" href="#intArgRegs">var intArgRegs</a>

```
searchKey: runtime.intArgRegs
```

```Go
var intArgRegs = abi.IntArgRegs * goexperiment.RegabiArgsInt
```

intArgRegs is used by the various register assignment algorithm implementations in the runtime. These include:. - Finalizers (mfinal.go) - Windows callbacks (syscall_windows.go) 

Both are stripped-down versions of the algorithm since they only have to deal with a subset of cases (finalizers only take a pointer or interface argument, Go Windows callbacks don't support floating point). 

It should be modified with care and are generally only modified when testing this package. 

It should never be set higher than its internal/abi constant counterparts, because the system relies on a structure that is at least large enough to hold the registers the system supports. 

Currently it's set to zero because using the actual constant will break every part of the toolchain that uses finalizers or Windows callbacks to call functions The value that is currently commented out there should be the actual value once we're ready to use the register ABI everywhere. 

Protected by finlock. 

### <a id="pinnedTypemaps" href="#pinnedTypemaps">var pinnedTypemaps</a>

```
searchKey: runtime.pinnedTypemaps
```

```Go
var pinnedTypemaps []map[typeOff]*_type
```

pinnedTypemaps are the map[typeOff]*_type from the moduledata objects. 

These typemap objects are allocated at run time on the heap, but the only direct reference to them is in the moduledata, created by the linker and marked SNOPTRDATA so it is ignored by the GC. 

To make sure the map isn't collected, we keep a second reference here. 

### <a id="firstmoduledata" href="#firstmoduledata">var firstmoduledata</a>

```
searchKey: runtime.firstmoduledata
```

```Go
var firstmoduledata moduledata // linker symbol

```

### <a id="lastmoduledatap" href="#lastmoduledatap">var lastmoduledatap</a>

```
searchKey: runtime.lastmoduledatap
```

```Go
var lastmoduledatap *moduledata // linker symbol

```

### <a id="modulesSlice" href="#modulesSlice">var modulesSlice</a>

```
searchKey: runtime.modulesSlice
```

```Go
var modulesSlice *[]*moduledata // see activeModules

```

### <a id="faketime" href="#faketime">var faketime</a>

```
searchKey: runtime.faketime
```

```Go
var faketime int64
```

faketime is the simulated time in nanoseconds since 1970 for the playground. 

Zero means not to use faketime. 

### <a id="trace" href="#trace">var trace</a>

```
searchKey: runtime.trace
```

```Go
var trace struct {
	lock          mutex       // protects the following members
	lockOwner     *g          // to avoid deadlocks during recursive lock locks
	enabled       bool        // when set runtime traces events
	shutdown      bool        // set when we are waiting for trace reader to finish after setting enabled to false
	headerWritten bool        // whether ReadTrace has emitted trace header
	footerWritten bool        // whether ReadTrace has emitted trace footer
	shutdownSema  uint32      // used to wait for ReadTrace completion
	seqStart      uint64      // sequence number when tracing was started
	ticksStart    int64       // cputicks when tracing was started
	ticksEnd      int64       // cputicks when tracing was stopped
	timeStart     int64       // nanotime when tracing was started
	timeEnd       int64       // nanotime when tracing was stopped
	seqGC         uint64      // GC start/done sequencer
	reading       traceBufPtr // buffer currently handed off to user
	empty         traceBufPtr // stack of empty buffers
	fullHead      traceBufPtr // queue of full buffers
	fullTail      traceBufPtr
	reader        guintptr        // goroutine that called ReadTrace, or nil
	stackTab      traceStackTable // maps stack traces to unique ids

	// Dictionary for traceEvString.
	//
	// TODO: central lock to access the map is not ideal.
	//   option: pre-assign ids to all user annotation region names and tags
	//   option: per-P cache
	//   option: sync.Map like data structure
	stringsLock mutex
	strings     map[string]uint64
	stringSeq   uint64

	// markWorkerLabels maps gcMarkWorkerMode to string ID.
	markWorkerLabels [len(gcMarkWorkerModeStrings)]uint64

	bufLock mutex       // protects buf
	buf     traceBufPtr // global trace buffer, used when running without a p
} = ...
```

trace is global tracing context. 

### <a id="gStatusStrings" href="#gStatusStrings">var gStatusStrings</a>

```
searchKey: runtime.gStatusStrings
```

```Go
var gStatusStrings = ...
```

### <a id="cgoTraceback" href="#cgoTraceback">var cgoTraceback</a>

```
searchKey: runtime.cgoTraceback
```

```Go
var cgoTraceback unsafe.Pointer
```

### <a id="cgoContext" href="#cgoContext">var cgoContext</a>

```
searchKey: runtime.cgoContext
```

```Go
var cgoContext unsafe.Pointer
```

### <a id="cgoSymbolizer" href="#cgoSymbolizer">var cgoSymbolizer</a>

```
searchKey: runtime.cgoSymbolizer
```

```Go
var cgoSymbolizer unsafe.Pointer
```

### <a id="reflectOffs" href="#reflectOffs">var reflectOffs</a>

```
searchKey: runtime.reflectOffs
```

```Go
var reflectOffs struct {
	lock mutex
	next int32
	m    map[int32]unsafe.Pointer
	minv map[unsafe.Pointer]int32
} = ...
```

reflectOffs holds type offsets defined at run time by the reflect package. 

When a type is defined at run time, its *rtype data lives on the heap. There are a wide range of possible addresses the heap may use, that may not be representable as a 32-bit offset. Moreover the GC may one day start moving heap memory, in which case there is no stable offset that can be defined. 

To provide stable offsets, we add pin *rtype objects in a global map and treat the offset as an identifier. We use negative offsets that do not overlap with any compile-time module offsets. 

Entries are created by reflect.addReflectOff. 

### <a id="Dlog" href="#Dlog">var Dlog</a>

```
searchKey: runtime.Dlog
```

```Go
var Dlog = dlog
```

### <a id="Mmap" href="#Mmap">var Mmap</a>

```
searchKey: runtime.Mmap
```

```Go
var Mmap = mmap
```

### <a id="Munmap" href="#Munmap">var Munmap</a>

```
searchKey: runtime.Munmap
```

```Go
var Munmap = munmap
```

### <a id="Pipe" href="#Pipe">var Pipe</a>

```
searchKey: runtime.Pipe
```

```Go
var Pipe = pipe
```

### <a id="Fadd64" href="#Fadd64">var Fadd64</a>

```
searchKey: runtime.Fadd64
```

```Go
var Fadd64 = fadd64
```

### <a id="Fsub64" href="#Fsub64">var Fsub64</a>

```
searchKey: runtime.Fsub64
```

```Go
var Fsub64 = fsub64
```

### <a id="Fmul64" href="#Fmul64">var Fmul64</a>

```
searchKey: runtime.Fmul64
```

```Go
var Fmul64 = fmul64
```

### <a id="Fdiv64" href="#Fdiv64">var Fdiv64</a>

```
searchKey: runtime.Fdiv64
```

```Go
var Fdiv64 = fdiv64
```

### <a id="F64to32" href="#F64to32">var F64to32</a>

```
searchKey: runtime.F64to32
```

```Go
var F64to32 = f64to32
```

### <a id="F32to64" href="#F32to64">var F32to64</a>

```
searchKey: runtime.F32to64
```

```Go
var F32to64 = f32to64
```

### <a id="Fcmp64" href="#Fcmp64">var Fcmp64</a>

```
searchKey: runtime.Fcmp64
```

```Go
var Fcmp64 = fcmp64
```

### <a id="Fintto64" href="#Fintto64">var Fintto64</a>

```
searchKey: runtime.Fintto64
```

```Go
var Fintto64 = fintto64
```

### <a id="F64toint" href="#F64toint">var F64toint</a>

```
searchKey: runtime.F64toint
```

```Go
var F64toint = f64toint
```

### <a id="Entersyscall" href="#Entersyscall">var Entersyscall</a>

```
searchKey: runtime.Entersyscall
```

```Go
var Entersyscall = entersyscall
```

### <a id="Exitsyscall" href="#Exitsyscall">var Exitsyscall</a>

```
searchKey: runtime.Exitsyscall
```

```Go
var Exitsyscall = exitsyscall
```

### <a id="LockedOSThread" href="#LockedOSThread">var LockedOSThread</a>

```
searchKey: runtime.LockedOSThread
```

```Go
var LockedOSThread = lockedOSThread
```

### <a id="Xadduintptr" href="#Xadduintptr">var Xadduintptr</a>

```
searchKey: runtime.Xadduintptr
```

```Go
var Xadduintptr = atomic.Xadduintptr
```

### <a id="FuncPC" href="#FuncPC">var FuncPC</a>

```
searchKey: runtime.FuncPC
```

```Go
var FuncPC = funcPC
```

### <a id="Fastlog2" href="#Fastlog2">var Fastlog2</a>

```
searchKey: runtime.Fastlog2
```

```Go
var Fastlog2 = fastlog2
```

### <a id="Atoi" href="#Atoi">var Atoi</a>

```
searchKey: runtime.Atoi
```

```Go
var Atoi = atoi
```

### <a id="Atoi32" href="#Atoi32">var Atoi32</a>

```
searchKey: runtime.Atoi32
```

```Go
var Atoi32 = atoi32
```

### <a id="Nanotime" href="#Nanotime">var Nanotime</a>

```
searchKey: runtime.Nanotime
```

```Go
var Nanotime = nanotime
```

### <a id="NetpollBreak" href="#NetpollBreak">var NetpollBreak</a>

```
searchKey: runtime.NetpollBreak
```

```Go
var NetpollBreak = netpollBreak
```

### <a id="Usleep" href="#Usleep">var Usleep</a>

```
searchKey: runtime.Usleep
```

```Go
var Usleep = usleep
```

### <a id="PhysPageSize" href="#PhysPageSize">var PhysPageSize</a>

```
searchKey: runtime.PhysPageSize
```

```Go
var PhysPageSize = physPageSize
```

### <a id="PhysHugePageSize" href="#PhysHugePageSize">var PhysHugePageSize</a>

```
searchKey: runtime.PhysHugePageSize
```

```Go
var PhysHugePageSize = physHugePageSize
```

### <a id="NetpollGenericInit" href="#NetpollGenericInit">var NetpollGenericInit</a>

```
searchKey: runtime.NetpollGenericInit
```

```Go
var NetpollGenericInit = netpollGenericInit
```

### <a id="Memmove" href="#Memmove">var Memmove</a>

```
searchKey: runtime.Memmove
```

```Go
var Memmove = memmove
```

### <a id="MemclrNoHeapPointers" href="#MemclrNoHeapPointers">var MemclrNoHeapPointers</a>

```
searchKey: runtime.MemclrNoHeapPointers
```

```Go
var MemclrNoHeapPointers = memclrNoHeapPointers
```

### <a id="LockPartialOrder" href="#LockPartialOrder">var LockPartialOrder</a>

```
searchKey: runtime.LockPartialOrder
```

```Go
var LockPartialOrder = lockPartialOrder
```

### <a id="RunSchedLocalQueueEmptyState" href="#RunSchedLocalQueueEmptyState">var RunSchedLocalQueueEmptyState</a>

```
searchKey: runtime.RunSchedLocalQueueEmptyState
```

```Go
var RunSchedLocalQueueEmptyState struct {
	done  chan bool
	ready *uint32
	p     *p
}
```

Temporary to enable register ABI bringup. TODO(register args): convert back to local variables in RunSchedLocalQueueEmptyTest that get passed to the "go" stmts there. 

### <a id="StringHash" href="#StringHash">var StringHash</a>

```
searchKey: runtime.StringHash
```

```Go
var StringHash = stringHash
```

### <a id="BytesHash" href="#BytesHash">var BytesHash</a>

```
searchKey: runtime.BytesHash
```

```Go
var BytesHash = bytesHash
```

### <a id="Int32Hash" href="#Int32Hash">var Int32Hash</a>

```
searchKey: runtime.Int32Hash
```

```Go
var Int32Hash = int32Hash
```

### <a id="Int64Hash" href="#Int64Hash">var Int64Hash</a>

```
searchKey: runtime.Int64Hash
```

```Go
var Int64Hash = int64Hash
```

### <a id="MemHash" href="#MemHash">var MemHash</a>

```
searchKey: runtime.MemHash
```

```Go
var MemHash = memhash
```

### <a id="MemHash32" href="#MemHash32">var MemHash32</a>

```
searchKey: runtime.MemHash32
```

```Go
var MemHash32 = memhash32
```

### <a id="MemHash64" href="#MemHash64">var MemHash64</a>

```
searchKey: runtime.MemHash64
```

```Go
var MemHash64 = memhash64
```

### <a id="EfaceHash" href="#EfaceHash">var EfaceHash</a>

```
searchKey: runtime.EfaceHash
```

```Go
var EfaceHash = efaceHash
```

### <a id="IfaceHash" href="#IfaceHash">var IfaceHash</a>

```
searchKey: runtime.IfaceHash
```

```Go
var IfaceHash = ifaceHash
```

### <a id="UseAeshash" href="#UseAeshash">var UseAeshash</a>

```
searchKey: runtime.UseAeshash
```

```Go
var UseAeshash = &useAeshash
```

### <a id="HashLoad" href="#HashLoad">var HashLoad</a>

```
searchKey: runtime.HashLoad
```

```Go
var HashLoad = &hashLoad
```

### <a id="Open" href="#Open">var Open</a>

```
searchKey: runtime.Open
```

```Go
var Open = open
```

### <a id="Close" href="#Close">var Close</a>

```
searchKey: runtime.Close
```

```Go
var Close = closefd
```

### <a id="Read" href="#Read">var Read</a>

```
searchKey: runtime.Read
```

```Go
var Read = read
```

### <a id="Write" href="#Write">var Write</a>

```
searchKey: runtime.Write
```

```Go
var Write = write
```

### <a id="BigEndian" href="#BigEndian">var BigEndian</a>

```
searchKey: runtime.BigEndian
```

```Go
var BigEndian = sys.BigEndian
```

### <a id="ForceGCPeriod" href="#ForceGCPeriod">var ForceGCPeriod</a>

```
searchKey: runtime.ForceGCPeriod
```

```Go
var ForceGCPeriod = &forcegcperiod
```

### <a id="ReadUnaligned32" href="#ReadUnaligned32">var ReadUnaligned32</a>

```
searchKey: runtime.ReadUnaligned32
```

```Go
var ReadUnaligned32 = readUnaligned32
```

### <a id="ReadUnaligned64" href="#ReadUnaligned64">var ReadUnaligned64</a>

```
searchKey: runtime.ReadUnaligned64
```

```Go
var ReadUnaligned64 = readUnaligned64
```

### <a id="BaseChunkIdx" href="#BaseChunkIdx">var BaseChunkIdx</a>

```
searchKey: runtime.BaseChunkIdx
```

```Go
var BaseChunkIdx = ...
```

BaseChunkIdx is a convenient chunkIdx value which works on both 64 bit and 32 bit platforms, allowing the tests to share code between the two. 

This should not be higher than 0x100*pallocChunkBytes to support mips and mipsle, which only have 31-bit address spaces. 

### <a id="Semacquire" href="#Semacquire">var Semacquire</a>

```
searchKey: runtime.Semacquire
```

```Go
var Semacquire = semacquire
```

### <a id="Semrelease1" href="#Semrelease1">var Semrelease1</a>

```
searchKey: runtime.Semrelease1
```

```Go
var Semrelease1 = semrelease1
```

### <a id="GCTestMoveStackOnNextCall" href="#GCTestMoveStackOnNextCall">var GCTestMoveStackOnNextCall</a>

```
searchKey: runtime.GCTestMoveStackOnNextCall
```

```Go
var GCTestMoveStackOnNextCall = gcTestMoveStackOnNextCall
```

For GCTestMoveStackOnNextCall, it's important not to introduce an extra layer of call, since then there's a return before the "real" next call. 

### <a id="NonblockingPipe" href="#NonblockingPipe">var NonblockingPipe</a>

```
searchKey: runtime.NonblockingPipe
```

```Go
var NonblockingPipe = nonblockingPipe
```

### <a id="SetNonblock" href="#SetNonblock">var SetNonblock</a>

```
searchKey: runtime.SetNonblock
```

```Go
var SetNonblock = setNonblock
```

### <a id="Closeonexec" href="#Closeonexec">var Closeonexec</a>

```
searchKey: runtime.Closeonexec
```

```Go
var Closeonexec = closeonexec
```

### <a id="waitForSigusr1" href="#waitForSigusr1">var waitForSigusr1</a>

```
searchKey: runtime.waitForSigusr1
```

```Go
var waitForSigusr1 struct {
	rdpipe int32
	wrpipe int32
	mID    int64
}
```

## <a id="type" href="#type">Types</a>

```
tags: [exported]
```

### <a id="cgoCallers" href="#cgoCallers">type cgoCallers [32]uintptr</a>

```
searchKey: runtime.cgoCallers
```

```Go
type cgoCallers [32]uintptr
```

Addresses collected in a cgo backtrace when crashing. Length must match arg.Max in x_cgo_callers in runtime/cgo/gcc_traceback.c. 

### <a id="argset" href="#argset">type argset struct</a>

```
searchKey: runtime.argset
```

```Go
type argset struct {
	args   unsafe.Pointer
	retval uintptr
}
```

argset matches runtime/cgo/linux_syscall.c:argset_t 

### <a id="hchan" href="#hchan">type hchan struct</a>

```
searchKey: runtime.hchan
```

```Go
type hchan struct {
	qcount   uint           // total data in the queue
	dataqsiz uint           // size of the circular queue
	buf      unsafe.Pointer // points to an array of dataqsiz elements
	elemsize uint16
	closed   uint32
	elemtype *_type // element type
	sendx    uint   // send index
	recvx    uint   // receive index
	recvq    waitq  // list of recv waiters
	sendq    waitq  // list of send waiters

	// lock protects all fields in hchan, as well as several
	// fields in sudogs blocked on this channel.
	//
	// Do not change another G's status while holding this lock
	// (in particular, do not ready a G), as this can deadlock
	// with stack shrinking.
	lock mutex
}
```

#### <a id="reflect_makechan" href="#reflect_makechan">func reflect_makechan(t *chantype, size int) *hchan</a>

```
searchKey: runtime.reflect_makechan
```

```Go
func reflect_makechan(t *chantype, size int) *hchan
```

#### <a id="makechan64" href="#makechan64">func makechan64(t *chantype, size int64) *hchan</a>

```
searchKey: runtime.makechan64
```

```Go
func makechan64(t *chantype, size int64) *hchan
```

#### <a id="makechan" href="#makechan">func makechan(t *chantype, size int) *hchan</a>

```
searchKey: runtime.makechan
```

```Go
func makechan(t *chantype, size int) *hchan
```

#### <a id="hchan.raceaddr" href="#hchan.raceaddr">func (c *hchan) raceaddr() unsafe.Pointer</a>

```
searchKey: runtime.hchan.raceaddr
```

```Go
func (c *hchan) raceaddr() unsafe.Pointer
```

#### <a id="hchan.sortkey" href="#hchan.sortkey">func (c *hchan) sortkey() uintptr</a>

```
searchKey: runtime.hchan.sortkey
```

```Go
func (c *hchan) sortkey() uintptr
```

### <a id="waitq" href="#waitq">type waitq struct</a>

```
searchKey: runtime.waitq
```

```Go
type waitq struct {
	first *sudog
	last  *sudog
}
```

#### <a id="waitq.enqueue" href="#waitq.enqueue">func (q *waitq) enqueue(sgp *sudog)</a>

```
searchKey: runtime.waitq.enqueue
```

```Go
func (q *waitq) enqueue(sgp *sudog)
```

#### <a id="waitq.dequeue" href="#waitq.dequeue">func (q *waitq) dequeue() *sudog</a>

```
searchKey: runtime.waitq.dequeue
```

```Go
func (q *waitq) dequeue() *sudog
```

#### <a id="waitq.dequeueSudoG" href="#waitq.dequeueSudoG">func (q *waitq) dequeueSudoG(sgp *sudog)</a>

```
searchKey: runtime.waitq.dequeueSudoG
```

```Go
func (q *waitq) dequeueSudoG(sgp *sudog)
```

### <a id="cpuProfile" href="#cpuProfile">type cpuProfile struct</a>

```
searchKey: runtime.cpuProfile
```

```Go
type cpuProfile struct {
	lock mutex
	on   bool     // profiling is on
	log  *profBuf // profile events written here

	// extra holds extra stacks accumulated in addNonGo
	// corresponding to profiling signals arriving on
	// non-Go-created threads. Those stacks are written
	// to log the next time a normal Go thread gets the
	// signal handler.
	// Assuming the stacks are 2 words each (we don't get
	// a full traceback from those threads), plus one word
	// size for framing, 100 Hz profiling would generate
	// 300 words per second.
	// Hopefully a normal Go thread will get the profiling
	// signal at least once every few seconds.
	extra      [1000]uintptr
	numExtra   int
	lostExtra  uint64 // count of frames lost because extra is full
	lostAtomic uint64 // count of frames lost because of being in atomic64 on mips/arm; updated racily
}
```

#### <a id="cpuProfile.add" href="#cpuProfile.add">func (p *cpuProfile) add(gp *g, stk []uintptr)</a>

```
searchKey: runtime.cpuProfile.add
```

```Go
func (p *cpuProfile) add(gp *g, stk []uintptr)
```

add adds the stack trace to the profile. It is called from signal handlers and other limited environments and cannot allocate memory or acquire locks that might be held at the time of the signal, nor can it use substantial amounts of stack. 

#### <a id="cpuProfile.addNonGo" href="#cpuProfile.addNonGo">func (p *cpuProfile) addNonGo(stk []uintptr)</a>

```
searchKey: runtime.cpuProfile.addNonGo
```

```Go
func (p *cpuProfile) addNonGo(stk []uintptr)
```

addNonGo adds the non-Go stack trace to the profile. It is called from a non-Go thread, so we cannot use much stack at all, nor do anything that needs a g or an m. In particular, we can't call cpuprof.log.write. Instead, we copy the stack into cpuprof.extra, which will be drained the next time a Go thread gets the signal handling event. 

#### <a id="cpuProfile.addExtra" href="#cpuProfile.addExtra">func (p *cpuProfile) addExtra()</a>

```
searchKey: runtime.cpuProfile.addExtra
```

```Go
func (p *cpuProfile) addExtra()
```

addExtra adds the "extra" profiling events, queued by addNonGo, to the profile log. addExtra is called either from a signal handler on a Go thread or from an ordinary goroutine; either way it can use stack and has a g. The world may be stopped, though. 

### <a id="debugCallWrapArgs" href="#debugCallWrapArgs">type debugCallWrapArgs struct</a>

```
searchKey: runtime.debugCallWrapArgs
```

```Go
type debugCallWrapArgs struct {
	dispatch uintptr
	callingG *g
}
```

### <a id="dlogger" href="#dlogger">type dlogger struct</a>

```
searchKey: runtime.dlogger
```

```Go
type dlogger struct {
	w debugLogWriter

	// allLink is the next dlogger in the allDloggers list.
	allLink *dlogger

	// owned indicates that this dlogger is owned by an M. This is
	// accessed atomically.
	owned uint32
}
```

A dlogger writes to the debug log. 

To obtain a dlogger, call dlog(). When done with the dlogger, call end(). 

#### <a id="dlog" href="#dlog">func dlog() *dlogger</a>

```
searchKey: runtime.dlog
```

```Go
func dlog() *dlogger
```

dlog returns a debug logger. The caller can use methods on the returned logger to add values, which will be space-separated in the final output, much like println. The caller must call end() to finish the message. 

dlog can be used from highly-constrained corners of the runtime: it is safe to use in the signal handler, from within the write barrier, from within the stack implementation, and in places that must be recursively nosplit. 

This will be compiled away if built without the debuglog build tag. However, argument construction may not be. If any of the arguments are not literals or trivial expressions, consider protecting the call with "if dlogEnabled". 

#### <a id="getCachedDlogger" href="#getCachedDlogger">func getCachedDlogger() *dlogger</a>

```
searchKey: runtime.getCachedDlogger
```

```Go
func getCachedDlogger() *dlogger
```

#### <a id="dlogger.end" href="#dlogger.end">func (l *dlogger) end()</a>

```
searchKey: runtime.dlogger.end
```

```Go
func (l *dlogger) end()
```

#### <a id="dlogger.b" href="#dlogger.b">func (l *dlogger) b(x bool) *dlogger</a>

```
searchKey: runtime.dlogger.b
```

```Go
func (l *dlogger) b(x bool) *dlogger
```

#### <a id="dlogger.i" href="#dlogger.i">func (l *dlogger) i(x int) *dlogger</a>

```
searchKey: runtime.dlogger.i
```

```Go
func (l *dlogger) i(x int) *dlogger
```

#### <a id="dlogger.i8" href="#dlogger.i8">func (l *dlogger) i8(x int8) *dlogger</a>

```
searchKey: runtime.dlogger.i8
```

```Go
func (l *dlogger) i8(x int8) *dlogger
```

#### <a id="dlogger.i16" href="#dlogger.i16">func (l *dlogger) i16(x int16) *dlogger</a>

```
searchKey: runtime.dlogger.i16
```

```Go
func (l *dlogger) i16(x int16) *dlogger
```

#### <a id="dlogger.i32" href="#dlogger.i32">func (l *dlogger) i32(x int32) *dlogger</a>

```
searchKey: runtime.dlogger.i32
```

```Go
func (l *dlogger) i32(x int32) *dlogger
```

#### <a id="dlogger.i64" href="#dlogger.i64">func (l *dlogger) i64(x int64) *dlogger</a>

```
searchKey: runtime.dlogger.i64
```

```Go
func (l *dlogger) i64(x int64) *dlogger
```

#### <a id="dlogger.u" href="#dlogger.u">func (l *dlogger) u(x uint) *dlogger</a>

```
searchKey: runtime.dlogger.u
```

```Go
func (l *dlogger) u(x uint) *dlogger
```

#### <a id="dlogger.uptr" href="#dlogger.uptr">func (l *dlogger) uptr(x uintptr) *dlogger</a>

```
searchKey: runtime.dlogger.uptr
```

```Go
func (l *dlogger) uptr(x uintptr) *dlogger
```

#### <a id="dlogger.u8" href="#dlogger.u8">func (l *dlogger) u8(x uint8) *dlogger</a>

```
searchKey: runtime.dlogger.u8
```

```Go
func (l *dlogger) u8(x uint8) *dlogger
```

#### <a id="dlogger.u16" href="#dlogger.u16">func (l *dlogger) u16(x uint16) *dlogger</a>

```
searchKey: runtime.dlogger.u16
```

```Go
func (l *dlogger) u16(x uint16) *dlogger
```

#### <a id="dlogger.u32" href="#dlogger.u32">func (l *dlogger) u32(x uint32) *dlogger</a>

```
searchKey: runtime.dlogger.u32
```

```Go
func (l *dlogger) u32(x uint32) *dlogger
```

#### <a id="dlogger.u64" href="#dlogger.u64">func (l *dlogger) u64(x uint64) *dlogger</a>

```
searchKey: runtime.dlogger.u64
```

```Go
func (l *dlogger) u64(x uint64) *dlogger
```

#### <a id="dlogger.hex" href="#dlogger.hex">func (l *dlogger) hex(x uint64) *dlogger</a>

```
searchKey: runtime.dlogger.hex
```

```Go
func (l *dlogger) hex(x uint64) *dlogger
```

#### <a id="dlogger.p" href="#dlogger.p">func (l *dlogger) p(x interface{}) *dlogger</a>

```
searchKey: runtime.dlogger.p
```

```Go
func (l *dlogger) p(x interface{}) *dlogger
```

#### <a id="dlogger.s" href="#dlogger.s">func (l *dlogger) s(x string) *dlogger</a>

```
searchKey: runtime.dlogger.s
```

```Go
func (l *dlogger) s(x string) *dlogger
```

#### <a id="dlogger.pc" href="#dlogger.pc">func (l *dlogger) pc(x uintptr) *dlogger</a>

```
searchKey: runtime.dlogger.pc
```

```Go
func (l *dlogger) pc(x uintptr) *dlogger
```

#### <a id="dlogger.traceback" href="#dlogger.traceback">func (l *dlogger) traceback(x []uintptr) *dlogger</a>

```
searchKey: runtime.dlogger.traceback
```

```Go
func (l *dlogger) traceback(x []uintptr) *dlogger
```

#### <a id="dlogger.End" href="#dlogger.End">func (l *dlogger) End()</a>

```
searchKey: runtime.dlogger.End
```

```Go
func (l *dlogger) End()
```

#### <a id="dlogger.B" href="#dlogger.B">func (l *dlogger) B(x bool) *dlogger</a>

```
searchKey: runtime.dlogger.B
```

```Go
func (l *dlogger) B(x bool) *dlogger
```

#### <a id="dlogger.I" href="#dlogger.I">func (l *dlogger) I(x int) *dlogger</a>

```
searchKey: runtime.dlogger.I
```

```Go
func (l *dlogger) I(x int) *dlogger
```

#### <a id="dlogger.I16" href="#dlogger.I16">func (l *dlogger) I16(x int16) *dlogger</a>

```
searchKey: runtime.dlogger.I16
```

```Go
func (l *dlogger) I16(x int16) *dlogger
```

#### <a id="dlogger.U64" href="#dlogger.U64">func (l *dlogger) U64(x uint64) *dlogger</a>

```
searchKey: runtime.dlogger.U64
```

```Go
func (l *dlogger) U64(x uint64) *dlogger
```

#### <a id="dlogger.Hex" href="#dlogger.Hex">func (l *dlogger) Hex(x uint64) *dlogger</a>

```
searchKey: runtime.dlogger.Hex
```

```Go
func (l *dlogger) Hex(x uint64) *dlogger
```

#### <a id="dlogger.P" href="#dlogger.P">func (l *dlogger) P(x interface{}) *dlogger</a>

```
searchKey: runtime.dlogger.P
```

```Go
func (l *dlogger) P(x interface{}) *dlogger
```

#### <a id="dlogger.S" href="#dlogger.S">func (l *dlogger) S(x string) *dlogger</a>

```
searchKey: runtime.dlogger.S
```

```Go
func (l *dlogger) S(x string) *dlogger
```

#### <a id="dlogger.PC" href="#dlogger.PC">func (l *dlogger) PC(x uintptr) *dlogger</a>

```
searchKey: runtime.dlogger.PC
```

```Go
func (l *dlogger) PC(x uintptr) *dlogger
```

### <a id="debugLogWriter" href="#debugLogWriter">type debugLogWriter struct</a>

```
searchKey: runtime.debugLogWriter
```

```Go
type debugLogWriter struct {
	write uint64
	data  debugLogBuf

	// tick and nano are the time bases from the most recently
	// written sync record.
	tick, nano uint64

	// r is a reader that consumes records as they get overwritten
	// by the writer. It also acts as the initial reader state
	// when printing the log.
	r debugLogReader

	// buf is a scratch buffer for encoding. This is here to
	// reduce stack usage.
	buf [10]byte
}
```

A debugLogWriter is a ring buffer of binary debug log records. 

A log record consists of a 2-byte framing header and a sequence of fields. The framing header gives the size of the record as a little endian 16-bit value. Each field starts with a byte indicating its type, followed by type-specific data. If the size in the framing header is 0, it's a sync record consisting of two little endian 64-bit values giving a new time base. 

Because this is a ring buffer, new records will eventually overwrite old records. Hence, it maintains a reader that consumes the log as it gets overwritten. That reader state is where an actual log reader would start. 

#### <a id="debugLogWriter.ensure" href="#debugLogWriter.ensure">func (l *debugLogWriter) ensure(n uint64)</a>

```
searchKey: runtime.debugLogWriter.ensure
```

```Go
func (l *debugLogWriter) ensure(n uint64)
```

#### <a id="debugLogWriter.writeFrameAt" href="#debugLogWriter.writeFrameAt">func (l *debugLogWriter) writeFrameAt(pos, size uint64) bool</a>

```
searchKey: runtime.debugLogWriter.writeFrameAt
```

```Go
func (l *debugLogWriter) writeFrameAt(pos, size uint64) bool
```

#### <a id="debugLogWriter.writeSync" href="#debugLogWriter.writeSync">func (l *debugLogWriter) writeSync(tick, nano uint64)</a>

```
searchKey: runtime.debugLogWriter.writeSync
```

```Go
func (l *debugLogWriter) writeSync(tick, nano uint64)
```

#### <a id="debugLogWriter.writeUint64LE" href="#debugLogWriter.writeUint64LE">func (l *debugLogWriter) writeUint64LE(x uint64)</a>

```
searchKey: runtime.debugLogWriter.writeUint64LE
```

```Go
func (l *debugLogWriter) writeUint64LE(x uint64)
```

#### <a id="debugLogWriter.byte" href="#debugLogWriter.byte">func (l *debugLogWriter) byte(x byte)</a>

```
searchKey: runtime.debugLogWriter.byte
```

```Go
func (l *debugLogWriter) byte(x byte)
```

#### <a id="debugLogWriter.bytes" href="#debugLogWriter.bytes">func (l *debugLogWriter) bytes(x []byte)</a>

```
searchKey: runtime.debugLogWriter.bytes
```

```Go
func (l *debugLogWriter) bytes(x []byte)
```

#### <a id="debugLogWriter.varint" href="#debugLogWriter.varint">func (l *debugLogWriter) varint(x int64)</a>

```
searchKey: runtime.debugLogWriter.varint
```

```Go
func (l *debugLogWriter) varint(x int64)
```

#### <a id="debugLogWriter.uvarint" href="#debugLogWriter.uvarint">func (l *debugLogWriter) uvarint(u uint64)</a>

```
searchKey: runtime.debugLogWriter.uvarint
```

```Go
func (l *debugLogWriter) uvarint(u uint64)
```

### <a id="debugLogBuf" href="#debugLogBuf">type debugLogBuf [16384]byte</a>

```
searchKey: runtime.debugLogBuf
```

```Go
type debugLogBuf [debugLogBytes]byte
```

### <a id="debugLogReader" href="#debugLogReader">type debugLogReader struct</a>

```
searchKey: runtime.debugLogReader
```

```Go
type debugLogReader struct {
	data *debugLogBuf

	// begin and end are the positions in the log of the beginning
	// and end of the log data, modulo len(data).
	begin, end uint64

	// tick and nano are the current time base at begin.
	tick, nano uint64
}
```

#### <a id="debugLogReader.skip" href="#debugLogReader.skip">func (r *debugLogReader) skip() uint64</a>

```
searchKey: runtime.debugLogReader.skip
```

```Go
func (r *debugLogReader) skip() uint64
```

#### <a id="debugLogReader.readUint16LEAt" href="#debugLogReader.readUint16LEAt">func (r *debugLogReader) readUint16LEAt(pos uint64) uint16</a>

```
searchKey: runtime.debugLogReader.readUint16LEAt
```

```Go
func (r *debugLogReader) readUint16LEAt(pos uint64) uint16
```

#### <a id="debugLogReader.readUint64LEAt" href="#debugLogReader.readUint64LEAt">func (r *debugLogReader) readUint64LEAt(pos uint64) uint64</a>

```
searchKey: runtime.debugLogReader.readUint64LEAt
```

```Go
func (r *debugLogReader) readUint64LEAt(pos uint64) uint64
```

#### <a id="debugLogReader.peek" href="#debugLogReader.peek">func (r *debugLogReader) peek() (tick uint64)</a>

```
searchKey: runtime.debugLogReader.peek
```

```Go
func (r *debugLogReader) peek() (tick uint64)
```

#### <a id="debugLogReader.header" href="#debugLogReader.header">func (r *debugLogReader) header() (end, tick, nano uint64, p int)</a>

```
searchKey: runtime.debugLogReader.header
```

```Go
func (r *debugLogReader) header() (end, tick, nano uint64, p int)
```

#### <a id="debugLogReader.uvarint" href="#debugLogReader.uvarint">func (r *debugLogReader) uvarint() uint64</a>

```
searchKey: runtime.debugLogReader.uvarint
```

```Go
func (r *debugLogReader) uvarint() uint64
```

#### <a id="debugLogReader.varint" href="#debugLogReader.varint">func (r *debugLogReader) varint() int64</a>

```
searchKey: runtime.debugLogReader.varint
```

```Go
func (r *debugLogReader) varint() int64
```

#### <a id="debugLogReader.printVal" href="#debugLogReader.printVal">func (r *debugLogReader) printVal() bool</a>

```
searchKey: runtime.debugLogReader.printVal
```

```Go
func (r *debugLogReader) printVal() bool
```

### <a id="dlogPerM" href="#dlogPerM">type dlogPerM struct{}</a>

```
searchKey: runtime.dlogPerM
```

```Go
type dlogPerM struct{}
```

### <a id="stackt" href="#stackt">type stackt struct</a>

```
searchKey: runtime.stackt
```

```Go
type stackt struct {
	ss_sp     *byte
	ss_size   uintptr
	ss_flags  int32
	pad_cgo_0 [4]byte
}
```

### <a id="sigactiont" href="#sigactiont">type sigactiont struct</a>

```
searchKey: runtime.sigactiont
```

```Go
type sigactiont struct {
	__sigaction_u [8]byte
	sa_tramp      unsafe.Pointer
	sa_mask       uint32
	sa_flags      int32
}
```

### <a id="usigactiont" href="#usigactiont">type usigactiont struct</a>

```
searchKey: runtime.usigactiont
```

```Go
type usigactiont struct {
	__sigaction_u [8]byte
	sa_mask       uint32
	sa_flags      int32
}
```

### <a id="siginfo" href="#siginfo">type siginfo struct</a>

```
searchKey: runtime.siginfo
```

```Go
type siginfo struct {
	si_signo  int32
	si_errno  int32
	si_code   int32
	si_pid    int32
	si_uid    uint32
	si_status int32
	si_addr   uint64
	si_value  [8]byte
	si_band   int64
	__pad     [7]uint64
}
```

### <a id="timeval" href="#timeval">type timeval struct</a>

```
searchKey: runtime.timeval
```

```Go
type timeval struct {
	tv_sec    int64
	tv_usec   int32
	pad_cgo_0 [4]byte
}
```

#### <a id="timeval.set_usec" href="#timeval.set_usec">func (tv *timeval) set_usec(x int32)</a>

```
searchKey: runtime.timeval.set_usec
```

```Go
func (tv *timeval) set_usec(x int32)
```

### <a id="itimerval" href="#itimerval">type itimerval struct</a>

```
searchKey: runtime.itimerval
```

```Go
type itimerval struct {
	it_interval timeval
	it_value    timeval
}
```

### <a id="timespec" href="#timespec">type timespec struct</a>

```
searchKey: runtime.timespec
```

```Go
type timespec struct {
	tv_sec  int64
	tv_nsec int64
}
```

#### <a id="timespec.setNsec" href="#timespec.setNsec">func (ts *timespec) setNsec(ns int64)</a>

```
searchKey: runtime.timespec.setNsec
```

```Go
func (ts *timespec) setNsec(ns int64)
```

### <a id="fpcontrol" href="#fpcontrol">type fpcontrol struct</a>

```
searchKey: runtime.fpcontrol
```

```Go
type fpcontrol struct {
	pad_cgo_0 [2]byte
}
```

### <a id="fpstatus" href="#fpstatus">type fpstatus struct</a>

```
searchKey: runtime.fpstatus
```

```Go
type fpstatus struct {
	pad_cgo_0 [2]byte
}
```

### <a id="regmmst" href="#regmmst">type regmmst struct</a>

```
searchKey: runtime.regmmst
```

```Go
type regmmst struct {
	mmst_reg  [10]int8
	mmst_rsrv [6]int8
}
```

### <a id="regxmm" href="#regxmm">type regxmm struct</a>

```
searchKey: runtime.regxmm
```

```Go
type regxmm struct {
	xmm_reg [16]int8
}
```

### <a id="regs64" href="#regs64">type regs64 struct</a>

```
searchKey: runtime.regs64
```

```Go
type regs64 struct {
	rax    uint64
	rbx    uint64
	rcx    uint64
	rdx    uint64
	rdi    uint64
	rsi    uint64
	rbp    uint64
	rsp    uint64
	r8     uint64
	r9     uint64
	r10    uint64
	r11    uint64
	r12    uint64
	r13    uint64
	r14    uint64
	r15    uint64
	rip    uint64
	rflags uint64
	cs     uint64
	fs     uint64
	gs     uint64
}
```

### <a id="floatstate64" href="#floatstate64">type floatstate64 struct</a>

```
searchKey: runtime.floatstate64
```

```Go
type floatstate64 struct {
	fpu_reserved  [2]int32
	fpu_fcw       fpcontrol
	fpu_fsw       fpstatus
	fpu_ftw       uint8
	fpu_rsrv1     uint8
	fpu_fop       uint16
	fpu_ip        uint32
	fpu_cs        uint16
	fpu_rsrv2     uint16
	fpu_dp        uint32
	fpu_ds        uint16
	fpu_rsrv3     uint16
	fpu_mxcsr     uint32
	fpu_mxcsrmask uint32
	fpu_stmm0     regmmst
	fpu_stmm1     regmmst
	fpu_stmm2     regmmst
	fpu_stmm3     regmmst
	fpu_stmm4     regmmst
	fpu_stmm5     regmmst
	fpu_stmm6     regmmst
	fpu_stmm7     regmmst
	fpu_xmm0      regxmm
	fpu_xmm1      regxmm
	fpu_xmm2      regxmm
	fpu_xmm3      regxmm
	fpu_xmm4      regxmm
	fpu_xmm5      regxmm
	fpu_xmm6      regxmm
	fpu_xmm7      regxmm
	fpu_xmm8      regxmm
	fpu_xmm9      regxmm
	fpu_xmm10     regxmm
	fpu_xmm11     regxmm
	fpu_xmm12     regxmm
	fpu_xmm13     regxmm
	fpu_xmm14     regxmm
	fpu_xmm15     regxmm
	fpu_rsrv4     [96]int8
	fpu_reserved1 int32
}
```

### <a id="exceptionstate64" href="#exceptionstate64">type exceptionstate64 struct</a>

```
searchKey: runtime.exceptionstate64
```

```Go
type exceptionstate64 struct {
	trapno     uint16
	cpu        uint16
	err        uint32
	faultvaddr uint64
}
```

### <a id="mcontext64" href="#mcontext64">type mcontext64 struct</a>

```
searchKey: runtime.mcontext64
```

```Go
type mcontext64 struct {
	es        exceptionstate64
	ss        regs64
	fs        floatstate64
	pad_cgo_0 [4]byte
}
```

### <a id="regs32" href="#regs32">type regs32 struct</a>

```
searchKey: runtime.regs32
```

```Go
type regs32 struct {
	eax    uint32
	ebx    uint32
	ecx    uint32
	edx    uint32
	edi    uint32
	esi    uint32
	ebp    uint32
	esp    uint32
	ss     uint32
	eflags uint32
	eip    uint32
	cs     uint32
	ds     uint32
	es     uint32
	fs     uint32
	gs     uint32
}
```

### <a id="floatstate32" href="#floatstate32">type floatstate32 struct</a>

```
searchKey: runtime.floatstate32
```

```Go
type floatstate32 struct {
	fpu_reserved  [2]int32
	fpu_fcw       fpcontrol
	fpu_fsw       fpstatus
	fpu_ftw       uint8
	fpu_rsrv1     uint8
	fpu_fop       uint16
	fpu_ip        uint32
	fpu_cs        uint16
	fpu_rsrv2     uint16
	fpu_dp        uint32
	fpu_ds        uint16
	fpu_rsrv3     uint16
	fpu_mxcsr     uint32
	fpu_mxcsrmask uint32
	fpu_stmm0     regmmst
	fpu_stmm1     regmmst
	fpu_stmm2     regmmst
	fpu_stmm3     regmmst
	fpu_stmm4     regmmst
	fpu_stmm5     regmmst
	fpu_stmm6     regmmst
	fpu_stmm7     regmmst
	fpu_xmm0      regxmm
	fpu_xmm1      regxmm
	fpu_xmm2      regxmm
	fpu_xmm3      regxmm
	fpu_xmm4      regxmm
	fpu_xmm5      regxmm
	fpu_xmm6      regxmm
	fpu_xmm7      regxmm
	fpu_rsrv4     [224]int8
	fpu_reserved1 int32
}
```

### <a id="exceptionstate32" href="#exceptionstate32">type exceptionstate32 struct</a>

```
searchKey: runtime.exceptionstate32
```

```Go
type exceptionstate32 struct {
	trapno     uint16
	cpu        uint16
	err        uint32
	faultvaddr uint32
}
```

### <a id="mcontext32" href="#mcontext32">type mcontext32 struct</a>

```
searchKey: runtime.mcontext32
```

```Go
type mcontext32 struct {
	es exceptionstate32
	ss regs32
	fs floatstate32
}
```

### <a id="ucontext" href="#ucontext">type ucontext struct</a>

```
searchKey: runtime.ucontext
```

```Go
type ucontext struct {
	uc_onstack  int32
	uc_sigmask  uint32
	uc_stack    stackt
	uc_link     *ucontext
	uc_mcsize   uint64
	uc_mcontext *mcontext64
}
```

### <a id="keventt" href="#keventt">type keventt struct</a>

```
searchKey: runtime.keventt
```

```Go
type keventt struct {
	ident  uint64
	filter int16
	flags  uint16
	fflags uint32
	data   int64
	udata  *byte
}
```

### <a id="pthread" href="#pthread">type pthread uintptr</a>

```
searchKey: runtime.pthread
```

```Go
type pthread uintptr
```

#### <a id="pthread_self" href="#pthread_self">func pthread_self() (t pthread)</a>

```
searchKey: runtime.pthread_self
```

```Go
func pthread_self() (t pthread)
```

### <a id="pthreadattr" href="#pthreadattr">type pthreadattr struct</a>

```
searchKey: runtime.pthreadattr
```

```Go
type pthreadattr struct {
	X__sig    int64
	X__opaque [56]int8
}
```

### <a id="pthreadmutex" href="#pthreadmutex">type pthreadmutex struct</a>

```
searchKey: runtime.pthreadmutex
```

```Go
type pthreadmutex struct {
	X__sig    int64
	X__opaque [56]int8
}
```

### <a id="pthreadmutexattr" href="#pthreadmutexattr">type pthreadmutexattr struct</a>

```
searchKey: runtime.pthreadmutexattr
```

```Go
type pthreadmutexattr struct {
	X__sig    int64
	X__opaque [8]int8
}
```

### <a id="pthreadcond" href="#pthreadcond">type pthreadcond struct</a>

```
searchKey: runtime.pthreadcond
```

```Go
type pthreadcond struct {
	X__sig    int64
	X__opaque [40]int8
}
```

### <a id="pthreadcondattr" href="#pthreadcondattr">type pthreadcondattr struct</a>

```
searchKey: runtime.pthreadcondattr
```

```Go
type pthreadcondattr struct {
	X__sig    int64
	X__opaque [8]int8
}
```

### <a id="machTimebaseInfo" href="#machTimebaseInfo">type machTimebaseInfo struct</a>

```
searchKey: runtime.machTimebaseInfo
```

```Go
type machTimebaseInfo struct {
	numer uint32
	denom uint32
}
```

### <a id="Error" href="#Error">type Error interface</a>

```
searchKey: runtime.Error
tags: [exported]
```

```Go
type Error interface {
	error

	// RuntimeError is a no-op function but
	// serves to distinguish types that are run time
	// errors from ordinary errors: a type is a
	// run time error if it has a RuntimeError method.
	RuntimeError()
}
```

The Error interface identifies a run time error. 

### <a id="TypeAssertionError" href="#TypeAssertionError">type TypeAssertionError struct</a>

```
searchKey: runtime.TypeAssertionError
tags: [exported]
```

```Go
type TypeAssertionError struct {
	_interface    *_type
	concrete      *_type
	asserted      *_type
	missingMethod string // one method needed by Interface, missing from Concrete
}
```

A TypeAssertionError explains a failed type assertion. 

#### <a id="TypeAssertionError.RuntimeError" href="#TypeAssertionError.RuntimeError">func (*TypeAssertionError) RuntimeError()</a>

```
searchKey: runtime.TypeAssertionError.RuntimeError
tags: [exported]
```

```Go
func (*TypeAssertionError) RuntimeError()
```

#### <a id="TypeAssertionError.Error" href="#TypeAssertionError.Error">func (e *TypeAssertionError) Error() string</a>

```
searchKey: runtime.TypeAssertionError.Error
tags: [exported]
```

```Go
func (e *TypeAssertionError) Error() string
```

### <a id="errorString" href="#errorString">type errorString string</a>

```
searchKey: runtime.errorString
```

```Go
type errorString string
```

An errorString represents a runtime error described by a single string. 

#### <a id="errorString.RuntimeError" href="#errorString.RuntimeError">func (e errorString) RuntimeError()</a>

```
searchKey: runtime.errorString.RuntimeError
```

```Go
func (e errorString) RuntimeError()
```

#### <a id="errorString.Error" href="#errorString.Error">func (e errorString) Error() string</a>

```
searchKey: runtime.errorString.Error
```

```Go
func (e errorString) Error() string
```

### <a id="errorAddressString" href="#errorAddressString">type errorAddressString struct</a>

```
searchKey: runtime.errorAddressString
```

```Go
type errorAddressString struct {
	msg  string  // error message
	addr uintptr // memory address where the error occurred
}
```

#### <a id="errorAddressString.RuntimeError" href="#errorAddressString.RuntimeError">func (e errorAddressString) RuntimeError()</a>

```
searchKey: runtime.errorAddressString.RuntimeError
```

```Go
func (e errorAddressString) RuntimeError()
```

#### <a id="errorAddressString.Error" href="#errorAddressString.Error">func (e errorAddressString) Error() string</a>

```
searchKey: runtime.errorAddressString.Error
```

```Go
func (e errorAddressString) Error() string
```

#### <a id="errorAddressString.Addr" href="#errorAddressString.Addr">func (e errorAddressString) Addr() uintptr</a>

```
searchKey: runtime.errorAddressString.Addr
```

```Go
func (e errorAddressString) Addr() uintptr
```

Addr returns the memory address where a fault occurred. The address provided is best-effort. The veracity of the result may depend on the platform. Errors providing this method will only be returned as a result of using runtime/debug.SetPanicOnFault. 

### <a id="plainError" href="#plainError">type plainError string</a>

```
searchKey: runtime.plainError
```

```Go
type plainError string
```

plainError represents a runtime error described a string without the prefix "runtime error: " after invoking errorString.Error(). See Issue #14965. 

#### <a id="plainError.RuntimeError" href="#plainError.RuntimeError">func (e plainError) RuntimeError()</a>

```
searchKey: runtime.plainError.RuntimeError
```

```Go
func (e plainError) RuntimeError()
```

#### <a id="plainError.Error" href="#plainError.Error">func (e plainError) Error() string</a>

```
searchKey: runtime.plainError.Error
```

```Go
func (e plainError) Error() string
```

### <a id="boundsError" href="#boundsError">type boundsError struct</a>

```
searchKey: runtime.boundsError
```

```Go
type boundsError struct {
	x int64
	y int
	// Values in an index or slice expression can be signed or unsigned.
	// That means we'd need 65 bits to encode all possible indexes, from -2^63 to 2^64-1.
	// Instead, we keep track of whether x should be interpreted as signed or unsigned.
	// y is known to be nonnegative and to fit in an int.
	signed bool
	code   boundsErrorCode
}
```

A boundsError represents an indexing or slicing operation gone wrong. 

#### <a id="boundsError.RuntimeError" href="#boundsError.RuntimeError">func (e boundsError) RuntimeError()</a>

```
searchKey: runtime.boundsError.RuntimeError
```

```Go
func (e boundsError) RuntimeError()
```

#### <a id="boundsError.Error" href="#boundsError.Error">func (e boundsError) Error() string</a>

```
searchKey: runtime.boundsError.Error
```

```Go
func (e boundsError) Error() string
```

### <a id="boundsErrorCode" href="#boundsErrorCode">type boundsErrorCode uint8</a>

```
searchKey: runtime.boundsErrorCode
```

```Go
type boundsErrorCode uint8
```

### <a id="stringer" href="#stringer">type stringer interface</a>

```
searchKey: runtime.stringer
```

```Go
type stringer interface {
	String() string
}
```

### <a id="typeCacheBucket" href="#typeCacheBucket">type typeCacheBucket struct</a>

```
searchKey: runtime.typeCacheBucket
```

```Go
type typeCacheBucket struct {
	t [typeCacheAssoc]*_type
}
```

### <a id="childInfo" href="#childInfo">type childInfo struct</a>

```
searchKey: runtime.childInfo
```

```Go
type childInfo struct {
	// Information passed up from the callee frame about
	// the layout of the outargs region.
	argoff uintptr   // where the arguments start in the frame
	arglen uintptr   // size of args region
	args   bitvector // if args.n >= 0, pointer map of args region
	sp     *uint8    // callee sp
	depth  uintptr   // depth in call stack (0 == most recent)
}
```

### <a id="timeHistogram" href="#timeHistogram">type timeHistogram struct</a>

```
searchKey: runtime.timeHistogram
```

```Go
type timeHistogram struct {
	counts [timeHistNumSuperBuckets * timeHistNumSubBuckets]uint64

	// underflow counts all the times we got a negative duration
	// sample. Because of how time works on some platforms, it's
	// possible to measure negative durations. We could ignore them,
	// but we record them anyway because it's better to have some
	// signal that it's happening than just missing samples.
	underflow uint64
}
```

timeHistogram represents a distribution of durations in nanoseconds. 

The accuracy and range of the histogram is defined by the timeHistSubBucketBits and timeHistNumSuperBuckets constants. 

It is an HDR histogram with exponentially-distributed buckets and linearly distributed sub-buckets. 

Counts in the histogram are updated atomically, so it is safe for concurrent use. It is also safe to read all the values atomically. 

#### <a id="timeHistogram.record" href="#timeHistogram.record">func (h *timeHistogram) record(duration int64)</a>

```
searchKey: runtime.timeHistogram.record
```

```Go
func (h *timeHistogram) record(duration int64)
```

record adds the given duration to the distribution. 

Disallow preemptions and stack growths because this function may run in sensitive locations. 

### <a id="itabTableType" href="#itabTableType">type itabTableType struct</a>

```
searchKey: runtime.itabTableType
```

```Go
type itabTableType struct {
	size    uintptr             // length of entries array. Always a power of 2.
	count   uintptr             // current number of filled entries.
	entries [itabInitSize]*itab // really [size] large
}
```

Note: change the formula in the mallocgc call in itabAdd if you change these fields. 

#### <a id="itabTableType.find" href="#itabTableType.find">func (t *itabTableType) find(inter *interfacetype, typ *_type) *itab</a>

```
searchKey: runtime.itabTableType.find
```

```Go
func (t *itabTableType) find(inter *interfacetype, typ *_type) *itab
```

find finds the given interface/type pair in t. Returns nil if the given interface/type pair isn't present. 

#### <a id="itabTableType.add" href="#itabTableType.add">func (t *itabTableType) add(m *itab)</a>

```
searchKey: runtime.itabTableType.add
```

```Go
func (t *itabTableType) add(m *itab)
```

add adds the given itab to itab table t. itabLock must be held. 

### <a id="uint16InterfacePtr" href="#uint16InterfacePtr">type uint16InterfacePtr uint16</a>

```
searchKey: runtime.uint16InterfacePtr
```

```Go
type uint16InterfacePtr uint16
```

The specialized convTx routines need a type descriptor to use when calling mallocgc. We don't need the type to be exact, just to have the correct size, alignment, and pointer-ness. However, when debugging, it'd be nice to have some indication in mallocgc where the types came from, so we use named types here. We then construct interface values of these types, and then extract the type word to use as needed. 

### <a id="uint32InterfacePtr" href="#uint32InterfacePtr">type uint32InterfacePtr uint32</a>

```
searchKey: runtime.uint32InterfacePtr
```

```Go
type uint32InterfacePtr uint32
```

The specialized convTx routines need a type descriptor to use when calling mallocgc. We don't need the type to be exact, just to have the correct size, alignment, and pointer-ness. However, when debugging, it'd be nice to have some indication in mallocgc where the types came from, so we use named types here. We then construct interface values of these types, and then extract the type word to use as needed. 

### <a id="uint64InterfacePtr" href="#uint64InterfacePtr">type uint64InterfacePtr uint64</a>

```
searchKey: runtime.uint64InterfacePtr
```

```Go
type uint64InterfacePtr uint64
```

The specialized convTx routines need a type descriptor to use when calling mallocgc. We don't need the type to be exact, just to have the correct size, alignment, and pointer-ness. However, when debugging, it'd be nice to have some indication in mallocgc where the types came from, so we use named types here. We then construct interface values of these types, and then extract the type word to use as needed. 

### <a id="stringInterfacePtr" href="#stringInterfacePtr">type stringInterfacePtr string</a>

```
searchKey: runtime.stringInterfacePtr
```

```Go
type stringInterfacePtr string
```

The specialized convTx routines need a type descriptor to use when calling mallocgc. We don't need the type to be exact, just to have the correct size, alignment, and pointer-ness. However, when debugging, it'd be nice to have some indication in mallocgc where the types came from, so we use named types here. We then construct interface values of these types, and then extract the type word to use as needed. 

### <a id="sliceInterfacePtr" href="#sliceInterfacePtr">type sliceInterfacePtr []byte</a>

```
searchKey: runtime.sliceInterfacePtr
```

```Go
type sliceInterfacePtr []byte
```

The specialized convTx routines need a type descriptor to use when calling mallocgc. We don't need the type to be exact, just to have the correct size, alignment, and pointer-ness. However, when debugging, it'd be nice to have some indication in mallocgc where the types came from, so we use named types here. We then construct interface values of these types, and then extract the type word to use as needed. 

### <a id="lfstack" href="#lfstack">type lfstack uint64</a>

```
searchKey: runtime.lfstack
```

```Go
type lfstack uint64
```

lfstack is the head of a lock-free stack. 

The zero value of lfstack is an empty list. 

This stack is intrusive. Nodes must embed lfnode as the first field. 

The stack does not keep GC-visible pointers to nodes, so the caller is responsible for ensuring the nodes are not garbage collected (typically by allocating them from manually-managed memory). 

#### <a id="lfstack.push" href="#lfstack.push">func (head *lfstack) push(node *lfnode)</a>

```
searchKey: runtime.lfstack.push
```

```Go
func (head *lfstack) push(node *lfnode)
```

#### <a id="lfstack.pop" href="#lfstack.pop">func (head *lfstack) pop() unsafe.Pointer</a>

```
searchKey: runtime.lfstack.pop
```

```Go
func (head *lfstack) pop() unsafe.Pointer
```

#### <a id="lfstack.empty" href="#lfstack.empty">func (head *lfstack) empty() bool</a>

```
searchKey: runtime.lfstack.empty
```

```Go
func (head *lfstack) empty() bool
```

### <a id="lockRank" href="#lockRank">type lockRank int</a>

```
searchKey: runtime.lockRank
```

```Go
type lockRank int
```

#### <a id="getLockRank" href="#getLockRank">func getLockRank(l *mutex) lockRank</a>

```
searchKey: runtime.getLockRank
```

```Go
func getLockRank(l *mutex) lockRank
```

#### <a id="lockRank.String" href="#lockRank.String">func (rank lockRank) String() string</a>

```
searchKey: runtime.lockRank.String
```

```Go
func (rank lockRank) String() string
```

### <a id="lockRankStruct" href="#lockRankStruct">type lockRankStruct struct{}</a>

```
searchKey: runtime.lockRankStruct
```

```Go
type lockRankStruct struct {
}
```

// lockRankStruct is embedded in mutex, but is empty when staticklockranking is disabled (the default) 

### <a id="persistentAlloc" href="#persistentAlloc">type persistentAlloc struct</a>

```
searchKey: runtime.persistentAlloc
```

```Go
type persistentAlloc struct {
	base *notInHeap
	off  uintptr
}
```

### <a id="linearAlloc" href="#linearAlloc">type linearAlloc struct</a>

```
searchKey: runtime.linearAlloc
```

```Go
type linearAlloc struct {
	next   uintptr // next free byte
	mapped uintptr // one byte past end of mapped space
	end    uintptr // end of reserved space

	mapMemory bool // transition memory from Reserved to Ready if true
}
```

linearAlloc is a simple linear allocator that pre-reserves a region of memory and then optionally maps that region into the Ready state as needed. 

The caller is responsible for locking. 

#### <a id="linearAlloc.init" href="#linearAlloc.init">func (l *linearAlloc) init(base, size uintptr, mapMemory bool)</a>

```
searchKey: runtime.linearAlloc.init
```

```Go
func (l *linearAlloc) init(base, size uintptr, mapMemory bool)
```

#### <a id="linearAlloc.alloc" href="#linearAlloc.alloc">func (l *linearAlloc) alloc(size, align uintptr, sysStat *sysMemStat) unsafe.Pointer</a>

```
searchKey: runtime.linearAlloc.alloc
```

```Go
func (l *linearAlloc) alloc(size, align uintptr, sysStat *sysMemStat) unsafe.Pointer
```

### <a id="notInHeap" href="#notInHeap">type notInHeap struct{}</a>

```
searchKey: runtime.notInHeap
```

```Go
type notInHeap struct{}
```

notInHeap is off-heap memory allocated by a lower-level allocator like sysAlloc or persistentAlloc. 

In general, it's better to use real types marked as go:notinheap, but this serves as a generic type for situations where that isn't possible (like in the allocators). 

TODO: Use this as the return type of sysAlloc, persistentAlloc, etc? 

#### <a id="persistentalloc1" href="#persistentalloc1">func persistentalloc1(size, align uintptr, sysStat *sysMemStat) *notInHeap</a>

```
searchKey: runtime.persistentalloc1
```

```Go
func persistentalloc1(size, align uintptr, sysStat *sysMemStat) *notInHeap
```

Must run on system stack because stack growth can (re)invoke it. See issue 9174. 

#### <a id="notInHeap.add" href="#notInHeap.add">func (p *notInHeap) add(bytes uintptr) *notInHeap</a>

```
searchKey: runtime.notInHeap.add
```

```Go
func (p *notInHeap) add(bytes uintptr) *notInHeap
```

### <a id="hmap" href="#hmap">type hmap struct</a>

```
searchKey: runtime.hmap
```

```Go
type hmap struct {
	// Note: the format of the hmap is also encoded in cmd/compile/internal/reflectdata/reflect.go.
	// Make sure this stays in sync with the compiler's definition.
	count     int // # live cells == size of map.  Must be first (used by len() builtin)
	flags     uint8
	B         uint8  // log_2 of # of buckets (can hold up to loadFactor * 2^B items)
	noverflow uint16 // approximate number of overflow buckets; see incrnoverflow for details
	hash0     uint32 // hash seed

	buckets    unsafe.Pointer // array of 2^B Buckets. may be nil if count==0.
	oldbuckets unsafe.Pointer // previous bucket array of half the size, non-nil only when growing
	nevacuate  uintptr        // progress counter for evacuation (buckets less than this have been evacuated)

	extra *mapextra // optional fields
}
```

A header for a Go map. 

#### <a id="makemap64" href="#makemap64">func makemap64(t *maptype, hint int64, h *hmap) *hmap</a>

```
searchKey: runtime.makemap64
```

```Go
func makemap64(t *maptype, hint int64, h *hmap) *hmap
```

#### <a id="makemap_small" href="#makemap_small">func makemap_small() *hmap</a>

```
searchKey: runtime.makemap_small
```

```Go
func makemap_small() *hmap
```

makemap_small implements Go map creation for make(map[k]v) and make(map[k]v, hint) when hint is known to be at most bucketCnt at compile time and the map needs to be allocated on the heap. 

#### <a id="makemap" href="#makemap">func makemap(t *maptype, hint int, h *hmap) *hmap</a>

```
searchKey: runtime.makemap
```

```Go
func makemap(t *maptype, hint int, h *hmap) *hmap
```

makemap implements Go map creation for make(map[k]v, hint). If the compiler has determined that the map or the first bucket can be created on the stack, h and/or bucket may be non-nil. If h != nil, the map can be created directly in h. If h.buckets != nil, bucket pointed to can be used as the first bucket. 

#### <a id="reflect_makemap" href="#reflect_makemap">func reflect_makemap(t *maptype, cap int) *hmap</a>

```
searchKey: runtime.reflect_makemap
```

```Go
func reflect_makemap(t *maptype, cap int) *hmap
```

#### <a id="hmap.incrnoverflow" href="#hmap.incrnoverflow">func (h *hmap) incrnoverflow()</a>

```
searchKey: runtime.hmap.incrnoverflow
```

```Go
func (h *hmap) incrnoverflow()
```

incrnoverflow increments h.noverflow. noverflow counts the number of overflow buckets. This is used to trigger same-size map growth. See also tooManyOverflowBuckets. To keep hmap small, noverflow is a uint16. When there are few buckets, noverflow is an exact count. When there are many buckets, noverflow is an approximate count. 

#### <a id="hmap.newoverflow" href="#hmap.newoverflow">func (h *hmap) newoverflow(t *maptype, b *bmap) *bmap</a>

```
searchKey: runtime.hmap.newoverflow
```

```Go
func (h *hmap) newoverflow(t *maptype, b *bmap) *bmap
```

#### <a id="hmap.createOverflow" href="#hmap.createOverflow">func (h *hmap) createOverflow()</a>

```
searchKey: runtime.hmap.createOverflow
```

```Go
func (h *hmap) createOverflow()
```

#### <a id="hmap.growing" href="#hmap.growing">func (h *hmap) growing() bool</a>

```
searchKey: runtime.hmap.growing
```

```Go
func (h *hmap) growing() bool
```

growing reports whether h is growing. The growth may be to the same size or bigger. 

#### <a id="hmap.sameSizeGrow" href="#hmap.sameSizeGrow">func (h *hmap) sameSizeGrow() bool</a>

```
searchKey: runtime.hmap.sameSizeGrow
```

```Go
func (h *hmap) sameSizeGrow() bool
```

sameSizeGrow reports whether the current growth is to a map of the same size. 

#### <a id="hmap.noldbuckets" href="#hmap.noldbuckets">func (h *hmap) noldbuckets() uintptr</a>

```
searchKey: runtime.hmap.noldbuckets
```

```Go
func (h *hmap) noldbuckets() uintptr
```

noldbuckets calculates the number of buckets prior to the current map growth. 

#### <a id="hmap.oldbucketmask" href="#hmap.oldbucketmask">func (h *hmap) oldbucketmask() uintptr</a>

```
searchKey: runtime.hmap.oldbucketmask
```

```Go
func (h *hmap) oldbucketmask() uintptr
```

oldbucketmask provides a mask that can be applied to calculate n % noldbuckets(). 

### <a id="mapextra" href="#mapextra">type mapextra struct</a>

```
searchKey: runtime.mapextra
```

```Go
type mapextra struct {
	// If both key and elem do not contain pointers and are inline, then we mark bucket
	// type as containing no pointers. This avoids scanning such maps.
	// However, bmap.overflow is a pointer. In order to keep overflow buckets
	// alive, we store pointers to all overflow buckets in hmap.extra.overflow and hmap.extra.oldoverflow.
	// overflow and oldoverflow are only used if key and elem do not contain pointers.
	// overflow contains overflow buckets for hmap.buckets.
	// oldoverflow contains overflow buckets for hmap.oldbuckets.
	// The indirection allows to store a pointer to the slice in hiter.
	overflow    *[]*bmap
	oldoverflow *[]*bmap

	// nextOverflow holds a pointer to a free overflow bucket.
	nextOverflow *bmap
}
```

mapextra holds fields that are not present on all maps. 

### <a id="bmap" href="#bmap">type bmap struct</a>

```
searchKey: runtime.bmap
```

```Go
type bmap struct {
	// tophash generally contains the top byte of the hash value
	// for each key in this bucket. If tophash[0] < minTopHash,
	// tophash[0] is a bucket evacuation state instead.
	tophash [bucketCnt]uint8
}
```

A bucket for a Go map. 

#### <a id="makeBucketArray" href="#makeBucketArray">func makeBucketArray(t *maptype, b uint8, dirtyalloc unsafe.Pointer) (buckets unsafe.Pointer, nextOverflow *bmap)</a>

```
searchKey: runtime.makeBucketArray
```

```Go
func makeBucketArray(t *maptype, b uint8, dirtyalloc unsafe.Pointer) (buckets unsafe.Pointer, nextOverflow *bmap)
```

makeBucketArray initializes a backing array for map buckets. 1<<b is the minimum number of buckets to allocate. dirtyalloc should either be nil or a bucket array previously allocated by makeBucketArray with the same t and b parameters. If dirtyalloc is nil a new backing array will be alloced and otherwise dirtyalloc will be cleared and reused as backing array. 

#### <a id="bmap.overflow" href="#bmap.overflow">func (b *bmap) overflow(t *maptype) *bmap</a>

```
searchKey: runtime.bmap.overflow
```

```Go
func (b *bmap) overflow(t *maptype) *bmap
```

#### <a id="bmap.setoverflow" href="#bmap.setoverflow">func (b *bmap) setoverflow(t *maptype, ovf *bmap)</a>

```
searchKey: runtime.bmap.setoverflow
```

```Go
func (b *bmap) setoverflow(t *maptype, ovf *bmap)
```

#### <a id="bmap.keys" href="#bmap.keys">func (b *bmap) keys() unsafe.Pointer</a>

```
searchKey: runtime.bmap.keys
```

```Go
func (b *bmap) keys() unsafe.Pointer
```

### <a id="hiter" href="#hiter">type hiter struct</a>

```
searchKey: runtime.hiter
```

```Go
type hiter struct {
	key         unsafe.Pointer // Must be in first position.  Write nil to indicate iteration end (see cmd/compile/internal/walk/range.go).
	elem        unsafe.Pointer // Must be in second position (see cmd/compile/internal/walk/range.go).
	t           *maptype
	h           *hmap
	buckets     unsafe.Pointer // bucket ptr at hash_iter initialization time
	bptr        *bmap          // current bucket
	overflow    *[]*bmap       // keeps overflow buckets of hmap.buckets alive
	oldoverflow *[]*bmap       // keeps overflow buckets of hmap.oldbuckets alive
	startBucket uintptr        // bucket iteration started at
	offset      uint8          // intra-bucket offset to start from during iteration (should be big enough to hold bucketCnt-1)
	wrapped     bool           // already wrapped around from end of bucket array to beginning
	B           uint8
	i           uint8
	bucket      uintptr
	checkBucket uintptr
}
```

A hash iteration structure. If you modify hiter, also change cmd/compile/internal/reflectdata/reflect.go to indicate the layout of this structure. 

#### <a id="reflect_mapiterinit" href="#reflect_mapiterinit">func reflect_mapiterinit(t *maptype, h *hmap) *hiter</a>

```
searchKey: runtime.reflect_mapiterinit
```

```Go
func reflect_mapiterinit(t *maptype, h *hmap) *hiter
```

### <a id="evacDst" href="#evacDst">type evacDst struct</a>

```
searchKey: runtime.evacDst
```

```Go
type evacDst struct {
	b *bmap          // current destination bucket
	i int            // key/elem index into b
	k unsafe.Pointer // pointer to current key storage
	e unsafe.Pointer // pointer to current elem storage
}
```

evacDst is an evacuation destination. 

### <a id="heapBits" href="#heapBits">type heapBits struct</a>

```
searchKey: runtime.heapBits
```

```Go
type heapBits struct {
	bitp  *uint8
	shift uint32
	arena uint32 // Index of heap arena containing bitp
	last  *uint8 // Last byte arena's bitmap
}
```

heapBits provides access to the bitmap bits for a single heap word. The methods on heapBits take value receivers so that the compiler can more easily inline calls to those methods and registerize the struct fields independently. 

#### <a id="heapBitsForAddr" href="#heapBitsForAddr">func heapBitsForAddr(addr uintptr) (h heapBits)</a>

```
searchKey: runtime.heapBitsForAddr
```

```Go
func heapBitsForAddr(addr uintptr) (h heapBits)
```

heapBitsForAddr returns the heapBits for the address addr. The caller must ensure addr is in an allocated span. In particular, be careful not to point past the end of an object. 

nosplit because it is used during write barriers and must not be preempted. 

#### <a id="heapBits.next" href="#heapBits.next">func (h heapBits) next() heapBits</a>

```
searchKey: runtime.heapBits.next
```

```Go
func (h heapBits) next() heapBits
```

next returns the heapBits describing the next pointer-sized word in memory. That is, if h describes address p, h.next() describes p+ptrSize. Note that next does not modify h. The caller must record the result. 

nosplit because it is used during write barriers and must not be preempted. 

#### <a id="heapBits.nextArena" href="#heapBits.nextArena">func (h heapBits) nextArena() heapBits</a>

```
searchKey: runtime.heapBits.nextArena
```

```Go
func (h heapBits) nextArena() heapBits
```

nextArena advances h to the beginning of the next heap arena. 

This is a slow-path helper to next. gc's inliner knows that heapBits.next can be inlined even though it calls this. This is marked noinline so it doesn't get inlined into next and cause next to be too big to inline. 

#### <a id="heapBits.forward" href="#heapBits.forward">func (h heapBits) forward(n uintptr) heapBits</a>

```
searchKey: runtime.heapBits.forward
```

```Go
func (h heapBits) forward(n uintptr) heapBits
```

forward returns the heapBits describing n pointer-sized words ahead of h in memory. That is, if h describes address p, h.forward(n) describes p+n*ptrSize. h.forward(1) is equivalent to h.next(), just slower. Note that forward does not modify h. The caller must record the result. bits returns the heap bits for the current word. 

#### <a id="heapBits.forwardOrBoundary" href="#heapBits.forwardOrBoundary">func (h heapBits) forwardOrBoundary(n uintptr) (heapBits, uintptr)</a>

```
searchKey: runtime.heapBits.forwardOrBoundary
```

```Go
func (h heapBits) forwardOrBoundary(n uintptr) (heapBits, uintptr)
```

forwardOrBoundary is like forward, but stops at boundaries between contiguous sections of the bitmap. It returns the number of words advanced over, which will be <= n. 

#### <a id="heapBits.bits" href="#heapBits.bits">func (h heapBits) bits() uint32</a>

```
searchKey: runtime.heapBits.bits
```

```Go
func (h heapBits) bits() uint32
```

The caller can test morePointers and isPointer by &-ing with bitScan and bitPointer. The result includes in its higher bits the bits for subsequent words described by the same bitmap byte. 

nosplit because it is used during write barriers and must not be preempted. 

#### <a id="heapBits.morePointers" href="#heapBits.morePointers">func (h heapBits) morePointers() bool</a>

```
searchKey: runtime.heapBits.morePointers
```

```Go
func (h heapBits) morePointers() bool
```

morePointers reports whether this word and all remaining words in this object are scalars. h must not describe the second word of the object. 

#### <a id="heapBits.isPointer" href="#heapBits.isPointer">func (h heapBits) isPointer() bool</a>

```
searchKey: runtime.heapBits.isPointer
```

```Go
func (h heapBits) isPointer() bool
```

isPointer reports whether the heap bits describe a pointer word. 

nosplit because it is used during write barriers and must not be preempted. 

#### <a id="heapBits.initSpan" href="#heapBits.initSpan">func (h heapBits) initSpan(s *mspan)</a>

```
searchKey: runtime.heapBits.initSpan
```

```Go
func (h heapBits) initSpan(s *mspan)
```

initSpan initializes the heap bitmap for a span. If this is a span of pointer-sized objects, it initializes all words to pointer/scan. Otherwise, it initializes all words to scalar/dead. 

### <a id="markBits" href="#markBits">type markBits struct</a>

```
searchKey: runtime.markBits
```

```Go
type markBits struct {
	bytep *uint8
	mask  uint8
	index uintptr
}
```

markBits provides access to the mark bit for an object in the heap. bytep points to the byte holding the mark bit. mask is a byte with a single bit set that can be &ed with *bytep to see if the bit has been set. *m.byte&m.mask != 0 indicates the mark bit is set. index can be used along with span information to generate the address of the object in the heap. We maintain one set of mark bits for allocation and one for marking purposes. 

#### <a id="markBitsForAddr" href="#markBitsForAddr">func markBitsForAddr(p uintptr) markBits</a>

```
searchKey: runtime.markBitsForAddr
```

```Go
func markBitsForAddr(p uintptr) markBits
```

#### <a id="markBitsForSpan" href="#markBitsForSpan">func markBitsForSpan(base uintptr) (mbits markBits)</a>

```
searchKey: runtime.markBitsForSpan
```

```Go
func markBitsForSpan(base uintptr) (mbits markBits)
```

markBitsForSpan returns the markBits for the span base address base. 

#### <a id="markBits.isMarked" href="#markBits.isMarked">func (m markBits) isMarked() bool</a>

```
searchKey: runtime.markBits.isMarked
```

```Go
func (m markBits) isMarked() bool
```

isMarked reports whether mark bit m is set. 

#### <a id="markBits.setMarked" href="#markBits.setMarked">func (m markBits) setMarked()</a>

```
searchKey: runtime.markBits.setMarked
```

```Go
func (m markBits) setMarked()
```

setMarked sets the marked bit in the markbits, atomically. 

#### <a id="markBits.setMarkedNonAtomic" href="#markBits.setMarkedNonAtomic">func (m markBits) setMarkedNonAtomic()</a>

```
searchKey: runtime.markBits.setMarkedNonAtomic
```

```Go
func (m markBits) setMarkedNonAtomic()
```

setMarkedNonAtomic sets the marked bit in the markbits, non-atomically. 

#### <a id="markBits.clearMarked" href="#markBits.clearMarked">func (m markBits) clearMarked()</a>

```
searchKey: runtime.markBits.clearMarked
```

```Go
func (m markBits) clearMarked()
```

clearMarked clears the marked bit in the markbits, atomically. 

#### <a id="markBits.advance" href="#markBits.advance">func (m *markBits) advance()</a>

```
searchKey: runtime.markBits.advance
```

```Go
func (m *markBits) advance()
```

advance advances the markBits to the next object in the span. 

### <a id="mcache" href="#mcache">type mcache struct</a>

```
searchKey: runtime.mcache
```

```Go
type mcache struct {
	// The following members are accessed on every malloc,
	// so they are grouped here for better caching.
	nextSample uintptr // trigger heap sample after allocating this many bytes
	scanAlloc  uintptr // bytes of scannable heap allocated

	// tiny points to the beginning of the current tiny block, or
	// nil if there is no current tiny block.
	//
	// tiny is a heap pointer. Since mcache is in non-GC'd memory,
	// we handle it by clearing it in releaseAll during mark
	// termination.
	//
	// tinyAllocs is the number of tiny allocations performed
	// by the P that owns this mcache.
	tiny       uintptr
	tinyoffset uintptr
	tinyAllocs uintptr

	alloc [numSpanClasses]*mspan // spans to allocate from, indexed by spanClass

	stackcache [_NumStackOrders]stackfreelist

	// flushGen indicates the sweepgen during which this mcache
	// was last flushed. If flushGen != mheap_.sweepgen, the spans
	// in this mcache are stale and need to the flushed so they
	// can be swept. This is done in acquirep.
	flushGen uint32
}
```

Per-thread (in Go, per-P) cache for small objects. This includes a small object cache and local allocation stats. No locking needed because it is per-thread (per-P). 

mcaches are allocated from non-GC'd memory, so any heap pointers must be specially handled. 

#### <a id="allocmcache" href="#allocmcache">func allocmcache() *mcache</a>

```
searchKey: runtime.allocmcache
```

```Go
func allocmcache() *mcache
```

#### <a id="getMCache" href="#getMCache">func getMCache() *mcache</a>

```
searchKey: runtime.getMCache
```

```Go
func getMCache() *mcache
```

getMCache is a convenience function which tries to obtain an mcache. 

Returns nil if we're not bootstrapping or we don't have a P. The caller's P must not change, so we must be in a non-preemptible state. 

#### <a id="mcache.nextFree" href="#mcache.nextFree">func (c *mcache) nextFree(spc spanClass) (v gclinkptr, s *mspan, shouldhelpgc bool)</a>

```
searchKey: runtime.mcache.nextFree
```

```Go
func (c *mcache) nextFree(spc spanClass) (v gclinkptr, s *mspan, shouldhelpgc bool)
```

nextFree returns the next free object from the cached span if one is available. Otherwise it refills the cache with a span with an available object and returns that object along with a flag indicating that this was a heavy weight allocation. If it is a heavy weight allocation the caller must determine whether a new GC cycle needs to be started or if the GC is active whether this goroutine needs to assist the GC. 

Must run in a non-preemptible context since otherwise the owner of c could change. 

#### <a id="mcache.refill" href="#mcache.refill">func (c *mcache) refill(spc spanClass)</a>

```
searchKey: runtime.mcache.refill
```

```Go
func (c *mcache) refill(spc spanClass)
```

refill acquires a new span of span class spc for c. This span will have at least one free object. The current span in c must be full. 

Must run in a non-preemptible context since otherwise the owner of c could change. 

#### <a id="mcache.allocLarge" href="#mcache.allocLarge">func (c *mcache) allocLarge(size uintptr, needzero bool, noscan bool) (*mspan, bool)</a>

```
searchKey: runtime.mcache.allocLarge
```

```Go
func (c *mcache) allocLarge(size uintptr, needzero bool, noscan bool) (*mspan, bool)
```

allocLarge allocates a span for a large object. The boolean result indicates whether the span is known-zeroed. If it did not need to be zeroed, it may not have been zeroed; but if it came directly from the OS, it is already zeroed. 

#### <a id="mcache.releaseAll" href="#mcache.releaseAll">func (c *mcache) releaseAll()</a>

```
searchKey: runtime.mcache.releaseAll
```

```Go
func (c *mcache) releaseAll()
```

#### <a id="mcache.prepareForSweep" href="#mcache.prepareForSweep">func (c *mcache) prepareForSweep()</a>

```
searchKey: runtime.mcache.prepareForSweep
```

```Go
func (c *mcache) prepareForSweep()
```

prepareForSweep flushes c if the system has entered a new sweep phase since c was populated. This must happen between the sweep phase starting and the first allocation from c. 

### <a id="gclink" href="#gclink">type gclink struct</a>

```
searchKey: runtime.gclink
```

```Go
type gclink struct {
	next gclinkptr
}
```

A gclink is a node in a linked list of blocks, like mlink, but it is opaque to the garbage collector. The GC does not trace the pointers during collection, and the compiler does not emit write barriers for assignments of gclinkptr values. Code should store references to gclinks as gclinkptr, not as *gclink. 

### <a id="gclinkptr" href="#gclinkptr">type gclinkptr uintptr</a>

```
searchKey: runtime.gclinkptr
```

```Go
type gclinkptr uintptr
```

A gclinkptr is a pointer to a gclink, but it is opaque to the garbage collector. 

#### <a id="nextFreeFast" href="#nextFreeFast">func nextFreeFast(s *mspan) gclinkptr</a>

```
searchKey: runtime.nextFreeFast
```

```Go
func nextFreeFast(s *mspan) gclinkptr
```

nextFreeFast returns the next free object if one is quickly available. Otherwise it returns 0. 

#### <a id="stackpoolalloc" href="#stackpoolalloc">func stackpoolalloc(order uint8) gclinkptr</a>

```
searchKey: runtime.stackpoolalloc
```

```Go
func stackpoolalloc(order uint8) gclinkptr
```

Allocates a stack from the free pool. Must be called with stackpool[order].item.mu held. 

#### <a id="gclinkptr.ptr" href="#gclinkptr.ptr">func (p gclinkptr) ptr() *gclink</a>

```
searchKey: runtime.gclinkptr.ptr
```

```Go
func (p gclinkptr) ptr() *gclink
```

ptr returns the *gclink form of p. The result should be used for accessing fields, not stored in other data structures. 

### <a id="stackfreelist" href="#stackfreelist">type stackfreelist struct</a>

```
searchKey: runtime.stackfreelist
```

```Go
type stackfreelist struct {
	list gclinkptr // linked list of free stacks
	size uintptr   // total size of stacks in list
}
```

### <a id="mcentral" href="#mcentral">type mcentral struct</a>

```
searchKey: runtime.mcentral
```

```Go
type mcentral struct {
	spanclass spanClass

	// partial and full contain two mspan sets: one of swept in-use
	// spans, and one of unswept in-use spans. These two trade
	// roles on each GC cycle. The unswept set is drained either by
	// allocation or by the background sweeper in every GC cycle,
	// so only two roles are necessary.
	//
	// sweepgen is increased by 2 on each GC cycle, so the swept
	// spans are in partial[sweepgen/2%2] and the unswept spans are in
	// partial[1-sweepgen/2%2]. Sweeping pops spans from the
	// unswept set and pushes spans that are still in-use on the
	// swept set. Likewise, allocating an in-use span pushes it
	// on the swept set.
	//
	// Some parts of the sweeper can sweep arbitrary spans, and hence
	// can't remove them from the unswept set, but will add the span
	// to the appropriate swept list. As a result, the parts of the
	// sweeper and mcentral that do consume from the unswept list may
	// encounter swept spans, and these should be ignored.
	partial [2]spanSet // list of spans with a free object
	full    [2]spanSet // list of spans with no free objects
}
```

Central list of free objects of a given size. 

#### <a id="mcentral.init" href="#mcentral.init">func (c *mcentral) init(spc spanClass)</a>

```
searchKey: runtime.mcentral.init
```

```Go
func (c *mcentral) init(spc spanClass)
```

Initialize a single central free list. 

#### <a id="mcentral.partialUnswept" href="#mcentral.partialUnswept">func (c *mcentral) partialUnswept(sweepgen uint32) *spanSet</a>

```
searchKey: runtime.mcentral.partialUnswept
```

```Go
func (c *mcentral) partialUnswept(sweepgen uint32) *spanSet
```

partialUnswept returns the spanSet which holds partially-filled unswept spans for this sweepgen. 

#### <a id="mcentral.partialSwept" href="#mcentral.partialSwept">func (c *mcentral) partialSwept(sweepgen uint32) *spanSet</a>

```
searchKey: runtime.mcentral.partialSwept
```

```Go
func (c *mcentral) partialSwept(sweepgen uint32) *spanSet
```

partialSwept returns the spanSet which holds partially-filled swept spans for this sweepgen. 

#### <a id="mcentral.fullUnswept" href="#mcentral.fullUnswept">func (c *mcentral) fullUnswept(sweepgen uint32) *spanSet</a>

```
searchKey: runtime.mcentral.fullUnswept
```

```Go
func (c *mcentral) fullUnswept(sweepgen uint32) *spanSet
```

fullUnswept returns the spanSet which holds unswept spans without any free slots for this sweepgen. 

#### <a id="mcentral.fullSwept" href="#mcentral.fullSwept">func (c *mcentral) fullSwept(sweepgen uint32) *spanSet</a>

```
searchKey: runtime.mcentral.fullSwept
```

```Go
func (c *mcentral) fullSwept(sweepgen uint32) *spanSet
```

fullSwept returns the spanSet which holds swept spans without any free slots for this sweepgen. 

#### <a id="mcentral.cacheSpan" href="#mcentral.cacheSpan">func (c *mcentral) cacheSpan() *mspan</a>

```
searchKey: runtime.mcentral.cacheSpan
```

```Go
func (c *mcentral) cacheSpan() *mspan
```

Allocate a span to use in an mcache. 

#### <a id="mcentral.uncacheSpan" href="#mcentral.uncacheSpan">func (c *mcentral) uncacheSpan(s *mspan)</a>

```
searchKey: runtime.mcentral.uncacheSpan
```

```Go
func (c *mcentral) uncacheSpan(s *mspan)
```

Return span from an mcache. 

s must have a span class corresponding to this mcentral and it must not be empty. 

#### <a id="mcentral.grow" href="#mcentral.grow">func (c *mcentral) grow() *mspan</a>

```
searchKey: runtime.mcentral.grow
```

```Go
func (c *mcentral) grow() *mspan
```

grow allocates a new empty span from the heap and initializes it for c's size class. 

### <a id="checkmarksMap" href="#checkmarksMap">type checkmarksMap [1048576]uint8</a>

```
searchKey: runtime.checkmarksMap
```

```Go
type checkmarksMap [heapArenaBytes / sys.PtrSize / 8]uint8
```

A checkmarksMap stores the GC marks in "checkmarks" mode. It is a per-arena bitmap with a bit for every word in the arena. The mark is stored on the bit corresponding to the first word of the marked allocation. 

### <a id="metricData" href="#metricData">type metricData struct</a>

```
searchKey: runtime.metricData
```

```Go
type metricData struct {
	// deps is the set of runtime statistics that this metric
	// depends on. Before compute is called, the statAggregate
	// which will be passed must ensure() these dependencies.
	deps statDepSet

	// compute is a function that populates a metricValue
	// given a populated statAggregate structure.
	compute func(in *statAggregate, out *metricValue)
}
```

### <a id="statDep" href="#statDep">type statDep uint</a>

```
searchKey: runtime.statDep
```

```Go
type statDep uint
```

statDep is a dependency on a group of statistics that a metric might have. 

### <a id="statDepSet" href="#statDepSet">type statDepSet [1]uint64</a>

```
searchKey: runtime.statDepSet
```

```Go
type statDepSet [1]uint64
```

statDepSet represents a set of statDeps. 

Under the hood, it's a bitmap. 

#### <a id="makeStatDepSet" href="#makeStatDepSet">func makeStatDepSet(deps ...statDep) statDepSet</a>

```
searchKey: runtime.makeStatDepSet
```

```Go
func makeStatDepSet(deps ...statDep) statDepSet
```

makeStatDepSet creates a new statDepSet from a list of statDeps. 

#### <a id="statDepSet.difference" href="#statDepSet.difference">func (s statDepSet) difference(b statDepSet) statDepSet</a>

```
searchKey: runtime.statDepSet.difference
```

```Go
func (s statDepSet) difference(b statDepSet) statDepSet
```

differennce returns set difference of s from b as a new set. 

#### <a id="statDepSet.union" href="#statDepSet.union">func (s statDepSet) union(b statDepSet) statDepSet</a>

```
searchKey: runtime.statDepSet.union
```

```Go
func (s statDepSet) union(b statDepSet) statDepSet
```

union returns the union of the two sets as a new set. 

#### <a id="statDepSet.empty" href="#statDepSet.empty">func (s *statDepSet) empty() bool</a>

```
searchKey: runtime.statDepSet.empty
```

```Go
func (s *statDepSet) empty() bool
```

empty returns true if there are no dependencies in the set. 

#### <a id="statDepSet.has" href="#statDepSet.has">func (s *statDepSet) has(d statDep) bool</a>

```
searchKey: runtime.statDepSet.has
```

```Go
func (s *statDepSet) has(d statDep) bool
```

has returns true if the set contains a given statDep. 

### <a id="heapStatsAggregate" href="#heapStatsAggregate">type heapStatsAggregate struct</a>

```
searchKey: runtime.heapStatsAggregate
```

```Go
type heapStatsAggregate struct {
	heapStatsDelta

	// inObjects is the bytes of memory occupied by objects,
	inObjects uint64

	// numObjects is the number of live objects in the heap.
	numObjects uint64

	// totalAllocated is the total bytes of heap objects allocated
	// over the lifetime of the program.
	totalAllocated uint64

	// totalFreed is the total bytes of heap objects freed
	// over the lifetime of the program.
	totalFreed uint64

	// totalAllocs is the number of heap objects allocated over
	// the lifetime of the program.
	totalAllocs uint64

	// totalFrees is the number of heap objects freed over
	// the lifetime of the program.
	totalFrees uint64
}
```

heapStatsAggregate represents memory stats obtained from the runtime. This set of stats is grouped together because they depend on each other in some way to make sense of the runtime's current heap memory use. They're also sharded across Ps, so it makes sense to grab them all at once. 

#### <a id="heapStatsAggregate.compute" href="#heapStatsAggregate.compute">func (a *heapStatsAggregate) compute()</a>

```
searchKey: runtime.heapStatsAggregate.compute
```

```Go
func (a *heapStatsAggregate) compute()
```

compute populates the heapStatsAggregate with values from the runtime. 

### <a id="sysStatsAggregate" href="#sysStatsAggregate">type sysStatsAggregate struct</a>

```
searchKey: runtime.sysStatsAggregate
```

```Go
type sysStatsAggregate struct {
	stacksSys      uint64
	mSpanSys       uint64
	mSpanInUse     uint64
	mCacheSys      uint64
	mCacheInUse    uint64
	buckHashSys    uint64
	gcMiscSys      uint64
	otherSys       uint64
	heapGoal       uint64
	gcCyclesDone   uint64
	gcCyclesForced uint64
}
```

sysStatsAggregate represents system memory stats obtained from the runtime. This set of stats is grouped together because they're all relatively cheap to acquire and generally independent of one another and other runtime memory stats. The fact that they may be acquired at different times, especially with respect to heapStatsAggregate, means there could be some skew, but because of these stats are independent, there's no real consistency issue here. 

#### <a id="sysStatsAggregate.compute" href="#sysStatsAggregate.compute">func (a *sysStatsAggregate) compute()</a>

```
searchKey: runtime.sysStatsAggregate.compute
```

```Go
func (a *sysStatsAggregate) compute()
```

compute populates the sysStatsAggregate with values from the runtime. 

### <a id="statAggregate" href="#statAggregate">type statAggregate struct</a>

```
searchKey: runtime.statAggregate
```

```Go
type statAggregate struct {
	ensured   statDepSet
	heapStats heapStatsAggregate
	sysStats  sysStatsAggregate
}
```

statAggregate is the main driver of the metrics implementation. 

It contains multiple aggregates of runtime statistics, as well as a set of these aggregates that it has populated. The aggergates are populated lazily by its ensure method. 

#### <a id="statAggregate.ensure" href="#statAggregate.ensure">func (a *statAggregate) ensure(deps *statDepSet)</a>

```
searchKey: runtime.statAggregate.ensure
```

```Go
func (a *statAggregate) ensure(deps *statDepSet)
```

ensure populates statistics aggregates determined by deps if they haven't yet been populated. 

### <a id="metricKind" href="#metricKind">type metricKind int</a>

```
searchKey: runtime.metricKind
```

```Go
type metricKind int
```

metricValidKind is a runtime copy of runtime/metrics.ValueKind and must be kept structurally identical to that type. 

### <a id="metricSample" href="#metricSample">type metricSample struct</a>

```
searchKey: runtime.metricSample
```

```Go
type metricSample struct {
	name  string
	value metricValue
}
```

metricSample is a runtime copy of runtime/metrics.Sample and must be kept structurally identical to that type. 

### <a id="metricValue" href="#metricValue">type metricValue struct</a>

```
searchKey: runtime.metricValue
```

```Go
type metricValue struct {
	kind    metricKind
	scalar  uint64         // contains scalar values for scalar Kinds.
	pointer unsafe.Pointer // contains non-scalar values.
}
```

metricValue is a runtime copy of runtime/metrics.Sample and must be kept structurally identical to that type. 

#### <a id="metricValue.float64HistOrInit" href="#metricValue.float64HistOrInit">func (v *metricValue) float64HistOrInit(buckets []float64) *metricFloat64Histogram</a>

```
searchKey: runtime.metricValue.float64HistOrInit
```

```Go
func (v *metricValue) float64HistOrInit(buckets []float64) *metricFloat64Histogram
```

float64HistOrInit tries to pull out an existing float64Histogram from the value, but if none exists, then it allocates one with the given buckets. 

### <a id="metricFloat64Histogram" href="#metricFloat64Histogram">type metricFloat64Histogram struct</a>

```
searchKey: runtime.metricFloat64Histogram
```

```Go
type metricFloat64Histogram struct {
	counts  []uint64
	buckets []float64
}
```

metricFloat64Histogram is a runtime copy of runtime/metrics.Float64Histogram and must be kept structurally identical to that type. 

### <a id="finblock" href="#finblock">type finblock struct</a>

```
searchKey: runtime.finblock
```

```Go
type finblock struct {
	alllink *finblock
	next    *finblock
	cnt     uint32
	_       int32
	fin     [(_FinBlockSize - 2*sys.PtrSize - 2*4) / unsafe.Sizeof(finalizer{})]finalizer
}
```

finblock is an array of finalizers to be executed. finblocks are arranged in a linked list for the finalizer queue. 

finblock is allocated from non-GC'd memory, so any heap pointers must be specially handled. GC currently assumes that the finalizer queue does not grow during marking (but it can shrink). 

### <a id="finalizer" href="#finalizer">type finalizer struct</a>

```
searchKey: runtime.finalizer
```

```Go
type finalizer struct {
	fn   *funcval       // function to call (may be a heap pointer)
	arg  unsafe.Pointer // ptr to object (may be a heap pointer)
	nret uintptr        // bytes of return values from fn
	fint *_type         // type of first argument of fn
	ot   *ptrtype       // type of ptr to object (may be a heap pointer)
}
```

NOTE: Layout known to queuefinalizer. 

### <a id="fixalloc" href="#fixalloc">type fixalloc struct</a>

```
searchKey: runtime.fixalloc
```

```Go
type fixalloc struct {
	size   uintptr
	first  func(arg, p unsafe.Pointer) // called first time p is returned
	arg    unsafe.Pointer
	list   *mlink
	chunk  uintptr // use uintptr instead of unsafe.Pointer to avoid write barriers
	nchunk uint32
	inuse  uintptr // in-use bytes now
	stat   *sysMemStat
	zero   bool // zero allocations
}
```

FixAlloc is a simple free-list allocator for fixed size objects. Malloc uses a FixAlloc wrapped around sysAlloc to manage its mcache and mspan objects. 

Memory returned by fixalloc.alloc is zeroed by default, but the caller may take responsibility for zeroing allocations by setting the zero flag to false. This is only safe if the memory never contains heap pointers. 

The caller is responsible for locking around FixAlloc calls. Callers can keep state in the object but the first word is smashed by freeing and reallocating. 

Consider marking fixalloc'd types go:notinheap. 

#### <a id="fixalloc.init" href="#fixalloc.init">func (f *fixalloc) init(size uintptr, first func(arg, p unsafe.Pointer), arg unsafe.Pointer, stat *sysMemStat)</a>

```
searchKey: runtime.fixalloc.init
```

```Go
func (f *fixalloc) init(size uintptr, first func(arg, p unsafe.Pointer), arg unsafe.Pointer, stat *sysMemStat)
```

Initialize f to allocate objects of the given size, using the allocator to obtain chunks of memory. 

#### <a id="fixalloc.alloc" href="#fixalloc.alloc">func (f *fixalloc) alloc() unsafe.Pointer</a>

```
searchKey: runtime.fixalloc.alloc
```

```Go
func (f *fixalloc) alloc() unsafe.Pointer
```

#### <a id="fixalloc.free" href="#fixalloc.free">func (f *fixalloc) free(p unsafe.Pointer)</a>

```
searchKey: runtime.fixalloc.free
```

```Go
func (f *fixalloc) free(p unsafe.Pointer)
```

### <a id="mlink" href="#mlink">type mlink struct</a>

```
searchKey: runtime.mlink
```

```Go
type mlink struct {
	next *mlink
}
```

A generic linked list of blocks.  (Typically the block is bigger than sizeof(MLink).) Since assignments to mlink.next will result in a write barrier being performed this cannot be used by some of the internal GC structures. For example when the sweeper is placing an unmarked object on the free list it does not want the write barrier to be called since that could result in the object being reachable. 

### <a id="gcMarkWorkerMode" href="#gcMarkWorkerMode">type gcMarkWorkerMode int</a>

```
searchKey: runtime.gcMarkWorkerMode
```

```Go
type gcMarkWorkerMode int
```

gcMarkWorkerMode represents the mode that a concurrent mark worker should operate in. 

Concurrent marking happens through four different mechanisms. One is mutator assists, which happen in response to allocations and are not scheduled. The other three are variations in the per-P mark workers and are distinguished by gcMarkWorkerMode. 

### <a id="gcMode" href="#gcMode">type gcMode int</a>

```
searchKey: runtime.gcMode
```

```Go
type gcMode int
```

gcMode indicates how concurrent a GC cycle should be. 

### <a id="gcTrigger" href="#gcTrigger">type gcTrigger struct</a>

```
searchKey: runtime.gcTrigger
```

```Go
type gcTrigger struct {
	kind gcTriggerKind
	now  int64  // gcTriggerTime: current time
	n    uint32 // gcTriggerCycle: cycle number to start
}
```

A gcTrigger is a predicate for starting a GC cycle. Specifically, it is an exit condition for the _GCoff phase. 

#### <a id="gcTrigger.test" href="#gcTrigger.test">func (t gcTrigger) test() bool</a>

```
searchKey: runtime.gcTrigger.test
```

```Go
func (t gcTrigger) test() bool
```

test reports whether the trigger condition is satisfied, meaning that the exit condition for the _GCoff phase has been met. The exit condition should be tested when allocating. 

### <a id="gcTriggerKind" href="#gcTriggerKind">type gcTriggerKind int</a>

```
searchKey: runtime.gcTriggerKind
```

```Go
type gcTriggerKind int
```

### <a id="gcBgMarkWorkerNode" href="#gcBgMarkWorkerNode">type gcBgMarkWorkerNode struct</a>

```
searchKey: runtime.gcBgMarkWorkerNode
```

```Go
type gcBgMarkWorkerNode struct {
	// Unused workers are managed in a lock-free stack. This field must be first.
	node lfnode

	// The g of this worker.
	gp guintptr

	// Release this m on park. This is used to communicate with the unlock
	// function, which cannot access the G's stack. It is unused outside of
	// gcBgMarkWorker().
	m muintptr
}
```

gcBgMarkWorker is an entry in the gcBgMarkWorkerPool. It points to a single gcBgMarkWorker goroutine. 

### <a id="gcDrainFlags" href="#gcDrainFlags">type gcDrainFlags int</a>

```
searchKey: runtime.gcDrainFlags
```

```Go
type gcDrainFlags int
```

### <a id="gcControllerState" href="#gcControllerState">type gcControllerState struct</a>

```
searchKey: runtime.gcControllerState
```

```Go
type gcControllerState struct {
	// Initialized from $GOGC. GOGC=off means no GC.
	gcPercent int32

	_ uint32 // padding so following 64-bit values are 8-byte aligned

	// heapMinimum is the minimum heap size at which to trigger GC.
	// For small heaps, this overrides the usual GOGC*live set rule.
	//
	// When there is a very small live set but a lot of allocation, simply
	// collecting when the heap reaches GOGC*live results in many GC
	// cycles and high total per-GC overhead. This minimum amortizes this
	// per-GC overhead while keeping the heap reasonably small.
	//
	// During initialization this is set to 4MB*GOGC/100. In the case of
	// GOGC==0, this will set heapMinimum to 0, resulting in constant
	// collection even when the heap size is small, which is useful for
	// debugging.
	heapMinimum uint64

	// triggerRatio is the heap growth ratio that triggers marking.
	//
	// E.g., if this is 0.6, then GC should start when the live
	// heap has reached 1.6 times the heap size marked by the
	// previous cycle. This should be ≤ GOGC/100 so the trigger
	// heap size is less than the goal heap size. This is set
	// during mark termination for the next cycle's trigger.
	//
	// Protected by mheap_.lock or a STW.
	triggerRatio float64

	// trigger is the heap size that triggers marking.
	//
	// When heapLive ≥ trigger, the mark phase will start.
	// This is also the heap size by which proportional sweeping
	// must be complete.
	//
	// This is computed from triggerRatio during mark termination
	// for the next cycle's trigger.
	//
	// Protected by mheap_.lock or a STW.
	trigger uint64

	// heapGoal is the goal heapLive for when next GC ends.
	// Set to ^uint64(0) if disabled.
	//
	// Read and written atomically, unless the world is stopped.
	heapGoal uint64

	// lastHeapGoal is the value of heapGoal for the previous GC.
	// Note that this is distinct from the last value heapGoal had,
	// because it could change if e.g. gcPercent changes.
	//
	// Read and written with the world stopped or with mheap_.lock held.
	lastHeapGoal uint64

	// heapLive is the number of bytes considered live by the GC.
	// That is: retained by the most recent GC plus allocated
	// since then. heapLive ≤ memstats.heapAlloc, since heapAlloc includes
	// unmarked objects that have not yet been swept (and hence goes up as we
	// allocate and down as we sweep) while heapLive excludes these
	// objects (and hence only goes up between GCs).
	//
	// This is updated atomically without locking. To reduce
	// contention, this is updated only when obtaining a span from
	// an mcentral and at this point it counts all of the
	// unallocated slots in that span (which will be allocated
	// before that mcache obtains another span from that
	// mcentral). Hence, it slightly overestimates the "true" live
	// heap size. It's better to overestimate than to
	// underestimate because 1) this triggers the GC earlier than
	// necessary rather than potentially too late and 2) this
	// leads to a conservative GC rate rather than a GC rate that
	// is potentially too low.
	//
	// Reads should likewise be atomic (or during STW).
	//
	// Whenever this is updated, call traceHeapAlloc() and
	// this gcControllerState's revise() method.
	heapLive uint64

	// heapScan is the number of bytes of "scannable" heap. This
	// is the live heap (as counted by heapLive), but omitting
	// no-scan objects and no-scan tails of objects.
	//
	// Whenever this is updated, call this gcControllerState's
	// revise() method.
	//
	// Read and written atomically or with the world stopped.
	heapScan uint64

	// heapMarked is the number of bytes marked by the previous
	// GC. After mark termination, heapLive == heapMarked, but
	// unlike heapLive, heapMarked does not change until the
	// next mark termination.
	heapMarked uint64

	// scanWork is the total scan work performed this cycle. This
	// is updated atomically during the cycle. Updates occur in
	// bounded batches, since it is both written and read
	// throughout the cycle. At the end of the cycle, this is how
	// much of the retained heap is scannable.
	//
	// Currently this is the bytes of heap scanned. For most uses,
	// this is an opaque unit of work, but for estimation the
	// definition is important.
	scanWork int64

	// bgScanCredit is the scan work credit accumulated by the
	// concurrent background scan. This credit is accumulated by
	// the background scan and stolen by mutator assists. This is
	// updated atomically. Updates occur in bounded batches, since
	// it is both written and read throughout the cycle.
	bgScanCredit int64

	// assistTime is the nanoseconds spent in mutator assists
	// during this cycle. This is updated atomically. Updates
	// occur in bounded batches, since it is both written and read
	// throughout the cycle.
	assistTime int64

	// dedicatedMarkTime is the nanoseconds spent in dedicated
	// mark workers during this cycle. This is updated atomically
	// at the end of the concurrent mark phase.
	dedicatedMarkTime int64

	// fractionalMarkTime is the nanoseconds spent in the
	// fractional mark worker during this cycle. This is updated
	// atomically throughout the cycle and will be up-to-date if
	// the fractional mark worker is not currently running.
	fractionalMarkTime int64

	// idleMarkTime is the nanoseconds spent in idle marking
	// during this cycle. This is updated atomically throughout
	// the cycle.
	idleMarkTime int64

	// markStartTime is the absolute start time in nanoseconds
	// that assists and background mark workers started.
	markStartTime int64

	// dedicatedMarkWorkersNeeded is the number of dedicated mark
	// workers that need to be started. This is computed at the
	// beginning of each cycle and decremented atomically as
	// dedicated mark workers get started.
	dedicatedMarkWorkersNeeded int64

	// assistWorkPerByte is the ratio of scan work to allocated
	// bytes that should be performed by mutator assists. This is
	// computed at the beginning of each cycle and updated every
	// time heapScan is updated.
	//
	// Stored as a uint64, but it's actually a float64. Use
	// float64frombits to get the value.
	//
	// Read and written atomically.
	assistWorkPerByte uint64

	// assistBytesPerWork is 1/assistWorkPerByte.
	//
	// Stored as a uint64, but it's actually a float64. Use
	// float64frombits to get the value.
	//
	// Read and written atomically.
	//
	// Note that because this is read and written independently
	// from assistWorkPerByte users may notice a skew between
	// the two values, and such a state should be safe.
	assistBytesPerWork uint64

	// fractionalUtilizationGoal is the fraction of wall clock
	// time that should be spent in the fractional mark worker on
	// each P that isn't running a dedicated worker.
	//
	// For example, if the utilization goal is 25% and there are
	// no dedicated workers, this will be 0.25. If the goal is
	// 25%, there is one dedicated worker, and GOMAXPROCS is 5,
	// this will be 0.05 to make up the missing 5%.
	//
	// If this is zero, no fractional workers are needed.
	fractionalUtilizationGoal float64

	_ cpu.CacheLinePad
}
```

#### <a id="gcControllerState.init" href="#gcControllerState.init">func (c *gcControllerState) init(gcPercent int32)</a>

```
searchKey: runtime.gcControllerState.init
```

```Go
func (c *gcControllerState) init(gcPercent int32)
```

#### <a id="gcControllerState.startCycle" href="#gcControllerState.startCycle">func (c *gcControllerState) startCycle()</a>

```
searchKey: runtime.gcControllerState.startCycle
```

```Go
func (c *gcControllerState) startCycle()
```

startCycle resets the GC controller's state and computes estimates for a new GC cycle. The caller must hold worldsema and the world must be stopped. 

#### <a id="gcControllerState.revise" href="#gcControllerState.revise">func (c *gcControllerState) revise()</a>

```
searchKey: runtime.gcControllerState.revise
```

```Go
func (c *gcControllerState) revise()
```

revise updates the assist ratio during the GC cycle to account for improved estimates. This should be called whenever gcController.heapScan, gcController.heapLive, or gcController.heapGoal is updated. It is safe to call concurrently, but it may race with other calls to revise. 

The result of this race is that the two assist ratio values may not line up or may be stale. In practice this is OK because the assist ratio moves slowly throughout a GC cycle, and the assist ratio is a best-effort heuristic anyway. Furthermore, no part of the heuristic depends on the two assist ratio values being exact reciprocals of one another, since the two values are used to convert values from different sources. 

The worst case result of this raciness is that we may miss a larger shift in the ratio (say, if we decide to pace more aggressively against the hard heap goal) but even this "hard goal" is best-effort (see #40460). The dedicated GC should ensure we don't exceed the hard goal by too much in the rare case we do exceed it. 

It should only be called when gcBlackenEnabled != 0 (because this is when assists are enabled and the necessary statistics are available). 

#### <a id="gcControllerState.endCycle" href="#gcControllerState.endCycle">func (c *gcControllerState) endCycle(userForced bool) float64</a>

```
searchKey: runtime.gcControllerState.endCycle
```

```Go
func (c *gcControllerState) endCycle(userForced bool) float64
```

endCycle computes the trigger ratio for the next cycle. userForced indicates whether the current GC cycle was forced by the application. 

#### <a id="gcControllerState.enlistWorker" href="#gcControllerState.enlistWorker">func (c *gcControllerState) enlistWorker()</a>

```
searchKey: runtime.gcControllerState.enlistWorker
```

```Go
func (c *gcControllerState) enlistWorker()
```

enlistWorker encourages another dedicated mark worker to start on another P if there are spare worker slots. It is used by putfull when more work is made available. 

#### <a id="gcControllerState.findRunnableGCWorker" href="#gcControllerState.findRunnableGCWorker">func (c *gcControllerState) findRunnableGCWorker(_p_ *p) *g</a>

```
searchKey: runtime.gcControllerState.findRunnableGCWorker
```

```Go
func (c *gcControllerState) findRunnableGCWorker(_p_ *p) *g
```

findRunnableGCWorker returns a background mark worker for _p_ if it should be run. This must only be called when gcBlackenEnabled != 0. 

#### <a id="gcControllerState.commit" href="#gcControllerState.commit">func (c *gcControllerState) commit(triggerRatio float64)</a>

```
searchKey: runtime.gcControllerState.commit
```

```Go
func (c *gcControllerState) commit(triggerRatio float64)
```

commit sets the trigger ratio and updates everything derived from it: the absolute trigger, the heap goal, mark pacing, and sweep pacing. 

This can be called any time. If GC is the in the middle of a concurrent phase, it will adjust the pacing of that phase. 

This depends on gcPercent, gcController.heapMarked, and gcController.heapLive. These must be up to date. 

mheap_.lock must be held or the world must be stopped. 

#### <a id="gcControllerState.effectiveGrowthRatio" href="#gcControllerState.effectiveGrowthRatio">func (c *gcControllerState) effectiveGrowthRatio() float64</a>

```
searchKey: runtime.gcControllerState.effectiveGrowthRatio
```

```Go
func (c *gcControllerState) effectiveGrowthRatio() float64
```

effectiveGrowthRatio returns the current effective heap growth ratio (GOGC/100) based on heapMarked from the previous GC and heapGoal for the current GC. 

This may differ from gcPercent/100 because of various upper and lower bounds on gcPercent. For example, if the heap is smaller than heapMinimum, this can be higher than gcPercent/100. 

mheap_.lock must be held or the world must be stopped. 

#### <a id="gcControllerState.setGCPercent" href="#gcControllerState.setGCPercent">func (c *gcControllerState) setGCPercent(in int32) int32</a>

```
searchKey: runtime.gcControllerState.setGCPercent
```

```Go
func (c *gcControllerState) setGCPercent(in int32) int32
```

setGCPercent updates gcPercent and all related pacer state. Returns the old value of gcPercent. 

The world must be stopped, or mheap_.lock must be held. 

### <a id="stackWorkBuf" href="#stackWorkBuf">type stackWorkBuf struct</a>

```
searchKey: runtime.stackWorkBuf
```

```Go
type stackWorkBuf struct {
	stackWorkBufHdr
	obj [(_WorkbufSize - unsafe.Sizeof(stackWorkBufHdr{})) / sys.PtrSize]uintptr
}
```

Buffer for pointers found during stack tracing. Must be smaller than or equal to workbuf. 

### <a id="stackWorkBufHdr" href="#stackWorkBufHdr">type stackWorkBufHdr struct</a>

```
searchKey: runtime.stackWorkBufHdr
```

```Go
type stackWorkBufHdr struct {
	workbufhdr
	next *stackWorkBuf // linked list of workbufs

}
```

Header declaration must come after the buf declaration above, because of issue #14620. 

### <a id="stackObjectBuf" href="#stackObjectBuf">type stackObjectBuf struct</a>

```
searchKey: runtime.stackObjectBuf
```

```Go
type stackObjectBuf struct {
	stackObjectBufHdr
	obj [(_WorkbufSize - unsafe.Sizeof(stackObjectBufHdr{})) / unsafe.Sizeof(stackObject{})]stackObject
}
```

Buffer for stack objects found on a goroutine stack. Must be smaller than or equal to workbuf. 

#### <a id="binarySearchTree" href="#binarySearchTree">func binarySearchTree(x *stackObjectBuf, idx int, n int) (root *stackObject, restBuf *stackObjectBuf, restIdx int)</a>

```
searchKey: runtime.binarySearchTree
```

```Go
func binarySearchTree(x *stackObjectBuf, idx int, n int) (root *stackObject, restBuf *stackObjectBuf, restIdx int)
```

Build a binary search tree with the n objects in the list x.obj[idx], x.obj[idx+1], ..., x.next.obj[0], ... Returns the root of that tree, and the buf+idx of the nth object after x.obj[idx]. (The first object that was not included in the binary search tree.) If n == 0, returns nil, x. 

### <a id="stackObjectBufHdr" href="#stackObjectBufHdr">type stackObjectBufHdr struct</a>

```
searchKey: runtime.stackObjectBufHdr
```

```Go
type stackObjectBufHdr struct {
	workbufhdr
	next *stackObjectBuf
}
```

### <a id="stackObject" href="#stackObject">type stackObject struct</a>

```
searchKey: runtime.stackObject
```

```Go
type stackObject struct {
	off   uint32             // offset above stack.lo
	size  uint32             // size of object
	r     *stackObjectRecord // info of the object (for ptr/nonptr bits). nil if object has been scanned.
	left  *stackObject       // objects with lower addresses
	right *stackObject       // objects with higher addresses
}
```

A stackObject represents a variable on the stack that has had its address taken. 

#### <a id="binarySearchTree" href="#binarySearchTree">func binarySearchTree(x *stackObjectBuf, idx int, n int) (root *stackObject, restBuf *stackObjectBuf, restIdx int)</a>

```
searchKey: runtime.binarySearchTree
```

```Go
func binarySearchTree(x *stackObjectBuf, idx int, n int) (root *stackObject, restBuf *stackObjectBuf, restIdx int)
```

Build a binary search tree with the n objects in the list x.obj[idx], x.obj[idx+1], ..., x.next.obj[0], ... Returns the root of that tree, and the buf+idx of the nth object after x.obj[idx]. (The first object that was not included in the binary search tree.) If n == 0, returns nil, x. 

#### <a id="stackObject.setRecord" href="#stackObject.setRecord">func (obj *stackObject) setRecord(r *stackObjectRecord)</a>

```
searchKey: runtime.stackObject.setRecord
```

```Go
func (obj *stackObject) setRecord(r *stackObjectRecord)
```

obj.r = r, but with no write barrier. 

### <a id="stackScanState" href="#stackScanState">type stackScanState struct</a>

```
searchKey: runtime.stackScanState
```

```Go
type stackScanState struct {
	cache pcvalueCache

	// stack limits
	stack stack

	// conservative indicates that the next frame must be scanned conservatively.
	// This applies only to the innermost frame at an async safe-point.
	conservative bool

	// buf contains the set of possible pointers to stack objects.
	// Organized as a LIFO linked list of buffers.
	// All buffers except possibly the head buffer are full.
	buf     *stackWorkBuf
	freeBuf *stackWorkBuf // keep around one free buffer for allocation hysteresis

	// cbuf contains conservative pointers to stack objects. If
	// all pointers to a stack object are obtained via
	// conservative scanning, then the stack object may be dead
	// and may contain dead pointers, so it must be scanned
	// defensively.
	cbuf *stackWorkBuf

	// list of stack objects
	// Objects are in increasing address order.
	head  *stackObjectBuf
	tail  *stackObjectBuf
	nobjs int

	// root of binary tree for fast object lookup by address
	// Initialized by buildIndex.
	root *stackObject
}
```

A stackScanState keeps track of the state used during the GC walk of a goroutine. 

#### <a id="stackScanState.putPtr" href="#stackScanState.putPtr">func (s *stackScanState) putPtr(p uintptr, conservative bool)</a>

```
searchKey: runtime.stackScanState.putPtr
```

```Go
func (s *stackScanState) putPtr(p uintptr, conservative bool)
```

Add p as a potential pointer to a stack object. p must be a stack address. 

#### <a id="stackScanState.getPtr" href="#stackScanState.getPtr">func (s *stackScanState) getPtr() (p uintptr, conservative bool)</a>

```
searchKey: runtime.stackScanState.getPtr
```

```Go
func (s *stackScanState) getPtr() (p uintptr, conservative bool)
```

Remove and return a potential pointer to a stack object. Returns 0 if there are no more pointers available. 

This prefers non-conservative pointers so we scan stack objects precisely if there are any non-conservative pointers to them. 

#### <a id="stackScanState.addObject" href="#stackScanState.addObject">func (s *stackScanState) addObject(addr uintptr, r *stackObjectRecord)</a>

```
searchKey: runtime.stackScanState.addObject
```

```Go
func (s *stackScanState) addObject(addr uintptr, r *stackObjectRecord)
```

addObject adds a stack object at addr of type typ to the set of stack objects. 

#### <a id="stackScanState.buildIndex" href="#stackScanState.buildIndex">func (s *stackScanState) buildIndex()</a>

```
searchKey: runtime.stackScanState.buildIndex
```

```Go
func (s *stackScanState) buildIndex()
```

buildIndex initializes s.root to a binary search tree. It should be called after all addObject calls but before any call of findObject. 

#### <a id="stackScanState.findObject" href="#stackScanState.findObject">func (s *stackScanState) findObject(a uintptr) *stackObject</a>

```
searchKey: runtime.stackScanState.findObject
```

```Go
func (s *stackScanState) findObject(a uintptr) *stackObject
```

findObject returns the stack object containing address a, if any. Must have called buildIndex previously. 

### <a id="sweepdata" href="#sweepdata">type sweepdata struct</a>

```
searchKey: runtime.sweepdata
```

```Go
type sweepdata struct {
	lock    mutex
	g       *g
	parked  bool
	started bool

	nbgsweep    uint32
	npausesweep uint32

	// centralIndex is the current unswept span class.
	// It represents an index into the mcentral span
	// sets. Accessed and updated via its load and
	// update methods. Not protected by a lock.
	//
	// Reset at mark termination.
	// Used by mheap.nextSpanForSweep.
	centralIndex sweepClass
}
```

State of background sweep. 

### <a id="sweepClass" href="#sweepClass">type sweepClass uint32</a>

```
searchKey: runtime.sweepClass
```

```Go
type sweepClass uint32
```

sweepClass is a spanClass and one bit to represent whether we're currently sweeping partial or full spans. 

#### <a id="sweepClass.load" href="#sweepClass.load">func (s *sweepClass) load() sweepClass</a>

```
searchKey: runtime.sweepClass.load
```

```Go
func (s *sweepClass) load() sweepClass
```

#### <a id="sweepClass.update" href="#sweepClass.update">func (s *sweepClass) update(sNew sweepClass)</a>

```
searchKey: runtime.sweepClass.update
```

```Go
func (s *sweepClass) update(sNew sweepClass)
```

#### <a id="sweepClass.clear" href="#sweepClass.clear">func (s *sweepClass) clear()</a>

```
searchKey: runtime.sweepClass.clear
```

```Go
func (s *sweepClass) clear()
```

#### <a id="sweepClass.split" href="#sweepClass.split">func (s sweepClass) split() (spc spanClass, full bool)</a>

```
searchKey: runtime.sweepClass.split
```

```Go
func (s sweepClass) split() (spc spanClass, full bool)
```

split returns the underlying span class as well as whether we're interested in the full or partial unswept lists for that class, indicated as a boolean (true means "full"). 

### <a id="sweepLocker" href="#sweepLocker">type sweepLocker struct</a>

```
searchKey: runtime.sweepLocker
```

```Go
type sweepLocker struct {
	// sweepGen is the sweep generation of the heap.
	sweepGen uint32
	// blocking indicates that this tracker is blocking sweep
	// completion, usually as a result of acquiring sweep
	// ownership of at least one span.
	blocking bool
}
```

sweepLocker acquires sweep ownership of spans and blocks sweep completion. 

#### <a id="newSweepLocker" href="#newSweepLocker">func newSweepLocker() sweepLocker</a>

```
searchKey: runtime.newSweepLocker
```

```Go
func newSweepLocker() sweepLocker
```

#### <a id="sweepLocker.tryAcquire" href="#sweepLocker.tryAcquire">func (l *sweepLocker) tryAcquire(s *mspan) (sweepLocked, bool)</a>

```
searchKey: runtime.sweepLocker.tryAcquire
```

```Go
func (l *sweepLocker) tryAcquire(s *mspan) (sweepLocked, bool)
```

tryAcquire attempts to acquire sweep ownership of span s. If it successfully acquires ownership, it blocks sweep completion. 

#### <a id="sweepLocker.blockCompletion" href="#sweepLocker.blockCompletion">func (l *sweepLocker) blockCompletion()</a>

```
searchKey: runtime.sweepLocker.blockCompletion
```

```Go
func (l *sweepLocker) blockCompletion()
```

blockCompletion blocks sweep completion without acquiring any specific spans. 

#### <a id="sweepLocker.dispose" href="#sweepLocker.dispose">func (l *sweepLocker) dispose()</a>

```
searchKey: runtime.sweepLocker.dispose
```

```Go
func (l *sweepLocker) dispose()
```

#### <a id="sweepLocker.sweepIsDone" href="#sweepLocker.sweepIsDone">func (l *sweepLocker) sweepIsDone()</a>

```
searchKey: runtime.sweepLocker.sweepIsDone
```

```Go
func (l *sweepLocker) sweepIsDone()
```

### <a id="sweepLocked" href="#sweepLocked">type sweepLocked struct</a>

```
searchKey: runtime.sweepLocked
```

```Go
type sweepLocked struct {
	*mspan
}
```

sweepLocked represents sweep ownership of a span. 

#### <a id="sweepLocked.sweep" href="#sweepLocked.sweep">func (sl *sweepLocked) sweep(preserve bool) bool</a>

```
searchKey: runtime.sweepLocked.sweep
```

```Go
func (sl *sweepLocked) sweep(preserve bool) bool
```

Sweep frees or collects finalizers for blocks not marked in the mark phase. It clears the mark bits in preparation for the next GC round. Returns true if the span was returned to heap. If preserve=true, don't return it to heap nor relink in mcentral lists; caller takes care of it. 

### <a id="gcWork" href="#gcWork">type gcWork struct</a>

```
searchKey: runtime.gcWork
```

```Go
type gcWork struct {
	// wbuf1 and wbuf2 are the primary and secondary work buffers.
	//
	// This can be thought of as a stack of both work buffers'
	// pointers concatenated. When we pop the last pointer, we
	// shift the stack up by one work buffer by bringing in a new
	// full buffer and discarding an empty one. When we fill both
	// buffers, we shift the stack down by one work buffer by
	// bringing in a new empty buffer and discarding a full one.
	// This way we have one buffer's worth of hysteresis, which
	// amortizes the cost of getting or putting a work buffer over
	// at least one buffer of work and reduces contention on the
	// global work lists.
	//
	// wbuf1 is always the buffer we're currently pushing to and
	// popping from and wbuf2 is the buffer that will be discarded
	// next.
	//
	// Invariant: Both wbuf1 and wbuf2 are nil or neither are.
	wbuf1, wbuf2 *workbuf

	// Bytes marked (blackened) on this gcWork. This is aggregated
	// into work.bytesMarked by dispose.
	bytesMarked uint64

	// Scan work performed on this gcWork. This is aggregated into
	// gcController by dispose and may also be flushed by callers.
	scanWork int64

	// flushedWork indicates that a non-empty work buffer was
	// flushed to the global work list since the last gcMarkDone
	// termination check. Specifically, this indicates that this
	// gcWork may have communicated work to another gcWork.
	flushedWork bool
}
```

A gcWork provides the interface to produce and consume work for the garbage collector. 

A gcWork can be used on the stack as follows: 

```
(preemption must be disabled)
gcw := &getg().m.p.ptr().gcw
.. call gcw.put() to produce and gcw.tryGet() to consume ..

```
It's important that any use of gcWork during the mark phase prevent the garbage collector from transitioning to mark termination since gcWork may locally hold GC work buffers. This can be done by disabling preemption (systemstack or acquirem). 

#### <a id="gcWork.init" href="#gcWork.init">func (w *gcWork) init()</a>

```
searchKey: runtime.gcWork.init
```

```Go
func (w *gcWork) init()
```

#### <a id="gcWork.put" href="#gcWork.put">func (w *gcWork) put(obj uintptr)</a>

```
searchKey: runtime.gcWork.put
```

```Go
func (w *gcWork) put(obj uintptr)
```

put enqueues a pointer for the garbage collector to trace. obj must point to the beginning of a heap object or an oblet. 

#### <a id="gcWork.putFast" href="#gcWork.putFast">func (w *gcWork) putFast(obj uintptr) bool</a>

```
searchKey: runtime.gcWork.putFast
```

```Go
func (w *gcWork) putFast(obj uintptr) bool
```

putFast does a put and reports whether it can be done quickly otherwise it returns false and the caller needs to call put. 

#### <a id="gcWork.putBatch" href="#gcWork.putBatch">func (w *gcWork) putBatch(obj []uintptr)</a>

```
searchKey: runtime.gcWork.putBatch
```

```Go
func (w *gcWork) putBatch(obj []uintptr)
```

putBatch performs a put on every pointer in obj. See put for constraints on these pointers. 

#### <a id="gcWork.tryGet" href="#gcWork.tryGet">func (w *gcWork) tryGet() uintptr</a>

```
searchKey: runtime.gcWork.tryGet
```

```Go
func (w *gcWork) tryGet() uintptr
```

tryGet dequeues a pointer for the garbage collector to trace. 

If there are no pointers remaining in this gcWork or in the global queue, tryGet returns 0.  Note that there may still be pointers in other gcWork instances or other caches. 

#### <a id="gcWork.tryGetFast" href="#gcWork.tryGetFast">func (w *gcWork) tryGetFast() uintptr</a>

```
searchKey: runtime.gcWork.tryGetFast
```

```Go
func (w *gcWork) tryGetFast() uintptr
```

tryGetFast dequeues a pointer for the garbage collector to trace if one is readily available. Otherwise it returns 0 and the caller is expected to call tryGet(). 

#### <a id="gcWork.dispose" href="#gcWork.dispose">func (w *gcWork) dispose()</a>

```
searchKey: runtime.gcWork.dispose
```

```Go
func (w *gcWork) dispose()
```

dispose returns any cached pointers to the global queue. The buffers are being put on the full queue so that the write barriers will not simply reacquire them before the GC can inspect them. This helps reduce the mutator's ability to hide pointers during the concurrent mark phase. 

#### <a id="gcWork.balance" href="#gcWork.balance">func (w *gcWork) balance()</a>

```
searchKey: runtime.gcWork.balance
```

```Go
func (w *gcWork) balance()
```

balance moves some work that's cached in this gcWork back on the global queue. 

#### <a id="gcWork.empty" href="#gcWork.empty">func (w *gcWork) empty() bool</a>

```
searchKey: runtime.gcWork.empty
```

```Go
func (w *gcWork) empty() bool
```

empty reports whether w has no mark work available. 

### <a id="workbufhdr" href="#workbufhdr">type workbufhdr struct</a>

```
searchKey: runtime.workbufhdr
```

```Go
type workbufhdr struct {
	node lfnode // must be first
	nobj int
}
```

### <a id="workbuf" href="#workbuf">type workbuf struct</a>

```
searchKey: runtime.workbuf
```

```Go
type workbuf struct {
	workbufhdr
	// account for the above fields
	obj [(_WorkbufSize - unsafe.Sizeof(workbufhdr{})) / sys.PtrSize]uintptr
}
```

#### <a id="getempty" href="#getempty">func getempty() *workbuf</a>

```
searchKey: runtime.getempty
```

```Go
func getempty() *workbuf
```

getempty pops an empty work buffer off the work.empty list, allocating new buffers if none are available. 

#### <a id="trygetfull" href="#trygetfull">func trygetfull() *workbuf</a>

```
searchKey: runtime.trygetfull
```

```Go
func trygetfull() *workbuf
```

trygetfull tries to get a full or partially empty workbuffer. If one is not immediately available return nil 

#### <a id="handoff" href="#handoff">func handoff(b *workbuf) *workbuf</a>

```
searchKey: runtime.handoff
```

```Go
func handoff(b *workbuf) *workbuf
```

#### <a id="workbuf.checknonempty" href="#workbuf.checknonempty">func (b *workbuf) checknonempty()</a>

```
searchKey: runtime.workbuf.checknonempty
```

```Go
func (b *workbuf) checknonempty()
```

#### <a id="workbuf.checkempty" href="#workbuf.checkempty">func (b *workbuf) checkempty()</a>

```
searchKey: runtime.workbuf.checkempty
```

```Go
func (b *workbuf) checkempty()
```

### <a id="mheap" href="#mheap">type mheap struct</a>

```
searchKey: runtime.mheap
```

```Go
type mheap struct {
	// lock must only be acquired on the system stack, otherwise a g
	// could self-deadlock if its stack grows with the lock held.
	lock  mutex
	pages pageAlloc // page allocation data structure

	sweepgen     uint32 // sweep generation, see comment in mspan; written during STW
	sweepDrained uint32 // all spans are swept or are being swept
	sweepers     uint32 // number of active sweepone calls

	// allspans is a slice of all mspans ever created. Each mspan
	// appears exactly once.
	//
	// The memory for allspans is manually managed and can be
	// reallocated and move as the heap grows.
	//
	// In general, allspans is protected by mheap_.lock, which
	// prevents concurrent access as well as freeing the backing
	// store. Accesses during STW might not hold the lock, but
	// must ensure that allocation cannot happen around the
	// access (since that may free the backing store).
	allspans []*mspan // all spans out there

	_ uint32 // align uint64 fields on 32-bit for atomics

	// Proportional sweep
	//
	// These parameters represent a linear function from gcController.heapLive
	// to page sweep count. The proportional sweep system works to
	// stay in the black by keeping the current page sweep count
	// above this line at the current gcController.heapLive.
	//
	// The line has slope sweepPagesPerByte and passes through a
	// basis point at (sweepHeapLiveBasis, pagesSweptBasis). At
	// any given time, the system is at (gcController.heapLive,
	// pagesSwept) in this space.
	//
	// It's important that the line pass through a point we
	// control rather than simply starting at a (0,0) origin
	// because that lets us adjust sweep pacing at any time while
	// accounting for current progress. If we could only adjust
	// the slope, it would create a discontinuity in debt if any
	// progress has already been made.
	pagesInUse         uint64  // pages of spans in stats mSpanInUse; updated atomically
	pagesSwept         uint64  // pages swept this cycle; updated atomically
	pagesSweptBasis    uint64  // pagesSwept to use as the origin of the sweep ratio; updated atomically
	sweepHeapLiveBasis uint64  // value of gcController.heapLive to use as the origin of sweep ratio; written with lock, read without
	sweepPagesPerByte  float64 // proportional sweep ratio; written with lock, read without

	// scavengeGoal is the amount of total retained heap memory (measured by
	// heapRetained) that the runtime will try to maintain by returning memory
	// to the OS.
	scavengeGoal uint64

	// reclaimIndex is the page index in allArenas of next page to
	// reclaim. Specifically, it refers to page (i %
	// pagesPerArena) of arena allArenas[i / pagesPerArena].
	//
	// If this is >= 1<<63, the page reclaimer is done scanning
	// the page marks.
	//
	// This is accessed atomically.
	reclaimIndex uint64
	// reclaimCredit is spare credit for extra pages swept. Since
	// the page reclaimer works in large chunks, it may reclaim
	// more than requested. Any spare pages released go to this
	// credit pool.
	//
	// This is accessed atomically.
	reclaimCredit uintptr

	// arenas is the heap arena map. It points to the metadata for
	// the heap for every arena frame of the entire usable virtual
	// address space.
	//
	// Use arenaIndex to compute indexes into this array.
	//
	// For regions of the address space that are not backed by the
	// Go heap, the arena map contains nil.
	//
	// Modifications are protected by mheap_.lock. Reads can be
	// performed without locking; however, a given entry can
	// transition from nil to non-nil at any time when the lock
	// isn't held. (Entries never transitions back to nil.)
	//
	// In general, this is a two-level mapping consisting of an L1
	// map and possibly many L2 maps. This saves space when there
	// are a huge number of arena frames. However, on many
	// platforms (even 64-bit), arenaL1Bits is 0, making this
	// effectively a single-level map. In this case, arenas[0]
	// will never be nil.
	arenas [1 << arenaL1Bits]*[1 << arenaL2Bits]*heapArena

	// heapArenaAlloc is pre-reserved space for allocating heapArena
	// objects. This is only used on 32-bit, where we pre-reserve
	// this space to avoid interleaving it with the heap itself.
	heapArenaAlloc linearAlloc

	// arenaHints is a list of addresses at which to attempt to
	// add more heap arenas. This is initially populated with a
	// set of general hint addresses, and grown with the bounds of
	// actual heap arena ranges.
	arenaHints *arenaHint

	// arena is a pre-reserved space for allocating heap arenas
	// (the actual arenas). This is only used on 32-bit.
	arena linearAlloc

	// allArenas is the arenaIndex of every mapped arena. This can
	// be used to iterate through the address space.
	//
	// Access is protected by mheap_.lock. However, since this is
	// append-only and old backing arrays are never freed, it is
	// safe to acquire mheap_.lock, copy the slice header, and
	// then release mheap_.lock.
	allArenas []arenaIdx

	// sweepArenas is a snapshot of allArenas taken at the
	// beginning of the sweep cycle. This can be read safely by
	// simply blocking GC (by disabling preemption).
	sweepArenas []arenaIdx

	// markArenas is a snapshot of allArenas taken at the beginning
	// of the mark cycle. Because allArenas is append-only, neither
	// this slice nor its contents will change during the mark, so
	// it can be read safely.
	markArenas []arenaIdx

	// curArena is the arena that the heap is currently growing
	// into. This should always be physPageSize-aligned.
	curArena struct {
		base, end uintptr
	}

	_ uint32 // ensure 64-bit alignment of central

	// central free lists for small size classes.
	// the padding makes sure that the mcentrals are
	// spaced CacheLinePadSize bytes apart, so that each mcentral.lock
	// gets its own cache line.
	// central is indexed by spanClass.
	central [numSpanClasses]struct {
		mcentral mcentral
		pad      [cpu.CacheLinePadSize - unsafe.Sizeof(mcentral{})%cpu.CacheLinePadSize]byte
	}

	spanalloc             fixalloc // allocator for span*
	cachealloc            fixalloc // allocator for mcache*
	specialfinalizeralloc fixalloc // allocator for specialfinalizer*
	specialprofilealloc   fixalloc // allocator for specialprofile*
	specialReachableAlloc fixalloc // allocator for specialReachable
	speciallock           mutex    // lock for special record allocators.
	arenaHintAlloc        fixalloc // allocator for arenaHints

	unused *specialfinalizer // never set, just here to force the specialfinalizer type into DWARF
}
```

Main malloc heap. The heap itself is the "free" and "scav" treaps, but all the other global data is here too. 

mheap must not be heap-allocated because it contains mSpanLists, which must not be heap-allocated. 

#### <a id="mheap.sysAlloc" href="#mheap.sysAlloc">func (h *mheap) sysAlloc(n uintptr) (v unsafe.Pointer, size uintptr)</a>

```
searchKey: runtime.mheap.sysAlloc
```

```Go
func (h *mheap) sysAlloc(n uintptr) (v unsafe.Pointer, size uintptr)
```

sysAlloc allocates heap arena space for at least n bytes. The returned pointer is always heapArenaBytes-aligned and backed by h.arenas metadata. The returned size is always a multiple of heapArenaBytes. sysAlloc returns nil on failure. There is no corresponding free function. 

sysAlloc returns a memory region in the Reserved state. This region must be transitioned to Prepared and then Ready before use. 

h must be locked. 

#### <a id="mheap.nextSpanForSweep" href="#mheap.nextSpanForSweep">func (h *mheap) nextSpanForSweep() *mspan</a>

```
searchKey: runtime.mheap.nextSpanForSweep
```

```Go
func (h *mheap) nextSpanForSweep() *mspan
```

nextSpanForSweep finds and pops the next span for sweeping from the central sweep buffers. It returns ownership of the span to the caller. Returns nil if no such span exists. 

#### <a id="mheap.init" href="#mheap.init">func (h *mheap) init()</a>

```
searchKey: runtime.mheap.init
```

```Go
func (h *mheap) init()
```

Initialize the heap. 

#### <a id="mheap.reclaim" href="#mheap.reclaim">func (h *mheap) reclaim(npage uintptr)</a>

```
searchKey: runtime.mheap.reclaim
```

```Go
func (h *mheap) reclaim(npage uintptr)
```

reclaim sweeps and reclaims at least npage pages into the heap. It is called before allocating npage pages to keep growth in check. 

reclaim implements the page-reclaimer half of the sweeper. 

h.lock must NOT be held. 

#### <a id="mheap.reclaimChunk" href="#mheap.reclaimChunk">func (h *mheap) reclaimChunk(arenas []arenaIdx, pageIdx, n uintptr) uintptr</a>

```
searchKey: runtime.mheap.reclaimChunk
```

```Go
func (h *mheap) reclaimChunk(arenas []arenaIdx, pageIdx, n uintptr) uintptr
```

reclaimChunk sweeps unmarked spans that start at page indexes [pageIdx, pageIdx+n). It returns the number of pages returned to the heap. 

h.lock must be held and the caller must be non-preemptible. Note: h.lock may be temporarily unlocked and re-locked in order to do sweeping or if tracing is enabled. 

#### <a id="mheap.alloc" href="#mheap.alloc">func (h *mheap) alloc(npages uintptr, spanclass spanClass, needzero bool) (*mspan, bool)</a>

```
searchKey: runtime.mheap.alloc
```

```Go
func (h *mheap) alloc(npages uintptr, spanclass spanClass, needzero bool) (*mspan, bool)
```

alloc allocates a new span of npage pages from the GC'd heap. 

spanclass indicates the span's size class and scannability. 

If needzero is true, the memory for the returned span will be zeroed. The boolean returned indicates whether the returned span contains zeroes, either because this was requested, or because it was already zeroed. 

#### <a id="mheap.allocManual" href="#mheap.allocManual">func (h *mheap) allocManual(npages uintptr, typ spanAllocType) *mspan</a>

```
searchKey: runtime.mheap.allocManual
```

```Go
func (h *mheap) allocManual(npages uintptr, typ spanAllocType) *mspan
```

allocManual allocates a manually-managed span of npage pages. allocManual returns nil if allocation fails. 

allocManual adds the bytes used to *stat, which should be a memstats in-use field. Unlike allocations in the GC'd heap, the allocation does *not* count toward heap_inuse or heap_sys. 

The memory backing the returned span may not be zeroed if span.needzero is set. 

allocManual must be called on the system stack because it may acquire the heap lock via allocSpan. See mheap for details. 

If new code is written to call allocManual, do NOT use an existing spanAllocType value and instead declare a new one. 

#### <a id="mheap.setSpans" href="#mheap.setSpans">func (h *mheap) setSpans(base, npage uintptr, s *mspan)</a>

```
searchKey: runtime.mheap.setSpans
```

```Go
func (h *mheap) setSpans(base, npage uintptr, s *mspan)
```

setSpans modifies the span map so [spanOf(base), spanOf(base+npage*pageSize)) is s. 

#### <a id="mheap.allocNeedsZero" href="#mheap.allocNeedsZero">func (h *mheap) allocNeedsZero(base, npage uintptr) (needZero bool)</a>

```
searchKey: runtime.mheap.allocNeedsZero
```

```Go
func (h *mheap) allocNeedsZero(base, npage uintptr) (needZero bool)
```

allocNeedsZero checks if the region of address space [base, base+npage*pageSize), assumed to be allocated, needs to be zeroed, updating heap arena metadata for future allocations. 

This must be called each time pages are allocated from the heap, even if the page allocator can otherwise prove the memory it's allocating is already zero because they're fresh from the operating system. It updates heapArena metadata that is critical for future page allocations. 

There are no locking constraints on this method. 

#### <a id="mheap.tryAllocMSpan" href="#mheap.tryAllocMSpan">func (h *mheap) tryAllocMSpan() *mspan</a>

```
searchKey: runtime.mheap.tryAllocMSpan
```

```Go
func (h *mheap) tryAllocMSpan() *mspan
```

tryAllocMSpan attempts to allocate an mspan object from the P-local cache, but may fail. 

h.lock need not be held. 

This caller must ensure that its P won't change underneath it during this function. Currently to ensure that we enforce that the function is run on the system stack, because that's the only place it is used now. In the future, this requirement may be relaxed if its use is necessary elsewhere. 

#### <a id="mheap.allocMSpanLocked" href="#mheap.allocMSpanLocked">func (h *mheap) allocMSpanLocked() *mspan</a>

```
searchKey: runtime.mheap.allocMSpanLocked
```

```Go
func (h *mheap) allocMSpanLocked() *mspan
```

allocMSpanLocked allocates an mspan object. 

h.lock must be held. 

allocMSpanLocked must be called on the system stack because its caller holds the heap lock. See mheap for details. Running on the system stack also ensures that we won't switch Ps during this function. See tryAllocMSpan for details. 

#### <a id="mheap.freeMSpanLocked" href="#mheap.freeMSpanLocked">func (h *mheap) freeMSpanLocked(s *mspan)</a>

```
searchKey: runtime.mheap.freeMSpanLocked
```

```Go
func (h *mheap) freeMSpanLocked(s *mspan)
```

freeMSpanLocked free an mspan object. 

h.lock must be held. 

freeMSpanLocked must be called on the system stack because its caller holds the heap lock. See mheap for details. Running on the system stack also ensures that we won't switch Ps during this function. See tryAllocMSpan for details. 

#### <a id="mheap.allocSpan" href="#mheap.allocSpan">func (h *mheap) allocSpan(npages uintptr, typ spanAllocType, spanclass spanClass) (s *mspan)</a>

```
searchKey: runtime.mheap.allocSpan
```

```Go
func (h *mheap) allocSpan(npages uintptr, typ spanAllocType, spanclass spanClass) (s *mspan)
```

allocSpan allocates an mspan which owns npages worth of memory. 

If typ.manual() == false, allocSpan allocates a heap span of class spanclass and updates heap accounting. If manual == true, allocSpan allocates a manually-managed span (spanclass is ignored), and the caller is responsible for any accounting related to its use of the span. Either way, allocSpan will atomically add the bytes in the newly allocated span to *sysStat. 

The returned span is fully initialized. 

h.lock must not be held. 

allocSpan must be called on the system stack both because it acquires the heap lock and because it must block GC transitions. 

#### <a id="mheap.grow" href="#mheap.grow">func (h *mheap) grow(npage uintptr) bool</a>

```
searchKey: runtime.mheap.grow
```

```Go
func (h *mheap) grow(npage uintptr) bool
```

Try to add at least npage pages of memory to the heap, returning whether it worked. 

h.lock must be held. 

#### <a id="mheap.freeSpan" href="#mheap.freeSpan">func (h *mheap) freeSpan(s *mspan)</a>

```
searchKey: runtime.mheap.freeSpan
```

```Go
func (h *mheap) freeSpan(s *mspan)
```

Free the span back into the heap. 

#### <a id="mheap.freeManual" href="#mheap.freeManual">func (h *mheap) freeManual(s *mspan, typ spanAllocType)</a>

```
searchKey: runtime.mheap.freeManual
```

```Go
func (h *mheap) freeManual(s *mspan, typ spanAllocType)
```

freeManual frees a manually-managed span returned by allocManual. typ must be the same as the spanAllocType passed to the allocManual that allocated s. 

This must only be called when gcphase == _GCoff. See mSpanState for an explanation. 

freeManual must be called on the system stack because it acquires the heap lock. See mheap for details. 

#### <a id="mheap.freeSpanLocked" href="#mheap.freeSpanLocked">func (h *mheap) freeSpanLocked(s *mspan, typ spanAllocType)</a>

```
searchKey: runtime.mheap.freeSpanLocked
```

```Go
func (h *mheap) freeSpanLocked(s *mspan, typ spanAllocType)
```

#### <a id="mheap.scavengeAll" href="#mheap.scavengeAll">func (h *mheap) scavengeAll()</a>

```
searchKey: runtime.mheap.scavengeAll
```

```Go
func (h *mheap) scavengeAll()
```

scavengeAll acquires the heap lock (blocking any additional manipulation of the page allocator) and iterates over the whole heap, scavenging every free page available. 

### <a id="heapArena" href="#heapArena">type heapArena struct</a>

```
searchKey: runtime.heapArena
```

```Go
type heapArena struct {
	// bitmap stores the pointer/scalar bitmap for the words in
	// this arena. See mbitmap.go for a description. Use the
	// heapBits type to access this.
	bitmap [heapArenaBitmapBytes]byte

	// spans maps from virtual address page ID within this arena to *mspan.
	// For allocated spans, their pages map to the span itself.
	// For free spans, only the lowest and highest pages map to the span itself.
	// Internal pages map to an arbitrary span.
	// For pages that have never been allocated, spans entries are nil.
	//
	// Modifications are protected by mheap.lock. Reads can be
	// performed without locking, but ONLY from indexes that are
	// known to contain in-use or stack spans. This means there
	// must not be a safe-point between establishing that an
	// address is live and looking it up in the spans array.
	spans [pagesPerArena]*mspan

	// pageInUse is a bitmap that indicates which spans are in
	// state mSpanInUse. This bitmap is indexed by page number,
	// but only the bit corresponding to the first page in each
	// span is used.
	//
	// Reads and writes are atomic.
	pageInUse [pagesPerArena / 8]uint8

	// pageMarks is a bitmap that indicates which spans have any
	// marked objects on them. Like pageInUse, only the bit
	// corresponding to the first page in each span is used.
	//
	// Writes are done atomically during marking. Reads are
	// non-atomic and lock-free since they only occur during
	// sweeping (and hence never race with writes).
	//
	// This is used to quickly find whole spans that can be freed.
	//
	// TODO(austin): It would be nice if this was uint64 for
	// faster scanning, but we don't have 64-bit atomic bit
	// operations.
	pageMarks [pagesPerArena / 8]uint8

	// pageSpecials is a bitmap that indicates which spans have
	// specials (finalizers or other). Like pageInUse, only the bit
	// corresponding to the first page in each span is used.
	//
	// Writes are done atomically whenever a special is added to
	// a span and whenever the last special is removed from a span.
	// Reads are done atomically to find spans containing specials
	// during marking.
	pageSpecials [pagesPerArena / 8]uint8

	// checkmarks stores the debug.gccheckmark state. It is only
	// used if debug.gccheckmark > 0.
	checkmarks *checkmarksMap

	// zeroedBase marks the first byte of the first page in this
	// arena which hasn't been used yet and is therefore already
	// zero. zeroedBase is relative to the arena base.
	// Increases monotonically until it hits heapArenaBytes.
	//
	// This field is sufficient to determine if an allocation
	// needs to be zeroed because the page allocator follows an
	// address-ordered first-fit policy.
	//
	// Read atomically and written with an atomic CAS.
	zeroedBase uintptr
}
```

A heapArena stores metadata for a heap arena. heapArenas are stored outside of the Go heap and accessed via the mheap_.arenas index. 

#### <a id="pageIndexOf" href="#pageIndexOf">func pageIndexOf(p uintptr) (arena *heapArena, pageIdx uintptr, pageMask uint8)</a>

```
searchKey: runtime.pageIndexOf
```

```Go
func pageIndexOf(p uintptr) (arena *heapArena, pageIdx uintptr, pageMask uint8)
```

pageIndexOf returns the arena, page index, and page mask for pointer p. The caller must ensure p is in the heap. 

### <a id="arenaHint" href="#arenaHint">type arenaHint struct</a>

```
searchKey: runtime.arenaHint
```

```Go
type arenaHint struct {
	addr uintptr
	down bool
	next *arenaHint
}
```

arenaHint is a hint for where to grow the heap arenas. See mheap_.arenaHints. 

### <a id="mSpanState" href="#mSpanState">type mSpanState uint8</a>

```
searchKey: runtime.mSpanState
```

```Go
type mSpanState uint8
```

An mspan representing actual memory has state mSpanInUse, mSpanManual, or mSpanFree. Transitions between these states are constrained as follows: 

* A span may transition from free to in-use or manual during any GC 

```
phase.

```
* During sweeping (gcphase == _GCoff), a span may transition from 

```
in-use to free (as a result of sweeping) or manual to free (as a
result of stacks being freed).

```
* During GC (gcphase != _GCoff), a span *must not* transition from 

```
manual or in-use to free. Because concurrent GC may read a pointer
and then look up its span, the span state must be monotonic.

```
Setting mspan.state to mSpanInUse or mSpanManual must be done atomically and only after all other span fields are valid. Likewise, if inspecting a span is contingent on it being mSpanInUse, the state should be loaded atomically and checked before depending on other fields. This allows the garbage collector to safely deal with potentially invalid pointers, since resolving such pointers may race with a span being allocated. 

### <a id="mSpanStateBox" href="#mSpanStateBox">type mSpanStateBox struct</a>

```
searchKey: runtime.mSpanStateBox
```

```Go
type mSpanStateBox struct {
	s mSpanState
}
```

mSpanStateBox holds an mSpanState and provides atomic operations on it. This is a separate type to disallow accidental comparison or assignment with mSpanState. 

#### <a id="mSpanStateBox.set" href="#mSpanStateBox.set">func (b *mSpanStateBox) set(s mSpanState)</a>

```
searchKey: runtime.mSpanStateBox.set
```

```Go
func (b *mSpanStateBox) set(s mSpanState)
```

#### <a id="mSpanStateBox.get" href="#mSpanStateBox.get">func (b *mSpanStateBox) get() mSpanState</a>

```
searchKey: runtime.mSpanStateBox.get
```

```Go
func (b *mSpanStateBox) get() mSpanState
```

### <a id="mSpanList" href="#mSpanList">type mSpanList struct</a>

```
searchKey: runtime.mSpanList
```

```Go
type mSpanList struct {
	first *mspan // first span in list, or nil if none
	last  *mspan // last span in list, or nil if none
}
```

mSpanList heads a linked list of spans. 

#### <a id="mSpanList.init" href="#mSpanList.init">func (list *mSpanList) init()</a>

```
searchKey: runtime.mSpanList.init
```

```Go
func (list *mSpanList) init()
```

Initialize an empty doubly-linked list. 

#### <a id="mSpanList.remove" href="#mSpanList.remove">func (list *mSpanList) remove(span *mspan)</a>

```
searchKey: runtime.mSpanList.remove
```

```Go
func (list *mSpanList) remove(span *mspan)
```

#### <a id="mSpanList.isEmpty" href="#mSpanList.isEmpty">func (list *mSpanList) isEmpty() bool</a>

```
searchKey: runtime.mSpanList.isEmpty
```

```Go
func (list *mSpanList) isEmpty() bool
```

#### <a id="mSpanList.insert" href="#mSpanList.insert">func (list *mSpanList) insert(span *mspan)</a>

```
searchKey: runtime.mSpanList.insert
```

```Go
func (list *mSpanList) insert(span *mspan)
```

#### <a id="mSpanList.insertBack" href="#mSpanList.insertBack">func (list *mSpanList) insertBack(span *mspan)</a>

```
searchKey: runtime.mSpanList.insertBack
```

```Go
func (list *mSpanList) insertBack(span *mspan)
```

#### <a id="mSpanList.takeAll" href="#mSpanList.takeAll">func (list *mSpanList) takeAll(other *mSpanList)</a>

```
searchKey: runtime.mSpanList.takeAll
```

```Go
func (list *mSpanList) takeAll(other *mSpanList)
```

takeAll removes all spans from other and inserts them at the front of list. 

### <a id="mspan" href="#mspan">type mspan struct</a>

```
searchKey: runtime.mspan
```

```Go
type mspan struct {
	next *mspan     // next span in list, or nil if none
	prev *mspan     // previous span in list, or nil if none
	list *mSpanList // For debugging. TODO: Remove.

	startAddr uintptr // address of first byte of span aka s.base()
	npages    uintptr // number of pages in span

	manualFreeList gclinkptr // list of free objects in mSpanManual spans

	// freeindex is the slot index between 0 and nelems at which to begin scanning
	// for the next free object in this span.
	// Each allocation scans allocBits starting at freeindex until it encounters a 0
	// indicating a free object. freeindex is then adjusted so that subsequent scans begin
	// just past the newly discovered free object.
	//
	// If freeindex == nelem, this span has no free objects.
	//
	// allocBits is a bitmap of objects in this span.
	// If n >= freeindex and allocBits[n/8] & (1<<(n%8)) is 0
	// then object n is free;
	// otherwise, object n is allocated. Bits starting at nelem are
	// undefined and should never be referenced.
	//
	// Object n starts at address n*elemsize + (start << pageShift).
	freeindex uintptr
	// TODO: Look up nelems from sizeclass and remove this field if it
	// helps performance.
	nelems uintptr // number of object in the span.

	// Cache of the allocBits at freeindex. allocCache is shifted
	// such that the lowest bit corresponds to the bit freeindex.
	// allocCache holds the complement of allocBits, thus allowing
	// ctz (count trailing zero) to use it directly.
	// allocCache may contain bits beyond s.nelems; the caller must ignore
	// these.
	allocCache uint64

	// allocBits and gcmarkBits hold pointers to a span's mark and
	// allocation bits. The pointers are 8 byte aligned.
	// There are three arenas where this data is held.
	// free: Dirty arenas that are no longer accessed
	//       and can be reused.
	// next: Holds information to be used in the next GC cycle.
	// current: Information being used during this GC cycle.
	// previous: Information being used during the last GC cycle.
	// A new GC cycle starts with the call to finishsweep_m.
	// finishsweep_m moves the previous arena to the free arena,
	// the current arena to the previous arena, and
	// the next arena to the current arena.
	// The next arena is populated as the spans request
	// memory to hold gcmarkBits for the next GC cycle as well
	// as allocBits for newly allocated spans.
	//
	// The pointer arithmetic is done "by hand" instead of using
	// arrays to avoid bounds checks along critical performance
	// paths.
	// The sweep will free the old allocBits and set allocBits to the
	// gcmarkBits. The gcmarkBits are replaced with a fresh zeroed
	// out memory.
	allocBits  *gcBits
	gcmarkBits *gcBits

	sweepgen    uint32
	divMul      uint32        // for divide by elemsize
	allocCount  uint16        // number of allocated objects
	spanclass   spanClass     // size class and noscan (uint8)
	state       mSpanStateBox // mSpanInUse etc; accessed atomically (get/set methods)
	needzero    uint8         // needs to be zeroed before allocation
	elemsize    uintptr       // computed from sizeclass or from npages
	limit       uintptr       // end of data in span
	speciallock mutex         // guards specials list
	specials    *special      // linked list of special records sorted by offset.
}
```

#### <a id="findObject" href="#findObject">func findObject(p, refBase, refOff uintptr) (base uintptr, s *mspan, objIndex uintptr)</a>

```
searchKey: runtime.findObject
```

```Go
func findObject(p, refBase, refOff uintptr) (base uintptr, s *mspan, objIndex uintptr)
```

findObject returns the base address for the heap object containing the address p, the object's span, and the index of the object in s. If p does not point into a heap object, it returns base == 0. 

If p points is an invalid heap pointer and debug.invalidptr != 0, findObject panics. 

refBase and refOff optionally give the base address of the object in which the pointer p was found and the byte offset at which it was found. These are used for error reporting. 

It is nosplit so it is safe for p to be a pointer to the current goroutine's stack. Since p is a uintptr, it would not be adjusted if the stack were to move. 

#### <a id="materializeGCProg" href="#materializeGCProg">func materializeGCProg(ptrdata uintptr, prog *byte) *mspan</a>

```
searchKey: runtime.materializeGCProg
```

```Go
func materializeGCProg(ptrdata uintptr, prog *byte) *mspan
```

materializeGCProg allocates space for the (1-bit) pointer bitmask for an object of size ptrdata.  Then it fills that space with the pointer bitmask specified by the program prog. The bitmask starts at s.startAddr. The result must be deallocated with dematerializeGCProg. 

#### <a id="spanOf" href="#spanOf">func spanOf(p uintptr) *mspan</a>

```
searchKey: runtime.spanOf
```

```Go
func spanOf(p uintptr) *mspan
```

spanOf returns the span of p. If p does not point into the heap arena or no span has ever contained p, spanOf returns nil. 

If p does not point to allocated memory, this may return a non-nil span that does *not* contain p. If this is a possibility, the caller should either call spanOfHeap or check the span bounds explicitly. 

Must be nosplit because it has callers that are nosplit. 

#### <a id="spanOfUnchecked" href="#spanOfUnchecked">func spanOfUnchecked(p uintptr) *mspan</a>

```
searchKey: runtime.spanOfUnchecked
```

```Go
func spanOfUnchecked(p uintptr) *mspan
```

spanOfUnchecked is equivalent to spanOf, but the caller must ensure that p points into an allocated heap arena. 

Must be nosplit because it has callers that are nosplit. 

#### <a id="spanOfHeap" href="#spanOfHeap">func spanOfHeap(p uintptr) *mspan</a>

```
searchKey: runtime.spanOfHeap
```

```Go
func spanOfHeap(p uintptr) *mspan
```

spanOfHeap is like spanOf, but returns nil if p does not point to a heap object. 

Must be nosplit because it has callers that are nosplit. 

#### <a id="mspan.allocBitsForIndex" href="#mspan.allocBitsForIndex">func (s *mspan) allocBitsForIndex(allocBitIndex uintptr) markBits</a>

```
searchKey: runtime.mspan.allocBitsForIndex
```

```Go
func (s *mspan) allocBitsForIndex(allocBitIndex uintptr) markBits
```

#### <a id="mspan.refillAllocCache" href="#mspan.refillAllocCache">func (s *mspan) refillAllocCache(whichByte uintptr)</a>

```
searchKey: runtime.mspan.refillAllocCache
```

```Go
func (s *mspan) refillAllocCache(whichByte uintptr)
```

refillAllocCache takes 8 bytes s.allocBits starting at whichByte and negates them so that ctz (count trailing zeros) instructions can be used. It then places these 8 bytes into the cached 64 bit s.allocCache. 

#### <a id="mspan.nextFreeIndex" href="#mspan.nextFreeIndex">func (s *mspan) nextFreeIndex() uintptr</a>

```
searchKey: runtime.mspan.nextFreeIndex
```

```Go
func (s *mspan) nextFreeIndex() uintptr
```

nextFreeIndex returns the index of the next free object in s at or after s.freeindex. There are hardware instructions that can be used to make this faster if profiling warrants it. 

#### <a id="mspan.isFree" href="#mspan.isFree">func (s *mspan) isFree(index uintptr) bool</a>

```
searchKey: runtime.mspan.isFree
```

```Go
func (s *mspan) isFree(index uintptr) bool
```

isFree reports whether the index'th object in s is unallocated. 

The caller must ensure s.state is mSpanInUse, and there must have been no preemption points since ensuring this (which could allow a GC transition, which would allow the state to change). 

#### <a id="mspan.divideByElemSize" href="#mspan.divideByElemSize">func (s *mspan) divideByElemSize(n uintptr) uintptr</a>

```
searchKey: runtime.mspan.divideByElemSize
```

```Go
func (s *mspan) divideByElemSize(n uintptr) uintptr
```

divideByElemSize returns n/s.elemsize. n must be within [0, s.npages*_PageSize), or may be exactly s.npages*_PageSize if s.elemsize is from sizeclasses.go. 

#### <a id="mspan.objIndex" href="#mspan.objIndex">func (s *mspan) objIndex(p uintptr) uintptr</a>

```
searchKey: runtime.mspan.objIndex
```

```Go
func (s *mspan) objIndex(p uintptr) uintptr
```

#### <a id="mspan.markBitsForIndex" href="#mspan.markBitsForIndex">func (s *mspan) markBitsForIndex(objIndex uintptr) markBits</a>

```
searchKey: runtime.mspan.markBitsForIndex
```

```Go
func (s *mspan) markBitsForIndex(objIndex uintptr) markBits
```

#### <a id="mspan.markBitsForBase" href="#mspan.markBitsForBase">func (s *mspan) markBitsForBase() markBits</a>

```
searchKey: runtime.mspan.markBitsForBase
```

```Go
func (s *mspan) markBitsForBase() markBits
```

#### <a id="mspan.countAlloc" href="#mspan.countAlloc">func (s *mspan) countAlloc() int</a>

```
searchKey: runtime.mspan.countAlloc
```

```Go
func (s *mspan) countAlloc() int
```

countAlloc returns the number of objects allocated in span s by scanning the allocation bitmap. 

#### <a id="mspan.ensureSwept" href="#mspan.ensureSwept">func (s *mspan) ensureSwept()</a>

```
searchKey: runtime.mspan.ensureSwept
```

```Go
func (s *mspan) ensureSwept()
```

Returns only when span s has been swept. 

#### <a id="mspan.reportZombies" href="#mspan.reportZombies">func (s *mspan) reportZombies()</a>

```
searchKey: runtime.mspan.reportZombies
```

```Go
func (s *mspan) reportZombies()
```

reportZombies reports any marked but free objects in s and throws. 

This generally means one of the following: 

1. User code converted a pointer to a uintptr and then back unsafely, and a GC ran while the uintptr was the only reference to an object. 

2. User code (or a compiler bug) constructed a bad pointer that points to a free slot, often a past-the-end pointer. 

3. The GC two cycles ago missed a pointer and freed a live object, but it was still live in the last cycle, so this GC cycle found a pointer to that object and marked it. 

#### <a id="mspan.base" href="#mspan.base">func (s *mspan) base() uintptr</a>

```
searchKey: runtime.mspan.base
```

```Go
func (s *mspan) base() uintptr
```

#### <a id="mspan.layout" href="#mspan.layout">func (s *mspan) layout() (size, n, total uintptr)</a>

```
searchKey: runtime.mspan.layout
```

```Go
func (s *mspan) layout() (size, n, total uintptr)
```

#### <a id="mspan.init" href="#mspan.init">func (span *mspan) init(base uintptr, npages uintptr)</a>

```
searchKey: runtime.mspan.init
```

```Go
func (span *mspan) init(base uintptr, npages uintptr)
```

Initialize a new span with the given start and npages. 

#### <a id="mspan.inList" href="#mspan.inList">func (span *mspan) inList() bool</a>

```
searchKey: runtime.mspan.inList
```

```Go
func (span *mspan) inList() bool
```

### <a id="spanClass" href="#spanClass">type spanClass uint8</a>

```
searchKey: runtime.spanClass
```

```Go
type spanClass uint8
```

A spanClass represents the size class and noscan-ness of a span. 

Each size class has a noscan spanClass and a scan spanClass. The noscan spanClass contains only noscan objects, which do not contain pointers and thus do not need to be scanned by the garbage collector. 

#### <a id="makeSpanClass" href="#makeSpanClass">func makeSpanClass(sizeclass uint8, noscan bool) spanClass</a>

```
searchKey: runtime.makeSpanClass
```

```Go
func makeSpanClass(sizeclass uint8, noscan bool) spanClass
```

#### <a id="spanClass.sizeclass" href="#spanClass.sizeclass">func (sc spanClass) sizeclass() int8</a>

```
searchKey: runtime.spanClass.sizeclass
```

```Go
func (sc spanClass) sizeclass() int8
```

#### <a id="spanClass.noscan" href="#spanClass.noscan">func (sc spanClass) noscan() bool</a>

```
searchKey: runtime.spanClass.noscan
```

```Go
func (sc spanClass) noscan() bool
```

### <a id="arenaIdx" href="#arenaIdx">type arenaIdx uint</a>

```
searchKey: runtime.arenaIdx
```

```Go
type arenaIdx uint
```

#### <a id="arenaIndex" href="#arenaIndex">func arenaIndex(p uintptr) arenaIdx</a>

```
searchKey: runtime.arenaIndex
```

```Go
func arenaIndex(p uintptr) arenaIdx
```

arenaIndex returns the index into mheap_.arenas of the arena containing metadata for p. This index combines of an index into the L1 map and an index into the L2 map and should be used as mheap_.arenas[ai.l1()][ai.l2()]. 

If p is outside the range of valid heap addresses, either l1() or l2() will be out of bounds. 

It is nosplit because it's called by spanOf and several other nosplit functions. 

#### <a id="arenaIdx.l1" href="#arenaIdx.l1">func (i arenaIdx) l1() uint</a>

```
searchKey: runtime.arenaIdx.l1
```

```Go
func (i arenaIdx) l1() uint
```

#### <a id="arenaIdx.l2" href="#arenaIdx.l2">func (i arenaIdx) l2() uint</a>

```
searchKey: runtime.arenaIdx.l2
```

```Go
func (i arenaIdx) l2() uint
```

### <a id="spanAllocType" href="#spanAllocType">type spanAllocType uint8</a>

```
searchKey: runtime.spanAllocType
```

```Go
type spanAllocType uint8
```

spanAllocType represents the type of allocation to make, or the type of allocation to be freed. 

#### <a id="spanAllocType.manual" href="#spanAllocType.manual">func (s spanAllocType) manual() bool</a>

```
searchKey: runtime.spanAllocType.manual
```

```Go
func (s spanAllocType) manual() bool
```

manual returns true if the span allocation is manually managed. 

### <a id="special" href="#special">type special struct</a>

```
searchKey: runtime.special
```

```Go
type special struct {
	next   *special // linked list in span
	offset uint16   // span offset of object
	kind   byte     // kind of special
}
```

#### <a id="removespecial" href="#removespecial">func removespecial(p unsafe.Pointer, kind uint8) *special</a>

```
searchKey: runtime.removespecial
```

```Go
func removespecial(p unsafe.Pointer, kind uint8) *special
```

Removes the Special record of the given kind for the object p. Returns the record if the record existed, nil otherwise. The caller must FixAlloc_Free the result. 

### <a id="specialfinalizer" href="#specialfinalizer">type specialfinalizer struct</a>

```
searchKey: runtime.specialfinalizer
```

```Go
type specialfinalizer struct {
	special special
	fn      *funcval // May be a heap pointer.
	nret    uintptr
	fint    *_type   // May be a heap pointer, but always live.
	ot      *ptrtype // May be a heap pointer, but always live.
}
```

The described object has a finalizer set for it. 

specialfinalizer is allocated from non-GC'd memory, so any heap pointers must be specially handled. 

### <a id="specialprofile" href="#specialprofile">type specialprofile struct</a>

```
searchKey: runtime.specialprofile
```

```Go
type specialprofile struct {
	special special
	b       *bucket
}
```

The described object is being heap profiled. 

### <a id="specialReachable" href="#specialReachable">type specialReachable struct</a>

```
searchKey: runtime.specialReachable
```

```Go
type specialReachable struct {
	special   special
	done      bool
	reachable bool
}
```

specialReachable tracks whether an object is reachable on the next GC cycle. This is used by testing. 

### <a id="specialsIter" href="#specialsIter">type specialsIter struct</a>

```
searchKey: runtime.specialsIter
```

```Go
type specialsIter struct {
	pprev **special
	s     *special
}
```

specialsIter helps iterate over specials lists. 

#### <a id="newSpecialsIter" href="#newSpecialsIter">func newSpecialsIter(span *mspan) specialsIter</a>

```
searchKey: runtime.newSpecialsIter
```

```Go
func newSpecialsIter(span *mspan) specialsIter
```

#### <a id="specialsIter.valid" href="#specialsIter.valid">func (i *specialsIter) valid() bool</a>

```
searchKey: runtime.specialsIter.valid
```

```Go
func (i *specialsIter) valid() bool
```

#### <a id="specialsIter.next" href="#specialsIter.next">func (i *specialsIter) next()</a>

```
searchKey: runtime.specialsIter.next
```

```Go
func (i *specialsIter) next()
```

#### <a id="specialsIter.unlinkAndNext" href="#specialsIter.unlinkAndNext">func (i *specialsIter) unlinkAndNext() *special</a>

```
searchKey: runtime.specialsIter.unlinkAndNext
```

```Go
func (i *specialsIter) unlinkAndNext() *special
```

unlinkAndNext removes the current special from the list and moves the iterator to the next special. It returns the unlinked special. 

### <a id="gcBits" href="#gcBits">type gcBits uint8</a>

```
searchKey: runtime.gcBits
```

```Go
type gcBits uint8
```

gcBits is an alloc/mark bitmap. This is always used as *gcBits. 

#### <a id="newMarkBits" href="#newMarkBits">func newMarkBits(nelems uintptr) *gcBits</a>

```
searchKey: runtime.newMarkBits
```

```Go
func newMarkBits(nelems uintptr) *gcBits
```

newMarkBits returns a pointer to 8 byte aligned bytes to be used for a span's mark bits. 

#### <a id="newAllocBits" href="#newAllocBits">func newAllocBits(nelems uintptr) *gcBits</a>

```
searchKey: runtime.newAllocBits
```

```Go
func newAllocBits(nelems uintptr) *gcBits
```

newAllocBits returns a pointer to 8 byte aligned bytes to be used for this span's alloc bits. newAllocBits is used to provide newly initialized spans allocation bits. For spans not being initialized the mark bits are repurposed as allocation bits when the span is swept. 

#### <a id="gcBits.bytep" href="#gcBits.bytep">func (b *gcBits) bytep(n uintptr) *uint8</a>

```
searchKey: runtime.gcBits.bytep
```

```Go
func (b *gcBits) bytep(n uintptr) *uint8
```

bytep returns a pointer to the n'th byte of b. 

#### <a id="gcBits.bitp" href="#gcBits.bitp">func (b *gcBits) bitp(n uintptr) (bytep *uint8, mask uint8)</a>

```
searchKey: runtime.gcBits.bitp
```

```Go
func (b *gcBits) bitp(n uintptr) (bytep *uint8, mask uint8)
```

bitp returns a pointer to the byte containing bit n and a mask for selecting that bit from *bytep. 

### <a id="gcBitsHeader" href="#gcBitsHeader">type gcBitsHeader struct</a>

```
searchKey: runtime.gcBitsHeader
```

```Go
type gcBitsHeader struct {
	free uintptr // free is the index into bits of the next free byte.
	next uintptr // *gcBits triggers recursive type bug. (issue 14620)
}
```

### <a id="gcBitsArena" href="#gcBitsArena">type gcBitsArena struct</a>

```
searchKey: runtime.gcBitsArena
```

```Go
type gcBitsArena struct {
	// gcBitsHeader // side step recursive type bug (issue 14620) by including fields by hand.
	free uintptr // free is the index into bits of the next free byte; read/write atomically
	next *gcBitsArena
	bits [gcBitsChunkBytes - gcBitsHeaderBytes]gcBits
}
```

#### <a id="newArenaMayUnlock" href="#newArenaMayUnlock">func newArenaMayUnlock() *gcBitsArena</a>

```
searchKey: runtime.newArenaMayUnlock
```

```Go
func newArenaMayUnlock() *gcBitsArena
```

newArenaMayUnlock allocates and zeroes a gcBits arena. The caller must hold gcBitsArena.lock. This may temporarily release it. 

#### <a id="gcBitsArena.tryAlloc" href="#gcBitsArena.tryAlloc">func (b *gcBitsArena) tryAlloc(bytes uintptr) *gcBits</a>

```
searchKey: runtime.gcBitsArena.tryAlloc
```

```Go
func (b *gcBitsArena) tryAlloc(bytes uintptr) *gcBits
```

tryAlloc allocates from b or returns nil if b does not have enough room. This is safe to call concurrently. 

### <a id="chunkIdx" href="#chunkIdx">type chunkIdx uint</a>

```
searchKey: runtime.chunkIdx
```

```Go
type chunkIdx uint
```

Global chunk index. 

Represents an index into the leaf level of the radix tree. Similar to arenaIndex, except instead of arenas, it divides the address space into chunks. 

#### <a id="chunkIndex" href="#chunkIndex">func chunkIndex(p uintptr) chunkIdx</a>

```
searchKey: runtime.chunkIndex
```

```Go
func chunkIndex(p uintptr) chunkIdx
```

chunkIndex returns the global index of the palloc chunk containing the pointer p. 

#### <a id="chunkIdx.l1" href="#chunkIdx.l1">func (i chunkIdx) l1() uint</a>

```
searchKey: runtime.chunkIdx.l1
```

```Go
func (i chunkIdx) l1() uint
```

l1 returns the index into the first level of (*pageAlloc).chunks. 

#### <a id="chunkIdx.l2" href="#chunkIdx.l2">func (i chunkIdx) l2() uint</a>

```
searchKey: runtime.chunkIdx.l2
```

```Go
func (i chunkIdx) l2() uint
```

l2 returns the index into the second level of (*pageAlloc).chunks. 

### <a id="pageAlloc" href="#pageAlloc">type pageAlloc struct</a>

```
searchKey: runtime.pageAlloc
```

```Go
type pageAlloc struct {
	// Radix tree of summaries.
	//
	// Each slice's cap represents the whole memory reservation.
	// Each slice's len reflects the allocator's maximum known
	// mapped heap address for that level.
	//
	// The backing store of each summary level is reserved in init
	// and may or may not be committed in grow (small address spaces
	// may commit all the memory in init).
	//
	// The purpose of keeping len <= cap is to enforce bounds checks
	// on the top end of the slice so that instead of an unknown
	// runtime segmentation fault, we get a much friendlier out-of-bounds
	// error.
	//
	// To iterate over a summary level, use inUse to determine which ranges
	// are currently available. Otherwise one might try to access
	// memory which is only Reserved which may result in a hard fault.
	//
	// We may still get segmentation faults < len since some of that
	// memory may not be committed yet.
	summary [summaryLevels][]pallocSum

	// chunks is a slice of bitmap chunks.
	//
	// The total size of chunks is quite large on most 64-bit platforms
	// (O(GiB) or more) if flattened, so rather than making one large mapping
	// (which has problems on some platforms, even when PROT_NONE) we use a
	// two-level sparse array approach similar to the arena index in mheap.
	//
	// To find the chunk containing a memory address `a`, do:
	//   chunkOf(chunkIndex(a))
	//
	// Below is a table describing the configuration for chunks for various
	// heapAddrBits supported by the runtime.
	//
	// heapAddrBits | L1 Bits | L2 Bits | L2 Entry Size
	// ------------------------------------------------
	// 32           | 0       | 10      | 128 KiB
	// 33 (iOS)     | 0       | 11      | 256 KiB
	// 48           | 13      | 13      | 1 MiB
	//
	// There's no reason to use the L1 part of chunks on 32-bit, the
	// address space is small so the L2 is small. For platforms with a
	// 48-bit address space, we pick the L1 such that the L2 is 1 MiB
	// in size, which is a good balance between low granularity without
	// making the impact on BSS too high (note the L1 is stored directly
	// in pageAlloc).
	//
	// To iterate over the bitmap, use inUse to determine which ranges
	// are currently available. Otherwise one might iterate over unused
	// ranges.
	//
	// TODO(mknyszek): Consider changing the definition of the bitmap
	// such that 1 means free and 0 means in-use so that summaries and
	// the bitmaps align better on zero-values.
	chunks [1 << pallocChunksL1Bits]*[1 << pallocChunksL2Bits]pallocData

	// The address to start an allocation search with. It must never
	// point to any memory that is not contained in inUse, i.e.
	// inUse.contains(searchAddr.addr()) must always be true. The one
	// exception to this rule is that it may take on the value of
	// maxOffAddr to indicate that the heap is exhausted.
	//
	// We guarantee that all valid heap addresses below this value
	// are allocated and not worth searching.
	searchAddr offAddr

	// start and end represent the chunk indices
	// which pageAlloc knows about. It assumes
	// chunks in the range [start, end) are
	// currently ready to use.
	start, end chunkIdx

	// inUse is a slice of ranges of address space which are
	// known by the page allocator to be currently in-use (passed
	// to grow).
	//
	// This field is currently unused on 32-bit architectures but
	// is harmless to track. We care much more about having a
	// contiguous heap in these cases and take additional measures
	// to ensure that, so in nearly all cases this should have just
	// 1 element.
	//
	// All access is protected by the mheapLock.
	inUse addrRanges

	// scav stores the scavenger state.
	//
	// All fields are protected by mheapLock.
	scav struct {
		// inUse is a slice of ranges of address space which have not
		// yet been looked at by the scavenger.
		inUse addrRanges

		// gen is the scavenge generation number.
		gen uint32

		// reservationBytes is how large of a reservation should be made
		// in bytes of address space for each scavenge iteration.
		reservationBytes uintptr

		// released is the amount of memory released this generation.
		released uintptr

		// scavLWM is the lowest (offset) address that the scavenger reached this
		// scavenge generation.
		scavLWM offAddr

		// freeHWM is the highest (offset) address of a page that was freed to
		// the page allocator this scavenge generation.
		freeHWM offAddr
	}

	// mheap_.lock. This level of indirection makes it possible
	// to test pageAlloc indepedently of the runtime allocator.
	mheapLock *mutex

	// sysStat is the runtime memstat to update when new system
	// memory is committed by the pageAlloc for allocation metadata.
	sysStat *sysMemStat

	// Whether or not this struct is being used in tests.
	test bool
}
```

#### <a id="pageAlloc.scavenge" href="#pageAlloc.scavenge">func (p *pageAlloc) scavenge(nbytes uintptr, mayUnlock bool) uintptr</a>

```
searchKey: runtime.pageAlloc.scavenge
```

```Go
func (p *pageAlloc) scavenge(nbytes uintptr, mayUnlock bool) uintptr
```

scavenge scavenges nbytes worth of free pages, starting with the highest address first. Successive calls continue from where it left off until the heap is exhausted. Call scavengeStartGen to bring it back to the top of the heap. 

Returns the amount of memory scavenged in bytes. 

p.mheapLock must be held, but may be temporarily released if mayUnlock == true. 

Must run on the system stack because p.mheapLock must be held. 

#### <a id="pageAlloc.scavengeStartGen" href="#pageAlloc.scavengeStartGen">func (p *pageAlloc) scavengeStartGen()</a>

```
searchKey: runtime.pageAlloc.scavengeStartGen
```

```Go
func (p *pageAlloc) scavengeStartGen()
```

scavengeStartGen starts a new scavenge generation, resetting the scavenger's search space to the full in-use address space. 

p.mheapLock must be held. 

Must run on the system stack because p.mheapLock must be held. 

#### <a id="pageAlloc.scavengeReserve" href="#pageAlloc.scavengeReserve">func (p *pageAlloc) scavengeReserve() (addrRange, uint32)</a>

```
searchKey: runtime.pageAlloc.scavengeReserve
```

```Go
func (p *pageAlloc) scavengeReserve() (addrRange, uint32)
```

scavengeReserve reserves a contiguous range of the address space for scavenging. The maximum amount of space it reserves is proportional to the size of the heap. The ranges are reserved from the high addresses first. 

Returns the reserved range and the scavenge generation number for it. 

p.mheapLock must be held. 

Must run on the system stack because p.mheapLock must be held. 

#### <a id="pageAlloc.scavengeUnreserve" href="#pageAlloc.scavengeUnreserve">func (p *pageAlloc) scavengeUnreserve(r addrRange, gen uint32)</a>

```
searchKey: runtime.pageAlloc.scavengeUnreserve
```

```Go
func (p *pageAlloc) scavengeUnreserve(r addrRange, gen uint32)
```

scavengeUnreserve returns an unscavenged portion of a range that was previously reserved with scavengeReserve. 

p.mheapLock must be held. 

Must run on the system stack because p.mheapLock must be held. 

#### <a id="pageAlloc.scavengeOne" href="#pageAlloc.scavengeOne">func (p *pageAlloc) scavengeOne(work addrRange, max uintptr, mayUnlock bool) (uintptr, addrRange)</a>

```
searchKey: runtime.pageAlloc.scavengeOne
```

```Go
func (p *pageAlloc) scavengeOne(work addrRange, max uintptr, mayUnlock bool) (uintptr, addrRange)
```

scavengeOne walks over address range work until it finds a contiguous run of pages to scavenge. It will try to scavenge at most max bytes at once, but may scavenge more to avoid breaking huge pages. Once it scavenges some memory it returns how much it scavenged in bytes. 

Returns the number of bytes scavenged and the part of work which was not yet searched. 

work's base address must be aligned to pallocChunkBytes. 

p.mheapLock must be held, but may be temporarily released if mayUnlock == true. 

Must run on the system stack because p.mheapLock must be held. 

#### <a id="pageAlloc.scavengeRangeLocked" href="#pageAlloc.scavengeRangeLocked">func (p *pageAlloc) scavengeRangeLocked(ci chunkIdx, base, npages uint) uintptr</a>

```
searchKey: runtime.pageAlloc.scavengeRangeLocked
```

```Go
func (p *pageAlloc) scavengeRangeLocked(ci chunkIdx, base, npages uint) uintptr
```

scavengeRangeLocked scavenges the given region of memory. The region of memory is described by its chunk index (ci), the starting page index of the region relative to that chunk (base), and the length of the region in pages (npages). 

Returns the base address of the scavenged region. 

p.mheapLock must be held. 

#### <a id="pageAlloc.init" href="#pageAlloc.init">func (p *pageAlloc) init(mheapLock *mutex, sysStat *sysMemStat)</a>

```
searchKey: runtime.pageAlloc.init
```

```Go
func (p *pageAlloc) init(mheapLock *mutex, sysStat *sysMemStat)
```

#### <a id="pageAlloc.tryChunkOf" href="#pageAlloc.tryChunkOf">func (p *pageAlloc) tryChunkOf(ci chunkIdx) *pallocData</a>

```
searchKey: runtime.pageAlloc.tryChunkOf
```

```Go
func (p *pageAlloc) tryChunkOf(ci chunkIdx) *pallocData
```

tryChunkOf returns the bitmap data for the given chunk. 

Returns nil if the chunk data has not been mapped. 

#### <a id="pageAlloc.chunkOf" href="#pageAlloc.chunkOf">func (p *pageAlloc) chunkOf(ci chunkIdx) *pallocData</a>

```
searchKey: runtime.pageAlloc.chunkOf
```

```Go
func (p *pageAlloc) chunkOf(ci chunkIdx) *pallocData
```

chunkOf returns the chunk at the given chunk index. 

The chunk index must be valid or this method may throw. 

#### <a id="pageAlloc.grow" href="#pageAlloc.grow">func (p *pageAlloc) grow(base, size uintptr)</a>

```
searchKey: runtime.pageAlloc.grow
```

```Go
func (p *pageAlloc) grow(base, size uintptr)
```

grow sets up the metadata for the address range [base, base+size). It may allocate metadata, in which case *p.sysStat will be updated. 

p.mheapLock must be held. 

#### <a id="pageAlloc.update" href="#pageAlloc.update">func (p *pageAlloc) update(base, npages uintptr, contig, alloc bool)</a>

```
searchKey: runtime.pageAlloc.update
```

```Go
func (p *pageAlloc) update(base, npages uintptr, contig, alloc bool)
```

update updates heap metadata. It must be called each time the bitmap is updated. 

If contig is true, update does some optimizations assuming that there was a contiguous allocation or free between addr and addr+npages. alloc indicates whether the operation performed was an allocation or a free. 

p.mheapLock must be held. 

#### <a id="pageAlloc.allocRange" href="#pageAlloc.allocRange">func (p *pageAlloc) allocRange(base, npages uintptr) uintptr</a>

```
searchKey: runtime.pageAlloc.allocRange
```

```Go
func (p *pageAlloc) allocRange(base, npages uintptr) uintptr
```

allocRange marks the range of memory [base, base+npages*pageSize) as allocated. It also updates the summaries to reflect the newly-updated bitmap. 

Returns the amount of scavenged memory in bytes present in the allocated range. 

p.mheapLock must be held. 

#### <a id="pageAlloc.findMappedAddr" href="#pageAlloc.findMappedAddr">func (p *pageAlloc) findMappedAddr(addr offAddr) offAddr</a>

```
searchKey: runtime.pageAlloc.findMappedAddr
```

```Go
func (p *pageAlloc) findMappedAddr(addr offAddr) offAddr
```

findMappedAddr returns the smallest mapped offAddr that is >= addr. That is, if addr refers to mapped memory, then it is returned. If addr is higher than any mapped region, then it returns maxOffAddr. 

p.mheapLock must be held. 

#### <a id="pageAlloc.find" href="#pageAlloc.find">func (p *pageAlloc) find(npages uintptr) (uintptr, offAddr)</a>

```
searchKey: runtime.pageAlloc.find
```

```Go
func (p *pageAlloc) find(npages uintptr) (uintptr, offAddr)
```

find searches for the first (address-ordered) contiguous free region of npages in size and returns a base address for that region. 

It uses p.searchAddr to prune its search and assumes that no palloc chunks below chunkIndex(p.searchAddr) contain any free memory at all. 

find also computes and returns a candidate p.searchAddr, which may or may not prune more of the address space than p.searchAddr already does. This candidate is always a valid p.searchAddr. 

find represents the slow path and the full radix tree search. 

Returns a base address of 0 on failure, in which case the candidate searchAddr returned is invalid and must be ignored. 

p.mheapLock must be held. 

#### <a id="pageAlloc.alloc" href="#pageAlloc.alloc">func (p *pageAlloc) alloc(npages uintptr) (addr uintptr, scav uintptr)</a>

```
searchKey: runtime.pageAlloc.alloc
```

```Go
func (p *pageAlloc) alloc(npages uintptr) (addr uintptr, scav uintptr)
```

alloc allocates npages worth of memory from the page heap, returning the base address for the allocation and the amount of scavenged memory in bytes contained in the region [base address, base address + npages*pageSize). 

Returns a 0 base address on failure, in which case other returned values should be ignored. 

p.mheapLock must be held. 

Must run on the system stack because p.mheapLock must be held. 

#### <a id="pageAlloc.free" href="#pageAlloc.free">func (p *pageAlloc) free(base, npages uintptr)</a>

```
searchKey: runtime.pageAlloc.free
```

```Go
func (p *pageAlloc) free(base, npages uintptr)
```

free returns npages worth of memory starting at base back to the page heap. 

p.mheapLock must be held. 

Must run on the system stack because p.mheapLock must be held. 

#### <a id="pageAlloc.sysInit" href="#pageAlloc.sysInit">func (p *pageAlloc) sysInit()</a>

```
searchKey: runtime.pageAlloc.sysInit
```

```Go
func (p *pageAlloc) sysInit()
```

sysInit performs architecture-dependent initialization of fields in pageAlloc. pageAlloc should be uninitialized except for sysStat if any runtime statistic should be updated. 

#### <a id="pageAlloc.sysGrow" href="#pageAlloc.sysGrow">func (p *pageAlloc) sysGrow(base, limit uintptr)</a>

```
searchKey: runtime.pageAlloc.sysGrow
```

```Go
func (p *pageAlloc) sysGrow(base, limit uintptr)
```

sysGrow performs architecture-dependent operations on heap growth for the page allocator, such as mapping in new memory for summaries. It also updates the length of the slices in [.summary. 

base is the base of the newly-added heap memory and limit is the first address past the end of the newly-added heap memory. Both must be aligned to pallocChunkBytes. 

The caller must update p.start and p.end after calling sysGrow. 

#### <a id="pageAlloc.allocToCache" href="#pageAlloc.allocToCache">func (p *pageAlloc) allocToCache() pageCache</a>

```
searchKey: runtime.pageAlloc.allocToCache
```

```Go
func (p *pageAlloc) allocToCache() pageCache
```

allocToCache acquires a pageCachePages-aligned chunk of free pages which may not be contiguous, and returns a pageCache structure which owns the chunk. 

p.mheapLock must be held. 

Must run on the system stack because p.mheapLock must be held. 

### <a id="pallocSum" href="#pallocSum">type pallocSum uint64</a>

```
searchKey: runtime.pallocSum
```

```Go
type pallocSum uint64
```

pallocSum is a packed summary type which packs three numbers: start, max, and end into a single 8-byte value. Each of these values are a summary of a bitmap and are thus counts, each of which may have a maximum value of 2^21 - 1, or all three may be equal to 2^21. The latter case is represented by just setting the 64th bit. 

#### <a id="packPallocSum" href="#packPallocSum">func packPallocSum(start, max, end uint) pallocSum</a>

```
searchKey: runtime.packPallocSum
```

```Go
func packPallocSum(start, max, end uint) pallocSum
```

packPallocSum takes a start, max, and end value and produces a pallocSum. 

#### <a id="mergeSummaries" href="#mergeSummaries">func mergeSummaries(sums []pallocSum, logMaxPagesPerSum uint) pallocSum</a>

```
searchKey: runtime.mergeSummaries
```

```Go
func mergeSummaries(sums []pallocSum, logMaxPagesPerSum uint) pallocSum
```

mergeSummaries merges consecutive summaries which may each represent at most 1 << logMaxPagesPerSum pages each together into one. 

#### <a id="pallocSum.start" href="#pallocSum.start">func (p pallocSum) start() uint</a>

```
searchKey: runtime.pallocSum.start
```

```Go
func (p pallocSum) start() uint
```

start extracts the start value from a packed sum. 

#### <a id="pallocSum.max" href="#pallocSum.max">func (p pallocSum) max() uint</a>

```
searchKey: runtime.pallocSum.max
```

```Go
func (p pallocSum) max() uint
```

max extracts the max value from a packed sum. 

#### <a id="pallocSum.end" href="#pallocSum.end">func (p pallocSum) end() uint</a>

```
searchKey: runtime.pallocSum.end
```

```Go
func (p pallocSum) end() uint
```

end extracts the end value from a packed sum. 

#### <a id="pallocSum.unpack" href="#pallocSum.unpack">func (p pallocSum) unpack() (uint, uint, uint)</a>

```
searchKey: runtime.pallocSum.unpack
```

```Go
func (p pallocSum) unpack() (uint, uint, uint)
```

unpack unpacks all three values from the summary. 

### <a id="pageCache" href="#pageCache">type pageCache struct</a>

```
searchKey: runtime.pageCache
```

```Go
type pageCache struct {
	base  uintptr // base address of the chunk
	cache uint64  // 64-bit bitmap representing free pages (1 means free)
	scav  uint64  // 64-bit bitmap representing scavenged pages (1 means scavenged)
}
```

pageCache represents a per-p cache of pages the allocator can allocate from without a lock. More specifically, it represents a pageCachePages*pageSize chunk of memory with 0 or more free pages in it. 

#### <a id="pageCache.empty" href="#pageCache.empty">func (c *pageCache) empty() bool</a>

```
searchKey: runtime.pageCache.empty
```

```Go
func (c *pageCache) empty() bool
```

empty returns true if the pageCache has any free pages, and false otherwise. 

#### <a id="pageCache.alloc" href="#pageCache.alloc">func (c *pageCache) alloc(npages uintptr) (uintptr, uintptr)</a>

```
searchKey: runtime.pageCache.alloc
```

```Go
func (c *pageCache) alloc(npages uintptr) (uintptr, uintptr)
```

alloc allocates npages from the page cache and is the main entry point for allocation. 

Returns a base address and the amount of scavenged memory in the allocated region in bytes. 

Returns a base address of zero on failure, in which case the amount of scavenged memory should be ignored. 

#### <a id="pageCache.allocN" href="#pageCache.allocN">func (c *pageCache) allocN(npages uintptr) (uintptr, uintptr)</a>

```
searchKey: runtime.pageCache.allocN
```

```Go
func (c *pageCache) allocN(npages uintptr) (uintptr, uintptr)
```

allocN is a helper which attempts to allocate npages worth of pages from the cache. It represents the general case for allocating from the page cache. 

Returns a base address and the amount of scavenged memory in the allocated region in bytes. 

#### <a id="pageCache.flush" href="#pageCache.flush">func (c *pageCache) flush(p *pageAlloc)</a>

```
searchKey: runtime.pageCache.flush
```

```Go
func (c *pageCache) flush(p *pageAlloc)
```

flush empties out unallocated free pages in the given cache into s. Then, it clears the cache, such that empty returns true. 

p.mheapLock must be held. 

Must run on the system stack because p.mheapLock must be held. 

### <a id="pageBits" href="#pageBits">type pageBits [8]uint64</a>

```
searchKey: runtime.pageBits
```

```Go
type pageBits [pallocChunkPages / 64]uint64
```

pageBits is a bitmap representing one bit per page in a palloc chunk. 

#### <a id="pageBits.get" href="#pageBits.get">func (b *pageBits) get(i uint) uint</a>

```
searchKey: runtime.pageBits.get
```

```Go
func (b *pageBits) get(i uint) uint
```

get returns the value of the i'th bit in the bitmap. 

#### <a id="pageBits.block64" href="#pageBits.block64">func (b *pageBits) block64(i uint) uint64</a>

```
searchKey: runtime.pageBits.block64
```

```Go
func (b *pageBits) block64(i uint) uint64
```

block64 returns the 64-bit aligned block of bits containing the i'th bit. 

#### <a id="pageBits.set" href="#pageBits.set">func (b *pageBits) set(i uint)</a>

```
searchKey: runtime.pageBits.set
```

```Go
func (b *pageBits) set(i uint)
```

set sets bit i of pageBits. 

#### <a id="pageBits.setRange" href="#pageBits.setRange">func (b *pageBits) setRange(i, n uint)</a>

```
searchKey: runtime.pageBits.setRange
```

```Go
func (b *pageBits) setRange(i, n uint)
```

setRange sets bits in the range [i, i+n). 

#### <a id="pageBits.setAll" href="#pageBits.setAll">func (b *pageBits) setAll()</a>

```
searchKey: runtime.pageBits.setAll
```

```Go
func (b *pageBits) setAll()
```

setAll sets all the bits of b. 

#### <a id="pageBits.clear" href="#pageBits.clear">func (b *pageBits) clear(i uint)</a>

```
searchKey: runtime.pageBits.clear
```

```Go
func (b *pageBits) clear(i uint)
```

clear clears bit i of pageBits. 

#### <a id="pageBits.clearRange" href="#pageBits.clearRange">func (b *pageBits) clearRange(i, n uint)</a>

```
searchKey: runtime.pageBits.clearRange
```

```Go
func (b *pageBits) clearRange(i, n uint)
```

clearRange clears bits in the range [i, i+n). 

#### <a id="pageBits.clearAll" href="#pageBits.clearAll">func (b *pageBits) clearAll()</a>

```
searchKey: runtime.pageBits.clearAll
```

```Go
func (b *pageBits) clearAll()
```

clearAll frees all the bits of b. 

#### <a id="pageBits.popcntRange" href="#pageBits.popcntRange">func (b *pageBits) popcntRange(i, n uint) (s uint)</a>

```
searchKey: runtime.pageBits.popcntRange
```

```Go
func (b *pageBits) popcntRange(i, n uint) (s uint)
```

popcntRange counts the number of set bits in the range [i, i+n). 

### <a id="pallocBits" href="#pallocBits">type pallocBits runtime.pageBits</a>

```
searchKey: runtime.pallocBits
```

```Go
type pallocBits pageBits
```

pallocBits is a bitmap that tracks page allocations for at most one palloc chunk. 

The precise representation is an implementation detail, but for the sake of documentation, 0s are free pages and 1s are allocated pages. 

#### <a id="pallocBits.summarize" href="#pallocBits.summarize">func (b *pallocBits) summarize() pallocSum</a>

```
searchKey: runtime.pallocBits.summarize
```

```Go
func (b *pallocBits) summarize() pallocSum
```

summarize returns a packed summary of the bitmap in pallocBits. 

#### <a id="pallocBits.find" href="#pallocBits.find">func (b *pallocBits) find(npages uintptr, searchIdx uint) (uint, uint)</a>

```
searchKey: runtime.pallocBits.find
```

```Go
func (b *pallocBits) find(npages uintptr, searchIdx uint) (uint, uint)
```

find searches for npages contiguous free pages in pallocBits and returns the index where that run starts, as well as the index of the first free page it found in the search. searchIdx represents the first known free page and where to begin the next search from. 

If find fails to find any free space, it returns an index of ^uint(0) and the new searchIdx should be ignored. 

Note that if npages == 1, the two returned values will always be identical. 

#### <a id="pallocBits.find1" href="#pallocBits.find1">func (b *pallocBits) find1(searchIdx uint) uint</a>

```
searchKey: runtime.pallocBits.find1
```

```Go
func (b *pallocBits) find1(searchIdx uint) uint
```

find1 is a helper for find which searches for a single free page in the pallocBits and returns the index. 

See find for an explanation of the searchIdx parameter. 

#### <a id="pallocBits.findSmallN" href="#pallocBits.findSmallN">func (b *pallocBits) findSmallN(npages uintptr, searchIdx uint) (uint, uint)</a>

```
searchKey: runtime.pallocBits.findSmallN
```

```Go
func (b *pallocBits) findSmallN(npages uintptr, searchIdx uint) (uint, uint)
```

findSmallN is a helper for find which searches for npages contiguous free pages in this pallocBits and returns the index where that run of contiguous pages starts as well as the index of the first free page it finds in its search. 

See find for an explanation of the searchIdx parameter. 

Returns a ^uint(0) index on failure and the new searchIdx should be ignored. 

findSmallN assumes npages <= 64, where any such contiguous run of pages crosses at most one aligned 64-bit boundary in the bits. 

#### <a id="pallocBits.findLargeN" href="#pallocBits.findLargeN">func (b *pallocBits) findLargeN(npages uintptr, searchIdx uint) (uint, uint)</a>

```
searchKey: runtime.pallocBits.findLargeN
```

```Go
func (b *pallocBits) findLargeN(npages uintptr, searchIdx uint) (uint, uint)
```

findLargeN is a helper for find which searches for npages contiguous free pages in this pallocBits and returns the index where that run starts, as well as the index of the first free page it found it its search. 

See alloc for an explanation of the searchIdx parameter. 

Returns a ^uint(0) index on failure and the new searchIdx should be ignored. 

findLargeN assumes npages > 64, where any such run of free pages crosses at least one aligned 64-bit boundary in the bits. 

#### <a id="pallocBits.allocRange" href="#pallocBits.allocRange">func (b *pallocBits) allocRange(i, n uint)</a>

```
searchKey: runtime.pallocBits.allocRange
```

```Go
func (b *pallocBits) allocRange(i, n uint)
```

allocRange allocates the range [i, i+n). 

#### <a id="pallocBits.allocAll" href="#pallocBits.allocAll">func (b *pallocBits) allocAll()</a>

```
searchKey: runtime.pallocBits.allocAll
```

```Go
func (b *pallocBits) allocAll()
```

allocAll allocates all the bits of b. 

#### <a id="pallocBits.free1" href="#pallocBits.free1">func (b *pallocBits) free1(i uint)</a>

```
searchKey: runtime.pallocBits.free1
```

```Go
func (b *pallocBits) free1(i uint)
```

free1 frees a single page in the pallocBits at i. 

#### <a id="pallocBits.free" href="#pallocBits.free">func (b *pallocBits) free(i, n uint)</a>

```
searchKey: runtime.pallocBits.free
```

```Go
func (b *pallocBits) free(i, n uint)
```

free frees the range [i, i+n) of pages in the pallocBits. 

#### <a id="pallocBits.freeAll" href="#pallocBits.freeAll">func (b *pallocBits) freeAll()</a>

```
searchKey: runtime.pallocBits.freeAll
```

```Go
func (b *pallocBits) freeAll()
```

freeAll frees all the bits of b. 

#### <a id="pallocBits.pages64" href="#pallocBits.pages64">func (b *pallocBits) pages64(i uint) uint64</a>

```
searchKey: runtime.pallocBits.pages64
```

```Go
func (b *pallocBits) pages64(i uint) uint64
```

pages64 returns a 64-bit bitmap representing a block of 64 pages aligned to 64 pages. The returned block of pages is the one containing the i'th page in this pallocBits. Each bit represents whether the page is in-use. 

### <a id="pallocData" href="#pallocData">type pallocData struct</a>

```
searchKey: runtime.pallocData
```

```Go
type pallocData struct {
	pallocBits
	scavenged pageBits
}
```

pallocData encapsulates pallocBits and a bitmap for whether or not a given page is scavenged in a single structure. It's effectively a pallocBits with additional functionality. 

Update the comment on (*pageAlloc).chunks should this structure change. 

#### <a id="pallocData.hasScavengeCandidate" href="#pallocData.hasScavengeCandidate">func (m *pallocData) hasScavengeCandidate(min uintptr) bool</a>

```
searchKey: runtime.pallocData.hasScavengeCandidate
```

```Go
func (m *pallocData) hasScavengeCandidate(min uintptr) bool
```

hasScavengeCandidate returns true if there's any min-page-aligned groups of min pages of free-and-unscavenged memory in the region represented by this pallocData. 

min must be a non-zero power of 2 <= maxPagesPerPhysPage. 

#### <a id="pallocData.findScavengeCandidate" href="#pallocData.findScavengeCandidate">func (m *pallocData) findScavengeCandidate(searchIdx uint, min, max uintptr) (uint, uint)</a>

```
searchKey: runtime.pallocData.findScavengeCandidate
```

```Go
func (m *pallocData) findScavengeCandidate(searchIdx uint, min, max uintptr) (uint, uint)
```

findScavengeCandidate returns a start index and a size for this pallocData segment which represents a contiguous region of free and unscavenged memory. 

searchIdx indicates the page index within this chunk to start the search, but note that findScavengeCandidate searches backwards through the pallocData. As a a result, it will return the highest scavenge candidate in address order. 

min indicates a hard minimum size and alignment for runs of pages. That is, findScavengeCandidate will not return a region smaller than min pages in size, or that is min pages or greater in size but not aligned to min. min must be a non-zero power of 2 <= maxPagesPerPhysPage. 

max is a hint for how big of a region is desired. If max >= pallocChunkPages, then findScavengeCandidate effectively returns entire free and unscavenged regions. If max < pallocChunkPages, it may truncate the returned region such that size is max. However, findScavengeCandidate may still return a larger region if, for example, it chooses to preserve huge pages, or if max is not aligned to min (it will round up). That is, even if max is small, the returned size is not guaranteed to be equal to max. max is allowed to be less than min, in which case it is as if max == min. 

#### <a id="pallocData.allocRange" href="#pallocData.allocRange">func (m *pallocData) allocRange(i, n uint)</a>

```
searchKey: runtime.pallocData.allocRange
```

```Go
func (m *pallocData) allocRange(i, n uint)
```

allocRange sets bits [i, i+n) in the bitmap to 1 and updates the scavenged bits appropriately. 

#### <a id="pallocData.allocAll" href="#pallocData.allocAll">func (m *pallocData) allocAll()</a>

```
searchKey: runtime.pallocData.allocAll
```

```Go
func (m *pallocData) allocAll()
```

allocAll sets every bit in the bitmap to 1 and updates the scavenged bits appropriately. 

### <a id="bucketType" href="#bucketType">type bucketType int</a>

```
searchKey: runtime.bucketType
```

```Go
type bucketType int
```

### <a id="bucket" href="#bucket">type bucket struct</a>

```
searchKey: runtime.bucket
```

```Go
type bucket struct {
	next    *bucket
	allnext *bucket
	typ     bucketType // memBucket or blockBucket (includes mutexProfile)
	hash    uintptr
	size    uintptr
	nstk    uintptr
}
```

A bucket holds per-call-stack profiling information. The representation is a bit sleazy, inherited from C. This struct defines the bucket header. It is followed in memory by the stack words and then the actual record data, either a memRecord or a blockRecord. 

Per-call-stack profiling information. Lookup by hashing call stack into a linked-list hash table. 

No heap pointers. 

#### <a id="newBucket" href="#newBucket">func newBucket(typ bucketType, nstk int) *bucket</a>

```
searchKey: runtime.newBucket
```

```Go
func newBucket(typ bucketType, nstk int) *bucket
```

newBucket allocates a bucket with the given type and number of stack entries. 

#### <a id="stkbucket" href="#stkbucket">func stkbucket(typ bucketType, size uintptr, stk []uintptr, alloc bool) *bucket</a>

```
searchKey: runtime.stkbucket
```

```Go
func stkbucket(typ bucketType, size uintptr, stk []uintptr, alloc bool) *bucket
```

Return the bucket for stk[0:nstk], allocating new bucket if needed. 

#### <a id="bucket.stk" href="#bucket.stk">func (b *bucket) stk() []uintptr</a>

```
searchKey: runtime.bucket.stk
```

```Go
func (b *bucket) stk() []uintptr
```

stk returns the slice in b holding the stack. 

#### <a id="bucket.mp" href="#bucket.mp">func (b *bucket) mp() *memRecord</a>

```
searchKey: runtime.bucket.mp
```

```Go
func (b *bucket) mp() *memRecord
```

mp returns the memRecord associated with the memProfile bucket b. 

#### <a id="bucket.bp" href="#bucket.bp">func (b *bucket) bp() *blockRecord</a>

```
searchKey: runtime.bucket.bp
```

```Go
func (b *bucket) bp() *blockRecord
```

bp returns the blockRecord associated with the blockProfile bucket b. 

### <a id="memRecord" href="#memRecord">type memRecord struct</a>

```
searchKey: runtime.memRecord
```

```Go
type memRecord struct {

	// active is the currently published profile. A profiling
	// cycle can be accumulated into active once its complete.
	active memRecordCycle

	// future records the profile events we're counting for cycles
	// that have not yet been published. This is ring buffer
	// indexed by the global heap profile cycle C and stores
	// cycles C, C+1, and C+2. Unlike active, these counts are
	// only for a single cycle; they are not cumulative across
	// cycles.
	//
	// We store cycle C here because there's a window between when
	// C becomes the active cycle and when we've flushed it to
	// active.
	future [3]memRecordCycle
}
```

A memRecord is the bucket data for a bucket of type memProfile, part of the memory profile. 

### <a id="memRecordCycle" href="#memRecordCycle">type memRecordCycle struct</a>

```
searchKey: runtime.memRecordCycle
```

```Go
type memRecordCycle struct {
	allocs, frees           uintptr
	alloc_bytes, free_bytes uintptr
}
```

memRecordCycle 

#### <a id="memRecordCycle.add" href="#memRecordCycle.add">func (a *memRecordCycle) add(b *memRecordCycle)</a>

```
searchKey: runtime.memRecordCycle.add
```

```Go
func (a *memRecordCycle) add(b *memRecordCycle)
```

add accumulates b into a. It does not zero b. 

### <a id="blockRecord" href="#blockRecord">type blockRecord struct</a>

```
searchKey: runtime.blockRecord
```

```Go
type blockRecord struct {
	count  float64
	cycles int64
}
```

A blockRecord is the bucket data for a bucket of type blockProfile, which is used in blocking and mutex profiles. 

### <a id="StackRecord" href="#StackRecord">type StackRecord struct</a>

```
searchKey: runtime.StackRecord
tags: [exported]
```

```Go
type StackRecord struct {
	Stack0 [32]uintptr // stack trace for this record; ends at first 0 entry
}
```

A StackRecord describes a single execution stack. 

#### <a id="StackRecord.Stack" href="#StackRecord.Stack">func (r *StackRecord) Stack() []uintptr</a>

```
searchKey: runtime.StackRecord.Stack
tags: [exported]
```

```Go
func (r *StackRecord) Stack() []uintptr
```

Stack returns the stack trace associated with the record, a prefix of r.Stack0. 

### <a id="MemProfileRecord" href="#MemProfileRecord">type MemProfileRecord struct</a>

```
searchKey: runtime.MemProfileRecord
tags: [exported]
```

```Go
type MemProfileRecord struct {
	AllocBytes, FreeBytes     int64       // number of bytes allocated, freed
	AllocObjects, FreeObjects int64       // number of objects allocated, freed
	Stack0                    [32]uintptr // stack trace for this record; ends at first 0 entry
}
```

A MemProfileRecord describes the live objects allocated by a particular call sequence (stack trace). 

#### <a id="MemProfileRecord.InUseBytes" href="#MemProfileRecord.InUseBytes">func (r *MemProfileRecord) InUseBytes() int64</a>

```
searchKey: runtime.MemProfileRecord.InUseBytes
tags: [exported]
```

```Go
func (r *MemProfileRecord) InUseBytes() int64
```

InUseBytes returns the number of bytes in use (AllocBytes - FreeBytes). 

#### <a id="MemProfileRecord.InUseObjects" href="#MemProfileRecord.InUseObjects">func (r *MemProfileRecord) InUseObjects() int64</a>

```
searchKey: runtime.MemProfileRecord.InUseObjects
tags: [exported]
```

```Go
func (r *MemProfileRecord) InUseObjects() int64
```

InUseObjects returns the number of objects in use (AllocObjects - FreeObjects). 

#### <a id="MemProfileRecord.Stack" href="#MemProfileRecord.Stack">func (r *MemProfileRecord) Stack() []uintptr</a>

```
searchKey: runtime.MemProfileRecord.Stack
tags: [exported]
```

```Go
func (r *MemProfileRecord) Stack() []uintptr
```

Stack returns the stack trace associated with the record, a prefix of r.Stack0. 

### <a id="BlockProfileRecord" href="#BlockProfileRecord">type BlockProfileRecord struct</a>

```
searchKey: runtime.BlockProfileRecord
tags: [exported]
```

```Go
type BlockProfileRecord struct {
	Count  int64
	Cycles int64
	StackRecord
}
```

BlockProfileRecord describes blocking events originated at a particular call sequence (stack trace). 

### <a id="addrRange" href="#addrRange">type addrRange struct</a>

```
searchKey: runtime.addrRange
```

```Go
type addrRange struct {
	// base and limit together represent the region of address space
	// [base, limit). That is, base is inclusive, limit is exclusive.
	// These are address over an offset view of the address space on
	// platforms with a segmented address space, that is, on platforms
	// where arenaBaseOffset != 0.
	base, limit offAddr
}
```

addrRange represents a region of address space. 

An addrRange must never span a gap in the address space. 

#### <a id="makeAddrRange" href="#makeAddrRange">func makeAddrRange(base, limit uintptr) addrRange</a>

```
searchKey: runtime.makeAddrRange
```

```Go
func makeAddrRange(base, limit uintptr) addrRange
```

makeAddrRange creates a new address range from two virtual addresses. 

Throws if the base and limit are not in the same memory segment. 

#### <a id="addrRange.size" href="#addrRange.size">func (a addrRange) size() uintptr</a>

```
searchKey: runtime.addrRange.size
```

```Go
func (a addrRange) size() uintptr
```

size returns the size of the range represented in bytes. 

#### <a id="addrRange.contains" href="#addrRange.contains">func (a addrRange) contains(addr uintptr) bool</a>

```
searchKey: runtime.addrRange.contains
```

```Go
func (a addrRange) contains(addr uintptr) bool
```

contains returns whether or not the range contains a given address. 

#### <a id="addrRange.subtract" href="#addrRange.subtract">func (a addrRange) subtract(b addrRange) addrRange</a>

```
searchKey: runtime.addrRange.subtract
```

```Go
func (a addrRange) subtract(b addrRange) addrRange
```

subtract takes the addrRange toPrune and cuts out any overlap with from, then returns the new range. subtract assumes that a and b either don't overlap at all, only overlap on one side, or are equal. If b is strictly contained in a, thus forcing a split, it will throw. 

#### <a id="addrRange.removeGreaterEqual" href="#addrRange.removeGreaterEqual">func (a addrRange) removeGreaterEqual(addr uintptr) addrRange</a>

```
searchKey: runtime.addrRange.removeGreaterEqual
```

```Go
func (a addrRange) removeGreaterEqual(addr uintptr) addrRange
```

removeGreaterEqual removes all addresses in a greater than or equal to addr and returns the new range. 

### <a id="offAddr" href="#offAddr">type offAddr struct</a>

```
searchKey: runtime.offAddr
```

```Go
type offAddr struct {
	// a is just the virtual address, but should never be used
	// directly. Call addr() to get this value instead.
	a uintptr
}
```

offAddr represents an address in a contiguous view of the address space on systems where the address space is segmented. On other systems, it's just a normal address. 

#### <a id="levelIndexToOffAddr" href="#levelIndexToOffAddr">func levelIndexToOffAddr(level, idx int) offAddr</a>

```
searchKey: runtime.levelIndexToOffAddr
```

```Go
func levelIndexToOffAddr(level, idx int) offAddr
```

levelIndexToOffAddr converts an index into summary[level] into the corresponding address in the offset address space. 

#### <a id="offAddr.add" href="#offAddr.add">func (l offAddr) add(bytes uintptr) offAddr</a>

```
searchKey: runtime.offAddr.add
```

```Go
func (l offAddr) add(bytes uintptr) offAddr
```

add adds a uintptr offset to the offAddr. 

#### <a id="offAddr.sub" href="#offAddr.sub">func (l offAddr) sub(bytes uintptr) offAddr</a>

```
searchKey: runtime.offAddr.sub
```

```Go
func (l offAddr) sub(bytes uintptr) offAddr
```

sub subtracts a uintptr offset from the offAddr. 

#### <a id="offAddr.diff" href="#offAddr.diff">func (l1 offAddr) diff(l2 offAddr) uintptr</a>

```
searchKey: runtime.offAddr.diff
```

```Go
func (l1 offAddr) diff(l2 offAddr) uintptr
```

diff returns the amount of bytes in between the two offAddrs. 

#### <a id="offAddr.lessThan" href="#offAddr.lessThan">func (l1 offAddr) lessThan(l2 offAddr) bool</a>

```
searchKey: runtime.offAddr.lessThan
```

```Go
func (l1 offAddr) lessThan(l2 offAddr) bool
```

lessThan returns true if l1 is less than l2 in the offset address space. 

#### <a id="offAddr.lessEqual" href="#offAddr.lessEqual">func (l1 offAddr) lessEqual(l2 offAddr) bool</a>

```
searchKey: runtime.offAddr.lessEqual
```

```Go
func (l1 offAddr) lessEqual(l2 offAddr) bool
```

lessEqual returns true if l1 is less than or equal to l2 in the offset address space. 

#### <a id="offAddr.equal" href="#offAddr.equal">func (l1 offAddr) equal(l2 offAddr) bool</a>

```
searchKey: runtime.offAddr.equal
```

```Go
func (l1 offAddr) equal(l2 offAddr) bool
```

equal returns true if the two offAddr values are equal. 

#### <a id="offAddr.addr" href="#offAddr.addr">func (l offAddr) addr() uintptr</a>

```
searchKey: runtime.offAddr.addr
```

```Go
func (l offAddr) addr() uintptr
```

addr returns the virtual address for this offset address. 

### <a id="addrRanges" href="#addrRanges">type addrRanges struct</a>

```
searchKey: runtime.addrRanges
```

```Go
type addrRanges struct {
	// ranges is a slice of ranges sorted by base.
	ranges []addrRange

	// totalBytes is the total amount of address space in bytes counted by
	// this addrRanges.
	totalBytes uintptr

	// sysStat is the stat to track allocations by this type
	sysStat *sysMemStat
}
```

addrRanges is a data structure holding a collection of ranges of address space. 

The ranges are coalesced eagerly to reduce the number ranges it holds. 

The slice backing store for this field is persistentalloc'd and thus there is no way to free it. 

addrRanges is not thread-safe. 

#### <a id="addrRanges.init" href="#addrRanges.init">func (a *addrRanges) init(sysStat *sysMemStat)</a>

```
searchKey: runtime.addrRanges.init
```

```Go
func (a *addrRanges) init(sysStat *sysMemStat)
```

#### <a id="addrRanges.findSucc" href="#addrRanges.findSucc">func (a *addrRanges) findSucc(addr uintptr) int</a>

```
searchKey: runtime.addrRanges.findSucc
```

```Go
func (a *addrRanges) findSucc(addr uintptr) int
```

findSucc returns the first index in a such that addr is less than the base of the addrRange at that index. 

#### <a id="addrRanges.findAddrGreaterEqual" href="#addrRanges.findAddrGreaterEqual">func (a *addrRanges) findAddrGreaterEqual(addr uintptr) (uintptr, bool)</a>

```
searchKey: runtime.addrRanges.findAddrGreaterEqual
```

```Go
func (a *addrRanges) findAddrGreaterEqual(addr uintptr) (uintptr, bool)
```

findAddrGreaterEqual returns the smallest address represented by a that is >= addr. Thus, if the address is represented by a, then it returns addr. The second return value indicates whether such an address exists for addr in a. That is, if addr is larger than any address known to a, the second return value will be false. 

#### <a id="addrRanges.contains" href="#addrRanges.contains">func (a *addrRanges) contains(addr uintptr) bool</a>

```
searchKey: runtime.addrRanges.contains
```

```Go
func (a *addrRanges) contains(addr uintptr) bool
```

contains returns true if a covers the address addr. 

#### <a id="addrRanges.add" href="#addrRanges.add">func (a *addrRanges) add(r addrRange)</a>

```
searchKey: runtime.addrRanges.add
```

```Go
func (a *addrRanges) add(r addrRange)
```

add inserts a new address range to a. 

r must not overlap with any address range in a and r.size() must be > 0. 

#### <a id="addrRanges.removeLast" href="#addrRanges.removeLast">func (a *addrRanges) removeLast(nBytes uintptr) addrRange</a>

```
searchKey: runtime.addrRanges.removeLast
```

```Go
func (a *addrRanges) removeLast(nBytes uintptr) addrRange
```

removeLast removes and returns the highest-addressed contiguous range of a, or the last nBytes of that range, whichever is smaller. If a is empty, it returns an empty range. 

#### <a id="addrRanges.removeGreaterEqual" href="#addrRanges.removeGreaterEqual">func (a *addrRanges) removeGreaterEqual(addr uintptr)</a>

```
searchKey: runtime.addrRanges.removeGreaterEqual
```

```Go
func (a *addrRanges) removeGreaterEqual(addr uintptr)
```

removeGreaterEqual removes the ranges of a which are above addr, and additionally splits any range containing addr. 

#### <a id="addrRanges.cloneInto" href="#addrRanges.cloneInto">func (a *addrRanges) cloneInto(b *addrRanges)</a>

```
searchKey: runtime.addrRanges.cloneInto
```

```Go
func (a *addrRanges) cloneInto(b *addrRanges)
```

cloneInto makes a deep clone of a's state into b, re-using b's ranges if able. 

### <a id="spanSet" href="#spanSet">type spanSet struct</a>

```
searchKey: runtime.spanSet
```

```Go
type spanSet struct {
	spineLock mutex
	spine     unsafe.Pointer // *[N]*spanSetBlock, accessed atomically
	spineLen  uintptr        // Spine array length, accessed atomically
	spineCap  uintptr        // Spine array cap, accessed under lock

	// index is the head and tail of the spanSet in a single field.
	// The head and the tail both represent an index into the logical
	// concatenation of all blocks, with the head always behind or
	// equal to the tail (indicating an empty set). This field is
	// always accessed atomically.
	//
	// The head and the tail are only 32 bits wide, which means we
	// can only support up to 2^32 pushes before a reset. If every
	// span in the heap were stored in this set, and each span were
	// the minimum size (1 runtime page, 8 KiB), then roughly the
	// smallest heap which would be unrepresentable is 32 TiB in size.
	index headTailIndex
}
```

A spanSet is a set of *mspans. 

spanSet is safe for concurrent push and pop operations. 

#### <a id="spanSet.push" href="#spanSet.push">func (b *spanSet) push(s *mspan)</a>

```
searchKey: runtime.spanSet.push
```

```Go
func (b *spanSet) push(s *mspan)
```

push adds span s to buffer b. push is safe to call concurrently with other push and pop operations. 

#### <a id="spanSet.pop" href="#spanSet.pop">func (b *spanSet) pop() *mspan</a>

```
searchKey: runtime.spanSet.pop
```

```Go
func (b *spanSet) pop() *mspan
```

pop removes and returns a span from buffer b, or nil if b is empty. pop is safe to call concurrently with other pop and push operations. 

#### <a id="spanSet.reset" href="#spanSet.reset">func (b *spanSet) reset()</a>

```
searchKey: runtime.spanSet.reset
```

```Go
func (b *spanSet) reset()
```

reset resets a spanSet which is empty. It will also clean up any left over blocks. 

Throws if the buf is not empty. 

reset may not be called concurrently with any other operations on the span set. 

### <a id="spanSetBlock" href="#spanSetBlock">type spanSetBlock struct</a>

```
searchKey: runtime.spanSetBlock
```

```Go
type spanSetBlock struct {
	// Free spanSetBlocks are managed via a lock-free stack.
	lfnode

	// popped is the number of pop operations that have occurred on
	// this block. This number is used to help determine when a block
	// may be safely recycled.
	popped uint32

	// spans is the set of spans in this block.
	spans [spanSetBlockEntries]*mspan
}
```

### <a id="spanSetBlockAlloc" href="#spanSetBlockAlloc">type spanSetBlockAlloc struct</a>

```
searchKey: runtime.spanSetBlockAlloc
```

```Go
type spanSetBlockAlloc struct {
	stack lfstack
}
```

spanSetBlockAlloc represents a concurrent pool of spanSetBlocks. 

#### <a id="spanSetBlockAlloc.alloc" href="#spanSetBlockAlloc.alloc">func (p *spanSetBlockAlloc) alloc() *spanSetBlock</a>

```
searchKey: runtime.spanSetBlockAlloc.alloc
```

```Go
func (p *spanSetBlockAlloc) alloc() *spanSetBlock
```

alloc tries to grab a spanSetBlock out of the pool, and if it fails persistentallocs a new one and returns it. 

#### <a id="spanSetBlockAlloc.free" href="#spanSetBlockAlloc.free">func (p *spanSetBlockAlloc) free(block *spanSetBlock)</a>

```
searchKey: runtime.spanSetBlockAlloc.free
```

```Go
func (p *spanSetBlockAlloc) free(block *spanSetBlock)
```

free returns a spanSetBlock back to the pool. 

### <a id="headTailIndex" href="#headTailIndex">type headTailIndex uint64</a>

```
searchKey: runtime.headTailIndex
```

```Go
type headTailIndex uint64
```

haidTailIndex represents a combined 32-bit head and 32-bit tail of a queue into a single 64-bit value. 

#### <a id="makeHeadTailIndex" href="#makeHeadTailIndex">func makeHeadTailIndex(head, tail uint32) headTailIndex</a>

```
searchKey: runtime.makeHeadTailIndex
```

```Go
func makeHeadTailIndex(head, tail uint32) headTailIndex
```

makeHeadTailIndex creates a headTailIndex value from a separate head and tail. 

#### <a id="headTailIndex.head" href="#headTailIndex.head">func (h headTailIndex) head() uint32</a>

```
searchKey: runtime.headTailIndex.head
```

```Go
func (h headTailIndex) head() uint32
```

head returns the head of a headTailIndex value. 

#### <a id="headTailIndex.tail" href="#headTailIndex.tail">func (h headTailIndex) tail() uint32</a>

```
searchKey: runtime.headTailIndex.tail
```

```Go
func (h headTailIndex) tail() uint32
```

tail returns the tail of a headTailIndex value. 

#### <a id="headTailIndex.split" href="#headTailIndex.split">func (h headTailIndex) split() (head uint32, tail uint32)</a>

```
searchKey: runtime.headTailIndex.split
```

```Go
func (h headTailIndex) split() (head uint32, tail uint32)
```

split splits the headTailIndex value into its parts. 

#### <a id="headTailIndex.load" href="#headTailIndex.load">func (h *headTailIndex) load() headTailIndex</a>

```
searchKey: runtime.headTailIndex.load
```

```Go
func (h *headTailIndex) load() headTailIndex
```

load atomically reads a headTailIndex value. 

#### <a id="headTailIndex.cas" href="#headTailIndex.cas">func (h *headTailIndex) cas(old, new headTailIndex) bool</a>

```
searchKey: runtime.headTailIndex.cas
```

```Go
func (h *headTailIndex) cas(old, new headTailIndex) bool
```

cas atomically compares-and-swaps a headTailIndex value. 

#### <a id="headTailIndex.incHead" href="#headTailIndex.incHead">func (h *headTailIndex) incHead() headTailIndex</a>

```
searchKey: runtime.headTailIndex.incHead
```

```Go
func (h *headTailIndex) incHead() headTailIndex
```

incHead atomically increments the head of a headTailIndex. 

#### <a id="headTailIndex.decHead" href="#headTailIndex.decHead">func (h *headTailIndex) decHead() headTailIndex</a>

```
searchKey: runtime.headTailIndex.decHead
```

```Go
func (h *headTailIndex) decHead() headTailIndex
```

decHead atomically decrements the head of a headTailIndex. 

#### <a id="headTailIndex.incTail" href="#headTailIndex.incTail">func (h *headTailIndex) incTail() headTailIndex</a>

```
searchKey: runtime.headTailIndex.incTail
```

```Go
func (h *headTailIndex) incTail() headTailIndex
```

incTail atomically increments the tail of a headTailIndex. 

#### <a id="headTailIndex.reset" href="#headTailIndex.reset">func (h *headTailIndex) reset()</a>

```
searchKey: runtime.headTailIndex.reset
```

```Go
func (h *headTailIndex) reset()
```

reset clears the headTailIndex to (0, 0). 

### <a id="mstats" href="#mstats">type mstats struct</a>

```
searchKey: runtime.mstats
```

```Go
type mstats struct {
	// General statistics.
	alloc       uint64 // bytes allocated and not yet freed
	total_alloc uint64 // bytes allocated (even if freed)
	sys         uint64 // bytes obtained from system (should be sum of xxx_sys below, no locking, approximate)
	nlookup     uint64 // number of pointer lookups (unused)
	nmalloc     uint64 // number of mallocs
	nfree       uint64 // number of frees

	// Statistics about malloc heap.
	// Updated atomically, or with the world stopped.
	//
	// Like MemStats, heap_sys and heap_inuse do not count memory
	// in manually-managed spans.
	heap_sys      sysMemStat // virtual address space obtained from system for GC'd heap
	heap_inuse    uint64     // bytes in mSpanInUse spans
	heap_released uint64     // bytes released to the os

	// heap_objects is not used by the runtime directly and instead
	// computed on the fly by updatememstats.
	heap_objects uint64 // total number of allocated objects

	// Statistics about stacks.
	stacks_inuse uint64     // bytes in manually-managed stack spans; computed by updatememstats
	stacks_sys   sysMemStat // only counts newosproc0 stack in mstats; differs from MemStats.StackSys

	// Statistics about allocation of low-level fixed-size structures.
	// Protected by FixAlloc locks.
	mspan_inuse  uint64 // mspan structures
	mspan_sys    sysMemStat
	mcache_inuse uint64 // mcache structures
	mcache_sys   sysMemStat
	buckhash_sys sysMemStat // profiling bucket hash table

	// Statistics about GC overhead.
	gcWorkBufInUse           uint64     // computed by updatememstats
	gcProgPtrScalarBitsInUse uint64     // computed by updatememstats
	gcMiscSys                sysMemStat // updated atomically or during STW

	// Miscellaneous statistics.
	other_sys sysMemStat // updated atomically or during STW

	// Protected by mheap or stopping the world during GC.
	last_gc_unix    uint64 // last gc (in unix time)
	pause_total_ns  uint64
	pause_ns        [256]uint64 // circular buffer of recent gc pause lengths
	pause_end       [256]uint64 // circular buffer of recent gc end times (nanoseconds since 1970)
	numgc           uint32
	numforcedgc     uint32  // number of user-forced GCs
	gc_cpu_fraction float64 // fraction of CPU time used by GC
	enablegc        bool
	debuggc         bool

	by_size [_NumSizeClasses]struct {
		size    uint32
		nmalloc uint64
		nfree   uint64
	}

	// Add an uint32 for even number of size classes to align below fields
	// to 64 bits for atomic operations on 32 bit platforms.
	_ [1 - _NumSizeClasses%2]uint32

	last_gc_nanotime uint64 // last gc (monotonic time)
	last_heap_inuse  uint64 // heap_inuse at mark termination of the previous GC

	// heapStats is a set of statistics
	heapStats consistentHeapStats

	// gcPauseDist represents the distribution of all GC-related
	// application pauses in the runtime.
	//
	// Each individual pause is counted separately, unlike pause_ns.
	gcPauseDist timeHistogram
}
```

Statistics. 

For detailed descriptions see the documentation for MemStats. Fields that differ from MemStats are further documented here. 

Many of these fields are updated on the fly, while others are only updated when updatememstats is called. 

### <a id="MemStats" href="#MemStats">type MemStats struct</a>

```
searchKey: runtime.MemStats
tags: [exported]
```

```Go
type MemStats struct {

	// Alloc is bytes of allocated heap objects.
	//
	// This is the same as HeapAlloc (see below).
	Alloc uint64

	// TotalAlloc is cumulative bytes allocated for heap objects.
	//
	// TotalAlloc increases as heap objects are allocated, but
	// unlike Alloc and HeapAlloc, it does not decrease when
	// objects are freed.
	TotalAlloc uint64

	// Sys is the total bytes of memory obtained from the OS.
	//
	// Sys is the sum of the XSys fields below. Sys measures the
	// virtual address space reserved by the Go runtime for the
	// heap, stacks, and other internal data structures. It's
	// likely that not all of the virtual address space is backed
	// by physical memory at any given moment, though in general
	// it all was at some point.
	Sys uint64

	// Lookups is the number of pointer lookups performed by the
	// runtime.
	//
	// This is primarily useful for debugging runtime internals.
	Lookups uint64

	// Mallocs is the cumulative count of heap objects allocated.
	// The number of live objects is Mallocs - Frees.
	Mallocs uint64

	// Frees is the cumulative count of heap objects freed.
	Frees uint64

	// HeapAlloc is bytes of allocated heap objects.
	//
	// "Allocated" heap objects include all reachable objects, as
	// well as unreachable objects that the garbage collector has
	// not yet freed. Specifically, HeapAlloc increases as heap
	// objects are allocated and decreases as the heap is swept
	// and unreachable objects are freed. Sweeping occurs
	// incrementally between GC cycles, so these two processes
	// occur simultaneously, and as a result HeapAlloc tends to
	// change smoothly (in contrast with the sawtooth that is
	// typical of stop-the-world garbage collectors).
	HeapAlloc uint64

	// HeapSys is bytes of heap memory obtained from the OS.
	//
	// HeapSys measures the amount of virtual address space
	// reserved for the heap. This includes virtual address space
	// that has been reserved but not yet used, which consumes no
	// physical memory, but tends to be small, as well as virtual
	// address space for which the physical memory has been
	// returned to the OS after it became unused (see HeapReleased
	// for a measure of the latter).
	//
	// HeapSys estimates the largest size the heap has had.
	HeapSys uint64

	// HeapIdle is bytes in idle (unused) spans.
	//
	// Idle spans have no objects in them. These spans could be
	// (and may already have been) returned to the OS, or they can
	// be reused for heap allocations, or they can be reused as
	// stack memory.
	//
	// HeapIdle minus HeapReleased estimates the amount of memory
	// that could be returned to the OS, but is being retained by
	// the runtime so it can grow the heap without requesting more
	// memory from the OS. If this difference is significantly
	// larger than the heap size, it indicates there was a recent
	// transient spike in live heap size.
	HeapIdle uint64

	// HeapInuse is bytes in in-use spans.
	//
	// In-use spans have at least one object in them. These spans
	// can only be used for other objects of roughly the same
	// size.
	//
	// HeapInuse minus HeapAlloc estimates the amount of memory
	// that has been dedicated to particular size classes, but is
	// not currently being used. This is an upper bound on
	// fragmentation, but in general this memory can be reused
	// efficiently.
	HeapInuse uint64

	// HeapReleased is bytes of physical memory returned to the OS.
	//
	// This counts heap memory from idle spans that was returned
	// to the OS and has not yet been reacquired for the heap.
	HeapReleased uint64

	// HeapObjects is the number of allocated heap objects.
	//
	// Like HeapAlloc, this increases as objects are allocated and
	// decreases as the heap is swept and unreachable objects are
	// freed.
	HeapObjects uint64

	// StackInuse is bytes in stack spans.
	//
	// In-use stack spans have at least one stack in them. These
	// spans can only be used for other stacks of the same size.
	//
	// There is no StackIdle because unused stack spans are
	// returned to the heap (and hence counted toward HeapIdle).
	StackInuse uint64

	// StackSys is bytes of stack memory obtained from the OS.
	//
	// StackSys is StackInuse, plus any memory obtained directly
	// from the OS for OS thread stacks (which should be minimal).
	StackSys uint64

	// MSpanInuse is bytes of allocated mspan structures.
	MSpanInuse uint64

	// MSpanSys is bytes of memory obtained from the OS for mspan
	// structures.
	MSpanSys uint64

	// MCacheInuse is bytes of allocated mcache structures.
	MCacheInuse uint64

	// MCacheSys is bytes of memory obtained from the OS for
	// mcache structures.
	MCacheSys uint64

	// BuckHashSys is bytes of memory in profiling bucket hash tables.
	BuckHashSys uint64

	// GCSys is bytes of memory in garbage collection metadata.
	GCSys uint64

	// OtherSys is bytes of memory in miscellaneous off-heap
	// runtime allocations.
	OtherSys uint64

	// NextGC is the target heap size of the next GC cycle.
	//
	// The garbage collector's goal is to keep HeapAlloc ≤ NextGC.
	// At the end of each GC cycle, the target for the next cycle
	// is computed based on the amount of reachable data and the
	// value of GOGC.
	NextGC uint64

	// LastGC is the time the last garbage collection finished, as
	// nanoseconds since 1970 (the UNIX epoch).
	LastGC uint64

	// PauseTotalNs is the cumulative nanoseconds in GC
	// stop-the-world pauses since the program started.
	//
	// During a stop-the-world pause, all goroutines are paused
	// and only the garbage collector can run.
	PauseTotalNs uint64

	// PauseNs is a circular buffer of recent GC stop-the-world
	// pause times in nanoseconds.
	//
	// The most recent pause is at PauseNs[(NumGC+255)%256]. In
	// general, PauseNs[N%256] records the time paused in the most
	// recent N%256th GC cycle. There may be multiple pauses per
	// GC cycle; this is the sum of all pauses during a cycle.
	PauseNs [256]uint64

	// PauseEnd is a circular buffer of recent GC pause end times,
	// as nanoseconds since 1970 (the UNIX epoch).
	//
	// This buffer is filled the same way as PauseNs. There may be
	// multiple pauses per GC cycle; this records the end of the
	// last pause in a cycle.
	PauseEnd [256]uint64

	// NumGC is the number of completed GC cycles.
	NumGC uint32

	// NumForcedGC is the number of GC cycles that were forced by
	// the application calling the GC function.
	NumForcedGC uint32

	// GCCPUFraction is the fraction of this program's available
	// CPU time used by the GC since the program started.
	//
	// GCCPUFraction is expressed as a number between 0 and 1,
	// where 0 means GC has consumed none of this program's CPU. A
	// program's available CPU time is defined as the integral of
	// GOMAXPROCS since the program started. That is, if
	// GOMAXPROCS is 2 and a program has been running for 10
	// seconds, its "available CPU" is 20 seconds. GCCPUFraction
	// does not include CPU time used for write barrier activity.
	//
	// This is the same as the fraction of CPU reported by
	// GODEBUG=gctrace=1.
	GCCPUFraction float64

	// EnableGC indicates that GC is enabled. It is always true,
	// even if GOGC=off.
	EnableGC bool

	// DebugGC is currently unused.
	DebugGC bool

	// BySize reports per-size class allocation statistics.
	//
	// BySize[N] gives statistics for allocations of size S where
	// BySize[N-1].Size < S ≤ BySize[N].Size.
	//
	// This does not report allocations larger than BySize[60].Size.
	BySize [61]struct {
		// Size is the maximum byte size of an object in this
		// size class.
		Size uint32

		// Mallocs is the cumulative count of heap objects
		// allocated in this size class. The cumulative bytes
		// of allocation is Size*Mallocs. The number of live
		// objects in this size class is Mallocs - Frees.
		Mallocs uint64

		// Frees is the cumulative count of heap objects freed
		// in this size class.
		Frees uint64
	}
}
```

A MemStats records statistics about the memory allocator. 

#### <a id="ReadMemStatsSlow" href="#ReadMemStatsSlow">func ReadMemStatsSlow() (base, slow MemStats)</a>

```
searchKey: runtime.ReadMemStatsSlow
```

```Go
func ReadMemStatsSlow() (base, slow MemStats)
```

ReadMemStatsSlow returns both the runtime-computed MemStats and MemStats accumulated by scanning the heap. 

### <a id="sysMemStat" href="#sysMemStat">type sysMemStat uint64</a>

```
searchKey: runtime.sysMemStat
```

```Go
type sysMemStat uint64
```

sysMemStat represents a global system statistic that is managed atomically. 

This type must structurally be a uint64 so that mstats aligns with MemStats. 

#### <a id="sysMemStat.load" href="#sysMemStat.load">func (s *sysMemStat) load() uint64</a>

```
searchKey: runtime.sysMemStat.load
```

```Go
func (s *sysMemStat) load() uint64
```

load atomically reads the value of the stat. 

Must be nosplit as it is called in runtime initialization, e.g. newosproc0. 

#### <a id="sysMemStat.add" href="#sysMemStat.add">func (s *sysMemStat) add(n int64)</a>

```
searchKey: runtime.sysMemStat.add
```

```Go
func (s *sysMemStat) add(n int64)
```

add atomically adds the sysMemStat by n. 

Must be nosplit as it is called in runtime initialization, e.g. newosproc0. 

### <a id="heapStatsDelta" href="#heapStatsDelta">type heapStatsDelta struct</a>

```
searchKey: runtime.heapStatsDelta
```

```Go
type heapStatsDelta struct {
	// Memory stats.
	committed       int64 // byte delta of memory committed
	released        int64 // byte delta of released memory generated
	inHeap          int64 // byte delta of memory placed in the heap
	inStacks        int64 // byte delta of memory reserved for stacks
	inWorkBufs      int64 // byte delta of memory reserved for work bufs
	inPtrScalarBits int64 // byte delta of memory reserved for unrolled GC prog bits

	// Allocator stats.
	tinyAllocCount  uintptr                  // number of tiny allocations
	largeAlloc      uintptr                  // bytes allocated for large objects
	largeAllocCount uintptr                  // number of large object allocations
	smallAllocCount [_NumSizeClasses]uintptr // number of allocs for small objects
	largeFree       uintptr                  // bytes freed for large objects (>maxSmallSize)
	largeFreeCount  uintptr                  // number of frees for large objects (>maxSmallSize)
	smallFreeCount  [_NumSizeClasses]uintptr // number of frees for small objects (<=maxSmallSize)

	// Add a uint32 to ensure this struct is a multiple of 8 bytes in size.
	// Only necessary on 32-bit platforms.
	_ [(sys.PtrSize / 4) % 2]uint32
}
```

heapStatsDelta contains deltas of various runtime memory statistics that need to be updated together in order for them to be kept consistent with one another. 

#### <a id="heapStatsDelta.merge" href="#heapStatsDelta.merge">func (a *heapStatsDelta) merge(b *heapStatsDelta)</a>

```
searchKey: runtime.heapStatsDelta.merge
```

```Go
func (a *heapStatsDelta) merge(b *heapStatsDelta)
```

merge adds in the deltas from b into a. 

### <a id="consistentHeapStats" href="#consistentHeapStats">type consistentHeapStats struct</a>

```
searchKey: runtime.consistentHeapStats
```

```Go
type consistentHeapStats struct {
	// stats is a ring buffer of heapStatsDelta values.
	// Writers always atomically update the delta at index gen.
	//
	// Readers operate by rotating gen (0 -> 1 -> 2 -> 0 -> ...)
	// and synchronizing with writers by observing each P's
	// statsSeq field. If the reader observes a P not writing,
	// it can be sure that it will pick up the new gen value the
	// next time it writes.
	//
	// The reader then takes responsibility by clearing space
	// in the ring buffer for the next reader to rotate gen to
	// that space (i.e. it merges in values from index (gen-2) mod 3
	// to index (gen-1) mod 3, then clears the former).
	//
	// Note that this means only one reader can be reading at a time.
	// There is no way for readers to synchronize.
	//
	// This process is why we need a ring buffer of size 3 instead
	// of 2: one is for the writers, one contains the most recent
	// data, and the last one is clear so writers can begin writing
	// to it the moment gen is updated.
	stats [3]heapStatsDelta

	// gen represents the current index into which writers
	// are writing, and can take on the value of 0, 1, or 2.
	// This value is updated atomically.
	gen uint32

	// noPLock is intended to provide mutual exclusion for updating
	// stats when no P is available. It does not block other writers
	// with a P, only other writers without a P and the reader. Because
	// stats are usually updated when a P is available, contention on
	// this lock should be minimal.
	noPLock mutex
}
```

consistentHeapStats represents a set of various memory statistics whose updates must be viewed completely to get a consistent state of the world. 

To write updates to memory stats use the acquire and release methods. To obtain a consistent global snapshot of these statistics, use read. 

#### <a id="consistentHeapStats.acquire" href="#consistentHeapStats.acquire">func (m *consistentHeapStats) acquire() *heapStatsDelta</a>

```
searchKey: runtime.consistentHeapStats.acquire
```

```Go
func (m *consistentHeapStats) acquire() *heapStatsDelta
```

acquire returns a heapStatsDelta to be updated. In effect, it acquires the shard for writing. release must be called as soon as the relevant deltas are updated. 

The returned heapStatsDelta must be updated atomically. 

The caller's P must not change between acquire and release. This also means that the caller should not acquire a P or release its P in between. 

#### <a id="consistentHeapStats.release" href="#consistentHeapStats.release">func (m *consistentHeapStats) release()</a>

```
searchKey: runtime.consistentHeapStats.release
```

```Go
func (m *consistentHeapStats) release()
```

release indicates that the writer is done modifying the delta. The value returned by the corresponding acquire must no longer be accessed or modified after release is called. 

The caller's P must not change between acquire and release. This also means that the caller should not acquire a P or release its P in between. 

#### <a id="consistentHeapStats.unsafeRead" href="#consistentHeapStats.unsafeRead">func (m *consistentHeapStats) unsafeRead(out *heapStatsDelta)</a>

```
searchKey: runtime.consistentHeapStats.unsafeRead
```

```Go
func (m *consistentHeapStats) unsafeRead(out *heapStatsDelta)
```

unsafeRead aggregates the delta for this shard into out. 

Unsafe because it does so without any synchronization. The world must be stopped. 

#### <a id="consistentHeapStats.unsafeClear" href="#consistentHeapStats.unsafeClear">func (m *consistentHeapStats) unsafeClear()</a>

```
searchKey: runtime.consistentHeapStats.unsafeClear
```

```Go
func (m *consistentHeapStats) unsafeClear()
```

unsafeClear clears the shard. 

Unsafe because the world must be stopped and values should be donated elsewhere before clearing. 

#### <a id="consistentHeapStats.read" href="#consistentHeapStats.read">func (m *consistentHeapStats) read(out *heapStatsDelta)</a>

```
searchKey: runtime.consistentHeapStats.read
```

```Go
func (m *consistentHeapStats) read(out *heapStatsDelta)
```

read takes a globally consistent snapshot of m and puts the aggregated value in out. Even though out is a heapStatsDelta, the resulting values should be complete and valid statistic values. 

Not safe to call concurrently. The world must be stopped or metricsSema must be held. 

### <a id="wbBuf" href="#wbBuf">type wbBuf struct</a>

```
searchKey: runtime.wbBuf
```

```Go
type wbBuf struct {
	// next points to the next slot in buf. It must not be a
	// pointer type because it can point past the end of buf and
	// must be updated without write barriers.
	//
	// This is a pointer rather than an index to optimize the
	// write barrier assembly.
	next uintptr

	// end points to just past the end of buf. It must not be a
	// pointer type because it points past the end of buf and must
	// be updated without write barriers.
	end uintptr

	// buf stores a series of pointers to execute write barriers
	// on. This must be a multiple of wbBufEntryPointers because
	// the write barrier only checks for overflow once per entry.
	buf [wbBufEntryPointers * wbBufEntries]uintptr
}
```

wbBuf is a per-P buffer of pointers queued by the write barrier. This buffer is flushed to the GC workbufs when it fills up and on various GC transitions. 

This is closely related to a "sequential store buffer" (SSB), except that SSBs are usually used for maintaining remembered sets, while this is used for marking. 

#### <a id="wbBuf.reset" href="#wbBuf.reset">func (b *wbBuf) reset()</a>

```
searchKey: runtime.wbBuf.reset
```

```Go
func (b *wbBuf) reset()
```

reset empties b by resetting its next and end pointers. 

#### <a id="wbBuf.discard" href="#wbBuf.discard">func (b *wbBuf) discard()</a>

```
searchKey: runtime.wbBuf.discard
```

```Go
func (b *wbBuf) discard()
```

discard resets b's next pointer, but not its end pointer. 

This must be nosplit because it's called by wbBufFlush. 

#### <a id="wbBuf.empty" href="#wbBuf.empty">func (b *wbBuf) empty() bool</a>

```
searchKey: runtime.wbBuf.empty
```

```Go
func (b *wbBuf) empty() bool
```

empty reports whether b contains no pointers. 

#### <a id="wbBuf.putFast" href="#wbBuf.putFast">func (b *wbBuf) putFast(old, new uintptr) bool</a>

```
searchKey: runtime.wbBuf.putFast
```

```Go
func (b *wbBuf) putFast(old, new uintptr) bool
```

putFast adds old and new to the write barrier buffer and returns false if a flush is necessary. Callers should use this as: 

```
buf := &getg().m.p.ptr().wbBuf
if !buf.putFast(old, new) {
    wbBufFlush(...)
}
... actual memory write ...

```
The arguments to wbBufFlush depend on whether the caller is doing its own cgo pointer checks. If it is, then this can be wbBufFlush(nil, 0). Otherwise, it must pass the slot address and new. 

The caller must ensure there are no preemption points during the above sequence. There must be no preemption points while buf is in use because it is a per-P resource. There must be no preemption points between the buffer put and the write to memory because this could allow a GC phase change, which could result in missed write barriers. 

putFast must be nowritebarrierrec to because write barriers here would corrupt the write barrier buffer. It (and everything it calls, if it called anything) has to be nosplit to avoid scheduling on to a different P and a different buffer. 

### <a id="pollDesc" href="#pollDesc">type pollDesc struct</a>

```
searchKey: runtime.pollDesc
```

```Go
type pollDesc struct {
	link *pollDesc // in pollcache, protected by pollcache.lock

	// The lock protects pollOpen, pollSetDeadline, pollUnblock and deadlineimpl operations.
	// This fully covers seq, rt and wt variables. fd is constant throughout the PollDesc lifetime.
	// pollReset, pollWait, pollWaitCanceled and runtime·netpollready (IO readiness notification)
	// proceed w/o taking the lock. So closing, everr, rg, rd, wg and wd are manipulated
	// in a lock-free way by all operations.
	// NOTE(dvyukov): the following code uses uintptr to store *g (rg/wg),
	// that will blow up when GC starts moving objects.
	lock    mutex // protects the following fields
	fd      uintptr
	closing bool
	everr   bool      // marks event scanning error happened
	user    uint32    // user settable cookie
	rseq    uintptr   // protects from stale read timers
	rg      uintptr   // pdReady, pdWait, G waiting for read or nil
	rt      timer     // read deadline timer (set if rt.f != nil)
	rd      int64     // read deadline
	wseq    uintptr   // protects from stale write timers
	wg      uintptr   // pdReady, pdWait, G waiting for write or nil
	wt      timer     // write deadline timer
	wd      int64     // write deadline
	self    *pollDesc // storage for indirect interface. See (*pollDesc).makeArg.
}
```

Network poller descriptor. 

No heap pointers. 

#### <a id="poll_runtime_pollOpen" href="#poll_runtime_pollOpen">func poll_runtime_pollOpen(fd uintptr) (*pollDesc, int)</a>

```
searchKey: runtime.poll_runtime_pollOpen
```

```Go
func poll_runtime_pollOpen(fd uintptr) (*pollDesc, int)
```

#### <a id="pollDesc.makeArg" href="#pollDesc.makeArg">func (pd *pollDesc) makeArg() (i interface{})</a>

```
searchKey: runtime.pollDesc.makeArg
```

```Go
func (pd *pollDesc) makeArg() (i interface{})
```

makeArg converts pd to an interface{}. makeArg does not do any allocation. Normally, such a conversion requires an allocation because pointers to go:notinheap types (which pollDesc is) must be stored in interfaces indirectly. See issue 42076. 

### <a id="pollCache" href="#pollCache">type pollCache struct</a>

```
searchKey: runtime.pollCache
```

```Go
type pollCache struct {
	lock  mutex
	first *pollDesc
}
```

#### <a id="pollCache.free" href="#pollCache.free">func (c *pollCache) free(pd *pollDesc)</a>

```
searchKey: runtime.pollCache.free
```

```Go
func (c *pollCache) free(pd *pollDesc)
```

#### <a id="pollCache.alloc" href="#pollCache.alloc">func (c *pollCache) alloc() *pollDesc</a>

```
searchKey: runtime.pollCache.alloc
```

```Go
func (c *pollCache) alloc() *pollDesc
```

### <a id="mOS" href="#mOS">type mOS struct</a>

```
searchKey: runtime.mOS
```

```Go
type mOS struct {
	initialized bool
	mutex       pthreadmutex
	cond        pthreadcond
	count       int
}
```

### <a id="sigset" href="#sigset">type sigset uint32</a>

```
searchKey: runtime.sigset
```

```Go
type sigset uint32
```

### <a id="ptabEntry" href="#ptabEntry">type ptabEntry struct</a>

```
searchKey: runtime.ptabEntry
```

```Go
type ptabEntry struct {
	name nameOff
	typ  typeOff
}
```

A ptabEntry is generated by the compiler for each exported function and global variable in the main package of a plugin. It is used to initialize the plugin module's symbol map. 

### <a id="suspendGState" href="#suspendGState">type suspendGState struct</a>

```
searchKey: runtime.suspendGState
```

```Go
type suspendGState struct {
	g *g

	// dead indicates the goroutine was not suspended because it
	// is dead. This goroutine could be reused after the dead
	// state was observed, so the caller must not assume that it
	// remains dead.
	dead bool

	// stopped indicates that this suspendG transitioned the G to
	// _Gwaiting via g.preemptStop and thus is responsible for
	// readying it when done.
	stopped bool
}
```

#### <a id="suspendG" href="#suspendG">func suspendG(gp *g) suspendGState</a>

```
searchKey: runtime.suspendG
```

```Go
func suspendG(gp *g) suspendGState
```

suspendG suspends goroutine gp at a safe-point and returns the state of the suspended goroutine. The caller gets read access to the goroutine until it calls resumeG. 

It is safe for multiple callers to attempt to suspend the same goroutine at the same time. The goroutine may execute between subsequent successful suspend operations. The current implementation grants exclusive access to the goroutine, and hence multiple callers will serialize. However, the intent is to grant shared read access, so please don't depend on exclusive access. 

This must be called from the system stack and the user goroutine on the current M (if any) must be in a preemptible state. This prevents deadlocks where two goroutines attempt to suspend each other and both are in non-preemptible states. There are other ways to resolve this deadlock, but this seems simplest. 

TODO(austin): What if we instead required this to be called from a user goroutine? Then we could deschedule the goroutine while waiting instead of blocking the thread. If two goroutines tried to suspend each other, one of them would win and the other wouldn't complete the suspend until it was resumed. We would have to be careful that they couldn't actually queue up suspend for each other and then both be suspended. This would also avoid the need for a kernel context switch in the synchronous case because we could just directly schedule the waiter. The context switch is unavoidable in the signal case. 

### <a id="hex" href="#hex">type hex uint64</a>

```
searchKey: runtime.hex
```

```Go
type hex uint64
```

The compiler knows that a print of a value of this type should use printhex instead of printuint (decimal). 

### <a id="cgothreadstart" href="#cgothreadstart">type cgothreadstart struct</a>

```
searchKey: runtime.cgothreadstart
```

```Go
type cgothreadstart struct {
	g   guintptr
	tls *uint64
	fn  unsafe.Pointer
}
```

### <a id="sysmontick" href="#sysmontick">type sysmontick struct</a>

```
searchKey: runtime.sysmontick
```

```Go
type sysmontick struct {
	schedtick   uint32
	schedwhen   int64
	syscalltick uint32
	syscallwhen int64
}
```

### <a id="pMask" href="#pMask">type pMask []uint32</a>

```
searchKey: runtime.pMask
```

```Go
type pMask []uint32
```

pMask is an atomic bitstring with one bit per P. 

#### <a id="pMask.read" href="#pMask.read">func (p pMask) read(id uint32) bool</a>

```
searchKey: runtime.pMask.read
```

```Go
func (p pMask) read(id uint32) bool
```

read returns true if P id's bit is set. 

#### <a id="pMask.set" href="#pMask.set">func (p pMask) set(id int32)</a>

```
searchKey: runtime.pMask.set
```

```Go
func (p pMask) set(id int32)
```

set sets P id's bit. 

#### <a id="pMask.clear" href="#pMask.clear">func (p pMask) clear(id int32)</a>

```
searchKey: runtime.pMask.clear
```

```Go
func (p pMask) clear(id int32)
```

clear clears P id's bit. 

### <a id="gQueue" href="#gQueue">type gQueue struct</a>

```
searchKey: runtime.gQueue
```

```Go
type gQueue struct {
	head guintptr
	tail guintptr
}
```

A gQueue is a dequeue of Gs linked through g.schedlink. A G can only be on one gQueue or gList at a time. 

#### <a id="runqdrain" href="#runqdrain">func runqdrain(_p_ *p) (drainQ gQueue, n uint32)</a>

```
searchKey: runtime.runqdrain
```

```Go
func runqdrain(_p_ *p) (drainQ gQueue, n uint32)
```

runqdrain drains the local runnable queue of _p_ and returns all goroutines in it. Executed only by the owner P. 

#### <a id="gQueue.empty" href="#gQueue.empty">func (q *gQueue) empty() bool</a>

```
searchKey: runtime.gQueue.empty
```

```Go
func (q *gQueue) empty() bool
```

empty reports whether q is empty. 

#### <a id="gQueue.push" href="#gQueue.push">func (q *gQueue) push(gp *g)</a>

```
searchKey: runtime.gQueue.push
```

```Go
func (q *gQueue) push(gp *g)
```

push adds gp to the head of q. 

#### <a id="gQueue.pushBack" href="#gQueue.pushBack">func (q *gQueue) pushBack(gp *g)</a>

```
searchKey: runtime.gQueue.pushBack
```

```Go
func (q *gQueue) pushBack(gp *g)
```

pushBack adds gp to the tail of q. 

#### <a id="gQueue.pushBackAll" href="#gQueue.pushBackAll">func (q *gQueue) pushBackAll(q2 gQueue)</a>

```
searchKey: runtime.gQueue.pushBackAll
```

```Go
func (q *gQueue) pushBackAll(q2 gQueue)
```

pushBackAll adds all Gs in l2 to the tail of q. After this q2 must not be used. 

#### <a id="gQueue.pop" href="#gQueue.pop">func (q *gQueue) pop() *g</a>

```
searchKey: runtime.gQueue.pop
```

```Go
func (q *gQueue) pop() *g
```

pop removes and returns the head of queue q. It returns nil if q is empty. 

#### <a id="gQueue.popList" href="#gQueue.popList">func (q *gQueue) popList() gList</a>

```
searchKey: runtime.gQueue.popList
```

```Go
func (q *gQueue) popList() gList
```

popList takes all Gs in q and returns them as a gList. 

### <a id="gList" href="#gList">type gList struct</a>

```
searchKey: runtime.gList
```

```Go
type gList struct {
	head guintptr
}
```

A gList is a list of Gs linked through g.schedlink. A G can only be on one gQueue or gList at a time. 

#### <a id="netpoll" href="#netpoll">func netpoll(delay int64) gList</a>

```
searchKey: runtime.netpoll
```

```Go
func netpoll(delay int64) gList
```

netpoll checks for ready network connections. Returns list of goroutines that become runnable. delay < 0: blocks indefinitely delay == 0: does not block, just polls delay > 0: block for up to that many nanoseconds 

#### <a id="gList.empty" href="#gList.empty">func (l *gList) empty() bool</a>

```
searchKey: runtime.gList.empty
```

```Go
func (l *gList) empty() bool
```

empty reports whether l is empty. 

#### <a id="gList.push" href="#gList.push">func (l *gList) push(gp *g)</a>

```
searchKey: runtime.gList.push
```

```Go
func (l *gList) push(gp *g)
```

push adds gp to the head of l. 

#### <a id="gList.pushAll" href="#gList.pushAll">func (l *gList) pushAll(q gQueue)</a>

```
searchKey: runtime.gList.pushAll
```

```Go
func (l *gList) pushAll(q gQueue)
```

pushAll prepends all Gs in q to l. 

#### <a id="gList.pop" href="#gList.pop">func (l *gList) pop() *g</a>

```
searchKey: runtime.gList.pop
```

```Go
func (l *gList) pop() *g
```

pop removes and returns the head of l. If l is empty, it returns nil. 

### <a id="randomOrder" href="#randomOrder">type randomOrder struct</a>

```
searchKey: runtime.randomOrder
```

```Go
type randomOrder struct {
	count    uint32
	coprimes []uint32
}
```

randomOrder/randomEnum are helper types for randomized work stealing. They allow to enumerate all Ps in different pseudo-random orders without repetitions. The algorithm is based on the fact that if we have X such that X and GOMAXPROCS are coprime, then a sequences of (i + X) % GOMAXPROCS gives the required enumeration. 

#### <a id="randomOrder.reset" href="#randomOrder.reset">func (ord *randomOrder) reset(count uint32)</a>

```
searchKey: runtime.randomOrder.reset
```

```Go
func (ord *randomOrder) reset(count uint32)
```

#### <a id="randomOrder.start" href="#randomOrder.start">func (ord *randomOrder) start(i uint32) randomEnum</a>

```
searchKey: runtime.randomOrder.start
```

```Go
func (ord *randomOrder) start(i uint32) randomEnum
```

### <a id="randomEnum" href="#randomEnum">type randomEnum struct</a>

```
searchKey: runtime.randomEnum
```

```Go
type randomEnum struct {
	i     uint32
	count uint32
	pos   uint32
	inc   uint32
}
```

#### <a id="randomEnum.done" href="#randomEnum.done">func (enum *randomEnum) done() bool</a>

```
searchKey: runtime.randomEnum.done
```

```Go
func (enum *randomEnum) done() bool
```

#### <a id="randomEnum.next" href="#randomEnum.next">func (enum *randomEnum) next()</a>

```
searchKey: runtime.randomEnum.next
```

```Go
func (enum *randomEnum) next()
```

#### <a id="randomEnum.position" href="#randomEnum.position">func (enum *randomEnum) position() uint32</a>

```
searchKey: runtime.randomEnum.position
```

```Go
func (enum *randomEnum) position() uint32
```

### <a id="initTask" href="#initTask">type initTask struct</a>

```
searchKey: runtime.initTask
```

```Go
type initTask struct {
	// TODO: pack the first 3 fields more tightly?
	state uintptr // 0 = uninitialized, 1 = in progress, 2 = done
	ndeps uintptr
	nfns  uintptr
}
```

An initTask represents the set of initializations that need to be done for a package. Keep in sync with ../../test/initempty.go:initTask 

### <a id="tracestat" href="#tracestat">type tracestat struct</a>

```
searchKey: runtime.tracestat
```

```Go
type tracestat struct {
	active bool   // init tracing activation status
	id     int64  // init goroutine id
	allocs uint64 // heap allocations
	bytes  uint64 // heap allocated bytes
}
```

### <a id="profBuf" href="#profBuf">type profBuf struct</a>

```
searchKey: runtime.profBuf
```

```Go
type profBuf struct {
	// accessed atomically
	r, w         profAtomic
	overflow     uint64
	overflowTime uint64
	eof          uint32

	// immutable (excluding slice content)
	hdrsize uintptr
	data    []uint64
	tags    []unsafe.Pointer

	// owned by reader
	rNext       profIndex
	overflowBuf []uint64 // for use by reader to return overflow record
	wait        note
}
```

A profBuf is a lock-free buffer for profiling events, safe for concurrent use by one reader and one writer. The writer may be a signal handler running without a user g. The reader is assumed to be a user g. 

Each logged event corresponds to a fixed size header, a list of uintptrs (typically a stack), and exactly one unsafe.Pointer tag. The header and uintptrs are stored in the circular buffer data and the tag is stored in a circular buffer tags, running in parallel. In the circular buffer data, each event takes 2+hdrsize+len(stk) words: the value 2+hdrsize+len(stk), then the time of the event, then hdrsize words giving the fixed-size header, and then len(stk) words for the stack. 

The current effective offsets into the tags and data circular buffers for reading and writing are stored in the high 30 and low 32 bits of r and w. The bottom bits of the high 32 are additional flag bits in w, unused in r. "Effective" offsets means the total number of reads or writes, mod 2^length. The offset in the buffer is the effective offset mod the length of the buffer. To make wraparound mod 2^length match wraparound mod length of the buffer, the length of the buffer must be a power of two. 

If the reader catches up to the writer, a flag passed to read controls whether the read blocks until more data is available. A read returns a pointer to the buffer data itself; the caller is assumed to be done with that data at the next read. The read offset rNext tracks the next offset to be returned by read. By definition, r ≤ rNext ≤ w (before wraparound), and rNext is only used by the reader, so it can be accessed without atomics. 

If the writer gets ahead of the reader, so that the buffer fills, future writes are discarded and replaced in the output stream by an overflow entry, which has size 2+hdrsize+1, time set to the time of the first discarded write, a header of all zeroed words, and a "stack" containing one word, the number of discarded writes. 

Between the time the buffer fills and the buffer becomes empty enough to hold more data, the overflow entry is stored as a pending overflow entry in the fields overflow and overflowTime. The pending overflow entry can be turned into a real record by either the writer or the reader. If the writer is called to write a new record and finds that the output buffer has room for both the pending overflow entry and the new record, the writer emits the pending overflow entry and the new record into the buffer. If the reader is called to read data and finds that the output buffer is empty but that there is a pending overflow entry, the reader will return a synthesized record for the pending overflow entry. 

Only the writer can create or add to a pending overflow entry, but either the reader or the writer can clear the pending overflow entry. A pending overflow entry is indicated by the low 32 bits of 'overflow' holding the number of discarded writes, and overflowTime holding the time of the first discarded write. The high 32 bits of 'overflow' increment each time the low 32 bits transition from zero to non-zero or vice versa. This sequence number avoids ABA problems in the use of compare-and-swap to coordinate between reader and writer. The overflowTime is only written when the low 32 bits of overflow are zero, that is, only when there is no pending overflow entry, in preparation for creating a new one. The reader can therefore fetch and clear the entry atomically using 

```
for {
	overflow = load(&b.overflow)
	if uint32(overflow) == 0 {
		// no pending entry
		break
	}
	time = load(&b.overflowTime)
	if cas(&b.overflow, overflow, ((overflow>>32)+1)<<32) {
		// pending entry cleared
		break
	}
}
if uint32(overflow) > 0 {
	emit entry for uint32(overflow), time
}

```
#### <a id="newProfBuf" href="#newProfBuf">func newProfBuf(hdrsize, bufwords, tags int) *profBuf</a>

```
searchKey: runtime.newProfBuf
```

```Go
func newProfBuf(hdrsize, bufwords, tags int) *profBuf
```

newProfBuf returns a new profiling buffer with room for a header of hdrsize words and a buffer of at least bufwords words. 

#### <a id="profBuf.hasOverflow" href="#profBuf.hasOverflow">func (b *profBuf) hasOverflow() bool</a>

```
searchKey: runtime.profBuf.hasOverflow
```

```Go
func (b *profBuf) hasOverflow() bool
```

hasOverflow reports whether b has any overflow records pending. 

#### <a id="profBuf.takeOverflow" href="#profBuf.takeOverflow">func (b *profBuf) takeOverflow() (count uint32, time uint64)</a>

```
searchKey: runtime.profBuf.takeOverflow
```

```Go
func (b *profBuf) takeOverflow() (count uint32, time uint64)
```

takeOverflow consumes the pending overflow records, returning the overflow count and the time of the first overflow. When called by the reader, it is racing against incrementOverflow. 

#### <a id="profBuf.incrementOverflow" href="#profBuf.incrementOverflow">func (b *profBuf) incrementOverflow(now int64)</a>

```
searchKey: runtime.profBuf.incrementOverflow
```

```Go
func (b *profBuf) incrementOverflow(now int64)
```

incrementOverflow records a single overflow at time now. It is racing against a possible takeOverflow in the reader. 

#### <a id="profBuf.canWriteRecord" href="#profBuf.canWriteRecord">func (b *profBuf) canWriteRecord(nstk int) bool</a>

```
searchKey: runtime.profBuf.canWriteRecord
```

```Go
func (b *profBuf) canWriteRecord(nstk int) bool
```

canWriteRecord reports whether the buffer has room for a single contiguous record with a stack of length nstk. 

#### <a id="profBuf.canWriteTwoRecords" href="#profBuf.canWriteTwoRecords">func (b *profBuf) canWriteTwoRecords(nstk1, nstk2 int) bool</a>

```
searchKey: runtime.profBuf.canWriteTwoRecords
```

```Go
func (b *profBuf) canWriteTwoRecords(nstk1, nstk2 int) bool
```

canWriteTwoRecords reports whether the buffer has room for two records with stack lengths nstk1, nstk2, in that order. Each record must be contiguous on its own, but the two records need not be contiguous (one can be at the end of the buffer and the other can wrap around and start at the beginning of the buffer). 

#### <a id="profBuf.write" href="#profBuf.write">func (b *profBuf) write(tagPtr *unsafe.Pointer, now int64, hdr []uint64, stk []uintptr)</a>

```
searchKey: runtime.profBuf.write
```

```Go
func (b *profBuf) write(tagPtr *unsafe.Pointer, now int64, hdr []uint64, stk []uintptr)
```

write writes an entry to the profiling buffer b. The entry begins with a fixed hdr, which must have length b.hdrsize, followed by a variable-sized stack and a single tag pointer *tagPtr (or nil if tagPtr is nil). No write barriers allowed because this might be called from a signal handler. 

#### <a id="profBuf.close" href="#profBuf.close">func (b *profBuf) close()</a>

```
searchKey: runtime.profBuf.close
```

```Go
func (b *profBuf) close()
```

close signals that there will be no more writes on the buffer. Once all the data has been read from the buffer, reads will return eof=true. 

#### <a id="profBuf.wakeupExtra" href="#profBuf.wakeupExtra">func (b *profBuf) wakeupExtra()</a>

```
searchKey: runtime.profBuf.wakeupExtra
```

```Go
func (b *profBuf) wakeupExtra()
```

wakeupExtra must be called after setting one of the "extra" atomic fields b.overflow or b.eof. It records the change in b.w and wakes up the reader if needed. 

#### <a id="profBuf.read" href="#profBuf.read">func (b *profBuf) read(mode profBufReadMode) (data []uint64, tags []unsafe.Pointer, eof bool)</a>

```
searchKey: runtime.profBuf.read
```

```Go
func (b *profBuf) read(mode profBufReadMode) (data []uint64, tags []unsafe.Pointer, eof bool)
```

### <a id="profAtomic" href="#profAtomic">type profAtomic uint64</a>

```
searchKey: runtime.profAtomic
```

```Go
type profAtomic uint64
```

A profAtomic is the atomically-accessed word holding a profIndex. 

#### <a id="profAtomic.load" href="#profAtomic.load">func (x *profAtomic) load() profIndex</a>

```
searchKey: runtime.profAtomic.load
```

```Go
func (x *profAtomic) load() profIndex
```

#### <a id="profAtomic.store" href="#profAtomic.store">func (x *profAtomic) store(new profIndex)</a>

```
searchKey: runtime.profAtomic.store
```

```Go
func (x *profAtomic) store(new profIndex)
```

#### <a id="profAtomic.cas" href="#profAtomic.cas">func (x *profAtomic) cas(old, new profIndex) bool</a>

```
searchKey: runtime.profAtomic.cas
```

```Go
func (x *profAtomic) cas(old, new profIndex) bool
```

### <a id="profIndex" href="#profIndex">type profIndex uint64</a>

```
searchKey: runtime.profIndex
```

```Go
type profIndex uint64
```

A profIndex is the packet tag and data counts and flags bits, described above. 

#### <a id="profIndex.dataCount" href="#profIndex.dataCount">func (x profIndex) dataCount() uint32</a>

```
searchKey: runtime.profIndex.dataCount
```

```Go
func (x profIndex) dataCount() uint32
```

#### <a id="profIndex.tagCount" href="#profIndex.tagCount">func (x profIndex) tagCount() uint32</a>

```
searchKey: runtime.profIndex.tagCount
```

```Go
func (x profIndex) tagCount() uint32
```

#### <a id="profIndex.addCountsAndClearFlags" href="#profIndex.addCountsAndClearFlags">func (x profIndex) addCountsAndClearFlags(data, tag int) profIndex</a>

```
searchKey: runtime.profIndex.addCountsAndClearFlags
```

```Go
func (x profIndex) addCountsAndClearFlags(data, tag int) profIndex
```

addCountsAndClearFlags returns the packed form of "x + (data, tag) - all flags". 

### <a id="profBufReadMode" href="#profBufReadMode">type profBufReadMode int</a>

```
searchKey: runtime.profBufReadMode
```

```Go
type profBufReadMode int
```

profBufReadMode specifies whether to block when no data is available to read. 

### <a id="dbgVar" href="#dbgVar">type dbgVar struct</a>

```
searchKey: runtime.dbgVar
```

```Go
type dbgVar struct {
	name  string
	value *int32
}
```

### <a id="mutex" href="#mutex">type mutex struct</a>

```
searchKey: runtime.mutex
```

```Go
type mutex struct {
	// Empty struct if lock ranking is disabled, otherwise includes the lock rank
	lockRankStruct
	// Futex-based impl treats it as uint32 key,
	// while sema-based impl as M* waitm.
	// Used to be a union, but unions break precise GC.
	key uintptr
}
```

Mutual exclusion locks.  In the uncontended case, as fast as spin locks (just a few user-level instructions), but on the contention path they sleep in the kernel. A zeroed Mutex is unlocked (no need to initialize each lock). Initialization is helpful for static lock ranking, but not required. 

### <a id="note" href="#note">type note struct</a>

```
searchKey: runtime.note
```

```Go
type note struct {
	// Futex-based impl treats it as uint32 key,
	// while sema-based impl as M* waitm.
	// Used to be a union, but unions break precise GC.
	key uintptr
}
```

sleep and wakeup on one-time events. before any calls to notesleep or notewakeup, must call noteclear to initialize the Note. then, exactly one thread can call notesleep and exactly one thread can call notewakeup (once). once notewakeup has been called, the notesleep will return.  future notesleep will return immediately. subsequent noteclear must be called only after previous notesleep has returned, e.g. it's disallowed to call noteclear straight after notewakeup. 

notetsleep is like notesleep but wakes up after a given number of nanoseconds even if the event has not yet happened.  if a goroutine uses notetsleep to wake up early, it must wait to call noteclear until it can be sure that no other goroutine is calling notewakeup. 

notesleep/notetsleep are generally called on g0, notetsleepg is similar to notetsleep but is called on user g. 

### <a id="funcval" href="#funcval">type funcval struct</a>

```
searchKey: runtime.funcval
```

```Go
type funcval struct {
	fn uintptr
}
```

### <a id="iface" href="#iface">type iface struct</a>

```
searchKey: runtime.iface
```

```Go
type iface struct {
	tab  *itab
	data unsafe.Pointer
}
```

#### <a id="convT2I" href="#convT2I">func convT2I(tab *itab, elem unsafe.Pointer) (i iface)</a>

```
searchKey: runtime.convT2I
```

```Go
func convT2I(tab *itab, elem unsafe.Pointer) (i iface)
```

#### <a id="convT2Inoptr" href="#convT2Inoptr">func convT2Inoptr(tab *itab, elem unsafe.Pointer) (i iface)</a>

```
searchKey: runtime.convT2Inoptr
```

```Go
func convT2Inoptr(tab *itab, elem unsafe.Pointer) (i iface)
```

#### <a id="convI2I" href="#convI2I">func convI2I(inter *interfacetype, i iface) (r iface)</a>

```
searchKey: runtime.convI2I
```

```Go
func convI2I(inter *interfacetype, i iface) (r iface)
```

#### <a id="assertI2I2" href="#assertI2I2">func assertI2I2(inter *interfacetype, i iface) (r iface)</a>

```
searchKey: runtime.assertI2I2
```

```Go
func assertI2I2(inter *interfacetype, i iface) (r iface)
```

#### <a id="assertE2I2" href="#assertE2I2">func assertE2I2(inter *interfacetype, e eface) (r iface)</a>

```
searchKey: runtime.assertE2I2
```

```Go
func assertE2I2(inter *interfacetype, e eface) (r iface)
```

### <a id="eface" href="#eface">type eface struct</a>

```
searchKey: runtime.eface
```

```Go
type eface struct {
	_type *_type
	data  unsafe.Pointer
}
```

#### <a id="convT2E" href="#convT2E">func convT2E(t *_type, elem unsafe.Pointer) (e eface)</a>

```
searchKey: runtime.convT2E
```

```Go
func convT2E(t *_type, elem unsafe.Pointer) (e eface)
```

#### <a id="convT2Enoptr" href="#convT2Enoptr">func convT2Enoptr(t *_type, elem unsafe.Pointer) (e eface)</a>

```
searchKey: runtime.convT2Enoptr
```

```Go
func convT2Enoptr(t *_type, elem unsafe.Pointer) (e eface)
```

#### <a id="efaceOf" href="#efaceOf">func efaceOf(ep *interface{}) *eface</a>

```
searchKey: runtime.efaceOf
```

```Go
func efaceOf(ep *interface{}) *eface
```

### <a id="guintptr" href="#guintptr">type guintptr uintptr</a>

```
searchKey: runtime.guintptr
```

```Go
type guintptr uintptr
```

A guintptr holds a goroutine pointer, but typed as a uintptr to bypass write barriers. It is used in the Gobuf goroutine state and in scheduling lists that are manipulated without a P. 

The Gobuf.g goroutine pointer is almost always updated by assembly code. In one of the few places it is updated by Go code - func save - it must be treated as a uintptr to avoid a write barrier being emitted at a bad time. Instead of figuring out how to emit the write barriers missing in the assembly manipulation, we change the type of the field to uintptr, so that it does not require write barriers at all. 

Goroutine structs are published in the allg list and never freed. That will keep the goroutine structs from being collected. There is never a time that Gobuf.g's contain the only references to a goroutine: the publishing of the goroutine in allg comes first. Goroutine pointers are also kept in non-GC-visible places like TLS, so I can't see them ever moving. If we did want to start moving data in the GC, we'd need to allocate the goroutine structs from an alternate arena. Using guintptr doesn't make that problem any worse. 

#### <a id="guintptr.ptr" href="#guintptr.ptr">func (gp guintptr) ptr() *g</a>

```
searchKey: runtime.guintptr.ptr
```

```Go
func (gp guintptr) ptr() *g
```

#### <a id="guintptr.set" href="#guintptr.set">func (gp *guintptr) set(g *g)</a>

```
searchKey: runtime.guintptr.set
```

```Go
func (gp *guintptr) set(g *g)
```

#### <a id="guintptr.cas" href="#guintptr.cas">func (gp *guintptr) cas(old, new guintptr) bool</a>

```
searchKey: runtime.guintptr.cas
```

```Go
func (gp *guintptr) cas(old, new guintptr) bool
```

### <a id="puintptr" href="#puintptr">type puintptr uintptr</a>

```
searchKey: runtime.puintptr
```

```Go
type puintptr uintptr
```

#### <a id="puintptr.ptr" href="#puintptr.ptr">func (pp puintptr) ptr() *p</a>

```
searchKey: runtime.puintptr.ptr
```

```Go
func (pp puintptr) ptr() *p
```

#### <a id="puintptr.set" href="#puintptr.set">func (pp *puintptr) set(p *p)</a>

```
searchKey: runtime.puintptr.set
```

```Go
func (pp *puintptr) set(p *p)
```

### <a id="muintptr" href="#muintptr">type muintptr uintptr</a>

```
searchKey: runtime.muintptr
```

```Go
type muintptr uintptr
```

muintptr is a *m that is not tracked by the garbage collector. 

Because we do free Ms, there are some additional constrains on muintptrs: 

1. Never hold an muintptr locally across a safe point. 

2. Any muintptr in the heap must be owned by the M itself so it can 

```
ensure it is not in use when the last true *m is released.

```
#### <a id="muintptr.ptr" href="#muintptr.ptr">func (mp muintptr) ptr() *m</a>

```
searchKey: runtime.muintptr.ptr
```

```Go
func (mp muintptr) ptr() *m
```

#### <a id="muintptr.set" href="#muintptr.set">func (mp *muintptr) set(m *m)</a>

```
searchKey: runtime.muintptr.set
```

```Go
func (mp *muintptr) set(m *m)
```

### <a id="gobuf" href="#gobuf">type gobuf struct</a>

```
searchKey: runtime.gobuf
```

```Go
type gobuf struct {
	// The offsets of sp, pc, and g are known to (hard-coded in) libmach.
	//
	// ctxt is unusual with respect to GC: it may be a
	// heap-allocated funcval, so GC needs to track it, but it
	// needs to be set and cleared from assembly, where it's
	// difficult to have write barriers. However, ctxt is really a
	// saved, live register, and we only ever exchange it between
	// the real register and the gobuf. Hence, we treat it as a
	// root during stack scanning, which means assembly that saves
	// and restores it doesn't need write barriers. It's still
	// typed as a pointer so that any other writes from Go get
	// write barriers.
	sp   uintptr
	pc   uintptr
	g    guintptr
	ctxt unsafe.Pointer
	ret  uintptr
	lr   uintptr
	bp   uintptr // for framepointer-enabled architectures
}
```

### <a id="sudog" href="#sudog">type sudog struct</a>

```
searchKey: runtime.sudog
```

```Go
type sudog struct {
	g *g

	next *sudog
	prev *sudog
	elem unsafe.Pointer // data element (may point to stack)

	acquiretime int64
	releasetime int64
	ticket      uint32

	// isSelect indicates g is participating in a select, so
	// g.selectDone must be CAS'd to win the wake-up race.
	isSelect bool

	// success indicates whether communication over channel c
	// succeeded. It is true if the goroutine was awoken because a
	// value was delivered over channel c, and false if awoken
	// because c was closed.
	success bool

	parent   *sudog // semaRoot binary tree
	waitlink *sudog // g.waiting list or semaRoot
	waittail *sudog // semaRoot
	c        *hchan // channel
}
```

sudog represents a g in a wait list, such as for sending/receiving on a channel. 

sudog is necessary because the g ↔ synchronization object relation is many-to-many. A g can be on many wait lists, so there may be many sudogs for one g; and many gs may be waiting on the same synchronization object, so there may be many sudogs for one object. 

sudogs are allocated from a special pool. Use acquireSudog and releaseSudog to allocate and free them. 

#### <a id="acquireSudog" href="#acquireSudog">func acquireSudog() *sudog</a>

```
searchKey: runtime.acquireSudog
```

```Go
func acquireSudog() *sudog
```

### <a id="libcall" href="#libcall">type libcall struct</a>

```
searchKey: runtime.libcall
```

```Go
type libcall struct {
	fn   uintptr
	n    uintptr // number of parameters
	args uintptr // parameters
	r1   uintptr // return values
	r2   uintptr
	err  uintptr // error number
}
```

### <a id="stack" href="#stack">type stack struct</a>

```
searchKey: runtime.stack
```

```Go
type stack struct {
	lo uintptr
	hi uintptr
}
```

Stack describes a Go execution stack. The bounds of the stack are exactly [lo, hi), with no implicit data structures on either side. 

#### <a id="stackalloc" href="#stackalloc">func stackalloc(n uint32) stack</a>

```
searchKey: runtime.stackalloc
```

```Go
func stackalloc(n uint32) stack
```

stackalloc allocates an n byte stack. 

stackalloc must run on the system stack because it uses per-P resources and must not split the stack. 

### <a id="heldLockInfo" href="#heldLockInfo">type heldLockInfo struct</a>

```
searchKey: runtime.heldLockInfo
```

```Go
type heldLockInfo struct {
	lockAddr uintptr
	rank     lockRank
}
```

heldLockInfo gives info on a held lock and the rank of that lock 

### <a id="g" href="#g">type g struct</a>

```
searchKey: runtime.g
```

```Go
type g struct {
	// Stack parameters.
	// stack describes the actual stack memory: [stack.lo, stack.hi).
	// stackguard0 is the stack pointer compared in the Go stack growth prologue.
	// It is stack.lo+StackGuard normally, but can be StackPreempt to trigger a preemption.
	// stackguard1 is the stack pointer compared in the C stack growth prologue.
	// It is stack.lo+StackGuard on g0 and gsignal stacks.
	// It is ~0 on other goroutine stacks, to trigger a call to morestackc (and crash).
	stack       stack   // offset known to runtime/cgo
	stackguard0 uintptr // offset known to liblink
	stackguard1 uintptr // offset known to liblink

	_panic    *_panic // innermost panic - offset known to liblink
	_defer    *_defer // innermost defer
	m         *m      // current m; offset known to arm liblink
	sched     gobuf
	syscallsp uintptr // if status==Gsyscall, syscallsp = sched.sp to use during gc
	syscallpc uintptr // if status==Gsyscall, syscallpc = sched.pc to use during gc
	stktopsp  uintptr // expected sp at top of stack, to check in traceback
	// param is a generic pointer parameter field used to pass
	// values in particular contexts where other storage for the
	// parameter would be difficult to find. It is currently used
	// in three ways:
	// 1. When a channel operation wakes up a blocked goroutine, it sets param to
	//    point to the sudog of the completed blocking operation.
	// 2. By gcAssistAlloc1 to signal back to its caller that the goroutine completed
	//    the GC cycle. It is unsafe to do so in any other way, because the goroutine's
	//    stack may have moved in the meantime.
	// 3. By debugCallWrap to pass parameters to a new goroutine because allocating a
	//    closure in the runtime is forbidden.
	param        unsafe.Pointer
	atomicstatus uint32
	stackLock    uint32 // sigprof/scang lock; TODO: fold in to atomicstatus
	goid         int64
	schedlink    guintptr
	waitsince    int64      // approx time when the g become blocked
	waitreason   waitReason // if status==Gwaiting

	preempt       bool // preemption signal, duplicates stackguard0 = stackpreempt
	preemptStop   bool // transition to _Gpreempted on preemption; otherwise, just deschedule
	preemptShrink bool // shrink stack at synchronous safe point

	// asyncSafePoint is set if g is stopped at an asynchronous
	// safe point. This means there are frames on the stack
	// without precise pointer information.
	asyncSafePoint bool

	paniconfault bool // panic (instead of crash) on unexpected fault address
	gcscandone   bool // g has scanned stack; protected by _Gscan bit in status
	throwsplit   bool // must not split stack
	// activeStackChans indicates that there are unlocked channels
	// pointing into this goroutine's stack. If true, stack
	// copying needs to acquire channel locks to protect these
	// areas of the stack.
	activeStackChans bool
	// parkingOnChan indicates that the goroutine is about to
	// park on a chansend or chanrecv. Used to signal an unsafe point
	// for stack shrinking. It's a boolean value, but is updated atomically.
	parkingOnChan uint8

	raceignore     int8     // ignore race detection events
	sysblocktraced bool     // StartTrace has emitted EvGoInSyscall about this goroutine
	tracking       bool     // whether we're tracking this G for sched latency statistics
	trackingSeq    uint8    // used to decide whether to track this G
	runnableStamp  int64    // timestamp of when the G last became runnable, only used when tracking
	runnableTime   int64    // the amount of time spent runnable, cleared when running, only used when tracking
	sysexitticks   int64    // cputicks when syscall has returned (for tracing)
	traceseq       uint64   // trace event sequencer
	tracelastp     puintptr // last P emitted an event for this goroutine
	lockedm        muintptr
	sig            uint32
	writebuf       []byte
	sigcode0       uintptr
	sigcode1       uintptr
	sigpc          uintptr
	gopc           uintptr         // pc of go statement that created this goroutine
	ancestors      *[]ancestorInfo // ancestor information goroutine(s) that created this goroutine (only used if debug.tracebackancestors)
	startpc        uintptr         // pc of goroutine function
	racectx        uintptr
	waiting        *sudog         // sudog structures this g is waiting on (that have a valid elem ptr); in lock order
	cgoCtxt        []uintptr      // cgo traceback context
	labels         unsafe.Pointer // profiler labels
	timer          *timer         // cached timer for time.Sleep
	selectDone     uint32         // are we participating in a select and did someone win the race?

	// gcAssistBytes is this G's GC assist credit in terms of
	// bytes allocated. If this is positive, then the G has credit
	// to allocate gcAssistBytes bytes without assisting. If this
	// is negative, then the G must correct this by performing
	// scan work. We track this in bytes to make it fast to update
	// and check for debt in the malloc hot path. The assist ratio
	// determines how this corresponds to scan work debt.
	gcAssistBytes int64
}
```

#### <a id="beforeIdle" href="#beforeIdle">func beforeIdle(int64, int64) (*g, bool)</a>

```
searchKey: runtime.beforeIdle
```

```Go
func beforeIdle(int64, int64) (*g, bool)
```

#### <a id="wakefing" href="#wakefing">func wakefing() *g</a>

```
searchKey: runtime.wakefing
```

```Go
func wakefing() *g
```

#### <a id="netpollunblock" href="#netpollunblock">func netpollunblock(pd *pollDesc, mode int32, ioready bool) *g</a>

```
searchKey: runtime.netpollunblock
```

```Go
func netpollunblock(pd *pollDesc, mode int32, ioready bool) *g
```

#### <a id="atomicAllG" href="#atomicAllG">func atomicAllG() (**g, uintptr)</a>

```
searchKey: runtime.atomicAllG
```

```Go
func atomicAllG() (**g, uintptr)
```

atomicAllG returns &allgs[0] and len(allgs) for use with atomicAllGIndex. 

#### <a id="atomicAllGIndex" href="#atomicAllGIndex">func atomicAllGIndex(ptr **g, i uintptr) *g</a>

```
searchKey: runtime.atomicAllGIndex
```

```Go
func atomicAllGIndex(ptr **g, i uintptr) *g
```

atomicAllGIndex returns ptr[i] with the allgptr returned from atomicAllG. 

#### <a id="findrunnable" href="#findrunnable">func findrunnable() (gp *g, inheritTime bool)</a>

```
searchKey: runtime.findrunnable
```

```Go
func findrunnable() (gp *g, inheritTime bool)
```

Finds a runnable goroutine to execute. Tries to steal from other P's, get g from local or global queue, poll network. 

#### <a id="stealWork" href="#stealWork">func stealWork(now int64) (gp *g, inheritTime bool, rnow, pollUntil int64, newWork bool)</a>

```
searchKey: runtime.stealWork
```

```Go
func stealWork(now int64) (gp *g, inheritTime bool, rnow, pollUntil int64, newWork bool)
```

stealWork attempts to steal a runnable goroutine or timer from any P. 

If newWork is true, new work may have been readied. 

If now is not 0 it is the current time. stealWork returns the passed time or the current time if now was passed as 0. 

#### <a id="checkIdleGCNoP" href="#checkIdleGCNoP">func checkIdleGCNoP() (*p, *g)</a>

```
searchKey: runtime.checkIdleGCNoP
```

```Go
func checkIdleGCNoP() (*p, *g)
```

Check for idle-priority GC, without a P on entry. 

If some GC work, a P, and a worker G are all available, the P and G will be returned. The returned P has not been wired yet. 

#### <a id="malg" href="#malg">func malg(stacksize int32) *g</a>

```
searchKey: runtime.malg
```

```Go
func malg(stacksize int32) *g
```

Allocate a new g, with a stack big enough for stacksize bytes. 

#### <a id="newproc1" href="#newproc1">func newproc1(fn *funcval, argp unsafe.Pointer, narg int32, callergp *g, callerpc uintptr) *g</a>

```
searchKey: runtime.newproc1
```

```Go
func newproc1(fn *funcval, argp unsafe.Pointer, narg int32, callergp *g, callerpc uintptr) *g
```

Create a new g in state _Grunnable, starting at fn, with narg bytes of arguments starting at argp. callerpc is the address of the go statement that created this. The caller is responsible for adding the new g to the scheduler. 

This must run on the system stack because it's the continuation of newproc, which cannot split the stack. 

#### <a id="gfget" href="#gfget">func gfget(_p_ *p) *g</a>

```
searchKey: runtime.gfget
```

```Go
func gfget(_p_ *p) *g
```

Get from gfree list. If local list is empty, grab a batch from global list. 

#### <a id="globrunqget" href="#globrunqget">func globrunqget(_p_ *p, max int32) *g</a>

```
searchKey: runtime.globrunqget
```

```Go
func globrunqget(_p_ *p, max int32) *g
```

Try get a batch of G's from the global runnable queue. sched.lock must be held. 

#### <a id="runqget" href="#runqget">func runqget(_p_ *p) (gp *g, inheritTime bool)</a>

```
searchKey: runtime.runqget
```

```Go
func runqget(_p_ *p) (gp *g, inheritTime bool)
```

Get g from local runnable queue. If inheritTime is true, gp should inherit the remaining time in the current time slice. Otherwise, it should start a new time slice. Executed only by the owner P. 

#### <a id="runqsteal" href="#runqsteal">func runqsteal(_p_, p2 *p, stealRunNextG bool) *g</a>

```
searchKey: runtime.runqsteal
```

```Go
func runqsteal(_p_, p2 *p, stealRunNextG bool) *g
```

Steal half of elements from local runnable queue of p2 and put onto local runnable queue of p. Returns one of the stolen elements (or nil if failed). 

#### <a id="sigFetchG" href="#sigFetchG">func sigFetchG(c *sigctxt) *g</a>

```
searchKey: runtime.sigFetchG
```

```Go
func sigFetchG(c *sigctxt) *g
```

sigFetchG fetches the value of G safely when running in a signal handler. On some architectures, the g value may be clobbered when running in a VDSO. See issue #32912. 

#### <a id="getg" href="#getg">func getg() *g</a>

```
searchKey: runtime.getg
```

```Go
func getg() *g
```

getg returns the pointer to the current g. The compiler rewrites calls to this function into instructions that fetch the g directly (from TLS or from the dedicated register). 

#### <a id="traceReader" href="#traceReader">func traceReader() *g</a>

```
searchKey: runtime.traceReader
```

```Go
func traceReader() *g
```

traceReader returns the trace reader that should be woken up, if any. 

#### <a id="Getg" href="#Getg">func Getg() *G</a>

```
searchKey: runtime.Getg
```

```Go
func Getg() *G
```

### <a id="m" href="#m">type m struct</a>

```
searchKey: runtime.m
```

```Go
type m struct {
	g0      *g     // goroutine with scheduling stack
	morebuf gobuf  // gobuf arg to morestack
	divmod  uint32 // div/mod denominator for arm - known to liblink

	// Fields not known to debuggers.
	procid        uint64            // for debuggers, but offset not hard-coded
	gsignal       *g                // signal-handling g
	goSigStack    gsignalStack      // Go-allocated signal handling stack
	sigmask       sigset            // storage for saved signal mask
	tls           [tlsSlots]uintptr // thread-local storage (for x86 extern register)
	mstartfn      func()
	curg          *g       // current running goroutine
	caughtsig     guintptr // goroutine running during fatal signal
	p             puintptr // attached p for executing go code (nil if not executing go code)
	nextp         puintptr
	oldp          puintptr // the p that was attached before executing a syscall
	id            int64
	mallocing     int32
	throwing      int32
	preemptoff    string // if != "", keep curg running on this m
	locks         int32
	dying         int32
	profilehz     int32
	spinning      bool // m is out of work and is actively looking for work
	blocked       bool // m is blocked on a note
	newSigstack   bool // minit on C thread called sigaltstack
	printlock     int8
	incgo         bool   // m is executing a cgo call
	freeWait      uint32 // if == 0, safe to free g0 and delete m (atomic)
	fastrand      [2]uint32
	needextram    bool
	traceback     uint8
	ncgocall      uint64      // number of cgo calls in total
	ncgo          int32       // number of cgo calls currently in progress
	cgoCallersUse uint32      // if non-zero, cgoCallers in use temporarily
	cgoCallers    *cgoCallers // cgo traceback if crashing in cgo call
	doesPark      bool        // non-P running threads: sysmon and newmHandoff never use .park
	park          note
	alllink       *m // on allm
	schedlink     muintptr
	lockedg       guintptr
	createstack   [32]uintptr // stack that created this thread.
	lockedExt     uint32      // tracking for external LockOSThread
	lockedInt     uint32      // tracking for internal lockOSThread
	nextwaitm     muintptr    // next m waiting for lock
	waitunlockf   func(*g, unsafe.Pointer) bool
	waitlock      unsafe.Pointer
	waittraceev   byte
	waittraceskip int
	startingtrace bool
	syscalltick   uint32
	freelink      *m // on sched.freem

	// mFixup is used to synchronize OS related m state
	// (credentials etc) use mutex to access. To avoid deadlocks
	// an atomic.Load() of used being zero in mDoFixupFn()
	// guarantees fn is nil.
	mFixup struct {
		lock mutex
		used uint32
		fn   func(bool) bool
	}

	// these are here because they are too large to be on the stack
	// of low-level NOSPLIT functions.
	libcall   libcall
	libcallpc uintptr // for cpu profiler
	libcallsp uintptr
	libcallg  guintptr
	syscall   libcall // stores syscall parameters on windows

	vdsoSP uintptr // SP for traceback while in VDSO call (0 if not in call)
	vdsoPC uintptr // PC for traceback while in VDSO call

	// preemptGen counts the number of completed preemption
	// signals. This is used to detect when a preemption is
	// requested, but fails. Accessed atomically.
	preemptGen uint32

	// Whether this is a pending preemption signal on this M.
	// Accessed atomically.
	signalPending uint32

	dlogPerM

	mOS

	// Up to 10 locks held by this m, maintained by the lock ranking code.
	locksHeldLen int
	locksHeld    [10]heldLockInfo
}
```

#### <a id="allocm" href="#allocm">func allocm(_p_ *p, fn func(), id int64) *m</a>

```
searchKey: runtime.allocm
```

```Go
func allocm(_p_ *p, fn func(), id int64) *m
```

Allocate a new m unassociated with any thread. Can use p for allocation context if needed. fn is recorded as the new m's m.mstartfn. id is optional pre-allocated m ID. Omit by passing -1. 

This function is allowed to have write barriers even if the caller isn't because it borrows _p_. 

#### <a id="lockextra" href="#lockextra">func lockextra(nilokay bool) *m</a>

```
searchKey: runtime.lockextra
```

```Go
func lockextra(nilokay bool) *m
```

lockextra locks the extra list and returns the list head. The caller must unlock the list by storing a new list head to extram. If nilokay is true, then lockextra will return a nil list head if that's what it finds. If nilokay is false, lockextra will keep waiting until the list head is no longer nil. 

#### <a id="mget" href="#mget">func mget() *m</a>

```
searchKey: runtime.mget
```

```Go
func mget() *m
```

Try to get an m from midle list. sched.lock must be held. May run during STW, so write barriers are not allowed. 

#### <a id="acquirem" href="#acquirem">func acquirem() *m</a>

```
searchKey: runtime.acquirem
```

```Go
func acquirem() *m
```

#### <a id="traceAcquireBuffer" href="#traceAcquireBuffer">func traceAcquireBuffer() (mp *m, pid int32, bufp *traceBufPtr)</a>

```
searchKey: runtime.traceAcquireBuffer
```

```Go
func traceAcquireBuffer() (mp *m, pid int32, bufp *traceBufPtr)
```

traceAcquireBuffer returns trace buffer to use and, if necessary, locks it. 

### <a id="p" href="#p">type p struct</a>

```
searchKey: runtime.p
```

```Go
type p struct {
	id          int32
	status      uint32 // one of pidle/prunning/...
	link        puintptr
	schedtick   uint32     // incremented on every scheduler call
	syscalltick uint32     // incremented on every system call
	sysmontick  sysmontick // last tick observed by sysmon
	m           muintptr   // back-link to associated m (nil if idle)
	mcache      *mcache
	pcache      pageCache
	raceprocctx uintptr

	deferpool    [5][]*_defer // pool of available defer structs of different sizes (see panic.go)
	deferpoolbuf [5][32]*_defer

	// Cache of goroutine ids, amortizes accesses to runtime·sched.goidgen.
	goidcache    uint64
	goidcacheend uint64

	// Queue of runnable goroutines. Accessed without lock.
	runqhead uint32
	runqtail uint32
	runq     [256]guintptr
	// runnext, if non-nil, is a runnable G that was ready'd by
	// the current G and should be run next instead of what's in
	// runq if there's time remaining in the running G's time
	// slice. It will inherit the time left in the current time
	// slice. If a set of goroutines is locked in a
	// communicate-and-wait pattern, this schedules that set as a
	// unit and eliminates the (potentially large) scheduling
	// latency that otherwise arises from adding the ready'd
	// goroutines to the end of the run queue.
	//
	// Note that while other P's may atomically CAS this to zero,
	// only the owner P can CAS it to a valid G.
	runnext guintptr

	// Available G's (status == Gdead)
	gFree struct {
		gList
		n int32
	}

	sudogcache []*sudog
	sudogbuf   [128]*sudog

	// Cache of mspan objects from the heap.
	mspancache struct {
		// We need an explicit length here because this field is used
		// in allocation codepaths where write barriers are not allowed,
		// and eliminating the write barrier/keeping it eliminated from
		// slice updates is tricky, moreso than just managing the length
		// ourselves.
		len int
		buf [128]*mspan
	}

	tracebuf traceBufPtr

	// traceSweep indicates the sweep events should be traced.
	// This is used to defer the sweep start event until a span
	// has actually been swept.
	traceSweep bool
	// traceSwept and traceReclaimed track the number of bytes
	// swept and reclaimed by sweeping in the current sweep loop.
	traceSwept, traceReclaimed uintptr

	palloc persistentAlloc // per-P to avoid mutex

	_ uint32 // Alignment for atomic fields below

	// The when field of the first entry on the timer heap.
	// This is updated using atomic functions.
	// This is 0 if the timer heap is empty.
	timer0When uint64

	// The earliest known nextwhen field of a timer with
	// timerModifiedEarlier status. Because the timer may have been
	// modified again, there need not be any timer with this value.
	// This is updated using atomic functions.
	// This is 0 if the value is unknown.
	timerModifiedEarliest uint64

	// Per-P GC state
	gcAssistTime         int64 // Nanoseconds in assistAlloc
	gcFractionalMarkTime int64 // Nanoseconds in fractional mark worker (atomic)

	// gcMarkWorkerMode is the mode for the next mark worker to run in.
	// That is, this is used to communicate with the worker goroutine
	// selected for immediate execution by
	// gcController.findRunnableGCWorker. When scheduling other goroutines,
	// this field must be set to gcMarkWorkerNotWorker.
	gcMarkWorkerMode gcMarkWorkerMode
	// gcMarkWorkerStartTime is the nanotime() at which the most recent
	// mark worker started.
	gcMarkWorkerStartTime int64

	// gcw is this P's GC work buffer cache. The work buffer is
	// filled by write barriers, drained by mutator assists, and
	// disposed on certain GC state transitions.
	gcw gcWork

	// wbBuf is this P's GC write barrier buffer.
	//
	// TODO: Consider caching this in the running G.
	wbBuf wbBuf

	runSafePointFn uint32 // if 1, run sched.safePointFn at next safe point

	// statsSeq is a counter indicating whether this P is currently
	// writing any stats. Its value is even when not, odd when it is.
	statsSeq uint32

	// Lock for timers. We normally access the timers while running
	// on this P, but the scheduler can also do it from a different P.
	timersLock mutex

	// Actions to take at some time. This is used to implement the
	// standard library's time package.
	// Must hold timersLock to access.
	timers []*timer

	// Number of timers in P's heap.
	// Modified using atomic instructions.
	numTimers uint32

	// Number of timerModifiedEarlier timers on P's heap.
	// This should only be modified while holding timersLock,
	// or while the timer status is in a transient state
	// such as timerModifying.
	adjustTimers uint32

	// Number of timerDeleted timers in P's heap.
	// Modified using atomic instructions.
	deletedTimers uint32

	// Race context used while executing timer functions.
	timerRaceCtx uintptr

	// preempt is set to indicate that this P should be enter the
	// scheduler ASAP (regardless of what G is running on it).
	preempt bool
}
```

#### <a id="checkRunqsNoP" href="#checkRunqsNoP">func checkRunqsNoP(allpSnapshot []*p, idlepMaskSnapshot pMask) *p</a>

```
searchKey: runtime.checkRunqsNoP
```

```Go
func checkRunqsNoP(allpSnapshot []*p, idlepMaskSnapshot pMask) *p
```

Check all Ps for a runnable G to steal. 

On entry we have no P. If a G is available to steal and a P is available, the P is returned which the caller should acquire and attempt to steal the work to. 

#### <a id="checkIdleGCNoP" href="#checkIdleGCNoP">func checkIdleGCNoP() (*p, *g)</a>

```
searchKey: runtime.checkIdleGCNoP
```

```Go
func checkIdleGCNoP() (*p, *g)
```

Check for idle-priority GC, without a P on entry. 

If some GC work, a P, and a worker G are all available, the P and G will be returned. The returned P has not been wired yet. 

#### <a id="procresize" href="#procresize">func procresize(nprocs int32) *p</a>

```
searchKey: runtime.procresize
```

```Go
func procresize(nprocs int32) *p
```

Change number of processors. 

sched.lock must be held, and the world must be stopped. 

gcworkbufs must not be being modified by either the GC or the write barrier code, so the GC must not be running if the number of Ps actually changes. 

Returns list of Ps with local work, they need to be scheduled by the caller. 

#### <a id="releasep" href="#releasep">func releasep() *p</a>

```
searchKey: runtime.releasep
```

```Go
func releasep() *p
```

Disassociate p and the current m. 

#### <a id="pidleget" href="#pidleget">func pidleget() *p</a>

```
searchKey: runtime.pidleget
```

```Go
func pidleget() *p
```

pidleget tries to get a p from the _Pidle list, acquiring ownership. 

sched.lock must be held. 

May run during STW, so write barriers are not allowed. 

#### <a id="timeSleepUntil" href="#timeSleepUntil">func timeSleepUntil() (int64, *p)</a>

```
searchKey: runtime.timeSleepUntil
```

```Go
func timeSleepUntil() (int64, *p)
```

timeSleepUntil returns the time when the next timer should fire, and the P that holds the timer heap that that timer is on. This is only called by sysmon and checkdead. 

#### <a id="p.init" href="#p.init">func (pp *p) init(id int32)</a>

```
searchKey: runtime.p.init
```

```Go
func (pp *p) init(id int32)
```

init initializes pp, which may be a freshly allocated p or a previously destroyed p, and transitions it to status _Pgcstop. 

#### <a id="p.destroy" href="#p.destroy">func (pp *p) destroy()</a>

```
searchKey: runtime.p.destroy
```

```Go
func (pp *p) destroy()
```

destroy releases all of the resources associated with pp and transitions it to status _Pdead. 

sched.lock must be held and the world must be stopped. 

### <a id="schedt" href="#schedt">type schedt struct</a>

```
searchKey: runtime.schedt
```

```Go
type schedt struct {
	// accessed atomically. keep at top to ensure alignment on 32-bit systems.
	goidgen   uint64
	lastpoll  uint64 // time of last network poll, 0 if currently polling
	pollUntil uint64 // time to which current poll is sleeping

	lock mutex

	midle        muintptr // idle m's waiting for work
	nmidle       int32    // number of idle m's waiting for work
	nmidlelocked int32    // number of locked m's waiting for work
	mnext        int64    // number of m's that have been created and next M ID
	maxmcount    int32    // maximum number of m's allowed (or die)
	nmsys        int32    // number of system m's not counted for deadlock
	nmfreed      int64    // cumulative number of freed m's

	ngsys uint32 // number of system goroutines; updated atomically

	pidle      puintptr // idle p's
	npidle     uint32
	nmspinning uint32 // See "Worker thread parking/unparking" comment in proc.go.

	// Global runnable queue.
	runq     gQueue
	runqsize int32

	// disable controls selective disabling of the scheduler.
	//
	// Use schedEnableUser to control this.
	//
	// disable is protected by sched.lock.
	disable struct {
		// user disables scheduling of user goroutines.
		user     bool
		runnable gQueue // pending runnable Gs
		n        int32  // length of runnable
	}

	// Global cache of dead G's.
	gFree struct {
		lock    mutex
		stack   gList // Gs with stacks
		noStack gList // Gs without stacks
		n       int32
	}

	// Central cache of sudog structs.
	sudoglock  mutex
	sudogcache *sudog

	// Central pool of available defer structs of different sizes.
	deferlock mutex
	deferpool [5]*_defer

	// freem is the list of m's waiting to be freed when their
	// m.exited is set. Linked through m.freelink.
	freem *m

	gcwaiting  uint32 // gc is waiting to run
	stopwait   int32
	stopnote   note
	sysmonwait uint32
	sysmonnote note

	// While true, sysmon not ready for mFixup calls.
	// Accessed atomically.
	sysmonStarting uint32

	// safepointFn should be called on each P at the next GC
	// safepoint if p.runSafePointFn is set.
	safePointFn   func(*p)
	safePointWait int32
	safePointNote note

	profilehz int32 // cpu profiling rate

	procresizetime int64 // nanotime() of last change to gomaxprocs
	totaltime      int64 // ∫gomaxprocs dt up to procresizetime

	// sysmonlock protects sysmon's actions on the runtime.
	//
	// Acquire and hold this mutex to block sysmon from interacting
	// with the rest of the runtime.
	sysmonlock mutex

	_ uint32 // ensure timeToRun has 8-byte alignment

	// timeToRun is a distribution of scheduling latencies, defined
	// as the sum of time a G spends in the _Grunnable state before
	// it transitions to _Grunning.
	//
	// timeToRun is protected by sched.lock.
	timeToRun timeHistogram
}
```

### <a id="_func" href="#_func">type _func struct</a>

```
searchKey: runtime._func
```

```Go
type _func struct {
	entry   uintptr // start pc
	nameoff int32   // function name

	args        int32  // in/out args size
	deferreturn uint32 // offset of start of a deferreturn call instruction from entry, if any.

	pcsp      uint32
	pcfile    uint32
	pcln      uint32
	npcdata   uint32
	cuOffset  uint32 // runtime.cutab offset of this function's CU
	funcID    funcID // set for certain special runtime functions
	flag      funcFlag
	_         [1]byte // pad
	nfuncdata uint8   // must be last, must end on a uint32-aligned boundary
}
```

Layout of in-memory per-function information prepared by linker See [https://golang.org/s/go12symtab](https://golang.org/s/go12symtab). Keep in sync with linker (../cmd/link/internal/ld/pcln.go:/pclntab) and with package debug/gosym and with symtab.go in package runtime. 

### <a id="funcinl" href="#funcinl">type funcinl struct</a>

```
searchKey: runtime.funcinl
```

```Go
type funcinl struct {
	zero  uintptr // set to 0 to distinguish from _func
	entry uintptr // entry of the real (the "outermost") frame.
	name  string
	file  string
	line  int
}
```

Pseudo-Func that is returned for PCs that occur in inlined code. A *Func can be either a *_func or a *funcinl, and they are distinguished by the first uintptr. 

### <a id="itab" href="#itab">type itab struct</a>

```
searchKey: runtime.itab
```

```Go
type itab struct {
	inter *interfacetype
	_type *_type
	hash  uint32 // copy of _type.hash. Used for type switches.
	_     [4]byte
	fun   [1]uintptr // variable sized. fun[0]==0 means _type does not implement inter.
}
```

layout of Itab known to compilers allocated in non-garbage-collected memory Needs to be in sync with ../cmd/compile/internal/gc/reflect.go:/^func.WriteTabs. 

#### <a id="getitab" href="#getitab">func getitab(inter *interfacetype, typ *_type, canfail bool) *itab</a>

```
searchKey: runtime.getitab
```

```Go
func getitab(inter *interfacetype, typ *_type, canfail bool) *itab
```

#### <a id="assertI2I" href="#assertI2I">func assertI2I(inter *interfacetype, tab *itab) *itab</a>

```
searchKey: runtime.assertI2I
```

```Go
func assertI2I(inter *interfacetype, tab *itab) *itab
```

#### <a id="assertE2I" href="#assertE2I">func assertE2I(inter *interfacetype, t *_type) *itab</a>

```
searchKey: runtime.assertE2I
```

```Go
func assertE2I(inter *interfacetype, t *_type) *itab
```

#### <a id="itab.init" href="#itab.init">func (m *itab) init() string</a>

```
searchKey: runtime.itab.init
```

```Go
func (m *itab) init() string
```

init fills in the m.fun array with all the code pointers for the m.inter/m._type pair. If the type does not implement the interface, it sets m.fun[0] to 0 and returns the name of an interface function that is missing. It is ok to call this multiple times on the same m, even concurrently. 

### <a id="lfnode" href="#lfnode">type lfnode struct</a>

```
searchKey: runtime.lfnode
```

```Go
type lfnode struct {
	next    uint64
	pushcnt uintptr
}
```

Lock-free stack node. Also known to export_test.go. 

#### <a id="lfstackUnpack" href="#lfstackUnpack">func lfstackUnpack(val uint64) *lfnode</a>

```
searchKey: runtime.lfstackUnpack
```

```Go
func lfstackUnpack(val uint64) *lfnode
```

### <a id="forcegcstate" href="#forcegcstate">type forcegcstate struct</a>

```
searchKey: runtime.forcegcstate
```

```Go
type forcegcstate struct {
	lock mutex
	g    *g
	idle uint32
}
```

### <a id="_defer" href="#_defer">type _defer struct</a>

```
searchKey: runtime._defer
```

```Go
type _defer struct {
	siz     int32 // includes both arguments and results
	started bool
	heap    bool
	// openDefer indicates that this _defer is for a frame with open-coded
	// defers. We have only one defer record for the entire frame (which may
	// currently have 0, 1, or more defers active).
	openDefer bool
	sp        uintptr  // sp at time of defer
	pc        uintptr  // pc at time of defer
	fn        *funcval // can be nil for open-coded defers
	_panic    *_panic  // panic that is running defer
	link      *_defer

	// If openDefer is true, the fields below record values about the stack
	// frame and associated function that has the open-coded defer(s). sp
	// above will be the sp for the frame, and pc will be address of the
	// deferreturn call in the function.
	fd   unsafe.Pointer // funcdata for the function associated with the frame
	varp uintptr        // value of varp for the stack frame
	// framepc is the current pc associated with the stack frame. Together,
	// with sp above (which is the sp associated with the stack frame),
	// framepc/sp can be used as pc/sp pair to continue a stack trace via
	// gentraceback().
	framepc uintptr
}
```

A _defer holds an entry on the list of deferred calls. If you add a field here, add code to clear it in freedefer and deferProcStack This struct must match the code in cmd/compile/internal/gc/reflect.go:deferstruct and cmd/compile/internal/gc/ssa.go:(*state).call. Some defers will be allocated on the stack and some on the heap. All defers are logically part of the stack, so write barriers to initialize them are not required. All defers must be manually scanned, and for heap defers, marked. 

#### <a id="newdefer" href="#newdefer">func newdefer(siz int32) *_defer</a>

```
searchKey: runtime.newdefer
```

```Go
func newdefer(siz int32) *_defer
```

Allocate a Defer, usually using per-P pool. Each defer must be released with freedefer.  The defer is not added to any defer chain yet. 

This must not grow the stack because there may be a frame without stack map information when this is called. 

### <a id="_panic" href="#_panic">type _panic struct</a>

```
searchKey: runtime._panic
```

```Go
type _panic struct {
	argp      unsafe.Pointer // pointer to arguments of deferred call run during panic; cannot move - known to liblink
	arg       interface{}    // argument to panic
	link      *_panic        // link to earlier panic
	pc        uintptr        // where to return to in runtime if this panic is bypassed
	sp        unsafe.Pointer // where to return to in runtime if this panic is bypassed
	recovered bool           // whether this panic is over
	aborted   bool           // the panic was aborted
	goexit    bool
}
```

A _panic holds information about an active panic. 

A _panic value must only ever live on the stack. 

The argp and link fields are stack pointers, but don't need special handling during stack growth: because they are pointer-typed and _panic values only live on the stack, regular stack pointer adjustment takes care of them. 

### <a id="stkframe" href="#stkframe">type stkframe struct</a>

```
searchKey: runtime.stkframe
```

```Go
type stkframe struct {
	fn       funcInfo   // function being run
	pc       uintptr    // program counter within fn
	continpc uintptr    // program counter where execution can continue, or 0 if not
	lr       uintptr    // program counter at caller aka link register
	sp       uintptr    // stack pointer at pc
	fp       uintptr    // stack pointer at caller aka frame pointer
	varp     uintptr    // top of local variables
	argp     uintptr    // pointer to function arguments
	arglen   uintptr    // number of bytes at argp
	argmap   *bitvector // force use of this argmap
}
```

stack traces 

### <a id="ancestorInfo" href="#ancestorInfo">type ancestorInfo struct</a>

```
searchKey: runtime.ancestorInfo
```

```Go
type ancestorInfo struct {
	pcs  []uintptr // pcs from the stack of this goroutine
	goid int64     // goroutine id of this goroutine; original goroutine possibly dead
	gopc uintptr   // pc of go statement that created this goroutine
}
```

ancestorInfo records details of where a goroutine was started. 

### <a id="waitReason" href="#waitReason">type waitReason uint8</a>

```
searchKey: runtime.waitReason
```

```Go
type waitReason uint8
```

A waitReason explains why a goroutine has been stopped. See gopark. Do not re-use waitReasons, add new ones. 

#### <a id="waitReason.String" href="#waitReason.String">func (w waitReason) String() string</a>

```
searchKey: runtime.waitReason.String
```

```Go
func (w waitReason) String() string
```

### <a id="rwmutex" href="#rwmutex">type rwmutex struct</a>

```
searchKey: runtime.rwmutex
```

```Go
type rwmutex struct {
	rLock      mutex    // protects readers, readerPass, writer
	readers    muintptr // list of pending readers
	readerPass uint32   // number of pending readers to skip readers list

	wLock  mutex    // serializes writers
	writer muintptr // pending writer waiting for completing readers

	readerCount uint32 // number of pending readers
	readerWait  uint32 // number of departing readers
}
```

A rwmutex is a reader/writer mutual exclusion lock. The lock can be held by an arbitrary number of readers or a single writer. This is a variant of sync.RWMutex, for the runtime package. Like mutex, rwmutex blocks the calling M. It does not interact with the goroutine scheduler. 

#### <a id="rwmutex.rlock" href="#rwmutex.rlock">func (rw *rwmutex) rlock()</a>

```
searchKey: runtime.rwmutex.rlock
```

```Go
func (rw *rwmutex) rlock()
```

rlock locks rw for reading. 

#### <a id="rwmutex.runlock" href="#rwmutex.runlock">func (rw *rwmutex) runlock()</a>

```
searchKey: runtime.rwmutex.runlock
```

```Go
func (rw *rwmutex) runlock()
```

runlock undoes a single rlock call on rw. 

#### <a id="rwmutex.lock" href="#rwmutex.lock">func (rw *rwmutex) lock()</a>

```
searchKey: runtime.rwmutex.lock
```

```Go
func (rw *rwmutex) lock()
```

lock locks rw for writing. 

#### <a id="rwmutex.unlock" href="#rwmutex.unlock">func (rw *rwmutex) unlock()</a>

```
searchKey: runtime.rwmutex.unlock
```

```Go
func (rw *rwmutex) unlock()
```

unlock unlocks rw for writing. 

### <a id="scase" href="#scase">type scase struct</a>

```
searchKey: runtime.scase
```

```Go
type scase struct {
	c    *hchan         // chan
	elem unsafe.Pointer // data element
}
```

Select case descriptor. Known to compiler. Changes here must also be made in src/cmd/internal/gc/select.go's scasetype. 

### <a id="runtimeSelect" href="#runtimeSelect">type runtimeSelect struct</a>

```
searchKey: runtime.runtimeSelect
```

```Go
type runtimeSelect struct {
	dir selectDir
	typ unsafe.Pointer // channel type (not used here)
	ch  *hchan         // channel
	val unsafe.Pointer // ptr to data (SendDir) or ptr to receive buffer (RecvDir)
}
```

A runtimeSelect is a single case passed to rselect. This must match ../reflect/value.go:/runtimeSelect 

### <a id="selectDir" href="#selectDir">type selectDir int</a>

```
searchKey: runtime.selectDir
```

```Go
type selectDir int
```

These values must match ../reflect/value.go:/SelectDir. 

### <a id="semaRoot" href="#semaRoot">type semaRoot struct</a>

```
searchKey: runtime.semaRoot
```

```Go
type semaRoot struct {
	lock  mutex
	treap *sudog // root of balanced tree of unique waiters.
	nwait uint32 // Number of waiters. Read w/o the lock.
}
```

A semaRoot holds a balanced tree of sudog with distinct addresses (s.elem). Each of those sudog may in turn point (through s.waitlink) to a list of other sudogs waiting on the same address. The operations on the inner lists of sudogs with the same address are all O(1). The scanning of the top-level semaRoot list is O(log n), where n is the number of distinct addresses with goroutines blocked on them that hash to the given semaRoot. See golang.org/issue/17953 for a program that worked badly before we introduced the second level of list, and test/locklinear.go for a test that exercises this. 

#### <a id="semroot" href="#semroot">func semroot(addr *uint32) *semaRoot</a>

```
searchKey: runtime.semroot
```

```Go
func semroot(addr *uint32) *semaRoot
```

#### <a id="semaRoot.queue" href="#semaRoot.queue">func (root *semaRoot) queue(addr *uint32, s *sudog, lifo bool)</a>

```
searchKey: runtime.semaRoot.queue
```

```Go
func (root *semaRoot) queue(addr *uint32, s *sudog, lifo bool)
```

queue adds s to the blocked goroutines in semaRoot. 

#### <a id="semaRoot.dequeue" href="#semaRoot.dequeue">func (root *semaRoot) dequeue(addr *uint32) (found *sudog, now int64)</a>

```
searchKey: runtime.semaRoot.dequeue
```

```Go
func (root *semaRoot) dequeue(addr *uint32) (found *sudog, now int64)
```

dequeue searches for and finds the first goroutine in semaRoot blocked on addr. If the sudog was being profiled, dequeue returns the time at which it was woken up as now. Otherwise now is 0. 

#### <a id="semaRoot.rotateLeft" href="#semaRoot.rotateLeft">func (root *semaRoot) rotateLeft(x *sudog)</a>

```
searchKey: runtime.semaRoot.rotateLeft
```

```Go
func (root *semaRoot) rotateLeft(x *sudog)
```

rotateLeft rotates the tree rooted at node x. turning (x a (y b c)) into (y (x a b) c). 

#### <a id="semaRoot.rotateRight" href="#semaRoot.rotateRight">func (root *semaRoot) rotateRight(y *sudog)</a>

```
searchKey: runtime.semaRoot.rotateRight
```

```Go
func (root *semaRoot) rotateRight(y *sudog)
```

rotateRight rotates the tree rooted at node y. turning (y (x a b) c) into (x a (y b c)). 

### <a id="semaProfileFlags" href="#semaProfileFlags">type semaProfileFlags int</a>

```
searchKey: runtime.semaProfileFlags
```

```Go
type semaProfileFlags int
```

### <a id="notifyList" href="#notifyList">type notifyList struct</a>

```
searchKey: runtime.notifyList
```

```Go
type notifyList struct {
	// wait is the ticket number of the next waiter. It is atomically
	// incremented outside the lock.
	wait uint32

	// notify is the ticket number of the next waiter to be notified. It can
	// be read outside the lock, but is only written to with lock held.
	//
	// Both wait & notify can wrap around, and such cases will be correctly
	// handled as long as their "unwrapped" difference is bounded by 2^31.
	// For this not to be the case, we'd need to have 2^31+ goroutines
	// blocked on the same condvar, which is currently not possible.
	notify uint32

	// List of parked waiters.
	lock mutex
	head *sudog
	tail *sudog
}
```

notifyList is a ticket-based notification list used to implement sync.Cond. 

It must be kept in sync with the sync package. 

### <a id="sigctxt" href="#sigctxt">type sigctxt struct</a>

```
searchKey: runtime.sigctxt
```

```Go
type sigctxt struct {
	info *siginfo
	ctxt unsafe.Pointer
}
```

#### <a id="sigctxt.sigpc" href="#sigctxt.sigpc">func (c *sigctxt) sigpc() uintptr</a>

```
searchKey: runtime.sigctxt.sigpc
```

```Go
func (c *sigctxt) sigpc() uintptr
```

#### <a id="sigctxt.sigsp" href="#sigctxt.sigsp">func (c *sigctxt) sigsp() uintptr</a>

```
searchKey: runtime.sigctxt.sigsp
```

```Go
func (c *sigctxt) sigsp() uintptr
```

#### <a id="sigctxt.siglr" href="#sigctxt.siglr">func (c *sigctxt) siglr() uintptr</a>

```
searchKey: runtime.sigctxt.siglr
```

```Go
func (c *sigctxt) siglr() uintptr
```

#### <a id="sigctxt.fault" href="#sigctxt.fault">func (c *sigctxt) fault() uintptr</a>

```
searchKey: runtime.sigctxt.fault
```

```Go
func (c *sigctxt) fault() uintptr
```

#### <a id="sigctxt.preparePanic" href="#sigctxt.preparePanic">func (c *sigctxt) preparePanic(sig uint32, gp *g)</a>

```
searchKey: runtime.sigctxt.preparePanic
```

```Go
func (c *sigctxt) preparePanic(sig uint32, gp *g)
```

preparePanic sets up the stack to look like a call to sigpanic. 

#### <a id="sigctxt.pushCall" href="#sigctxt.pushCall">func (c *sigctxt) pushCall(targetPC, resumePC uintptr)</a>

```
searchKey: runtime.sigctxt.pushCall
```

```Go
func (c *sigctxt) pushCall(targetPC, resumePC uintptr)
```

#### <a id="sigctxt.regs" href="#sigctxt.regs">func (c *sigctxt) regs() *regs64</a>

```
searchKey: runtime.sigctxt.regs
```

```Go
func (c *sigctxt) regs() *regs64
```

#### <a id="sigctxt.rax" href="#sigctxt.rax">func (c *sigctxt) rax() uint64</a>

```
searchKey: runtime.sigctxt.rax
```

```Go
func (c *sigctxt) rax() uint64
```

#### <a id="sigctxt.rbx" href="#sigctxt.rbx">func (c *sigctxt) rbx() uint64</a>

```
searchKey: runtime.sigctxt.rbx
```

```Go
func (c *sigctxt) rbx() uint64
```

#### <a id="sigctxt.rcx" href="#sigctxt.rcx">func (c *sigctxt) rcx() uint64</a>

```
searchKey: runtime.sigctxt.rcx
```

```Go
func (c *sigctxt) rcx() uint64
```

#### <a id="sigctxt.rdx" href="#sigctxt.rdx">func (c *sigctxt) rdx() uint64</a>

```
searchKey: runtime.sigctxt.rdx
```

```Go
func (c *sigctxt) rdx() uint64
```

#### <a id="sigctxt.rdi" href="#sigctxt.rdi">func (c *sigctxt) rdi() uint64</a>

```
searchKey: runtime.sigctxt.rdi
```

```Go
func (c *sigctxt) rdi() uint64
```

#### <a id="sigctxt.rsi" href="#sigctxt.rsi">func (c *sigctxt) rsi() uint64</a>

```
searchKey: runtime.sigctxt.rsi
```

```Go
func (c *sigctxt) rsi() uint64
```

#### <a id="sigctxt.rbp" href="#sigctxt.rbp">func (c *sigctxt) rbp() uint64</a>

```
searchKey: runtime.sigctxt.rbp
```

```Go
func (c *sigctxt) rbp() uint64
```

#### <a id="sigctxt.rsp" href="#sigctxt.rsp">func (c *sigctxt) rsp() uint64</a>

```
searchKey: runtime.sigctxt.rsp
```

```Go
func (c *sigctxt) rsp() uint64
```

#### <a id="sigctxt.r8" href="#sigctxt.r8">func (c *sigctxt) r8() uint64</a>

```
searchKey: runtime.sigctxt.r8
```

```Go
func (c *sigctxt) r8() uint64
```

#### <a id="sigctxt.r9" href="#sigctxt.r9">func (c *sigctxt) r9() uint64</a>

```
searchKey: runtime.sigctxt.r9
```

```Go
func (c *sigctxt) r9() uint64
```

#### <a id="sigctxt.r10" href="#sigctxt.r10">func (c *sigctxt) r10() uint64</a>

```
searchKey: runtime.sigctxt.r10
```

```Go
func (c *sigctxt) r10() uint64
```

#### <a id="sigctxt.r11" href="#sigctxt.r11">func (c *sigctxt) r11() uint64</a>

```
searchKey: runtime.sigctxt.r11
```

```Go
func (c *sigctxt) r11() uint64
```

#### <a id="sigctxt.r12" href="#sigctxt.r12">func (c *sigctxt) r12() uint64</a>

```
searchKey: runtime.sigctxt.r12
```

```Go
func (c *sigctxt) r12() uint64
```

#### <a id="sigctxt.r13" href="#sigctxt.r13">func (c *sigctxt) r13() uint64</a>

```
searchKey: runtime.sigctxt.r13
```

```Go
func (c *sigctxt) r13() uint64
```

#### <a id="sigctxt.r14" href="#sigctxt.r14">func (c *sigctxt) r14() uint64</a>

```
searchKey: runtime.sigctxt.r14
```

```Go
func (c *sigctxt) r14() uint64
```

#### <a id="sigctxt.r15" href="#sigctxt.r15">func (c *sigctxt) r15() uint64</a>

```
searchKey: runtime.sigctxt.r15
```

```Go
func (c *sigctxt) r15() uint64
```

#### <a id="sigctxt.rip" href="#sigctxt.rip">func (c *sigctxt) rip() uint64</a>

```
searchKey: runtime.sigctxt.rip
```

```Go
func (c *sigctxt) rip() uint64
```

#### <a id="sigctxt.rflags" href="#sigctxt.rflags">func (c *sigctxt) rflags() uint64</a>

```
searchKey: runtime.sigctxt.rflags
```

```Go
func (c *sigctxt) rflags() uint64
```

#### <a id="sigctxt.cs" href="#sigctxt.cs">func (c *sigctxt) cs() uint64</a>

```
searchKey: runtime.sigctxt.cs
```

```Go
func (c *sigctxt) cs() uint64
```

#### <a id="sigctxt.fs" href="#sigctxt.fs">func (c *sigctxt) fs() uint64</a>

```
searchKey: runtime.sigctxt.fs
```

```Go
func (c *sigctxt) fs() uint64
```

#### <a id="sigctxt.gs" href="#sigctxt.gs">func (c *sigctxt) gs() uint64</a>

```
searchKey: runtime.sigctxt.gs
```

```Go
func (c *sigctxt) gs() uint64
```

#### <a id="sigctxt.sigcode" href="#sigctxt.sigcode">func (c *sigctxt) sigcode() uint64</a>

```
searchKey: runtime.sigctxt.sigcode
```

```Go
func (c *sigctxt) sigcode() uint64
```

#### <a id="sigctxt.sigaddr" href="#sigctxt.sigaddr">func (c *sigctxt) sigaddr() uint64</a>

```
searchKey: runtime.sigctxt.sigaddr
```

```Go
func (c *sigctxt) sigaddr() uint64
```

#### <a id="sigctxt.set_rip" href="#sigctxt.set_rip">func (c *sigctxt) set_rip(x uint64)</a>

```
searchKey: runtime.sigctxt.set_rip
```

```Go
func (c *sigctxt) set_rip(x uint64)
```

#### <a id="sigctxt.set_rsp" href="#sigctxt.set_rsp">func (c *sigctxt) set_rsp(x uint64)</a>

```
searchKey: runtime.sigctxt.set_rsp
```

```Go
func (c *sigctxt) set_rsp(x uint64)
```

#### <a id="sigctxt.set_sigcode" href="#sigctxt.set_sigcode">func (c *sigctxt) set_sigcode(x uint64)</a>

```
searchKey: runtime.sigctxt.set_sigcode
```

```Go
func (c *sigctxt) set_sigcode(x uint64)
```

#### <a id="sigctxt.set_sigaddr" href="#sigctxt.set_sigaddr">func (c *sigctxt) set_sigaddr(x uint64)</a>

```
searchKey: runtime.sigctxt.set_sigaddr
```

```Go
func (c *sigctxt) set_sigaddr(x uint64)
```

#### <a id="sigctxt.fixsigcode" href="#sigctxt.fixsigcode">func (c *sigctxt) fixsigcode(sig uint32)</a>

```
searchKey: runtime.sigctxt.fixsigcode
```

```Go
func (c *sigctxt) fixsigcode(sig uint32)
```

### <a id="sigTabT" href="#sigTabT">type sigTabT struct</a>

```
searchKey: runtime.sigTabT
```

```Go
type sigTabT struct {
	flags int32
	name  string
}
```

sigTabT is the type of an entry in the global sigtable array. sigtable is inherently system dependent, and appears in OS-specific files, but sigTabT is the same for all Unixy systems. The sigtable array is indexed by a system signal number to get the flags and printable name of each signal. 

### <a id="gsignalStack" href="#gsignalStack">type gsignalStack struct</a>

```
searchKey: runtime.gsignalStack
```

```Go
type gsignalStack struct {
	stack       stack
	stackguard0 uintptr
	stackguard1 uintptr
	stktopsp    uintptr
}
```

gsignalStack saves the fields of the gsignal stack changed by setGsignalStack. 

### <a id="slice" href="#slice">type slice struct</a>

```
searchKey: runtime.slice
```

```Go
type slice struct {
	array unsafe.Pointer
	len   int
	cap   int
}
```

#### <a id="growslice" href="#growslice">func growslice(et *_type, old slice, cap int) slice</a>

```
searchKey: runtime.growslice
```

```Go
func growslice(et *_type, old slice, cap int) slice
```

growslice handles slice growth during append. It is passed the slice element type, the old slice, and the desired new minimum capacity, and it returns a new slice with at least that capacity, with the old data copied into it. The new slice's length is set to the old slice's length, NOT to the new requested capacity. This is for codegen convenience. The old slice's length is used immediately to calculate where to write new values during an append. TODO: When the old backend is gone, reconsider this decision. The SSA backend might prefer the new length or to return only ptr/cap and save stack space. 

### <a id="notInHeapSlice" href="#notInHeapSlice">type notInHeapSlice struct</a>

```
searchKey: runtime.notInHeapSlice
```

```Go
type notInHeapSlice struct {
	array *notInHeap
	len   int
	cap   int
}
```

A notInHeapSlice is a slice backed by go:notinheap memory. 

### <a id="stackpoolItem" href="#stackpoolItem">type stackpoolItem struct</a>

```
searchKey: runtime.stackpoolItem
```

```Go
type stackpoolItem struct {
	mu   mutex
	span mSpanList
}
```

### <a id="adjustinfo" href="#adjustinfo">type adjustinfo struct</a>

```
searchKey: runtime.adjustinfo
```

```Go
type adjustinfo struct {
	old   stack
	delta uintptr // ptr distance from old to new stack (newbase - oldbase)
	cache pcvalueCache

	// sghi is the highest sudog.elem on the stack.
	sghi uintptr
}
```

### <a id="bitvector" href="#bitvector">type bitvector struct</a>

```
searchKey: runtime.bitvector
```

```Go
type bitvector struct {
	n        int32 // # of bits
	bytedata *uint8
}
```

Information from the compiler about the layout of stack frames. Note: this type must agree with reflect.bitVector. 

#### <a id="makeheapobjbv" href="#makeheapobjbv">func makeheapobjbv(p uintptr, size uintptr) bitvector</a>

```
searchKey: runtime.makeheapobjbv
```

```Go
func makeheapobjbv(p uintptr, size uintptr) bitvector
```

#### <a id="progToPointerMask" href="#progToPointerMask">func progToPointerMask(prog *byte, size uintptr) bitvector</a>

```
searchKey: runtime.progToPointerMask
```

```Go
func progToPointerMask(prog *byte, size uintptr) bitvector
```

progToPointerMask returns the 1-bit pointer mask output by the GC program prog. size the size of the region described by prog, in bytes. The resulting bitvector will have no more than size/sys.PtrSize bits. 

#### <a id="getStackMap" href="#getStackMap">func getStackMap(frame *stkframe, cache *pcvalueCache, debug bool) (locals, args bitvector, objs []stackObjectRecord)</a>

```
searchKey: runtime.getStackMap
```

```Go
func getStackMap(frame *stkframe, cache *pcvalueCache, debug bool) (locals, args bitvector, objs []stackObjectRecord)
```

getStackMap returns the locals and arguments live pointer maps, and stack object list for frame. 

#### <a id="stackmapdata" href="#stackmapdata">func stackmapdata(stkmap *stackmap, n int32) bitvector</a>

```
searchKey: runtime.stackmapdata
```

```Go
func stackmapdata(stkmap *stackmap, n int32) bitvector
```

#### <a id="getArgInfoFast" href="#getArgInfoFast">func getArgInfoFast(f funcInfo, needArgMap bool) (arglen uintptr, argmap *bitvector, ok bool)</a>

```
searchKey: runtime.getArgInfoFast
```

```Go
func getArgInfoFast(f funcInfo, needArgMap bool) (arglen uintptr, argmap *bitvector, ok bool)
```

getArgInfoFast returns the argument frame information for a call to f. It is short and inlineable. However, it does not handle all functions. If ok reports false, you must call getArgInfo instead. TODO(josharian): once we do mid-stack inlining, call getArgInfo directly from getArgInfoFast and stop returning an ok bool. 

#### <a id="getArgInfo" href="#getArgInfo">func getArgInfo(frame *stkframe, f funcInfo, needArgMap bool, ctxt *funcval) (arglen uintptr, argmap *bitvector)</a>

```
searchKey: runtime.getArgInfo
```

```Go
func getArgInfo(frame *stkframe, f funcInfo, needArgMap bool, ctxt *funcval) (arglen uintptr, argmap *bitvector)
```

getArgInfo returns the argument frame information for a call to f with call frame frame. 

This is used for both actual calls with active stack frames and for deferred calls or goroutines that are not yet executing. If this is an actual call, ctxt must be nil (getArgInfo will retrieve what it needs from the active stack frame). If this is a deferred call or unstarted goroutine, ctxt must be the function object that was deferred or go'd. 

#### <a id="bitvector.ptrbit" href="#bitvector.ptrbit">func (bv *bitvector) ptrbit(i uintptr) uint8</a>

```
searchKey: runtime.bitvector.ptrbit
```

```Go
func (bv *bitvector) ptrbit(i uintptr) uint8
```

ptrbit returns the i'th bit in bv. ptrbit is less efficient than iterating directly over bitvector bits, and should only be used in non-performance-critical code. See adjustpointers for an example of a high-efficiency walk of a bitvector. 

### <a id="stackObjectRecord" href="#stackObjectRecord">type stackObjectRecord struct</a>

```
searchKey: runtime.stackObjectRecord
```

```Go
type stackObjectRecord struct {
	// offset in frame
	// if negative, offset from varp
	// if non-negative, offset from argp
	off      int32
	size     int32
	_ptrdata int32 // ptrdata, or -ptrdata is GC prog is used
	gcdata   *byte // pointer map or GC prog of the type
}
```

A stackObjectRecord is generated by the compiler for each stack object in a stack frame. This record must match the generator code in cmd/compile/internal/liveness/plive.go:emitStackObjects. 

#### <a id="stackObjectRecord.useGCProg" href="#stackObjectRecord.useGCProg">func (r *stackObjectRecord) useGCProg() bool</a>

```
searchKey: runtime.stackObjectRecord.useGCProg
```

```Go
func (r *stackObjectRecord) useGCProg() bool
```

#### <a id="stackObjectRecord.ptrdata" href="#stackObjectRecord.ptrdata">func (r *stackObjectRecord) ptrdata() uintptr</a>

```
searchKey: runtime.stackObjectRecord.ptrdata
```

```Go
func (r *stackObjectRecord) ptrdata() uintptr
```

### <a id="tmpBuf" href="#tmpBuf">type tmpBuf [32]byte</a>

```
searchKey: runtime.tmpBuf
```

```Go
type tmpBuf [tmpStringBufSize]byte
```

### <a id="stringStruct" href="#stringStruct">type stringStruct struct</a>

```
searchKey: runtime.stringStruct
```

```Go
type stringStruct struct {
	str unsafe.Pointer
	len int
}
```

#### <a id="stringStructOf" href="#stringStructOf">func stringStructOf(sp *string) *stringStruct</a>

```
searchKey: runtime.stringStructOf
```

```Go
func stringStructOf(sp *string) *stringStruct
```

### <a id="stringStructDWARF" href="#stringStructDWARF">type stringStructDWARF struct</a>

```
searchKey: runtime.stringStructDWARF
```

```Go
type stringStructDWARF struct {
	str *byte
	len int
}
```

Variant with *byte pointer type for DWARF debugging. 

### <a id="neverCallThisFunction" href="#neverCallThisFunction">type neverCallThisFunction struct{}</a>

```
searchKey: runtime.neverCallThisFunction
```

```Go
type neverCallThisFunction struct{}
```

### <a id="Frames" href="#Frames">type Frames struct</a>

```
searchKey: runtime.Frames
tags: [exported]
```

```Go
type Frames struct {
	// callers is a slice of PCs that have not yet been expanded to frames.
	callers []uintptr

	// frames is a slice of Frames that have yet to be returned.
	frames     []Frame
	frameStore [2]Frame
}
```

Frames may be used to get function/file/line information for a slice of PC values returned by Callers. 

#### <a id="CallersFrames" href="#CallersFrames">func CallersFrames(callers []uintptr) *Frames</a>

```
searchKey: runtime.CallersFrames
tags: [exported]
```

```Go
func CallersFrames(callers []uintptr) *Frames
```

CallersFrames takes a slice of PC values returned by Callers and prepares to return function/file/line information. Do not change the slice until you are done with the Frames. 

#### <a id="Frames.Next" href="#Frames.Next">func (ci *Frames) Next() (frame Frame, more bool)</a>

```
searchKey: runtime.Frames.Next
tags: [exported]
```

```Go
func (ci *Frames) Next() (frame Frame, more bool)
```

Next returns a Frame representing the next call frame in the slice of PC values. If it has already returned all call frames, Next returns a zero Frame. 

The more result indicates whether the next call to Next will return a valid Frame. It does not necessarily indicate whether this call returned one. 

See the Frames example for idiomatic usage. 

### <a id="Frame" href="#Frame">type Frame struct</a>

```
searchKey: runtime.Frame
tags: [exported]
```

```Go
type Frame struct {
	// PC is the program counter for the location in this frame.
	// For a frame that calls another frame, this will be the
	// program counter of a call instruction. Because of inlining,
	// multiple frames may have the same PC value, but different
	// symbolic information.
	PC uintptr

	// Func is the Func value of this call frame. This may be nil
	// for non-Go code or fully inlined functions.
	Func *Func

	// Function is the package path-qualified function name of
	// this call frame. If non-empty, this string uniquely
	// identifies a single function in the program.
	// This may be the empty string if not known.
	// If Func is not nil then Function == Func.Name().
	Function string

	// File and Line are the file name and line number of the
	// location in this frame. For non-leaf frames, this will be
	// the location of a call. These may be the empty string and
	// zero, respectively, if not known.
	File string
	Line int

	// Entry point program counter for the function; may be zero
	// if not known. If Func is not nil then Entry ==
	// Func.Entry().
	Entry uintptr

	// The runtime's internal view of the function. This field
	// is set (funcInfo.valid() returns true) only for Go functions,
	// not for C functions.
	funcInfo funcInfo
}
```

Frame is the information returned by Frames for each call frame. 

### <a id="Func" href="#Func">type Func struct</a>

```
searchKey: runtime.Func
tags: [exported]
```

```Go
type Func struct {
	opaque struct{} // unexported field to disallow conversions
}
```

A Func represents a Go function in the running binary. 

#### <a id="FuncForPC" href="#FuncForPC">func FuncForPC(pc uintptr) *Func</a>

```
searchKey: runtime.FuncForPC
tags: [exported]
```

```Go
func FuncForPC(pc uintptr) *Func
```

FuncForPC returns a *Func describing the function that contains the given program counter address, or else nil. 

If pc represents multiple functions because of inlining, it returns the *Func describing the innermost function, but with an entry of the outermost function. 

#### <a id="Func.raw" href="#Func.raw">func (f *Func) raw() *_func</a>

```
searchKey: runtime.Func.raw
```

```Go
func (f *Func) raw() *_func
```

#### <a id="Func.funcInfo" href="#Func.funcInfo">func (f *Func) funcInfo() funcInfo</a>

```
searchKey: runtime.Func.funcInfo
```

```Go
func (f *Func) funcInfo() funcInfo
```

#### <a id="Func.Name" href="#Func.Name">func (f *Func) Name() string</a>

```
searchKey: runtime.Func.Name
tags: [exported]
```

```Go
func (f *Func) Name() string
```

Name returns the name of the function. 

#### <a id="Func.Entry" href="#Func.Entry">func (f *Func) Entry() uintptr</a>

```
searchKey: runtime.Func.Entry
tags: [exported]
```

```Go
func (f *Func) Entry() uintptr
```

Entry returns the entry address of the function. 

#### <a id="Func.FileLine" href="#Func.FileLine">func (f *Func) FileLine(pc uintptr) (file string, line int)</a>

```
searchKey: runtime.Func.FileLine
tags: [exported]
```

```Go
func (f *Func) FileLine(pc uintptr) (file string, line int)
```

FileLine returns the file name and line number of the source code corresponding to the program counter pc. The result will not be accurate if pc is not a program counter within f. 

### <a id="funcID" href="#funcID">type funcID uint8</a>

```
searchKey: runtime.funcID
```

```Go
type funcID uint8
```

A FuncID identifies particular functions that need to be treated specially by the runtime. Note that in some situations involving plugins, there may be multiple copies of a particular special runtime function. Note: this list must match the list in cmd/internal/objabi/funcid.go. 

### <a id="funcFlag" href="#funcFlag">type funcFlag uint8</a>

```
searchKey: runtime.funcFlag
```

```Go
type funcFlag uint8
```

A FuncFlag holds bits about a function. This list must match the list in cmd/internal/objabi/funcid.go. 

### <a id="pcHeader" href="#pcHeader">type pcHeader struct</a>

```
searchKey: runtime.pcHeader
```

```Go
type pcHeader struct {
	magic          uint32  // 0xFFFFFFFA
	pad1, pad2     uint8   // 0,0
	minLC          uint8   // min instruction size
	ptrSize        uint8   // size of a ptr in bytes
	nfunc          int     // number of functions in the module
	nfiles         uint    // number of entries in the file tab.
	funcnameOffset uintptr // offset to the funcnametab variable from pcHeader
	cuOffset       uintptr // offset to the cutab variable from pcHeader
	filetabOffset  uintptr // offset to the filetab variable from pcHeader
	pctabOffset    uintptr // offset to the pctab varible from pcHeader
	pclnOffset     uintptr // offset to the pclntab variable from pcHeader
}
```

pcHeader holds data used by the pclntab lookups. 

### <a id="moduledata" href="#moduledata">type moduledata struct</a>

```
searchKey: runtime.moduledata
```

```Go
type moduledata struct {
	pcHeader     *pcHeader
	funcnametab  []byte
	cutab        []uint32
	filetab      []byte
	pctab        []byte
	pclntable    []byte
	ftab         []functab
	findfunctab  uintptr
	minpc, maxpc uintptr

	text, etext           uintptr
	noptrdata, enoptrdata uintptr
	data, edata           uintptr
	bss, ebss             uintptr
	noptrbss, enoptrbss   uintptr
	end, gcdata, gcbss    uintptr
	types, etypes         uintptr

	textsectmap []textsect
	typelinks   []int32 // offsets from types
	itablinks   []*itab

	ptab []ptabEntry

	pluginpath string
	pkghashes  []modulehash

	modulename   string
	modulehashes []modulehash

	hasmain uint8 // 1 if module contains the main function, 0 otherwise

	gcdatamask, gcbssmask bitvector

	typemap map[typeOff]*_type // offset to *_rtype in previous module

	bad bool // module failed to load and should be ignored

	next *moduledata
}
```

moduledata records information about the layout of the executable image. It is written by the linker. Any changes here must be matched changes to the code in cmd/internal/ld/symtab.go:symtab. moduledata is stored in statically allocated non-pointer memory; none of the pointers here are visible to the garbage collector. 

#### <a id="findmoduledatap" href="#findmoduledatap">func findmoduledatap(pc uintptr) *moduledata</a>

```
searchKey: runtime.findmoduledatap
```

```Go
func findmoduledatap(pc uintptr) *moduledata
```

findmoduledatap looks up the moduledata for a PC. 

It is nosplit because it's part of the isgoexception implementation. 

### <a id="modulehash" href="#modulehash">type modulehash struct</a>

```
searchKey: runtime.modulehash
```

```Go
type modulehash struct {
	modulename   string
	linktimehash string
	runtimehash  *string
}
```

A modulehash is used to compare the ABI of a new module or a package in a new module with the loaded program. 

For each shared library a module links against, the linker creates an entry in the moduledata.modulehashes slice containing the name of the module, the abi hash seen at link time and a pointer to the runtime abi hash. These are checked in moduledataverify1 below. 

For each loaded plugin, the pkghashes slice has a modulehash of the newly loaded package that can be used to check the plugin's version of a package against any previously loaded version of the package. This is done in plugin.lastmoduleinit. 

### <a id="functab" href="#functab">type functab struct</a>

```
searchKey: runtime.functab
```

```Go
type functab struct {
	entry   uintptr
	funcoff uintptr
}
```

### <a id="textsect" href="#textsect">type textsect struct</a>

```
searchKey: runtime.textsect
```

```Go
type textsect struct {
	vaddr    uintptr // prelinked section vaddr
	length   uintptr // section length
	baseaddr uintptr // relocated section address
}
```

### <a id="findfuncbucket" href="#findfuncbucket">type findfuncbucket struct</a>

```
searchKey: runtime.findfuncbucket
```

```Go
type findfuncbucket struct {
	idx        uint32
	subbuckets [16]byte
}
```

findfunctab is an array of these structures. Each bucket represents 4096 bytes of the text segment. Each subbucket represents 256 bytes of the text segment. To find a function given a pc, locate the bucket and subbucket for that pc. Add together the idx and subbucket value to obtain a function index. Then scan the functab array starting at that index to find the target function. This table uses 20 bytes for every 4096 bytes of code, or ~0.5% overhead. 

### <a id="funcInfo" href="#funcInfo">type funcInfo struct</a>

```
searchKey: runtime.funcInfo
```

```Go
type funcInfo struct {
	*_func
	datap *moduledata
}
```

#### <a id="findfunc" href="#findfunc">func findfunc(pc uintptr) funcInfo</a>

```
searchKey: runtime.findfunc
```

```Go
func findfunc(pc uintptr) funcInfo
```

findfunc looks up function metadata for a PC. 

It is nosplit because it's part of the isgoexception implementation. 

#### <a id="funcInfo.valid" href="#funcInfo.valid">func (f funcInfo) valid() bool</a>

```
searchKey: runtime.funcInfo.valid
```

```Go
func (f funcInfo) valid() bool
```

#### <a id="funcInfo._Func" href="#funcInfo._Func">func (f funcInfo) _Func() *Func</a>

```
searchKey: runtime.funcInfo._Func
```

```Go
func (f funcInfo) _Func() *Func
```

### <a id="pcvalueCache" href="#pcvalueCache">type pcvalueCache struct</a>

```
searchKey: runtime.pcvalueCache
```

```Go
type pcvalueCache struct {
	entries [2][8]pcvalueCacheEnt
}
```

### <a id="pcvalueCacheEnt" href="#pcvalueCacheEnt">type pcvalueCacheEnt struct</a>

```
searchKey: runtime.pcvalueCacheEnt
```

```Go
type pcvalueCacheEnt struct {
	// targetpc and off together are the key of this cache entry.
	targetpc uintptr
	off      uint32
	// val is the value of this cached pcvalue entry.
	val int32
}
```

### <a id="stackmap" href="#stackmap">type stackmap struct</a>

```
searchKey: runtime.stackmap
```

```Go
type stackmap struct {
	n        int32   // number of bitmaps
	nbit     int32   // number of bits in each bitmap
	bytedata [1]byte // bitmaps, each starting on a byte boundary
}
```

### <a id="inlinedCall" href="#inlinedCall">type inlinedCall struct</a>

```
searchKey: runtime.inlinedCall
```

```Go
type inlinedCall struct {
	parent   int16  // index of parent in the inltree, or < 0
	funcID   funcID // type of the called function
	_        byte
	file     int32 // perCU file index for inlined call. See cmd/link:pcln.go
	line     int32 // line number of the call site
	func_    int32 // offset into pclntab for name of called function
	parentPc int32 // position of an instruction whose source position is the call site (offset from entry)
}
```

inlinedCall is the encoding of entries in the FUNCDATA_InlTree table. 

### <a id="timer" href="#timer">type timer struct</a>

```
searchKey: runtime.timer
```

```Go
type timer struct {
	// If this timer is on a heap, which P's heap it is on.
	// puintptr rather than *p to match uintptr in the versions
	// of this struct defined in other packages.
	pp puintptr

	// Timer wakes up at when, and then at when+period, ... (period > 0 only)
	// each time calling f(arg, now) in the timer goroutine, so f must be
	// a well-behaved function and not block.
	//
	// when must be positive on an active timer.
	when   int64
	period int64
	f      func(interface{}, uintptr)
	arg    interface{}
	seq    uintptr

	// What to set the when field to in timerModifiedXX status.
	nextwhen int64

	// The status field holds one of the values below.
	status uint32
}
```

Package time knows the layout of this structure. If this struct changes, adjust ../time/sleep.go:/runtimeTimer. 

### <a id="traceBufHeader" href="#traceBufHeader">type traceBufHeader struct</a>

```
searchKey: runtime.traceBufHeader
```

```Go
type traceBufHeader struct {
	link      traceBufPtr             // in trace.empty/full
	lastTicks uint64                  // when we wrote the last event
	pos       int                     // next write offset in arr
	stk       [traceStackSize]uintptr // scratch buffer for traceback
}
```

traceBufHeader is per-P tracing buffer. 

### <a id="traceBuf" href="#traceBuf">type traceBuf struct</a>

```
searchKey: runtime.traceBuf
```

```Go
type traceBuf struct {
	traceBufHeader
	arr [64<<10 - unsafe.Sizeof(traceBufHeader{})]byte // underlying buffer for traceBufHeader.buf
}
```

traceBuf is per-P tracing buffer. 

#### <a id="traceBuf.varint" href="#traceBuf.varint">func (buf *traceBuf) varint(v uint64)</a>

```
searchKey: runtime.traceBuf.varint
```

```Go
func (buf *traceBuf) varint(v uint64)
```

varint appends v to buf in little-endian-base-128 encoding. 

#### <a id="traceBuf.byte" href="#traceBuf.byte">func (buf *traceBuf) byte(v byte)</a>

```
searchKey: runtime.traceBuf.byte
```

```Go
func (buf *traceBuf) byte(v byte)
```

byte appends v to buf. 

### <a id="traceBufPtr" href="#traceBufPtr">type traceBufPtr uintptr</a>

```
searchKey: runtime.traceBufPtr
```

```Go
type traceBufPtr uintptr
```

traceBufPtr is a *traceBuf that is not traced by the garbage collector and doesn't have write barriers. traceBufs are not allocated from the GC'd heap, so this is safe, and are often manipulated in contexts where write barriers are not allowed, so this is necessary. 

TODO: Since traceBuf is now go:notinheap, this isn't necessary. 

#### <a id="traceBufPtrOf" href="#traceBufPtrOf">func traceBufPtrOf(b *traceBuf) traceBufPtr</a>

```
searchKey: runtime.traceBufPtrOf
```

```Go
func traceBufPtrOf(b *traceBuf) traceBufPtr
```

#### <a id="traceFullDequeue" href="#traceFullDequeue">func traceFullDequeue() traceBufPtr</a>

```
searchKey: runtime.traceFullDequeue
```

```Go
func traceFullDequeue() traceBufPtr
```

traceFullDequeue dequeues from queue of full buffers. 

#### <a id="traceAcquireBuffer" href="#traceAcquireBuffer">func traceAcquireBuffer() (mp *m, pid int32, bufp *traceBufPtr)</a>

```
searchKey: runtime.traceAcquireBuffer
```

```Go
func traceAcquireBuffer() (mp *m, pid int32, bufp *traceBufPtr)
```

traceAcquireBuffer returns trace buffer to use and, if necessary, locks it. 

#### <a id="traceFlush" href="#traceFlush">func traceFlush(buf traceBufPtr, pid int32) traceBufPtr</a>

```
searchKey: runtime.traceFlush
```

```Go
func traceFlush(buf traceBufPtr, pid int32) traceBufPtr
```

traceFlush puts buf onto stack of full buffers and returns an empty buffer. 

#### <a id="traceString" href="#traceString">func traceString(bufp *traceBufPtr, pid int32, s string) (uint64, *traceBufPtr)</a>

```
searchKey: runtime.traceString
```

```Go
func traceString(bufp *traceBufPtr, pid int32, s string) (uint64, *traceBufPtr)
```

traceString adds a string to the trace.strings and returns the id. 

#### <a id="traceFrameForPC" href="#traceFrameForPC">func traceFrameForPC(buf traceBufPtr, pid int32, f Frame) (traceFrame, traceBufPtr)</a>

```
searchKey: runtime.traceFrameForPC
```

```Go
func traceFrameForPC(buf traceBufPtr, pid int32, f Frame) (traceFrame, traceBufPtr)
```

traceFrameForPC records the frame information. It may allocate memory. 

#### <a id="traceBufPtr.ptr" href="#traceBufPtr.ptr">func (tp traceBufPtr) ptr() *traceBuf</a>

```
searchKey: runtime.traceBufPtr.ptr
```

```Go
func (tp traceBufPtr) ptr() *traceBuf
```

#### <a id="traceBufPtr.set" href="#traceBufPtr.set">func (tp *traceBufPtr) set(b *traceBuf)</a>

```
searchKey: runtime.traceBufPtr.set
```

```Go
func (tp *traceBufPtr) set(b *traceBuf)
```

### <a id="traceStackTable" href="#traceStackTable">type traceStackTable struct</a>

```
searchKey: runtime.traceStackTable
```

```Go
type traceStackTable struct {
	lock mutex
	seq  uint32
	mem  traceAlloc
	tab  [1 << 13]traceStackPtr
}
```

traceStackTable maps stack traces (arrays of PC's) to unique uint32 ids. It is lock-free for reading. 

#### <a id="traceStackTable.put" href="#traceStackTable.put">func (tab *traceStackTable) put(pcs []uintptr) uint32</a>

```
searchKey: runtime.traceStackTable.put
```

```Go
func (tab *traceStackTable) put(pcs []uintptr) uint32
```

put returns a unique id for the stack trace pcs and caches it in the table, if it sees the trace for the first time. 

#### <a id="traceStackTable.find" href="#traceStackTable.find">func (tab *traceStackTable) find(pcs []uintptr, hash uintptr) uint32</a>

```
searchKey: runtime.traceStackTable.find
```

```Go
func (tab *traceStackTable) find(pcs []uintptr, hash uintptr) uint32
```

find checks if the stack trace pcs is already present in the table. 

#### <a id="traceStackTable.newStack" href="#traceStackTable.newStack">func (tab *traceStackTable) newStack(n int) *traceStack</a>

```
searchKey: runtime.traceStackTable.newStack
```

```Go
func (tab *traceStackTable) newStack(n int) *traceStack
```

newStack allocates a new stack of size n. 

#### <a id="traceStackTable.dump" href="#traceStackTable.dump">func (tab *traceStackTable) dump()</a>

```
searchKey: runtime.traceStackTable.dump
```

```Go
func (tab *traceStackTable) dump()
```

dump writes all previously cached stacks to trace buffers, releases all memory and resets state. 

### <a id="traceStack" href="#traceStack">type traceStack struct</a>

```
searchKey: runtime.traceStack
```

```Go
type traceStack struct {
	link traceStackPtr
	hash uintptr
	id   uint32
	n    int
	stk  [0]uintptr // real type [n]uintptr
}
```

traceStack is a single stack in traceStackTable. 

#### <a id="traceStack.stack" href="#traceStack.stack">func (ts *traceStack) stack() []uintptr</a>

```
searchKey: runtime.traceStack.stack
```

```Go
func (ts *traceStack) stack() []uintptr
```

stack returns slice of PCs. 

### <a id="traceStackPtr" href="#traceStackPtr">type traceStackPtr uintptr</a>

```
searchKey: runtime.traceStackPtr
```

```Go
type traceStackPtr uintptr
```

#### <a id="traceStackPtr.ptr" href="#traceStackPtr.ptr">func (tp traceStackPtr) ptr() *traceStack</a>

```
searchKey: runtime.traceStackPtr.ptr
```

```Go
func (tp traceStackPtr) ptr() *traceStack
```

### <a id="traceFrame" href="#traceFrame">type traceFrame struct</a>

```
searchKey: runtime.traceFrame
```

```Go
type traceFrame struct {
	funcID uint64
	fileID uint64
	line   uint64
}
```

#### <a id="traceFrameForPC" href="#traceFrameForPC">func traceFrameForPC(buf traceBufPtr, pid int32, f Frame) (traceFrame, traceBufPtr)</a>

```
searchKey: runtime.traceFrameForPC
```

```Go
func traceFrameForPC(buf traceBufPtr, pid int32, f Frame) (traceFrame, traceBufPtr)
```

traceFrameForPC records the frame information. It may allocate memory. 

### <a id="traceAlloc" href="#traceAlloc">type traceAlloc struct</a>

```
searchKey: runtime.traceAlloc
```

```Go
type traceAlloc struct {
	head traceAllocBlockPtr
	off  uintptr
}
```

traceAlloc is a non-thread-safe region allocator. It holds a linked list of traceAllocBlock. 

#### <a id="traceAlloc.alloc" href="#traceAlloc.alloc">func (a *traceAlloc) alloc(n uintptr) unsafe.Pointer</a>

```
searchKey: runtime.traceAlloc.alloc
```

```Go
func (a *traceAlloc) alloc(n uintptr) unsafe.Pointer
```

alloc allocates n-byte block. 

#### <a id="traceAlloc.drop" href="#traceAlloc.drop">func (a *traceAlloc) drop()</a>

```
searchKey: runtime.traceAlloc.drop
```

```Go
func (a *traceAlloc) drop()
```

drop frees all previously allocated memory and resets the allocator. 

### <a id="traceAllocBlock" href="#traceAllocBlock">type traceAllocBlock struct</a>

```
searchKey: runtime.traceAllocBlock
```

```Go
type traceAllocBlock struct {
	next traceAllocBlockPtr
	data [64<<10 - sys.PtrSize]byte
}
```

traceAllocBlock is a block in traceAlloc. 

traceAllocBlock is allocated from non-GC'd memory, so it must not contain heap pointers. Writes to pointers to traceAllocBlocks do not need write barriers. 

### <a id="traceAllocBlockPtr" href="#traceAllocBlockPtr">type traceAllocBlockPtr uintptr</a>

```
searchKey: runtime.traceAllocBlockPtr
```

```Go
type traceAllocBlockPtr uintptr
```

TODO: Since traceAllocBlock is now go:notinheap, this isn't necessary. 

#### <a id="traceAllocBlockPtr.ptr" href="#traceAllocBlockPtr.ptr">func (p traceAllocBlockPtr) ptr() *traceAllocBlock</a>

```
searchKey: runtime.traceAllocBlockPtr.ptr
```

```Go
func (p traceAllocBlockPtr) ptr() *traceAllocBlock
```

#### <a id="traceAllocBlockPtr.set" href="#traceAllocBlockPtr.set">func (p *traceAllocBlockPtr) set(x *traceAllocBlock)</a>

```
searchKey: runtime.traceAllocBlockPtr.set
```

```Go
func (p *traceAllocBlockPtr) set(x *traceAllocBlock)
```

### <a id="reflectMethodValue" href="#reflectMethodValue">type reflectMethodValue struct</a>

```
searchKey: runtime.reflectMethodValue
```

```Go
type reflectMethodValue struct {
	fn     uintptr
	stack  *bitvector // ptrmap for both args and results
	argLen uintptr    // just args
}
```

reflectMethodValue is a partial duplicate of reflect.makeFuncImpl and reflect.methodValue. 

### <a id="cgoTracebackArg" href="#cgoTracebackArg">type cgoTracebackArg struct</a>

```
searchKey: runtime.cgoTracebackArg
```

```Go
type cgoTracebackArg struct {
	context    uintptr
	sigContext uintptr
	buf        *uintptr
	max        uintptr
}
```

cgoTracebackArg is the type passed to cgoTraceback. 

### <a id="cgoContextArg" href="#cgoContextArg">type cgoContextArg struct</a>

```
searchKey: runtime.cgoContextArg
```

```Go
type cgoContextArg struct {
	context uintptr
}
```

cgoContextArg is the type passed to the context function. 

### <a id="cgoSymbolizerArg" href="#cgoSymbolizerArg">type cgoSymbolizerArg struct</a>

```
searchKey: runtime.cgoSymbolizerArg
```

```Go
type cgoSymbolizerArg struct {
	pc       uintptr
	file     *byte
	lineno   uintptr
	funcName *byte
	entry    uintptr
	more     uintptr
	data     uintptr
}
```

cgoSymbolizerArg is the type passed to cgoSymbolizer. 

### <a id="tflag" href="#tflag">type tflag uint8</a>

```
searchKey: runtime.tflag
```

```Go
type tflag uint8
```

tflag is documented in reflect/type.go. 

tflag values must be kept in sync with copies in: 

```
	cmd/compile/internal/gc/reflect.go
	cmd/link/internal/ld/decodesym.go
	reflect/type.go
     internal/reflectlite/type.go

```
### <a id="_type" href="#_type">type _type struct</a>

```
searchKey: runtime._type
```

```Go
type _type struct {
	size       uintptr
	ptrdata    uintptr // size of memory prefix holding all pointers
	hash       uint32
	tflag      tflag
	align      uint8
	fieldAlign uint8
	kind       uint8
	// function for comparing objects of this type
	// (ptr to object A, ptr to object B) -> ==?
	equal func(unsafe.Pointer, unsafe.Pointer) bool
	// gcdata stores the GC type data for the garbage collector.
	// If the KindGCProg bit is set in kind, gcdata is a GC program.
	// Otherwise it is a ptrmask bitmap. See mbitmap.go for details.
	gcdata    *byte
	str       nameOff
	ptrToThis typeOff
}
```

Needs to be in sync with ../cmd/link/internal/ld/decodesym.go:/^func.commonsize, ../cmd/compile/internal/gc/reflect.go:/^func.dcommontype and ../reflect/type.go:/^type.rtype. ../internal/reflectlite/type.go:/^type.rtype. 

#### <a id="resolveTypeOff" href="#resolveTypeOff">func resolveTypeOff(ptrInModule unsafe.Pointer, off typeOff) *_type</a>

```
searchKey: runtime.resolveTypeOff
```

```Go
func resolveTypeOff(ptrInModule unsafe.Pointer, off typeOff) *_type
```

#### <a id="_type.string" href="#_type.string">func (t *_type) string() string</a>

```
searchKey: runtime._type.string
```

```Go
func (t *_type) string() string
```

#### <a id="_type.uncommon" href="#_type.uncommon">func (t *_type) uncommon() *uncommontype</a>

```
searchKey: runtime._type.uncommon
```

```Go
func (t *_type) uncommon() *uncommontype
```

#### <a id="_type.name" href="#_type.name">func (t *_type) name() string</a>

```
searchKey: runtime._type.name
```

```Go
func (t *_type) name() string
```

#### <a id="_type.pkgpath" href="#_type.pkgpath">func (t *_type) pkgpath() string</a>

```
searchKey: runtime._type.pkgpath
```

```Go
func (t *_type) pkgpath() string
```

pkgpath returns the path of the package where t was defined, if available. This is not the same as the reflect package's PkgPath method, in that it returns the package path for struct and interface types, not just named types. 

#### <a id="_type.nameOff" href="#_type.nameOff">func (t *_type) nameOff(off nameOff) name</a>

```
searchKey: runtime._type.nameOff
```

```Go
func (t *_type) nameOff(off nameOff) name
```

#### <a id="_type.typeOff" href="#_type.typeOff">func (t *_type) typeOff(off typeOff) *_type</a>

```
searchKey: runtime._type.typeOff
```

```Go
func (t *_type) typeOff(off typeOff) *_type
```

#### <a id="_type.textOff" href="#_type.textOff">func (t *_type) textOff(off textOff) unsafe.Pointer</a>

```
searchKey: runtime._type.textOff
```

```Go
func (t *_type) textOff(off textOff) unsafe.Pointer
```

### <a id="nameOff" href="#nameOff">type nameOff int32</a>

```
searchKey: runtime.nameOff
```

```Go
type nameOff int32
```

### <a id="typeOff" href="#typeOff">type typeOff int32</a>

```
searchKey: runtime.typeOff
```

```Go
type typeOff int32
```

### <a id="textOff" href="#textOff">type textOff int32</a>

```
searchKey: runtime.textOff
```

```Go
type textOff int32
```

### <a id="method" href="#method">type method struct</a>

```
searchKey: runtime.method
```

```Go
type method struct {
	name nameOff
	mtyp typeOff
	ifn  textOff
	tfn  textOff
}
```

### <a id="uncommontype" href="#uncommontype">type uncommontype struct</a>

```
searchKey: runtime.uncommontype
```

```Go
type uncommontype struct {
	pkgpath nameOff
	mcount  uint16 // number of methods
	xcount  uint16 // number of exported methods
	moff    uint32 // offset from this uncommontype to [mcount]method
	_       uint32 // unused
}
```

### <a id="imethod" href="#imethod">type imethod struct</a>

```
searchKey: runtime.imethod
```

```Go
type imethod struct {
	name nameOff
	ityp typeOff
}
```

### <a id="interfacetype" href="#interfacetype">type interfacetype struct</a>

```
searchKey: runtime.interfacetype
```

```Go
type interfacetype struct {
	typ     _type
	pkgpath name
	mhdr    []imethod
}
```

### <a id="maptype" href="#maptype">type maptype struct</a>

```
searchKey: runtime.maptype
```

```Go
type maptype struct {
	typ    _type
	key    *_type
	elem   *_type
	bucket *_type // internal type representing a hash bucket
	// function for hashing keys (ptr to key, seed) -> hash
	hasher     func(unsafe.Pointer, uintptr) uintptr
	keysize    uint8  // size of key slot
	elemsize   uint8  // size of elem slot
	bucketsize uint16 // size of bucket
	flags      uint32
}
```

#### <a id="maptype.indirectkey" href="#maptype.indirectkey">func (mt *maptype) indirectkey() bool</a>

```
searchKey: runtime.maptype.indirectkey
```

```Go
func (mt *maptype) indirectkey() bool
```

Note: flag values must match those used in the TMAP case in ../cmd/compile/internal/gc/reflect.go:writeType. 

#### <a id="maptype.indirectelem" href="#maptype.indirectelem">func (mt *maptype) indirectelem() bool</a>

```
searchKey: runtime.maptype.indirectelem
```

```Go
func (mt *maptype) indirectelem() bool
```

#### <a id="maptype.reflexivekey" href="#maptype.reflexivekey">func (mt *maptype) reflexivekey() bool</a>

```
searchKey: runtime.maptype.reflexivekey
```

```Go
func (mt *maptype) reflexivekey() bool
```

#### <a id="maptype.needkeyupdate" href="#maptype.needkeyupdate">func (mt *maptype) needkeyupdate() bool</a>

```
searchKey: runtime.maptype.needkeyupdate
```

```Go
func (mt *maptype) needkeyupdate() bool
```

#### <a id="maptype.hashMightPanic" href="#maptype.hashMightPanic">func (mt *maptype) hashMightPanic() bool</a>

```
searchKey: runtime.maptype.hashMightPanic
```

```Go
func (mt *maptype) hashMightPanic() bool
```

### <a id="arraytype" href="#arraytype">type arraytype struct</a>

```
searchKey: runtime.arraytype
```

```Go
type arraytype struct {
	typ   _type
	elem  *_type
	slice *_type
	len   uintptr
}
```

### <a id="chantype" href="#chantype">type chantype struct</a>

```
searchKey: runtime.chantype
```

```Go
type chantype struct {
	typ  _type
	elem *_type
	dir  uintptr
}
```

### <a id="slicetype" href="#slicetype">type slicetype struct</a>

```
searchKey: runtime.slicetype
```

```Go
type slicetype struct {
	typ  _type
	elem *_type
}
```

### <a id="functype" href="#functype">type functype struct</a>

```
searchKey: runtime.functype
```

```Go
type functype struct {
	typ      _type
	inCount  uint16
	outCount uint16
}
```

#### <a id="functype.in" href="#functype.in">func (t *functype) in() []*_type</a>

```
searchKey: runtime.functype.in
```

```Go
func (t *functype) in() []*_type
```

#### <a id="functype.out" href="#functype.out">func (t *functype) out() []*_type</a>

```
searchKey: runtime.functype.out
```

```Go
func (t *functype) out() []*_type
```

#### <a id="functype.dotdotdot" href="#functype.dotdotdot">func (t *functype) dotdotdot() bool</a>

```
searchKey: runtime.functype.dotdotdot
```

```Go
func (t *functype) dotdotdot() bool
```

### <a id="ptrtype" href="#ptrtype">type ptrtype struct</a>

```
searchKey: runtime.ptrtype
```

```Go
type ptrtype struct {
	typ  _type
	elem *_type
}
```

### <a id="structfield" href="#structfield">type structfield struct</a>

```
searchKey: runtime.structfield
```

```Go
type structfield struct {
	name       name
	typ        *_type
	offsetAnon uintptr
}
```

#### <a id="structfield.offset" href="#structfield.offset">func (f *structfield) offset() uintptr</a>

```
searchKey: runtime.structfield.offset
```

```Go
func (f *structfield) offset() uintptr
```

### <a id="structtype" href="#structtype">type structtype struct</a>

```
searchKey: runtime.structtype
```

```Go
type structtype struct {
	typ     _type
	pkgPath name
	fields  []structfield
}
```

### <a id="name" href="#name">type name struct</a>

```
searchKey: runtime.name
```

```Go
type name struct {
	bytes *byte
}
```

name is an encoded type name with optional extra data. See reflect/type.go for details. 

#### <a id="resolveNameOff" href="#resolveNameOff">func resolveNameOff(ptrInModule unsafe.Pointer, off nameOff) name</a>

```
searchKey: runtime.resolveNameOff
```

```Go
func resolveNameOff(ptrInModule unsafe.Pointer, off nameOff) name
```

#### <a id="name.data" href="#name.data">func (n name) data(off int) *byte</a>

```
searchKey: runtime.name.data
```

```Go
func (n name) data(off int) *byte
```

#### <a id="name.isExported" href="#name.isExported">func (n name) isExported() bool</a>

```
searchKey: runtime.name.isExported
```

```Go
func (n name) isExported() bool
```

#### <a id="name.readvarint" href="#name.readvarint">func (n name) readvarint(off int) (int, int)</a>

```
searchKey: runtime.name.readvarint
```

```Go
func (n name) readvarint(off int) (int, int)
```

#### <a id="name.name" href="#name.name">func (n name) name() (s string)</a>

```
searchKey: runtime.name.name
```

```Go
func (n name) name() (s string)
```

#### <a id="name.tag" href="#name.tag">func (n name) tag() (s string)</a>

```
searchKey: runtime.name.tag
```

```Go
func (n name) tag() (s string)
```

#### <a id="name.pkgPath" href="#name.pkgPath">func (n name) pkgPath() string</a>

```
searchKey: runtime.name.pkgPath
```

```Go
func (n name) pkgPath() string
```

#### <a id="name.isBlank" href="#name.isBlank">func (n name) isBlank() bool</a>

```
searchKey: runtime.name.isBlank
```

```Go
func (n name) isBlank() bool
```

### <a id="_typePair" href="#_typePair">type _typePair struct</a>

```
searchKey: runtime._typePair
```

```Go
type _typePair struct {
	t1 *_type
	t2 *_type
}
```

### <a id="LockRank" href="#LockRank">type LockRank runtime.lockRank</a>

```
searchKey: runtime.LockRank
```

```Go
type LockRank lockRank
```

#### <a id="LockRank.String" href="#LockRank.String">func (l LockRank) String() string</a>

```
searchKey: runtime.LockRank.String
```

```Go
func (l LockRank) String() string
```

### <a id="LFNode" href="#LFNode">type LFNode struct</a>

```
searchKey: runtime.LFNode
```

```Go
type LFNode struct {
	Next    uint64
	Pushcnt uintptr
}
```

#### <a id="LFStackPop" href="#LFStackPop">func LFStackPop(head *uint64) *LFNode</a>

```
searchKey: runtime.LFStackPop
```

```Go
func LFStackPop(head *uint64) *LFNode
```

### <a id="ProfBuf" href="#ProfBuf">type ProfBuf runtime.profBuf</a>

```
searchKey: runtime.ProfBuf
```

```Go
type ProfBuf profBuf
```

#### <a id="NewProfBuf" href="#NewProfBuf">func NewProfBuf(hdrsize, bufwords, tags int) *ProfBuf</a>

```
searchKey: runtime.NewProfBuf
```

```Go
func NewProfBuf(hdrsize, bufwords, tags int) *ProfBuf
```

#### <a id="ProfBuf.Write" href="#ProfBuf.Write">func (p *ProfBuf) Write(tag *unsafe.Pointer, now int64, hdr []uint64, stk []uintptr)</a>

```
searchKey: runtime.ProfBuf.Write
```

```Go
func (p *ProfBuf) Write(tag *unsafe.Pointer, now int64, hdr []uint64, stk []uintptr)
```

#### <a id="ProfBuf.Read" href="#ProfBuf.Read">func (p *ProfBuf) Read(mode profBufReadMode) ([]uint64, []unsafe.Pointer, bool)</a>

```
searchKey: runtime.ProfBuf.Read
```

```Go
func (p *ProfBuf) Read(mode profBufReadMode) ([]uint64, []unsafe.Pointer, bool)
```

#### <a id="ProfBuf.Close" href="#ProfBuf.Close">func (p *ProfBuf) Close()</a>

```
searchKey: runtime.ProfBuf.Close
```

```Go
func (p *ProfBuf) Close()
```

### <a id="RWMutex" href="#RWMutex">type RWMutex struct</a>

```
searchKey: runtime.RWMutex
```

```Go
type RWMutex struct {
	rw rwmutex
}
```

#### <a id="RWMutex.RLock" href="#RWMutex.RLock">func (rw *RWMutex) RLock()</a>

```
searchKey: runtime.RWMutex.RLock
```

```Go
func (rw *RWMutex) RLock()
```

#### <a id="RWMutex.RUnlock" href="#RWMutex.RUnlock">func (rw *RWMutex) RUnlock()</a>

```
searchKey: runtime.RWMutex.RUnlock
```

```Go
func (rw *RWMutex) RUnlock()
```

#### <a id="RWMutex.Lock" href="#RWMutex.Lock">func (rw *RWMutex) Lock()</a>

```
searchKey: runtime.RWMutex.Lock
```

```Go
func (rw *RWMutex) Lock()
```

#### <a id="RWMutex.Unlock" href="#RWMutex.Unlock">func (rw *RWMutex) Unlock()</a>

```
searchKey: runtime.RWMutex.Unlock
```

```Go
func (rw *RWMutex) Unlock()
```

### <a id="G" href="#G">type G runtime.g</a>

```
searchKey: runtime.G
```

```Go
type G = g
```

#### <a id="beforeIdle" href="#beforeIdle">func beforeIdle(int64, int64) (*g, bool)</a>

```
searchKey: runtime.beforeIdle
```

```Go
func beforeIdle(int64, int64) (*g, bool)
```

#### <a id="wakefing" href="#wakefing">func wakefing() *g</a>

```
searchKey: runtime.wakefing
```

```Go
func wakefing() *g
```

#### <a id="netpollunblock" href="#netpollunblock">func netpollunblock(pd *pollDesc, mode int32, ioready bool) *g</a>

```
searchKey: runtime.netpollunblock
```

```Go
func netpollunblock(pd *pollDesc, mode int32, ioready bool) *g
```

#### <a id="atomicAllG" href="#atomicAllG">func atomicAllG() (**g, uintptr)</a>

```
searchKey: runtime.atomicAllG
```

```Go
func atomicAllG() (**g, uintptr)
```

atomicAllG returns &allgs[0] and len(allgs) for use with atomicAllGIndex. 

#### <a id="atomicAllGIndex" href="#atomicAllGIndex">func atomicAllGIndex(ptr **g, i uintptr) *g</a>

```
searchKey: runtime.atomicAllGIndex
```

```Go
func atomicAllGIndex(ptr **g, i uintptr) *g
```

atomicAllGIndex returns ptr[i] with the allgptr returned from atomicAllG. 

#### <a id="findrunnable" href="#findrunnable">func findrunnable() (gp *g, inheritTime bool)</a>

```
searchKey: runtime.findrunnable
```

```Go
func findrunnable() (gp *g, inheritTime bool)
```

Finds a runnable goroutine to execute. Tries to steal from other P's, get g from local or global queue, poll network. 

#### <a id="stealWork" href="#stealWork">func stealWork(now int64) (gp *g, inheritTime bool, rnow, pollUntil int64, newWork bool)</a>

```
searchKey: runtime.stealWork
```

```Go
func stealWork(now int64) (gp *g, inheritTime bool, rnow, pollUntil int64, newWork bool)
```

stealWork attempts to steal a runnable goroutine or timer from any P. 

If newWork is true, new work may have been readied. 

If now is not 0 it is the current time. stealWork returns the passed time or the current time if now was passed as 0. 

#### <a id="checkIdleGCNoP" href="#checkIdleGCNoP">func checkIdleGCNoP() (*p, *g)</a>

```
searchKey: runtime.checkIdleGCNoP
```

```Go
func checkIdleGCNoP() (*p, *g)
```

Check for idle-priority GC, without a P on entry. 

If some GC work, a P, and a worker G are all available, the P and G will be returned. The returned P has not been wired yet. 

#### <a id="malg" href="#malg">func malg(stacksize int32) *g</a>

```
searchKey: runtime.malg
```

```Go
func malg(stacksize int32) *g
```

Allocate a new g, with a stack big enough for stacksize bytes. 

#### <a id="newproc1" href="#newproc1">func newproc1(fn *funcval, argp unsafe.Pointer, narg int32, callergp *g, callerpc uintptr) *g</a>

```
searchKey: runtime.newproc1
```

```Go
func newproc1(fn *funcval, argp unsafe.Pointer, narg int32, callergp *g, callerpc uintptr) *g
```

Create a new g in state _Grunnable, starting at fn, with narg bytes of arguments starting at argp. callerpc is the address of the go statement that created this. The caller is responsible for adding the new g to the scheduler. 

This must run on the system stack because it's the continuation of newproc, which cannot split the stack. 

#### <a id="gfget" href="#gfget">func gfget(_p_ *p) *g</a>

```
searchKey: runtime.gfget
```

```Go
func gfget(_p_ *p) *g
```

Get from gfree list. If local list is empty, grab a batch from global list. 

#### <a id="globrunqget" href="#globrunqget">func globrunqget(_p_ *p, max int32) *g</a>

```
searchKey: runtime.globrunqget
```

```Go
func globrunqget(_p_ *p, max int32) *g
```

Try get a batch of G's from the global runnable queue. sched.lock must be held. 

#### <a id="runqget" href="#runqget">func runqget(_p_ *p) (gp *g, inheritTime bool)</a>

```
searchKey: runtime.runqget
```

```Go
func runqget(_p_ *p) (gp *g, inheritTime bool)
```

Get g from local runnable queue. If inheritTime is true, gp should inherit the remaining time in the current time slice. Otherwise, it should start a new time slice. Executed only by the owner P. 

#### <a id="runqsteal" href="#runqsteal">func runqsteal(_p_, p2 *p, stealRunNextG bool) *g</a>

```
searchKey: runtime.runqsteal
```

```Go
func runqsteal(_p_, p2 *p, stealRunNextG bool) *g
```

Steal half of elements from local runnable queue of p2 and put onto local runnable queue of p. Returns one of the stolen elements (or nil if failed). 

#### <a id="sigFetchG" href="#sigFetchG">func sigFetchG(c *sigctxt) *g</a>

```
searchKey: runtime.sigFetchG
```

```Go
func sigFetchG(c *sigctxt) *g
```

sigFetchG fetches the value of G safely when running in a signal handler. On some architectures, the g value may be clobbered when running in a VDSO. See issue #32912. 

#### <a id="getg" href="#getg">func getg() *g</a>

```
searchKey: runtime.getg
```

```Go
func getg() *g
```

getg returns the pointer to the current g. The compiler rewrites calls to this function into instructions that fetch the g directly (from TLS or from the dedicated register). 

#### <a id="traceReader" href="#traceReader">func traceReader() *g</a>

```
searchKey: runtime.traceReader
```

```Go
func traceReader() *g
```

traceReader returns the trace reader that should be woken up, if any. 

#### <a id="Getg" href="#Getg">func Getg() *G</a>

```
searchKey: runtime.Getg
```

```Go
func Getg() *G
```

### <a id="Sudog" href="#Sudog">type Sudog runtime.sudog</a>

```
searchKey: runtime.Sudog
```

```Go
type Sudog = sudog
```

#### <a id="acquireSudog" href="#acquireSudog">func acquireSudog() *sudog</a>

```
searchKey: runtime.acquireSudog
```

```Go
func acquireSudog() *sudog
```

### <a id="PallocSum" href="#PallocSum">type PallocSum runtime.pallocSum</a>

```
searchKey: runtime.PallocSum
```

```Go
type PallocSum pallocSum
```

Expose pallocSum for testing. 

#### <a id="PackPallocSum" href="#PackPallocSum">func PackPallocSum(start, max, end uint) PallocSum</a>

```
searchKey: runtime.PackPallocSum
```

```Go
func PackPallocSum(start, max, end uint) PallocSum
```

#### <a id="SummarizeSlow" href="#SummarizeSlow">func SummarizeSlow(b *PallocBits) PallocSum</a>

```
searchKey: runtime.SummarizeSlow
```

```Go
func SummarizeSlow(b *PallocBits) PallocSum
```

SummarizeSlow is a slow but more obviously correct implementation of (*pallocBits).summarize. Used for testing. 

#### <a id="PallocSum.Start" href="#PallocSum.Start">func (m PallocSum) Start() uint</a>

```
searchKey: runtime.PallocSum.Start
```

```Go
func (m PallocSum) Start() uint
```

#### <a id="PallocSum.Max" href="#PallocSum.Max">func (m PallocSum) Max() uint</a>

```
searchKey: runtime.PallocSum.Max
```

```Go
func (m PallocSum) Max() uint
```

#### <a id="PallocSum.End" href="#PallocSum.End">func (m PallocSum) End() uint</a>

```
searchKey: runtime.PallocSum.End
```

```Go
func (m PallocSum) End() uint
```

### <a id="PallocBits" href="#PallocBits">type PallocBits runtime.pallocBits</a>

```
searchKey: runtime.PallocBits
```

```Go
type PallocBits pallocBits
```

Expose pallocBits for testing. 

#### <a id="PallocBits.Find" href="#PallocBits.Find">func (b *PallocBits) Find(npages uintptr, searchIdx uint) (uint, uint)</a>

```
searchKey: runtime.PallocBits.Find
```

```Go
func (b *PallocBits) Find(npages uintptr, searchIdx uint) (uint, uint)
```

#### <a id="PallocBits.AllocRange" href="#PallocBits.AllocRange">func (b *PallocBits) AllocRange(i, n uint)</a>

```
searchKey: runtime.PallocBits.AllocRange
```

```Go
func (b *PallocBits) AllocRange(i, n uint)
```

#### <a id="PallocBits.Free" href="#PallocBits.Free">func (b *PallocBits) Free(i, n uint)</a>

```
searchKey: runtime.PallocBits.Free
```

```Go
func (b *PallocBits) Free(i, n uint)
```

#### <a id="PallocBits.Summarize" href="#PallocBits.Summarize">func (b *PallocBits) Summarize() PallocSum</a>

```
searchKey: runtime.PallocBits.Summarize
```

```Go
func (b *PallocBits) Summarize() PallocSum
```

#### <a id="PallocBits.PopcntRange" href="#PallocBits.PopcntRange">func (b *PallocBits) PopcntRange(i, n uint) uint</a>

```
searchKey: runtime.PallocBits.PopcntRange
```

```Go
func (b *PallocBits) PopcntRange(i, n uint) uint
```

### <a id="PallocData" href="#PallocData">type PallocData runtime.pallocData</a>

```
searchKey: runtime.PallocData
```

```Go
type PallocData pallocData
```

Expose pallocData for testing. 

#### <a id="PallocData.FindScavengeCandidate" href="#PallocData.FindScavengeCandidate">func (d *PallocData) FindScavengeCandidate(searchIdx uint, min, max uintptr) (uint, uint)</a>

```
searchKey: runtime.PallocData.FindScavengeCandidate
```

```Go
func (d *PallocData) FindScavengeCandidate(searchIdx uint, min, max uintptr) (uint, uint)
```

#### <a id="PallocData.AllocRange" href="#PallocData.AllocRange">func (d *PallocData) AllocRange(i, n uint)</a>

```
searchKey: runtime.PallocData.AllocRange
```

```Go
func (d *PallocData) AllocRange(i, n uint)
```

#### <a id="PallocData.ScavengedSetRange" href="#PallocData.ScavengedSetRange">func (d *PallocData) ScavengedSetRange(i, n uint)</a>

```
searchKey: runtime.PallocData.ScavengedSetRange
```

```Go
func (d *PallocData) ScavengedSetRange(i, n uint)
```

#### <a id="PallocData.PallocBits" href="#PallocData.PallocBits">func (d *PallocData) PallocBits() *PallocBits</a>

```
searchKey: runtime.PallocData.PallocBits
```

```Go
func (d *PallocData) PallocBits() *PallocBits
```

#### <a id="PallocData.Scavenged" href="#PallocData.Scavenged">func (d *PallocData) Scavenged() *PallocBits</a>

```
searchKey: runtime.PallocData.Scavenged
```

```Go
func (d *PallocData) Scavenged() *PallocBits
```

### <a id="PageCache" href="#PageCache">type PageCache runtime.pageCache</a>

```
searchKey: runtime.PageCache
```

```Go
type PageCache pageCache
```

Expose pageCache for testing. 

#### <a id="NewPageCache" href="#NewPageCache">func NewPageCache(base uintptr, cache, scav uint64) PageCache</a>

```
searchKey: runtime.NewPageCache
```

```Go
func NewPageCache(base uintptr, cache, scav uint64) PageCache
```

#### <a id="PageCache.Empty" href="#PageCache.Empty">func (c *PageCache) Empty() bool</a>

```
searchKey: runtime.PageCache.Empty
```

```Go
func (c *PageCache) Empty() bool
```

#### <a id="PageCache.Base" href="#PageCache.Base">func (c *PageCache) Base() uintptr</a>

```
searchKey: runtime.PageCache.Base
```

```Go
func (c *PageCache) Base() uintptr
```

#### <a id="PageCache.Cache" href="#PageCache.Cache">func (c *PageCache) Cache() uint64</a>

```
searchKey: runtime.PageCache.Cache
```

```Go
func (c *PageCache) Cache() uint64
```

#### <a id="PageCache.Scav" href="#PageCache.Scav">func (c *PageCache) Scav() uint64</a>

```
searchKey: runtime.PageCache.Scav
```

```Go
func (c *PageCache) Scav() uint64
```

#### <a id="PageCache.Alloc" href="#PageCache.Alloc">func (c *PageCache) Alloc(npages uintptr) (uintptr, uintptr)</a>

```
searchKey: runtime.PageCache.Alloc
```

```Go
func (c *PageCache) Alloc(npages uintptr) (uintptr, uintptr)
```

#### <a id="PageCache.Flush" href="#PageCache.Flush">func (c *PageCache) Flush(s *PageAlloc)</a>

```
searchKey: runtime.PageCache.Flush
```

```Go
func (c *PageCache) Flush(s *PageAlloc)
```

### <a id="ChunkIdx" href="#ChunkIdx">type ChunkIdx runtime.chunkIdx</a>

```
searchKey: runtime.ChunkIdx
```

```Go
type ChunkIdx chunkIdx
```

Expose chunk index type. 

### <a id="PageAlloc" href="#PageAlloc">type PageAlloc runtime.pageAlloc</a>

```
searchKey: runtime.PageAlloc
```

```Go
type PageAlloc pageAlloc
```

Expose pageAlloc for testing. Note that because pageAlloc is not in the heap, so is PageAlloc. 

#### <a id="NewPageAlloc" href="#NewPageAlloc">func NewPageAlloc(chunks, scav map[ChunkIdx][]BitRange) *PageAlloc</a>

```
searchKey: runtime.NewPageAlloc
```

```Go
func NewPageAlloc(chunks, scav map[ChunkIdx][]BitRange) *PageAlloc
```

NewPageAlloc creates a new page allocator for testing and initializes it with the scav and chunks maps. Each key in these maps represents a chunk index and each value is a series of bit ranges to set within each bitmap's chunk. 

The initialization of the pageAlloc preserves the invariant that if a scavenged bit is set the alloc bit is necessarily unset, so some of the bits described by scav may be cleared in the final bitmap if ranges in chunks overlap with them. 

scav is optional, and if nil, the scavenged bitmap will be cleared (as opposed to all 1s, which it usually is). Furthermore, every chunk index in scav must appear in chunks; ones that do not are ignored. 

#### <a id="PageAlloc.Alloc" href="#PageAlloc.Alloc">func (p *PageAlloc) Alloc(npages uintptr) (uintptr, uintptr)</a>

```
searchKey: runtime.PageAlloc.Alloc
```

```Go
func (p *PageAlloc) Alloc(npages uintptr) (uintptr, uintptr)
```

#### <a id="PageAlloc.AllocToCache" href="#PageAlloc.AllocToCache">func (p *PageAlloc) AllocToCache() PageCache</a>

```
searchKey: runtime.PageAlloc.AllocToCache
```

```Go
func (p *PageAlloc) AllocToCache() PageCache
```

#### <a id="PageAlloc.Free" href="#PageAlloc.Free">func (p *PageAlloc) Free(base, npages uintptr)</a>

```
searchKey: runtime.PageAlloc.Free
```

```Go
func (p *PageAlloc) Free(base, npages uintptr)
```

#### <a id="PageAlloc.Bounds" href="#PageAlloc.Bounds">func (p *PageAlloc) Bounds() (ChunkIdx, ChunkIdx)</a>

```
searchKey: runtime.PageAlloc.Bounds
```

```Go
func (p *PageAlloc) Bounds() (ChunkIdx, ChunkIdx)
```

#### <a id="PageAlloc.Scavenge" href="#PageAlloc.Scavenge">func (p *PageAlloc) Scavenge(nbytes uintptr, mayUnlock bool) (r uintptr)</a>

```
searchKey: runtime.PageAlloc.Scavenge
```

```Go
func (p *PageAlloc) Scavenge(nbytes uintptr, mayUnlock bool) (r uintptr)
```

#### <a id="PageAlloc.InUse" href="#PageAlloc.InUse">func (p *PageAlloc) InUse() []AddrRange</a>

```
searchKey: runtime.PageAlloc.InUse
```

```Go
func (p *PageAlloc) InUse() []AddrRange
```

#### <a id="PageAlloc.PallocData" href="#PageAlloc.PallocData">func (p *PageAlloc) PallocData(i ChunkIdx) *PallocData</a>

```
searchKey: runtime.PageAlloc.PallocData
```

```Go
func (p *PageAlloc) PallocData(i ChunkIdx) *PallocData
```

Returns nil if the PallocData's L2 is missing. 

### <a id="AddrRange" href="#AddrRange">type AddrRange struct</a>

```
searchKey: runtime.AddrRange
```

```Go
type AddrRange struct {
	addrRange
}
```

AddrRange is a wrapper around addrRange for testing. 

#### <a id="MakeAddrRange" href="#MakeAddrRange">func MakeAddrRange(base, limit uintptr) AddrRange</a>

```
searchKey: runtime.MakeAddrRange
```

```Go
func MakeAddrRange(base, limit uintptr) AddrRange
```

MakeAddrRange creates a new address range. 

#### <a id="AddrRange.Base" href="#AddrRange.Base">func (a AddrRange) Base() uintptr</a>

```
searchKey: runtime.AddrRange.Base
```

```Go
func (a AddrRange) Base() uintptr
```

Base returns the virtual base address of the address range. 

#### <a id="AddrRange.Limit" href="#AddrRange.Limit">func (a AddrRange) Limit() uintptr</a>

```
searchKey: runtime.AddrRange.Limit
```

```Go
func (a AddrRange) Limit() uintptr
```

Base returns the virtual address of the limit of the address range. 

#### <a id="AddrRange.Equals" href="#AddrRange.Equals">func (a AddrRange) Equals(b AddrRange) bool</a>

```
searchKey: runtime.AddrRange.Equals
```

```Go
func (a AddrRange) Equals(b AddrRange) bool
```

Equals returns true if the two address ranges are exactly equal. 

#### <a id="AddrRange.Size" href="#AddrRange.Size">func (a AddrRange) Size() uintptr</a>

```
searchKey: runtime.AddrRange.Size
```

```Go
func (a AddrRange) Size() uintptr
```

Size returns the size in bytes of the address range. 

### <a id="AddrRanges" href="#AddrRanges">type AddrRanges struct</a>

```
searchKey: runtime.AddrRanges
```

```Go
type AddrRanges struct {
	addrRanges
	mutable bool
}
```

AddrRanges is a wrapper around addrRanges for testing. 

#### <a id="NewAddrRanges" href="#NewAddrRanges">func NewAddrRanges() AddrRanges</a>

```
searchKey: runtime.NewAddrRanges
```

```Go
func NewAddrRanges() AddrRanges
```

NewAddrRanges creates a new empty addrRanges. 

Note that this initializes addrRanges just like in the runtime, so its memory is persistentalloc'd. Call this function sparingly since the memory it allocates is leaked. 

This AddrRanges is mutable, so we can test methods like Add. 

#### <a id="MakeAddrRanges" href="#MakeAddrRanges">func MakeAddrRanges(a ...AddrRange) AddrRanges</a>

```
searchKey: runtime.MakeAddrRanges
```

```Go
func MakeAddrRanges(a ...AddrRange) AddrRanges
```

MakeAddrRanges creates a new addrRanges populated with the ranges in a. 

The returned AddrRanges is immutable, so methods like Add will fail. 

#### <a id="AddrRanges.Ranges" href="#AddrRanges.Ranges">func (a *AddrRanges) Ranges() []AddrRange</a>

```
searchKey: runtime.AddrRanges.Ranges
```

```Go
func (a *AddrRanges) Ranges() []AddrRange
```

Ranges returns a copy of the ranges described by the addrRanges. 

#### <a id="AddrRanges.FindSucc" href="#AddrRanges.FindSucc">func (a *AddrRanges) FindSucc(base uintptr) int</a>

```
searchKey: runtime.AddrRanges.FindSucc
```

```Go
func (a *AddrRanges) FindSucc(base uintptr) int
```

FindSucc returns the successor to base. See addrRanges.findSucc for more details. 

#### <a id="AddrRanges.Add" href="#AddrRanges.Add">func (a *AddrRanges) Add(r AddrRange)</a>

```
searchKey: runtime.AddrRanges.Add
```

```Go
func (a *AddrRanges) Add(r AddrRange)
```

Add adds a new AddrRange to the AddrRanges. 

The AddrRange must be mutable (i.e. created by NewAddrRanges), otherwise this method will throw. 

#### <a id="AddrRanges.TotalBytes" href="#AddrRanges.TotalBytes">func (a *AddrRanges) TotalBytes() uintptr</a>

```
searchKey: runtime.AddrRanges.TotalBytes
```

```Go
func (a *AddrRanges) TotalBytes() uintptr
```

TotalBytes returns the totalBytes field of the addrRanges. 

### <a id="BitRange" href="#BitRange">type BitRange struct</a>

```
searchKey: runtime.BitRange
```

```Go
type BitRange struct {
	I, N uint // bit index and length in bits
}
```

BitRange represents a range over a bitmap. 

### <a id="BitsMismatch" href="#BitsMismatch">type BitsMismatch struct</a>

```
searchKey: runtime.BitsMismatch
```

```Go
type BitsMismatch struct {
	Base      uintptr
	Got, Want uint64
}
```

### <a id="MSpan" href="#MSpan">type MSpan runtime.mspan</a>

```
searchKey: runtime.MSpan
```

```Go
type MSpan mspan
```

mspan wrapper for testing. 

#### <a id="AllocMSpan" href="#AllocMSpan">func AllocMSpan() *MSpan</a>

```
searchKey: runtime.AllocMSpan
```

```Go
func AllocMSpan() *MSpan
```

Allocate an mspan for testing. 

### <a id="TimeHistogram" href="#TimeHistogram">type TimeHistogram runtime.timeHistogram</a>

```
searchKey: runtime.TimeHistogram
```

```Go
type TimeHistogram timeHistogram
```

#### <a id="TimeHistogram.Count" href="#TimeHistogram.Count">func (th *TimeHistogram) Count(bucket, subBucket uint) (uint64, bool)</a>

```
searchKey: runtime.TimeHistogram.Count
```

```Go
func (th *TimeHistogram) Count(bucket, subBucket uint) (uint64, bool)
```

Counts returns the counts for the given bucket, subBucket indices. Returns true if the bucket was valid, otherwise returns the counts for the underflow bucket and false. 

#### <a id="TimeHistogram.Record" href="#TimeHistogram.Record">func (th *TimeHistogram) Record(duration int64)</a>

```
searchKey: runtime.TimeHistogram.Record
```

```Go
func (th *TimeHistogram) Record(duration int64)
```

### <a id="M" href="#M">type M runtime.m</a>

```
searchKey: runtime.M
```

```Go
type M = m
```

#### <a id="allocm" href="#allocm">func allocm(_p_ *p, fn func(), id int64) *m</a>

```
searchKey: runtime.allocm
```

```Go
func allocm(_p_ *p, fn func(), id int64) *m
```

Allocate a new m unassociated with any thread. Can use p for allocation context if needed. fn is recorded as the new m's m.mstartfn. id is optional pre-allocated m ID. Omit by passing -1. 

This function is allowed to have write barriers even if the caller isn't because it borrows _p_. 

#### <a id="lockextra" href="#lockextra">func lockextra(nilokay bool) *m</a>

```
searchKey: runtime.lockextra
```

```Go
func lockextra(nilokay bool) *m
```

lockextra locks the extra list and returns the list head. The caller must unlock the list by storing a new list head to extram. If nilokay is true, then lockextra will return a nil list head if that's what it finds. If nilokay is false, lockextra will keep waiting until the list head is no longer nil. 

#### <a id="mget" href="#mget">func mget() *m</a>

```
searchKey: runtime.mget
```

```Go
func mget() *m
```

Try to get an m from midle list. sched.lock must be held. May run during STW, so write barriers are not allowed. 

#### <a id="acquirem" href="#acquirem">func acquirem() *m</a>

```
searchKey: runtime.acquirem
```

```Go
func acquirem() *m
```

#### <a id="traceAcquireBuffer" href="#traceAcquireBuffer">func traceAcquireBuffer() (mp *m, pid int32, bufp *traceBufPtr)</a>

```
searchKey: runtime.traceAcquireBuffer
```

```Go
func traceAcquireBuffer() (mp *m, pid int32, bufp *traceBufPtr)
```

traceAcquireBuffer returns trace buffer to use and, if necessary, locks it. 

## <a id="func" href="#func">Functions</a>

```
tags: [exported]
```

### <a id="memhash0" href="#memhash0">func memhash0(p unsafe.Pointer, h uintptr) uintptr</a>

```
searchKey: runtime.memhash0
```

```Go
func memhash0(p unsafe.Pointer, h uintptr) uintptr
```

### <a id="memhash8" href="#memhash8">func memhash8(p unsafe.Pointer, h uintptr) uintptr</a>

```
searchKey: runtime.memhash8
```

```Go
func memhash8(p unsafe.Pointer, h uintptr) uintptr
```

### <a id="memhash16" href="#memhash16">func memhash16(p unsafe.Pointer, h uintptr) uintptr</a>

```
searchKey: runtime.memhash16
```

```Go
func memhash16(p unsafe.Pointer, h uintptr) uintptr
```

### <a id="memhash128" href="#memhash128">func memhash128(p unsafe.Pointer, h uintptr) uintptr</a>

```
searchKey: runtime.memhash128
```

```Go
func memhash128(p unsafe.Pointer, h uintptr) uintptr
```

### <a id="memhash_varlen" href="#memhash_varlen">func memhash_varlen(p unsafe.Pointer, h uintptr) uintptr</a>

```
searchKey: runtime.memhash_varlen
```

```Go
func memhash_varlen(p unsafe.Pointer, h uintptr) uintptr
```

### <a id="memhash" href="#memhash">func memhash(p unsafe.Pointer, h, s uintptr) uintptr</a>

```
searchKey: runtime.memhash
```

```Go
func memhash(p unsafe.Pointer, h, s uintptr) uintptr
```

in asm_*.s 

### <a id="memhash32" href="#memhash32">func memhash32(p unsafe.Pointer, h uintptr) uintptr</a>

```
searchKey: runtime.memhash32
```

```Go
func memhash32(p unsafe.Pointer, h uintptr) uintptr
```

### <a id="memhash64" href="#memhash64">func memhash64(p unsafe.Pointer, h uintptr) uintptr</a>

```
searchKey: runtime.memhash64
```

```Go
func memhash64(p unsafe.Pointer, h uintptr) uintptr
```

### <a id="strhash" href="#strhash">func strhash(p unsafe.Pointer, h uintptr) uintptr</a>

```
searchKey: runtime.strhash
```

```Go
func strhash(p unsafe.Pointer, h uintptr) uintptr
```

### <a id="strhashFallback" href="#strhashFallback">func strhashFallback(a unsafe.Pointer, h uintptr) uintptr</a>

```
searchKey: runtime.strhashFallback
```

```Go
func strhashFallback(a unsafe.Pointer, h uintptr) uintptr
```

### <a id="f32hash" href="#f32hash">func f32hash(p unsafe.Pointer, h uintptr) uintptr</a>

```
searchKey: runtime.f32hash
```

```Go
func f32hash(p unsafe.Pointer, h uintptr) uintptr
```

### <a id="f64hash" href="#f64hash">func f64hash(p unsafe.Pointer, h uintptr) uintptr</a>

```
searchKey: runtime.f64hash
```

```Go
func f64hash(p unsafe.Pointer, h uintptr) uintptr
```

### <a id="c64hash" href="#c64hash">func c64hash(p unsafe.Pointer, h uintptr) uintptr</a>

```
searchKey: runtime.c64hash
```

```Go
func c64hash(p unsafe.Pointer, h uintptr) uintptr
```

### <a id="c128hash" href="#c128hash">func c128hash(p unsafe.Pointer, h uintptr) uintptr</a>

```
searchKey: runtime.c128hash
```

```Go
func c128hash(p unsafe.Pointer, h uintptr) uintptr
```

### <a id="interhash" href="#interhash">func interhash(p unsafe.Pointer, h uintptr) uintptr</a>

```
searchKey: runtime.interhash
```

```Go
func interhash(p unsafe.Pointer, h uintptr) uintptr
```

### <a id="nilinterhash" href="#nilinterhash">func nilinterhash(p unsafe.Pointer, h uintptr) uintptr</a>

```
searchKey: runtime.nilinterhash
```

```Go
func nilinterhash(p unsafe.Pointer, h uintptr) uintptr
```

### <a id="typehash" href="#typehash">func typehash(t *_type, p unsafe.Pointer, h uintptr) uintptr</a>

```
searchKey: runtime.typehash
```

```Go
func typehash(t *_type, p unsafe.Pointer, h uintptr) uintptr
```

typehash computes the hash of the object of type t at address p. h is the seed. This function is seldom used. Most maps use for hashing either fixed functions (e.g. f32hash) or compiler-generated functions (e.g. for a type like struct { x, y string }). This implementation is slower but more general and is used for hashing interface types (called from interhash or nilinterhash, above) or for hashing in maps generated by reflect.MapOf (reflect_typehash, below). Note: this function must match the compiler generated functions exactly. See issue 37716. 

### <a id="reflect_typehash" href="#reflect_typehash">func reflect_typehash(t *_type, p unsafe.Pointer, h uintptr) uintptr</a>

```
searchKey: runtime.reflect_typehash
```

```Go
func reflect_typehash(t *_type, p unsafe.Pointer, h uintptr) uintptr
```

### <a id="memequal0" href="#memequal0">func memequal0(p, q unsafe.Pointer) bool</a>

```
searchKey: runtime.memequal0
```

```Go
func memequal0(p, q unsafe.Pointer) bool
```

### <a id="memequal8" href="#memequal8">func memequal8(p, q unsafe.Pointer) bool</a>

```
searchKey: runtime.memequal8
```

```Go
func memequal8(p, q unsafe.Pointer) bool
```

### <a id="memequal16" href="#memequal16">func memequal16(p, q unsafe.Pointer) bool</a>

```
searchKey: runtime.memequal16
```

```Go
func memequal16(p, q unsafe.Pointer) bool
```

### <a id="memequal32" href="#memequal32">func memequal32(p, q unsafe.Pointer) bool</a>

```
searchKey: runtime.memequal32
```

```Go
func memequal32(p, q unsafe.Pointer) bool
```

### <a id="memequal64" href="#memequal64">func memequal64(p, q unsafe.Pointer) bool</a>

```
searchKey: runtime.memequal64
```

```Go
func memequal64(p, q unsafe.Pointer) bool
```

### <a id="memequal128" href="#memequal128">func memequal128(p, q unsafe.Pointer) bool</a>

```
searchKey: runtime.memequal128
```

```Go
func memequal128(p, q unsafe.Pointer) bool
```

### <a id="f32equal" href="#f32equal">func f32equal(p, q unsafe.Pointer) bool</a>

```
searchKey: runtime.f32equal
```

```Go
func f32equal(p, q unsafe.Pointer) bool
```

### <a id="f64equal" href="#f64equal">func f64equal(p, q unsafe.Pointer) bool</a>

```
searchKey: runtime.f64equal
```

```Go
func f64equal(p, q unsafe.Pointer) bool
```

### <a id="c64equal" href="#c64equal">func c64equal(p, q unsafe.Pointer) bool</a>

```
searchKey: runtime.c64equal
```

```Go
func c64equal(p, q unsafe.Pointer) bool
```

### <a id="c128equal" href="#c128equal">func c128equal(p, q unsafe.Pointer) bool</a>

```
searchKey: runtime.c128equal
```

```Go
func c128equal(p, q unsafe.Pointer) bool
```

### <a id="strequal" href="#strequal">func strequal(p, q unsafe.Pointer) bool</a>

```
searchKey: runtime.strequal
```

```Go
func strequal(p, q unsafe.Pointer) bool
```

### <a id="interequal" href="#interequal">func interequal(p, q unsafe.Pointer) bool</a>

```
searchKey: runtime.interequal
```

```Go
func interequal(p, q unsafe.Pointer) bool
```

### <a id="nilinterequal" href="#nilinterequal">func nilinterequal(p, q unsafe.Pointer) bool</a>

```
searchKey: runtime.nilinterequal
```

```Go
func nilinterequal(p, q unsafe.Pointer) bool
```

### <a id="efaceeq" href="#efaceeq">func efaceeq(t *_type, x, y unsafe.Pointer) bool</a>

```
searchKey: runtime.efaceeq
```

```Go
func efaceeq(t *_type, x, y unsafe.Pointer) bool
```

### <a id="ifaceeq" href="#ifaceeq">func ifaceeq(tab *itab, x, y unsafe.Pointer) bool</a>

```
searchKey: runtime.ifaceeq
```

```Go
func ifaceeq(tab *itab, x, y unsafe.Pointer) bool
```

### <a id="stringHash" href="#stringHash">func stringHash(s string, seed uintptr) uintptr</a>

```
searchKey: runtime.stringHash
```

```Go
func stringHash(s string, seed uintptr) uintptr
```

Testing adapters for hash quality tests (see hash_test.go) 

### <a id="bytesHash" href="#bytesHash">func bytesHash(b []byte, seed uintptr) uintptr</a>

```
searchKey: runtime.bytesHash
```

```Go
func bytesHash(b []byte, seed uintptr) uintptr
```

### <a id="int32Hash" href="#int32Hash">func int32Hash(i uint32, seed uintptr) uintptr</a>

```
searchKey: runtime.int32Hash
```

```Go
func int32Hash(i uint32, seed uintptr) uintptr
```

### <a id="int64Hash" href="#int64Hash">func int64Hash(i uint64, seed uintptr) uintptr</a>

```
searchKey: runtime.int64Hash
```

```Go
func int64Hash(i uint64, seed uintptr) uintptr
```

### <a id="efaceHash" href="#efaceHash">func efaceHash(i interface{}, seed uintptr) uintptr</a>

```
searchKey: runtime.efaceHash
```

```Go
func efaceHash(i interface{}, seed uintptr) uintptr
```

### <a id="ifaceHash" href="#ifaceHash">func ifaceHash(i interface {...</a>

```
searchKey: runtime.ifaceHash
```

```Go
func ifaceHash(i interface {
	F()
}, seed uintptr) uintptr
```

### <a id="alginit" href="#alginit">func alginit()</a>

```
searchKey: runtime.alginit
```

```Go
func alginit()
```

### <a id="initAlgAES" href="#initAlgAES">func initAlgAES()</a>

```
searchKey: runtime.initAlgAES
```

```Go
func initAlgAES()
```

### <a id="readUnaligned32" href="#readUnaligned32">func readUnaligned32(p unsafe.Pointer) uint32</a>

```
searchKey: runtime.readUnaligned32
```

```Go
func readUnaligned32(p unsafe.Pointer) uint32
```

Note: These routines perform the read with a native endianness. 

### <a id="readUnaligned64" href="#readUnaligned64">func readUnaligned64(p unsafe.Pointer) uint64</a>

```
searchKey: runtime.readUnaligned64
```

```Go
func readUnaligned64(p unsafe.Pointer) uint64
```

### <a id="atomicwb" href="#atomicwb">func atomicwb(ptr *unsafe.Pointer, new unsafe.Pointer)</a>

```
searchKey: runtime.atomicwb
```

```Go
func atomicwb(ptr *unsafe.Pointer, new unsafe.Pointer)
```

atomicwb performs a write barrier before an atomic pointer write. The caller should guard the call with "if writeBarrier.enabled". 

### <a id="atomicstorep" href="#atomicstorep">func atomicstorep(ptr unsafe.Pointer, new unsafe.Pointer)</a>

```
searchKey: runtime.atomicstorep
```

```Go
func atomicstorep(ptr unsafe.Pointer, new unsafe.Pointer)
```

atomicstorep performs *ptr = new atomically and invokes a write barrier. 

### <a id="sync_atomic_StoreUintptr" href="#sync_atomic_StoreUintptr">func sync_atomic_StoreUintptr(ptr *uintptr, new uintptr)</a>

```
searchKey: runtime.sync_atomic_StoreUintptr
```

```Go
func sync_atomic_StoreUintptr(ptr *uintptr, new uintptr)
```

### <a id="sync_atomic_StorePointer" href="#sync_atomic_StorePointer">func sync_atomic_StorePointer(ptr *unsafe.Pointer, new unsafe.Pointer)</a>

```
searchKey: runtime.sync_atomic_StorePointer
```

```Go
func sync_atomic_StorePointer(ptr *unsafe.Pointer, new unsafe.Pointer)
```

### <a id="sync_atomic_SwapUintptr" href="#sync_atomic_SwapUintptr">func sync_atomic_SwapUintptr(ptr *uintptr, new uintptr) uintptr</a>

```
searchKey: runtime.sync_atomic_SwapUintptr
```

```Go
func sync_atomic_SwapUintptr(ptr *uintptr, new uintptr) uintptr
```

### <a id="sync_atomic_SwapPointer" href="#sync_atomic_SwapPointer">func sync_atomic_SwapPointer(ptr *unsafe.Pointer, new unsafe.Pointer) unsafe.Pointer</a>

```
searchKey: runtime.sync_atomic_SwapPointer
```

```Go
func sync_atomic_SwapPointer(ptr *unsafe.Pointer, new unsafe.Pointer) unsafe.Pointer
```

### <a id="sync_atomic_CompareAndSwapUintptr" href="#sync_atomic_CompareAndSwapUintptr">func sync_atomic_CompareAndSwapUintptr(ptr *uintptr, old, new uintptr) bool</a>

```
searchKey: runtime.sync_atomic_CompareAndSwapUintptr
```

```Go
func sync_atomic_CompareAndSwapUintptr(ptr *uintptr, old, new uintptr) bool
```

### <a id="sync_atomic_CompareAndSwapPointer" href="#sync_atomic_CompareAndSwapPointer">func sync_atomic_CompareAndSwapPointer(ptr *unsafe.Pointer, old, new unsafe.Pointer) bool</a>

```
searchKey: runtime.sync_atomic_CompareAndSwapPointer
```

```Go
func sync_atomic_CompareAndSwapPointer(ptr *unsafe.Pointer, old, new unsafe.Pointer) bool
```

### <a id="cgoUse" href="#cgoUse">func cgoUse(interface{})</a>

```
searchKey: runtime.cgoUse
```

```Go
func cgoUse(interface{})
```

cgoUse is called by cgo-generated code (using go:linkname to get at an unexported name). The calls serve two purposes: 1) they are opaque to escape analysis, so the argument is considered to escape to the heap. 2) they keep the argument alive until the call site; the call is emitted after the end of the (presumed) use of the argument by C. cgoUse should not actually be called (see cgoAlwaysFalse). 

### <a id="syscall_cgocaller" href="#syscall_cgocaller">func syscall_cgocaller(fn unsafe.Pointer, args ...uintptr) uintptr</a>

```
searchKey: runtime.syscall_cgocaller
```

```Go
func syscall_cgocaller(fn unsafe.Pointer, args ...uintptr) uintptr
```

wrapper for syscall package to call cgocall for libc (cgo) calls. 

### <a id="cgocall" href="#cgocall">func cgocall(fn, arg unsafe.Pointer) int32</a>

```
searchKey: runtime.cgocall
```

```Go
func cgocall(fn, arg unsafe.Pointer) int32
```

Call from Go to C. 

This must be nosplit because it's used for syscalls on some platforms. Syscalls may have untyped arguments on the stack, so it's not safe to grow or scan the stack. 

### <a id="cgocallbackg" href="#cgocallbackg">func cgocallbackg(fn, frame unsafe.Pointer, ctxt uintptr)</a>

```
searchKey: runtime.cgocallbackg
```

```Go
func cgocallbackg(fn, frame unsafe.Pointer, ctxt uintptr)
```

Call from C back to Go. fn must point to an ABIInternal Go entry-point. 

### <a id="cgocallbackg1" href="#cgocallbackg1">func cgocallbackg1(fn, frame unsafe.Pointer, ctxt uintptr)</a>

```
searchKey: runtime.cgocallbackg1
```

```Go
func cgocallbackg1(fn, frame unsafe.Pointer, ctxt uintptr)
```

### <a id="unwindm" href="#unwindm">func unwindm(restore *bool)</a>

```
searchKey: runtime.unwindm
```

```Go
func unwindm(restore *bool)
```

### <a id="badcgocallback" href="#badcgocallback">func badcgocallback()</a>

```
searchKey: runtime.badcgocallback
```

```Go
func badcgocallback()
```

called from assembly 

### <a id="cgounimpl" href="#cgounimpl">func cgounimpl()</a>

```
searchKey: runtime.cgounimpl
```

```Go
func cgounimpl()
```

called from (incomplete) assembly 

### <a id="cgoCheckPointer" href="#cgoCheckPointer">func cgoCheckPointer(ptr interface{}, arg interface{})</a>

```
searchKey: runtime.cgoCheckPointer
```

```Go
func cgoCheckPointer(ptr interface{}, arg interface{})
```

cgoCheckPointer checks if the argument contains a Go pointer that points to a Go pointer, and panics if it does. 

### <a id="cgoCheckArg" href="#cgoCheckArg">func cgoCheckArg(t *_type, p unsafe.Pointer, indir, top bool, msg string)</a>

```
searchKey: runtime.cgoCheckArg
```

```Go
func cgoCheckArg(t *_type, p unsafe.Pointer, indir, top bool, msg string)
```

cgoCheckArg is the real work of cgoCheckPointer. The argument p is either a pointer to the value (of type t), or the value itself, depending on indir. The top parameter is whether we are at the top level, where Go pointers are allowed. 

### <a id="cgoCheckUnknownPointer" href="#cgoCheckUnknownPointer">func cgoCheckUnknownPointer(p unsafe.Pointer, msg string) (base, i uintptr)</a>

```
searchKey: runtime.cgoCheckUnknownPointer
```

```Go
func cgoCheckUnknownPointer(p unsafe.Pointer, msg string) (base, i uintptr)
```

cgoCheckUnknownPointer is called for an arbitrary pointer into Go memory. It checks whether that Go memory contains any other pointer into Go memory. If it does, we panic. The return values are unused but useful to see in panic tracebacks. 

### <a id="cgoIsGoPointer" href="#cgoIsGoPointer">func cgoIsGoPointer(p unsafe.Pointer) bool</a>

```
searchKey: runtime.cgoIsGoPointer
```

```Go
func cgoIsGoPointer(p unsafe.Pointer) bool
```

cgoIsGoPointer reports whether the pointer is a Go pointer--a pointer to Go memory. We only care about Go memory that might contain pointers. 

### <a id="cgoInRange" href="#cgoInRange">func cgoInRange(p unsafe.Pointer, start, end uintptr) bool</a>

```
searchKey: runtime.cgoInRange
```

```Go
func cgoInRange(p unsafe.Pointer, start, end uintptr) bool
```

cgoInRange reports whether p is between start and end. 

### <a id="cgoCheckResult" href="#cgoCheckResult">func cgoCheckResult(val interface{})</a>

```
searchKey: runtime.cgoCheckResult
```

```Go
func cgoCheckResult(val interface{})
```

cgoCheckResult is called to check the result parameter of an exported Go function. It panics if the result is or contains a Go pointer. 

### <a id="_cgo_panic_internal" href="#_cgo_panic_internal">func _cgo_panic_internal(p *byte)</a>

```
searchKey: runtime._cgo_panic_internal
```

```Go
func _cgo_panic_internal(p *byte)
```

### <a id="cgoCheckWriteBarrier" href="#cgoCheckWriteBarrier">func cgoCheckWriteBarrier(dst *uintptr, src uintptr)</a>

```
searchKey: runtime.cgoCheckWriteBarrier
```

```Go
func cgoCheckWriteBarrier(dst *uintptr, src uintptr)
```

cgoCheckWriteBarrier is called whenever a pointer is stored into memory. It throws if the program is storing a Go pointer into non-Go memory. 

This is called from the write barrier, so its entire call tree must be nosplit. 

### <a id="cgoCheckMemmove" href="#cgoCheckMemmove">func cgoCheckMemmove(typ *_type, dst, src unsafe.Pointer, off, size uintptr)</a>

```
searchKey: runtime.cgoCheckMemmove
```

```Go
func cgoCheckMemmove(typ *_type, dst, src unsafe.Pointer, off, size uintptr)
```

cgoCheckMemmove is called when moving a block of memory. dst and src point off bytes into the value to copy. size is the number of bytes to copy. It throws if the program is copying a block that contains a Go pointer into non-Go memory. 

### <a id="cgoCheckSliceCopy" href="#cgoCheckSliceCopy">func cgoCheckSliceCopy(typ *_type, dst, src unsafe.Pointer, n int)</a>

```
searchKey: runtime.cgoCheckSliceCopy
```

```Go
func cgoCheckSliceCopy(typ *_type, dst, src unsafe.Pointer, n int)
```

cgoCheckSliceCopy is called when copying n elements of a slice. src and dst are pointers to the first element of the slice. typ is the element type of the slice. It throws if the program is copying slice elements that contain Go pointers into non-Go memory. 

### <a id="cgoCheckTypedBlock" href="#cgoCheckTypedBlock">func cgoCheckTypedBlock(typ *_type, src unsafe.Pointer, off, size uintptr)</a>

```
searchKey: runtime.cgoCheckTypedBlock
```

```Go
func cgoCheckTypedBlock(typ *_type, src unsafe.Pointer, off, size uintptr)
```

cgoCheckTypedBlock checks the block of memory at src, for up to size bytes, and throws if it finds a Go pointer. The type of the memory is typ, and src is off bytes into that type. 

### <a id="cgoCheckBits" href="#cgoCheckBits">func cgoCheckBits(src unsafe.Pointer, gcbits *byte, off, size uintptr)</a>

```
searchKey: runtime.cgoCheckBits
```

```Go
func cgoCheckBits(src unsafe.Pointer, gcbits *byte, off, size uintptr)
```

cgoCheckBits checks the block of memory at src, for up to size bytes, and throws if it finds a Go pointer. The gcbits mark each pointer value. The src pointer is off bytes into the gcbits. 

### <a id="cgoCheckUsingType" href="#cgoCheckUsingType">func cgoCheckUsingType(typ *_type, src unsafe.Pointer, off, size uintptr)</a>

```
searchKey: runtime.cgoCheckUsingType
```

```Go
func cgoCheckUsingType(typ *_type, src unsafe.Pointer, off, size uintptr)
```

cgoCheckUsingType is like cgoCheckTypedBlock, but is a last ditch fall back to look for pointers in src using the type information. We only use this when looking at a value on the stack when the type uses a GC program, because otherwise it's more efficient to use the GC bits. This is called on the system stack. 

### <a id="chanbuf" href="#chanbuf">func chanbuf(c *hchan, i uint) unsafe.Pointer</a>

```
searchKey: runtime.chanbuf
```

```Go
func chanbuf(c *hchan, i uint) unsafe.Pointer
```

chanbuf(c, i) is pointer to the i'th slot in the buffer. 

### <a id="full" href="#full">func full(c *hchan) bool</a>

```
searchKey: runtime.full
```

```Go
func full(c *hchan) bool
```

full reports whether a send on c would block (that is, the channel is full). It uses a single word-sized read of mutable state, so although the answer is instantaneously true, the correct answer may have changed by the time the calling function receives the return value. 

### <a id="chansend1" href="#chansend1">func chansend1(c *hchan, elem unsafe.Pointer)</a>

```
searchKey: runtime.chansend1
```

```Go
func chansend1(c *hchan, elem unsafe.Pointer)
```

entry point for c <- x from compiled code 

### <a id="chansend" href="#chansend">func chansend(c *hchan, ep unsafe.Pointer, block bool, callerpc uintptr) bool</a>

```
searchKey: runtime.chansend
```

```Go
func chansend(c *hchan, ep unsafe.Pointer, block bool, callerpc uintptr) bool
```

* generic single channel send/recv * If block is not nil, * then the protocol will not * sleep but return if it could * not complete. * * sleep can wake up with g.param == nil * when a channel involved in the sleep has * been closed.  it is easiest to loop and re-run * the operation; we'll see that it's now closed. 

### <a id="send" href="#send">func send(c *hchan, sg *sudog, ep unsafe.Pointer, unlockf func(), skip int)</a>

```
searchKey: runtime.send
```

```Go
func send(c *hchan, sg *sudog, ep unsafe.Pointer, unlockf func(), skip int)
```

send processes a send operation on an empty channel c. The value ep sent by the sender is copied to the receiver sg. The receiver is then woken up to go on its merry way. Channel c must be empty and locked.  send unlocks c with unlockf. sg must already be dequeued from c. ep must be non-nil and point to the heap or the caller's stack. 

### <a id="sendDirect" href="#sendDirect">func sendDirect(t *_type, sg *sudog, src unsafe.Pointer)</a>

```
searchKey: runtime.sendDirect
```

```Go
func sendDirect(t *_type, sg *sudog, src unsafe.Pointer)
```

### <a id="recvDirect" href="#recvDirect">func recvDirect(t *_type, sg *sudog, dst unsafe.Pointer)</a>

```
searchKey: runtime.recvDirect
```

```Go
func recvDirect(t *_type, sg *sudog, dst unsafe.Pointer)
```

### <a id="closechan" href="#closechan">func closechan(c *hchan)</a>

```
searchKey: runtime.closechan
```

```Go
func closechan(c *hchan)
```

### <a id="empty" href="#empty">func empty(c *hchan) bool</a>

```
searchKey: runtime.empty
```

```Go
func empty(c *hchan) bool
```

empty reports whether a read from c would block (that is, the channel is empty).  It uses a single atomic read of mutable state. 

### <a id="chanrecv1" href="#chanrecv1">func chanrecv1(c *hchan, elem unsafe.Pointer)</a>

```
searchKey: runtime.chanrecv1
```

```Go
func chanrecv1(c *hchan, elem unsafe.Pointer)
```

entry points for <- c from compiled code 

### <a id="chanrecv2" href="#chanrecv2">func chanrecv2(c *hchan, elem unsafe.Pointer) (received bool)</a>

```
searchKey: runtime.chanrecv2
```

```Go
func chanrecv2(c *hchan, elem unsafe.Pointer) (received bool)
```

### <a id="chanrecv" href="#chanrecv">func chanrecv(c *hchan, ep unsafe.Pointer, block bool) (selected, received bool)</a>

```
searchKey: runtime.chanrecv
```

```Go
func chanrecv(c *hchan, ep unsafe.Pointer, block bool) (selected, received bool)
```

chanrecv receives on channel c and writes the received data to ep. ep may be nil, in which case received data is ignored. If block == false and no elements are available, returns (false, false). Otherwise, if c is closed, zeros *ep and returns (true, false). Otherwise, fills in *ep with an element and returns (true, true). A non-nil ep must point to the heap or the caller's stack. 

### <a id="recv" href="#recv">func recv(c *hchan, sg *sudog, ep unsafe.Pointer, unlockf func(), skip int)</a>

```
searchKey: runtime.recv
```

```Go
func recv(c *hchan, sg *sudog, ep unsafe.Pointer, unlockf func(), skip int)
```

recv processes a receive operation on a full channel c. There are 2 parts: 1) The value sent by the sender sg is put into the channel 

```
and the sender is woken up to go on its merry way.

```
2) The value received by the receiver (the current G) is 

```
written to ep.

```
For synchronous channels, both values are the same. For asynchronous channels, the receiver gets its data from the channel buffer and the sender's data is put in the channel buffer. Channel c must be full and locked. recv unlocks c with unlockf. sg must already be dequeued from c. A non-nil ep must point to the heap or the caller's stack. 

### <a id="chanparkcommit" href="#chanparkcommit">func chanparkcommit(gp *g, chanLock unsafe.Pointer) bool</a>

```
searchKey: runtime.chanparkcommit
```

```Go
func chanparkcommit(gp *g, chanLock unsafe.Pointer) bool
```

### <a id="selectnbsend" href="#selectnbsend">func selectnbsend(c *hchan, elem unsafe.Pointer) (selected bool)</a>

```
searchKey: runtime.selectnbsend
```

```Go
func selectnbsend(c *hchan, elem unsafe.Pointer) (selected bool)
```

compiler implements 

```
select {
case c <- v:
	... foo
default:
	... bar
}

```
as 

```
if selectnbsend(c, v) {
	... foo
} else {
	... bar
}

```
### <a id="selectnbrecv" href="#selectnbrecv">func selectnbrecv(elem unsafe.Pointer, c *hchan) (selected, received bool)</a>

```
searchKey: runtime.selectnbrecv
```

```Go
func selectnbrecv(elem unsafe.Pointer, c *hchan) (selected, received bool)
```

compiler implements 

```
select {
case v, ok = <-c:
	... foo
default:
	... bar
}

```
as 

```
if selected, ok = selectnbrecv(&v, c); selected {
	... foo
} else {
	... bar
}

```
### <a id="reflect_chansend" href="#reflect_chansend">func reflect_chansend(c *hchan, elem unsafe.Pointer, nb bool) (selected bool)</a>

```
searchKey: runtime.reflect_chansend
```

```Go
func reflect_chansend(c *hchan, elem unsafe.Pointer, nb bool) (selected bool)
```

### <a id="reflect_chanrecv" href="#reflect_chanrecv">func reflect_chanrecv(c *hchan, nb bool, elem unsafe.Pointer) (selected bool, received bool)</a>

```
searchKey: runtime.reflect_chanrecv
```

```Go
func reflect_chanrecv(c *hchan, nb bool, elem unsafe.Pointer) (selected bool, received bool)
```

### <a id="reflect_chanlen" href="#reflect_chanlen">func reflect_chanlen(c *hchan) int</a>

```
searchKey: runtime.reflect_chanlen
```

```Go
func reflect_chanlen(c *hchan) int
```

### <a id="reflectlite_chanlen" href="#reflectlite_chanlen">func reflectlite_chanlen(c *hchan) int</a>

```
searchKey: runtime.reflectlite_chanlen
```

```Go
func reflectlite_chanlen(c *hchan) int
```

### <a id="reflect_chancap" href="#reflect_chancap">func reflect_chancap(c *hchan) int</a>

```
searchKey: runtime.reflect_chancap
```

```Go
func reflect_chancap(c *hchan) int
```

### <a id="reflect_chanclose" href="#reflect_chanclose">func reflect_chanclose(c *hchan)</a>

```
searchKey: runtime.reflect_chanclose
```

```Go
func reflect_chanclose(c *hchan)
```

### <a id="racesync" href="#racesync">func racesync(c *hchan, sg *sudog)</a>

```
searchKey: runtime.racesync
```

```Go
func racesync(c *hchan, sg *sudog)
```

### <a id="racenotify" href="#racenotify">func racenotify(c *hchan, idx uint, sg *sudog)</a>

```
searchKey: runtime.racenotify
```

```Go
func racenotify(c *hchan, idx uint, sg *sudog)
```

Notify the race detector of a send or receive involving buffer entry idx and a channel c or its communicating partner sg. This function handles the special case of c.elemsize==0. 

### <a id="checkptrAlignment" href="#checkptrAlignment">func checkptrAlignment(p unsafe.Pointer, elem *_type, n uintptr)</a>

```
searchKey: runtime.checkptrAlignment
```

```Go
func checkptrAlignment(p unsafe.Pointer, elem *_type, n uintptr)
```

### <a id="checkptrArithmetic" href="#checkptrArithmetic">func checkptrArithmetic(p unsafe.Pointer, originals []unsafe.Pointer)</a>

```
searchKey: runtime.checkptrArithmetic
```

```Go
func checkptrArithmetic(p unsafe.Pointer, originals []unsafe.Pointer)
```

### <a id="checkptrBase" href="#checkptrBase">func checkptrBase(p unsafe.Pointer) uintptr</a>

```
searchKey: runtime.checkptrBase
```

```Go
func checkptrBase(p unsafe.Pointer) uintptr
```

checkptrBase returns the base address for the allocation containing the address p. 

Importantly, if p1 and p2 point into the same variable, then checkptrBase(p1) == checkptrBase(p2). However, the converse/inverse is not necessarily true as allocations can have trailing padding, and multiple variables may be packed into a single allocation. 

### <a id="inf2one" href="#inf2one">func inf2one(f float64) float64</a>

```
searchKey: runtime.inf2one
```

```Go
func inf2one(f float64) float64
```

inf2one returns a signed 1 if f is an infinity and a signed 0 otherwise. The sign of the result is the sign of f. 

### <a id="complex128div" href="#complex128div">func complex128div(n complex128, m complex128) complex128</a>

```
searchKey: runtime.complex128div
```

```Go
func complex128div(n complex128, m complex128) complex128
```

### <a id="init" href="#init">func init()</a>

```
searchKey: runtime.init
```

```Go
func init()
```

### <a id="SetCPUProfileRate" href="#SetCPUProfileRate">func SetCPUProfileRate(hz int)</a>

```
searchKey: runtime.SetCPUProfileRate
tags: [exported]
```

```Go
func SetCPUProfileRate(hz int)
```

SetCPUProfileRate sets the CPU profiling rate to hz samples per second. If hz <= 0, SetCPUProfileRate turns off profiling. If the profiler is on, the rate cannot be changed without first turning it off. 

Most clients should use the runtime/pprof package or the testing package's -test.cpuprofile flag instead of calling SetCPUProfileRate directly. 

### <a id="CPUProfile" href="#CPUProfile">func CPUProfile() []byte</a>

```
searchKey: runtime.CPUProfile
tags: [exported deprecated]
```

```Go
func CPUProfile() []byte
```

CPUProfile panics. It formerly provided raw access to chunks of a pprof-format profile generated by the runtime. The details of generating that format have changed, so this functionality has been removed. 

Deprecated: Use the runtime/pprof package, or the handlers in the net/http/pprof package, or the testing package's -test.cpuprofile flag instead. 

### <a id="runtime_pprof_runtime_cyclesPerSecond" href="#runtime_pprof_runtime_cyclesPerSecond">func runtime_pprof_runtime_cyclesPerSecond() int64</a>

```
searchKey: runtime.runtime_pprof_runtime_cyclesPerSecond
```

```Go
func runtime_pprof_runtime_cyclesPerSecond() int64
```

### <a id="runtime_pprof_readProfile" href="#runtime_pprof_readProfile">func runtime_pprof_readProfile() ([]uint64, []unsafe.Pointer, bool)</a>

```
searchKey: runtime.runtime_pprof_readProfile
```

```Go
func runtime_pprof_readProfile() ([]uint64, []unsafe.Pointer, bool)
```

readProfile, provided to runtime/pprof, returns the next chunk of binary CPU profiling stack trace data, blocking until data is available. If profiling is turned off and all the profile data accumulated while it was on has been returned, readProfile returns eof=true. The caller must save the returned data and tags before calling readProfile again. 

### <a id="cputicks" href="#cputicks">func cputicks() int64</a>

```
searchKey: runtime.cputicks
```

```Go
func cputicks() int64
```

careful: cputicks is not guaranteed to be monotonic! In particular, we have noticed drift between cpus on certain os/arch combinations. See issue 8976. 

### <a id="GOMAXPROCS" href="#GOMAXPROCS">func GOMAXPROCS(n int) int</a>

```
searchKey: runtime.GOMAXPROCS
tags: [exported]
```

```Go
func GOMAXPROCS(n int) int
```

GOMAXPROCS sets the maximum number of CPUs that can be executing simultaneously and returns the previous setting. It defaults to the value of runtime.NumCPU. If n < 1, it does not change the current setting. This call will go away when the scheduler improves. 

### <a id="NumCPU" href="#NumCPU">func NumCPU() int</a>

```
searchKey: runtime.NumCPU
tags: [exported]
```

```Go
func NumCPU() int
```

NumCPU returns the number of logical CPUs usable by the current process. 

The set of available CPUs is checked by querying the operating system at process startup. Changes to operating system CPU allocation after process startup are not reflected. 

### <a id="NumCgoCall" href="#NumCgoCall">func NumCgoCall() int64</a>

```
searchKey: runtime.NumCgoCall
tags: [exported]
```

```Go
func NumCgoCall() int64
```

NumCgoCall returns the number of cgo calls made by the current process. 

### <a id="NumGoroutine" href="#NumGoroutine">func NumGoroutine() int</a>

```
searchKey: runtime.NumGoroutine
tags: [exported]
```

```Go
func NumGoroutine() int
```

NumGoroutine returns the number of goroutines that currently exist. 

### <a id="debug_modinfo" href="#debug_modinfo">func debug_modinfo() string</a>

```
searchKey: runtime.debug_modinfo
```

```Go
func debug_modinfo() string
```

### <a id="debugCallV2" href="#debugCallV2">func debugCallV2()</a>

```
searchKey: runtime.debugCallV2
```

```Go
func debugCallV2()
```

### <a id="debugCallPanicked" href="#debugCallPanicked">func debugCallPanicked(val interface{})</a>

```
searchKey: runtime.debugCallPanicked
```

```Go
func debugCallPanicked(val interface{})
```

### <a id="debugCallCheck" href="#debugCallCheck">func debugCallCheck(pc uintptr) string</a>

```
searchKey: runtime.debugCallCheck
```

```Go
func debugCallCheck(pc uintptr) string
```

debugCallCheck checks whether it is safe to inject a debugger function call with return PC pc. If not, it returns a string explaining why. 

### <a id="debugCallWrap" href="#debugCallWrap">func debugCallWrap(dispatch uintptr)</a>

```
searchKey: runtime.debugCallWrap
```

```Go
func debugCallWrap(dispatch uintptr)
```

debugCallWrap starts a new goroutine to run a debug call and blocks the calling goroutine. On the goroutine, it prepares to recover panics from the debug call, and then calls the call dispatching function at PC dispatch. 

This must be deeply nosplit because there are untyped values on the stack from debugCallV2. 

### <a id="debugCallWrap1" href="#debugCallWrap1">func debugCallWrap1()</a>

```
searchKey: runtime.debugCallWrap1
```

```Go
func debugCallWrap1()
```

debugCallWrap1 is the continuation of debugCallWrap on the callee goroutine. 

### <a id="debugCallWrap2" href="#debugCallWrap2">func debugCallWrap2(dispatch uintptr)</a>

```
searchKey: runtime.debugCallWrap2
```

```Go
func debugCallWrap2(dispatch uintptr)
```

### <a id="printDebugLog" href="#printDebugLog">func printDebugLog()</a>

```
searchKey: runtime.printDebugLog
```

```Go
func printDebugLog()
```

printDebugLog prints the debug log. 

### <a id="printDebugLogPC" href="#printDebugLogPC">func printDebugLogPC(pc uintptr, returnPC bool)</a>

```
searchKey: runtime.printDebugLogPC
```

```Go
func printDebugLogPC(pc uintptr, returnPC bool)
```

printDebugLogPC prints a single symbolized PC. If returnPC is true, pc is a return PC that must first be converted to a call PC. 

### <a id="putCachedDlogger" href="#putCachedDlogger">func putCachedDlogger(l *dlogger) bool</a>

```
searchKey: runtime.putCachedDlogger
```

```Go
func putCachedDlogger(l *dlogger) bool
```

### <a id="gogetenv" href="#gogetenv">func gogetenv(key string) string</a>

```
searchKey: runtime.gogetenv
```

```Go
func gogetenv(key string) string
```

### <a id="envKeyEqual" href="#envKeyEqual">func envKeyEqual(a, b string) bool</a>

```
searchKey: runtime.envKeyEqual
```

```Go
func envKeyEqual(a, b string) bool
```

envKeyEqual reports whether a == b, with ASCII-only case insensitivity on Windows. The two strings must have the same length. 

### <a id="lowerASCII" href="#lowerASCII">func lowerASCII(c byte) byte</a>

```
searchKey: runtime.lowerASCII
```

```Go
func lowerASCII(c byte) byte
```

### <a id="syscall_setenv_c" href="#syscall_setenv_c">func syscall_setenv_c(k string, v string)</a>

```
searchKey: runtime.syscall_setenv_c
```

```Go
func syscall_setenv_c(k string, v string)
```

Update the C environment if cgo is loaded. Called from syscall.Setenv. 

### <a id="syscall_unsetenv_c" href="#syscall_unsetenv_c">func syscall_unsetenv_c(k string)</a>

```
searchKey: runtime.syscall_unsetenv_c
```

```Go
func syscall_unsetenv_c(k string)
```

Update the C environment if cgo is loaded. Called from syscall.unsetenv. 

### <a id="cstring" href="#cstring">func cstring(s string) unsafe.Pointer</a>

```
searchKey: runtime.cstring
```

```Go
func cstring(s string) unsafe.Pointer
```

### <a id="itoa" href="#itoa">func itoa(buf []byte, val uint64) []byte</a>

```
searchKey: runtime.itoa
```

```Go
func itoa(buf []byte, val uint64) []byte
```

itoa converts val to a decimal representation. The result is written somewhere within buf and the location of the result is returned. buf must be at least 20 bytes. 

### <a id="appendIntStr" href="#appendIntStr">func appendIntStr(b []byte, v int64, signed bool) []byte</a>

```
searchKey: runtime.appendIntStr
```

```Go
func appendIntStr(b []byte, v int64, signed bool) []byte
```

### <a id="printany" href="#printany">func printany(i interface{})</a>

```
searchKey: runtime.printany
```

```Go
func printany(i interface{})
```

printany prints an argument passed to panic. If panic is called with a value that has a String or Error method, it has already been converted into a string by preprintpanics. 

### <a id="printanycustomtype" href="#printanycustomtype">func printanycustomtype(i interface{})</a>

```
searchKey: runtime.printanycustomtype
```

```Go
func printanycustomtype(i interface{})
```

### <a id="panicwrap" href="#panicwrap">func panicwrap()</a>

```
searchKey: runtime.panicwrap
```

```Go
func panicwrap()
```

panicwrap generates a panic for a call to a wrapped value method with a nil pointer receiver. 

It is called from the generated wrapper code. 

### <a id="Caller" href="#Caller">func Caller(skip int) (pc uintptr, file string, line int, ok bool)</a>

```
searchKey: runtime.Caller
tags: [exported]
```

```Go
func Caller(skip int) (pc uintptr, file string, line int, ok bool)
```

Caller reports file and line number information about function invocations on the calling goroutine's stack. The argument skip is the number of stack frames to ascend, with 0 identifying the caller of Caller.  (For historical reasons the meaning of skip differs between Caller and Callers.) The return values report the program counter, file name, and line number within the file of the corresponding call. The boolean ok is false if it was not possible to recover the information. 

### <a id="Callers" href="#Callers">func Callers(skip int, pc []uintptr) int</a>

```
searchKey: runtime.Callers
tags: [exported]
```

```Go
func Callers(skip int, pc []uintptr) int
```

Callers fills the slice pc with the return program counters of function invocations on the calling goroutine's stack. The argument skip is the number of stack frames to skip before recording in pc, with 0 identifying the frame for Callers itself and 1 identifying the caller of Callers. It returns the number of entries written to pc. 

To translate these PCs into symbolic information such as function names and line numbers, use CallersFrames. CallersFrames accounts for inlined functions and adjusts the return program counters into call program counters. Iterating over the returned slice of PCs directly is discouraged, as is using FuncForPC on any of the returned PCs, since these cannot account for inlining or return program counter adjustment. 

### <a id="GOROOT" href="#GOROOT">func GOROOT() string</a>

```
searchKey: runtime.GOROOT
tags: [exported]
```

```Go
func GOROOT() string
```

GOROOT returns the root of the Go tree. It uses the GOROOT environment variable, if set at process start, or else the root used during the Go build. 

### <a id="Version" href="#Version">func Version() string</a>

```
searchKey: runtime.Version
tags: [exported]
```

```Go
func Version() string
```

Version returns the Go tree's version string. It is either the commit hash and date at the time of the build or, when possible, a release tag like "go1.3". 

### <a id="fastlog2" href="#fastlog2">func fastlog2(x float64) float64</a>

```
searchKey: runtime.fastlog2
```

```Go
func fastlog2(x float64) float64
```

fastlog2 implements a fast approximation to the base 2 log of a float64. This is used to compute a geometric distribution for heap sampling, without introducing dependencies into package math. This uses a very rough approximation using the float64 exponent and the first 25 bits of the mantissa. The top 5 bits of the mantissa are used to load limits from a table of constants and the rest are used to scale linearly between them. 

### <a id="isNaN" href="#isNaN">func isNaN(f float64) (is bool)</a>

```
searchKey: runtime.isNaN
```

```Go
func isNaN(f float64) (is bool)
```

isNaN reports whether f is an IEEE 754 `not-a-number' value. 

### <a id="isFinite" href="#isFinite">func isFinite(f float64) bool</a>

```
searchKey: runtime.isFinite
```

```Go
func isFinite(f float64) bool
```

isFinite reports whether f is neither NaN nor an infinity. 

### <a id="isInf" href="#isInf">func isInf(f float64) bool</a>

```
searchKey: runtime.isInf
```

```Go
func isInf(f float64) bool
```

isInf reports whether f is an infinity. 

### <a id="abs" href="#abs">func abs(x float64) float64</a>

```
searchKey: runtime.abs
```

```Go
func abs(x float64) float64
```

Abs returns the absolute value of x. 

Special cases are: 

```
Abs(±Inf) = +Inf
Abs(NaN) = NaN

```
### <a id="copysign" href="#copysign">func copysign(x, y float64) float64</a>

```
searchKey: runtime.copysign
```

```Go
func copysign(x, y float64) float64
```

copysign returns a value with the magnitude of x and the sign of y. 

### <a id="float64bits" href="#float64bits">func float64bits(f float64) uint64</a>

```
searchKey: runtime.float64bits
```

```Go
func float64bits(f float64) uint64
```

Float64bits returns the IEEE 754 binary representation of f. 

### <a id="float64frombits" href="#float64frombits">func float64frombits(b uint64) float64</a>

```
searchKey: runtime.float64frombits
```

```Go
func float64frombits(b uint64) float64
```

Float64frombits returns the floating point number corresponding the IEEE 754 binary representation b. 

### <a id="memhashFallback" href="#memhashFallback">func memhashFallback(p unsafe.Pointer, seed, s uintptr) uintptr</a>

```
searchKey: runtime.memhashFallback
```

```Go
func memhashFallback(p unsafe.Pointer, seed, s uintptr) uintptr
```

### <a id="memhash32Fallback" href="#memhash32Fallback">func memhash32Fallback(p unsafe.Pointer, seed uintptr) uintptr</a>

```
searchKey: runtime.memhash32Fallback
```

```Go
func memhash32Fallback(p unsafe.Pointer, seed uintptr) uintptr
```

### <a id="memhash64Fallback" href="#memhash64Fallback">func memhash64Fallback(p unsafe.Pointer, seed uintptr) uintptr</a>

```
searchKey: runtime.memhash64Fallback
```

```Go
func memhash64Fallback(p unsafe.Pointer, seed uintptr) uintptr
```

### <a id="mix" href="#mix">func mix(a, b uintptr) uintptr</a>

```
searchKey: runtime.mix
```

```Go
func mix(a, b uintptr) uintptr
```

### <a id="r4" href="#r4">func r4(p unsafe.Pointer) uintptr</a>

```
searchKey: runtime.r4
```

```Go
func r4(p unsafe.Pointer) uintptr
```

### <a id="r8" href="#r8">func r8(p unsafe.Pointer) uintptr</a>

```
searchKey: runtime.r8
```

```Go
func r8(p unsafe.Pointer) uintptr
```

### <a id="runtime_debug_WriteHeapDump" href="#runtime_debug_WriteHeapDump">func runtime_debug_WriteHeapDump(fd uintptr)</a>

```
searchKey: runtime.runtime_debug_WriteHeapDump
```

```Go
func runtime_debug_WriteHeapDump(fd uintptr)
```

### <a id="dwrite" href="#dwrite">func dwrite(data unsafe.Pointer, len uintptr)</a>

```
searchKey: runtime.dwrite
```

```Go
func dwrite(data unsafe.Pointer, len uintptr)
```

### <a id="dwritebyte" href="#dwritebyte">func dwritebyte(b byte)</a>

```
searchKey: runtime.dwritebyte
```

```Go
func dwritebyte(b byte)
```

### <a id="flush" href="#flush">func flush()</a>

```
searchKey: runtime.flush
```

```Go
func flush()
```

### <a id="dumpint" href="#dumpint">func dumpint(v uint64)</a>

```
searchKey: runtime.dumpint
```

```Go
func dumpint(v uint64)
```

dump a uint64 in a varint format parseable by encoding/binary 

### <a id="dumpbool" href="#dumpbool">func dumpbool(b bool)</a>

```
searchKey: runtime.dumpbool
```

```Go
func dumpbool(b bool)
```

### <a id="dumpmemrange" href="#dumpmemrange">func dumpmemrange(data unsafe.Pointer, len uintptr)</a>

```
searchKey: runtime.dumpmemrange
```

```Go
func dumpmemrange(data unsafe.Pointer, len uintptr)
```

dump varint uint64 length followed by memory contents 

### <a id="dumpslice" href="#dumpslice">func dumpslice(b []byte)</a>

```
searchKey: runtime.dumpslice
```

```Go
func dumpslice(b []byte)
```

### <a id="dumpstr" href="#dumpstr">func dumpstr(s string)</a>

```
searchKey: runtime.dumpstr
```

```Go
func dumpstr(s string)
```

### <a id="dumptype" href="#dumptype">func dumptype(t *_type)</a>

```
searchKey: runtime.dumptype
```

```Go
func dumptype(t *_type)
```

dump information for a type 

### <a id="dumpobj" href="#dumpobj">func dumpobj(obj unsafe.Pointer, size uintptr, bv bitvector)</a>

```
searchKey: runtime.dumpobj
```

```Go
func dumpobj(obj unsafe.Pointer, size uintptr, bv bitvector)
```

dump an object 

### <a id="dumpotherroot" href="#dumpotherroot">func dumpotherroot(description string, to unsafe.Pointer)</a>

```
searchKey: runtime.dumpotherroot
```

```Go
func dumpotherroot(description string, to unsafe.Pointer)
```

### <a id="dumpfinalizer" href="#dumpfinalizer">func dumpfinalizer(obj unsafe.Pointer, fn *funcval, fint *_type, ot *ptrtype)</a>

```
searchKey: runtime.dumpfinalizer
```

```Go
func dumpfinalizer(obj unsafe.Pointer, fn *funcval, fint *_type, ot *ptrtype)
```

### <a id="dumpbv" href="#dumpbv">func dumpbv(cbv *bitvector, offset uintptr)</a>

```
searchKey: runtime.dumpbv
```

```Go
func dumpbv(cbv *bitvector, offset uintptr)
```

dump kinds & offsets of interesting fields in bv 

### <a id="dumpframe" href="#dumpframe">func dumpframe(s *stkframe, arg unsafe.Pointer) bool</a>

```
searchKey: runtime.dumpframe
```

```Go
func dumpframe(s *stkframe, arg unsafe.Pointer) bool
```

### <a id="dumpgoroutine" href="#dumpgoroutine">func dumpgoroutine(gp *g)</a>

```
searchKey: runtime.dumpgoroutine
```

```Go
func dumpgoroutine(gp *g)
```

### <a id="dumpgs" href="#dumpgs">func dumpgs()</a>

```
searchKey: runtime.dumpgs
```

```Go
func dumpgs()
```

### <a id="finq_callback" href="#finq_callback">func finq_callback(fn *funcval, obj unsafe.Pointer, nret uintptr, fint *_type, ot *ptrtype)</a>

```
searchKey: runtime.finq_callback
```

```Go
func finq_callback(fn *funcval, obj unsafe.Pointer, nret uintptr, fint *_type, ot *ptrtype)
```

### <a id="dumproots" href="#dumproots">func dumproots()</a>

```
searchKey: runtime.dumproots
```

```Go
func dumproots()
```

### <a id="dumpobjs" href="#dumpobjs">func dumpobjs()</a>

```
searchKey: runtime.dumpobjs
```

```Go
func dumpobjs()
```

### <a id="dumpparams" href="#dumpparams">func dumpparams()</a>

```
searchKey: runtime.dumpparams
```

```Go
func dumpparams()
```

### <a id="itab_callback" href="#itab_callback">func itab_callback(tab *itab)</a>

```
searchKey: runtime.itab_callback
```

```Go
func itab_callback(tab *itab)
```

### <a id="dumpitabs" href="#dumpitabs">func dumpitabs()</a>

```
searchKey: runtime.dumpitabs
```

```Go
func dumpitabs()
```

### <a id="dumpms" href="#dumpms">func dumpms()</a>

```
searchKey: runtime.dumpms
```

```Go
func dumpms()
```

### <a id="dumpmemstats" href="#dumpmemstats">func dumpmemstats(m *MemStats)</a>

```
searchKey: runtime.dumpmemstats
```

```Go
func dumpmemstats(m *MemStats)
```

### <a id="dumpmemprof_callback" href="#dumpmemprof_callback">func dumpmemprof_callback(b *bucket, nstk uintptr, pstk *uintptr, size, allocs, frees uintptr)</a>

```
searchKey: runtime.dumpmemprof_callback
```

```Go
func dumpmemprof_callback(b *bucket, nstk uintptr, pstk *uintptr, size, allocs, frees uintptr)
```

### <a id="dumpmemprof" href="#dumpmemprof">func dumpmemprof()</a>

```
searchKey: runtime.dumpmemprof
```

```Go
func dumpmemprof()
```

### <a id="mdump" href="#mdump">func mdump(m *MemStats)</a>

```
searchKey: runtime.mdump
```

```Go
func mdump(m *MemStats)
```

### <a id="writeheapdump_m" href="#writeheapdump_m">func writeheapdump_m(fd uintptr, m *MemStats)</a>

```
searchKey: runtime.writeheapdump_m
```

```Go
func writeheapdump_m(fd uintptr, m *MemStats)
```

### <a id="dumpfields" href="#dumpfields">func dumpfields(bv bitvector)</a>

```
searchKey: runtime.dumpfields
```

```Go
func dumpfields(bv bitvector)
```

dumpint() the kind & offset of each field in an object. 

### <a id="float64Inf" href="#float64Inf">func float64Inf() float64</a>

```
searchKey: runtime.float64Inf
```

```Go
func float64Inf() float64
```

### <a id="float64NegInf" href="#float64NegInf">func float64NegInf() float64</a>

```
searchKey: runtime.float64NegInf
```

```Go
func float64NegInf() float64
```

### <a id="timeHistogramMetricsBuckets" href="#timeHistogramMetricsBuckets">func timeHistogramMetricsBuckets() []float64</a>

```
searchKey: runtime.timeHistogramMetricsBuckets
```

```Go
func timeHistogramMetricsBuckets() []float64
```

timeHistogramMetricsBuckets generates a slice of boundaries for the timeHistogram. These boundaries are represented in seconds, not nanoseconds like the timeHistogram represents durations. 

### <a id="itabHashFunc" href="#itabHashFunc">func itabHashFunc(inter *interfacetype, typ *_type) uintptr</a>

```
searchKey: runtime.itabHashFunc
```

```Go
func itabHashFunc(inter *interfacetype, typ *_type) uintptr
```

### <a id="itabAdd" href="#itabAdd">func itabAdd(m *itab)</a>

```
searchKey: runtime.itabAdd
```

```Go
func itabAdd(m *itab)
```

itabAdd adds the given itab to the itab hash table. itabLock must be held. 

### <a id="itabsinit" href="#itabsinit">func itabsinit()</a>

```
searchKey: runtime.itabsinit
```

```Go
func itabsinit()
```

### <a id="panicdottypeE" href="#panicdottypeE">func panicdottypeE(have, want, iface *_type)</a>

```
searchKey: runtime.panicdottypeE
```

```Go
func panicdottypeE(have, want, iface *_type)
```

panicdottypeE is called when doing an e.(T) conversion and the conversion fails. have = the dynamic type we have. want = the static type we're trying to convert to. iface = the static type we're converting from. 

### <a id="panicdottypeI" href="#panicdottypeI">func panicdottypeI(have *itab, want, iface *_type)</a>

```
searchKey: runtime.panicdottypeI
```

```Go
func panicdottypeI(have *itab, want, iface *_type)
```

panicdottypeI is called when doing an i.(T) conversion and the conversion fails. Same args as panicdottypeE, but "have" is the dynamic itab we have. 

### <a id="panicnildottype" href="#panicnildottype">func panicnildottype(want *_type)</a>

```
searchKey: runtime.panicnildottype
```

```Go
func panicnildottype(want *_type)
```

panicnildottype is called when doing a i.(T) conversion and the interface i is nil. want = the static type we're trying to convert to. 

### <a id="convT16" href="#convT16">func convT16(val uint16) (x unsafe.Pointer)</a>

```
searchKey: runtime.convT16
```

```Go
func convT16(val uint16) (x unsafe.Pointer)
```

### <a id="convT32" href="#convT32">func convT32(val uint32) (x unsafe.Pointer)</a>

```
searchKey: runtime.convT32
```

```Go
func convT32(val uint32) (x unsafe.Pointer)
```

### <a id="convT64" href="#convT64">func convT64(val uint64) (x unsafe.Pointer)</a>

```
searchKey: runtime.convT64
```

```Go
func convT64(val uint64) (x unsafe.Pointer)
```

### <a id="convTstring" href="#convTstring">func convTstring(val string) (x unsafe.Pointer)</a>

```
searchKey: runtime.convTstring
```

```Go
func convTstring(val string) (x unsafe.Pointer)
```

### <a id="convTslice" href="#convTslice">func convTslice(val []byte) (x unsafe.Pointer)</a>

```
searchKey: runtime.convTslice
```

```Go
func convTslice(val []byte) (x unsafe.Pointer)
```

### <a id="reflect_ifaceE2I" href="#reflect_ifaceE2I">func reflect_ifaceE2I(inter *interfacetype, e eface, dst *iface)</a>

```
searchKey: runtime.reflect_ifaceE2I
```

```Go
func reflect_ifaceE2I(inter *interfacetype, e eface, dst *iface)
```

### <a id="reflectlite_ifaceE2I" href="#reflectlite_ifaceE2I">func reflectlite_ifaceE2I(inter *interfacetype, e eface, dst *iface)</a>

```
searchKey: runtime.reflectlite_ifaceE2I
```

```Go
func reflectlite_ifaceE2I(inter *interfacetype, e eface, dst *iface)
```

### <a id="iterate_itabs" href="#iterate_itabs">func iterate_itabs(fn func(*itab))</a>

```
searchKey: runtime.iterate_itabs
```

```Go
func iterate_itabs(fn func(*itab))
```

### <a id="unreachableMethod" href="#unreachableMethod">func unreachableMethod()</a>

```
searchKey: runtime.unreachableMethod
```

```Go
func unreachableMethod()
```

The linker redirects a reference of a method that it determined unreachable to a reference to this function, so it will throw if ever called. 

### <a id="lfnodeValidate" href="#lfnodeValidate">func lfnodeValidate(node *lfnode)</a>

```
searchKey: runtime.lfnodeValidate
```

```Go
func lfnodeValidate(node *lfnode)
```

lfnodeValidate panics if node is not a valid address for use with lfstack.push. This only needs to be called when node is allocated. 

### <a id="lfstackPack" href="#lfstackPack">func lfstackPack(node *lfnode, cnt uintptr) uint64</a>

```
searchKey: runtime.lfstackPack
```

```Go
func lfstackPack(node *lfnode, cnt uintptr) uint64
```

### <a id="lock" href="#lock">func lock(l *mutex)</a>

```
searchKey: runtime.lock
```

```Go
func lock(l *mutex)
```

### <a id="lock2" href="#lock2">func lock2(l *mutex)</a>

```
searchKey: runtime.lock2
```

```Go
func lock2(l *mutex)
```

### <a id="unlock" href="#unlock">func unlock(l *mutex)</a>

```
searchKey: runtime.unlock
```

```Go
func unlock(l *mutex)
```

### <a id="unlock2" href="#unlock2">func unlock2(l *mutex)</a>

```
searchKey: runtime.unlock2
```

```Go
func unlock2(l *mutex)
```

We might not be holding a p in this code. 

### <a id="noteclear" href="#noteclear">func noteclear(n *note)</a>

```
searchKey: runtime.noteclear
```

```Go
func noteclear(n *note)
```

One-time notifications. 

### <a id="notewakeup" href="#notewakeup">func notewakeup(n *note)</a>

```
searchKey: runtime.notewakeup
```

```Go
func notewakeup(n *note)
```

### <a id="notesleep" href="#notesleep">func notesleep(n *note)</a>

```
searchKey: runtime.notesleep
```

```Go
func notesleep(n *note)
```

### <a id="notetsleep_internal" href="#notetsleep_internal">func notetsleep_internal(n *note, ns int64, gp *g, deadline int64) bool</a>

```
searchKey: runtime.notetsleep_internal
```

```Go
func notetsleep_internal(n *note, ns int64, gp *g, deadline int64) bool
```

### <a id="notetsleep" href="#notetsleep">func notetsleep(n *note, ns int64) bool</a>

```
searchKey: runtime.notetsleep
```

```Go
func notetsleep(n *note, ns int64) bool
```

### <a id="notetsleepg" href="#notetsleepg">func notetsleepg(n *note, ns int64) bool</a>

```
searchKey: runtime.notetsleepg
```

```Go
func notetsleepg(n *note, ns int64) bool
```

same as runtime·notetsleep, but called on user g (not g0) calls only nosplit functions between entersyscallblock/exitsyscall 

### <a id="checkTimeouts" href="#checkTimeouts">func checkTimeouts()</a>

```
searchKey: runtime.checkTimeouts
```

```Go
func checkTimeouts()
```

### <a id="lockInit" href="#lockInit">func lockInit(l *mutex, rank lockRank)</a>

```
searchKey: runtime.lockInit
```

```Go
func lockInit(l *mutex, rank lockRank)
```

### <a id="lockWithRank" href="#lockWithRank">func lockWithRank(l *mutex, rank lockRank)</a>

```
searchKey: runtime.lockWithRank
```

```Go
func lockWithRank(l *mutex, rank lockRank)
```

### <a id="acquireLockRank" href="#acquireLockRank">func acquireLockRank(rank lockRank)</a>

```
searchKey: runtime.acquireLockRank
```

```Go
func acquireLockRank(rank lockRank)
```

This function may be called in nosplit context and thus must be nosplit. 

### <a id="unlockWithRank" href="#unlockWithRank">func unlockWithRank(l *mutex)</a>

```
searchKey: runtime.unlockWithRank
```

```Go
func unlockWithRank(l *mutex)
```

### <a id="releaseLockRank" href="#releaseLockRank">func releaseLockRank(rank lockRank)</a>

```
searchKey: runtime.releaseLockRank
```

```Go
func releaseLockRank(rank lockRank)
```

This function may be called in nosplit context and thus must be nosplit. 

### <a id="lockWithRankMayAcquire" href="#lockWithRankMayAcquire">func lockWithRankMayAcquire(l *mutex, rank lockRank)</a>

```
searchKey: runtime.lockWithRankMayAcquire
```

```Go
func lockWithRankMayAcquire(l *mutex, rank lockRank)
```

### <a id="assertLockHeld" href="#assertLockHeld">func assertLockHeld(l *mutex)</a>

```
searchKey: runtime.assertLockHeld
```

```Go
func assertLockHeld(l *mutex)
```

### <a id="assertRankHeld" href="#assertRankHeld">func assertRankHeld(r lockRank)</a>

```
searchKey: runtime.assertRankHeld
```

```Go
func assertRankHeld(r lockRank)
```

### <a id="worldStopped" href="#worldStopped">func worldStopped()</a>

```
searchKey: runtime.worldStopped
```

```Go
func worldStopped()
```

### <a id="worldStarted" href="#worldStarted">func worldStarted()</a>

```
searchKey: runtime.worldStarted
```

```Go
func worldStarted()
```

### <a id="assertWorldStopped" href="#assertWorldStopped">func assertWorldStopped()</a>

```
searchKey: runtime.assertWorldStopped
```

```Go
func assertWorldStopped()
```

### <a id="assertWorldStoppedOrLockHeld" href="#assertWorldStoppedOrLockHeld">func assertWorldStoppedOrLockHeld(l *mutex)</a>

```
searchKey: runtime.assertWorldStoppedOrLockHeld
```

```Go
func assertWorldStoppedOrLockHeld(l *mutex)
```

### <a id="mallocinit" href="#mallocinit">func mallocinit()</a>

```
searchKey: runtime.mallocinit
```

```Go
func mallocinit()
```

### <a id="sysReserveAligned" href="#sysReserveAligned">func sysReserveAligned(v unsafe.Pointer, size, align uintptr) (unsafe.Pointer, uintptr)</a>

```
searchKey: runtime.sysReserveAligned
```

```Go
func sysReserveAligned(v unsafe.Pointer, size, align uintptr) (unsafe.Pointer, uintptr)
```

sysReserveAligned is like sysReserve, but the returned pointer is aligned to align bytes. It may reserve either n or n+align bytes, so it returns the size that was reserved. 

### <a id="mallocgc" href="#mallocgc">func mallocgc(size uintptr, typ *_type, needzero bool) unsafe.Pointer</a>

```
searchKey: runtime.mallocgc
```

```Go
func mallocgc(size uintptr, typ *_type, needzero bool) unsafe.Pointer
```

Allocate an object of size bytes. Small objects are allocated from the per-P cache's free lists. Large objects (> 32 kB) are allocated straight from the heap. 

### <a id="memclrNoHeapPointersChunked" href="#memclrNoHeapPointersChunked">func memclrNoHeapPointersChunked(size uintptr, x unsafe.Pointer)</a>

```
searchKey: runtime.memclrNoHeapPointersChunked
```

```Go
func memclrNoHeapPointersChunked(size uintptr, x unsafe.Pointer)
```

memclrNoHeapPointersChunked repeatedly calls memclrNoHeapPointers on chunks of the buffer to be zeroed, with opportunities for preemption along the way.  memclrNoHeapPointers contains no safepoints and also cannot be preemptively scheduled, so this provides a still-efficient block copy that can also be preempted on a reasonable granularity. 

Use this with care; if the data being cleared is tagged to contain pointers, this allows the GC to run before it is all cleared. 

### <a id="newobject" href="#newobject">func newobject(typ *_type) unsafe.Pointer</a>

```
searchKey: runtime.newobject
```

```Go
func newobject(typ *_type) unsafe.Pointer
```

implementation of new builtin compiler (both frontend and SSA backend) knows the signature of this function 

### <a id="reflect_unsafe_New" href="#reflect_unsafe_New">func reflect_unsafe_New(typ *_type) unsafe.Pointer</a>

```
searchKey: runtime.reflect_unsafe_New
```

```Go
func reflect_unsafe_New(typ *_type) unsafe.Pointer
```

### <a id="reflectlite_unsafe_New" href="#reflectlite_unsafe_New">func reflectlite_unsafe_New(typ *_type) unsafe.Pointer</a>

```
searchKey: runtime.reflectlite_unsafe_New
```

```Go
func reflectlite_unsafe_New(typ *_type) unsafe.Pointer
```

### <a id="newarray" href="#newarray">func newarray(typ *_type, n int) unsafe.Pointer</a>

```
searchKey: runtime.newarray
```

```Go
func newarray(typ *_type, n int) unsafe.Pointer
```

newarray allocates an array of n elements of type typ. 

### <a id="reflect_unsafe_NewArray" href="#reflect_unsafe_NewArray">func reflect_unsafe_NewArray(typ *_type, n int) unsafe.Pointer</a>

```
searchKey: runtime.reflect_unsafe_NewArray
```

```Go
func reflect_unsafe_NewArray(typ *_type, n int) unsafe.Pointer
```

### <a id="profilealloc" href="#profilealloc">func profilealloc(mp *m, x unsafe.Pointer, size uintptr)</a>

```
searchKey: runtime.profilealloc
```

```Go
func profilealloc(mp *m, x unsafe.Pointer, size uintptr)
```

### <a id="nextSample" href="#nextSample">func nextSample() uintptr</a>

```
searchKey: runtime.nextSample
```

```Go
func nextSample() uintptr
```

nextSample returns the next sampling point for heap profiling. The goal is to sample allocations on average every MemProfileRate bytes, but with a completely random distribution over the allocation timeline; this corresponds to a Poisson process with parameter MemProfileRate. In Poisson processes, the distance between two samples follows the exponential distribution (exp(MemProfileRate)), so the best return value is a random number taken from an exponential distribution whose mean is MemProfileRate. 

### <a id="fastexprand" href="#fastexprand">func fastexprand(mean int) int32</a>

```
searchKey: runtime.fastexprand
```

```Go
func fastexprand(mean int) int32
```

fastexprand returns a random number from an exponential distribution with the specified mean. 

### <a id="nextSampleNoFP" href="#nextSampleNoFP">func nextSampleNoFP() uintptr</a>

```
searchKey: runtime.nextSampleNoFP
```

```Go
func nextSampleNoFP() uintptr
```

nextSampleNoFP is similar to nextSample, but uses older, simpler code to avoid floating point. 

### <a id="persistentalloc" href="#persistentalloc">func persistentalloc(size, align uintptr, sysStat *sysMemStat) unsafe.Pointer</a>

```
searchKey: runtime.persistentalloc
```

```Go
func persistentalloc(size, align uintptr, sysStat *sysMemStat) unsafe.Pointer
```

Wrapper around sysAlloc that can allocate small chunks. There is no associated free operation. Intended for things like function/type/debug-related persistent data. If align is 0, uses default align (currently 8). The returned memory will be zeroed. 

Consider marking persistentalloc'd types go:notinheap. 

### <a id="inPersistentAlloc" href="#inPersistentAlloc">func inPersistentAlloc(p uintptr) bool</a>

```
searchKey: runtime.inPersistentAlloc
```

```Go
func inPersistentAlloc(p uintptr) bool
```

inPersistentAlloc reports whether p points to memory allocated by persistentalloc. This must be nosplit because it is called by the cgo checker code, which is called by the write barrier code. 

### <a id="isEmpty" href="#isEmpty">func isEmpty(x uint8) bool</a>

```
searchKey: runtime.isEmpty
```

```Go
func isEmpty(x uint8) bool
```

isEmpty reports whether the given tophash array entry represents an empty bucket entry. 

### <a id="bucketShift" href="#bucketShift">func bucketShift(b uint8) uintptr</a>

```
searchKey: runtime.bucketShift
```

```Go
func bucketShift(b uint8) uintptr
```

bucketShift returns 1<<b, optimized for code generation. 

### <a id="bucketMask" href="#bucketMask">func bucketMask(b uint8) uintptr</a>

```
searchKey: runtime.bucketMask
```

```Go
func bucketMask(b uint8) uintptr
```

bucketMask returns 1<<b - 1, optimized for code generation. 

### <a id="tophash" href="#tophash">func tophash(hash uintptr) uint8</a>

```
searchKey: runtime.tophash
```

```Go
func tophash(hash uintptr) uint8
```

tophash calculates the tophash value for hash. 

### <a id="evacuated" href="#evacuated">func evacuated(b *bmap) bool</a>

```
searchKey: runtime.evacuated
```

```Go
func evacuated(b *bmap) bool
```

### <a id="mapaccess1" href="#mapaccess1">func mapaccess1(t *maptype, h *hmap, key unsafe.Pointer) unsafe.Pointer</a>

```
searchKey: runtime.mapaccess1
```

```Go
func mapaccess1(t *maptype, h *hmap, key unsafe.Pointer) unsafe.Pointer
```

mapaccess1 returns a pointer to h[key].  Never returns nil, instead it will return a reference to the zero object for the elem type if the key is not in the map. NOTE: The returned pointer may keep the whole map live, so don't hold onto it for very long. 

### <a id="mapaccess2" href="#mapaccess2">func mapaccess2(t *maptype, h *hmap, key unsafe.Pointer) (unsafe.Pointer, bool)</a>

```
searchKey: runtime.mapaccess2
```

```Go
func mapaccess2(t *maptype, h *hmap, key unsafe.Pointer) (unsafe.Pointer, bool)
```

### <a id="mapaccessK" href="#mapaccessK">func mapaccessK(t *maptype, h *hmap, key unsafe.Pointer) (unsafe.Pointer, unsafe.Pointer)</a>

```
searchKey: runtime.mapaccessK
```

```Go
func mapaccessK(t *maptype, h *hmap, key unsafe.Pointer) (unsafe.Pointer, unsafe.Pointer)
```

returns both key and elem. Used by map iterator 

### <a id="mapaccess1_fat" href="#mapaccess1_fat">func mapaccess1_fat(t *maptype, h *hmap, key, zero unsafe.Pointer) unsafe.Pointer</a>

```
searchKey: runtime.mapaccess1_fat
```

```Go
func mapaccess1_fat(t *maptype, h *hmap, key, zero unsafe.Pointer) unsafe.Pointer
```

### <a id="mapaccess2_fat" href="#mapaccess2_fat">func mapaccess2_fat(t *maptype, h *hmap, key, zero unsafe.Pointer) (unsafe.Pointer, bool)</a>

```
searchKey: runtime.mapaccess2_fat
```

```Go
func mapaccess2_fat(t *maptype, h *hmap, key, zero unsafe.Pointer) (unsafe.Pointer, bool)
```

### <a id="mapassign" href="#mapassign">func mapassign(t *maptype, h *hmap, key unsafe.Pointer) unsafe.Pointer</a>

```
searchKey: runtime.mapassign
```

```Go
func mapassign(t *maptype, h *hmap, key unsafe.Pointer) unsafe.Pointer
```

Like mapaccess, but allocates a slot for the key if it is not present in the map. 

### <a id="mapdelete" href="#mapdelete">func mapdelete(t *maptype, h *hmap, key unsafe.Pointer)</a>

```
searchKey: runtime.mapdelete
```

```Go
func mapdelete(t *maptype, h *hmap, key unsafe.Pointer)
```

### <a id="mapiterinit" href="#mapiterinit">func mapiterinit(t *maptype, h *hmap, it *hiter)</a>

```
searchKey: runtime.mapiterinit
```

```Go
func mapiterinit(t *maptype, h *hmap, it *hiter)
```

mapiterinit initializes the hiter struct used for ranging over maps. The hiter struct pointed to by 'it' is allocated on the stack by the compilers order pass or on the heap by reflect_mapiterinit. Both need to have zeroed hiter since the struct contains pointers. 

### <a id="mapiternext" href="#mapiternext">func mapiternext(it *hiter)</a>

```
searchKey: runtime.mapiternext
```

```Go
func mapiternext(it *hiter)
```

### <a id="mapclear" href="#mapclear">func mapclear(t *maptype, h *hmap)</a>

```
searchKey: runtime.mapclear
```

```Go
func mapclear(t *maptype, h *hmap)
```

mapclear deletes all keys from a map. 

### <a id="hashGrow" href="#hashGrow">func hashGrow(t *maptype, h *hmap)</a>

```
searchKey: runtime.hashGrow
```

```Go
func hashGrow(t *maptype, h *hmap)
```

### <a id="overLoadFactor" href="#overLoadFactor">func overLoadFactor(count int, B uint8) bool</a>

```
searchKey: runtime.overLoadFactor
```

```Go
func overLoadFactor(count int, B uint8) bool
```

overLoadFactor reports whether count items placed in 1<<B buckets is over loadFactor. 

### <a id="tooManyOverflowBuckets" href="#tooManyOverflowBuckets">func tooManyOverflowBuckets(noverflow uint16, B uint8) bool</a>

```
searchKey: runtime.tooManyOverflowBuckets
```

```Go
func tooManyOverflowBuckets(noverflow uint16, B uint8) bool
```

tooManyOverflowBuckets reports whether noverflow buckets is too many for a map with 1<<B buckets. Note that most of these overflow buckets must be in sparse use; if use was dense, then we'd have already triggered regular map growth. 

### <a id="growWork" href="#growWork">func growWork(t *maptype, h *hmap, bucket uintptr)</a>

```
searchKey: runtime.growWork
```

```Go
func growWork(t *maptype, h *hmap, bucket uintptr)
```

### <a id="bucketEvacuated" href="#bucketEvacuated">func bucketEvacuated(t *maptype, h *hmap, bucket uintptr) bool</a>

```
searchKey: runtime.bucketEvacuated
```

```Go
func bucketEvacuated(t *maptype, h *hmap, bucket uintptr) bool
```

### <a id="evacuate" href="#evacuate">func evacuate(t *maptype, h *hmap, oldbucket uintptr)</a>

```
searchKey: runtime.evacuate
```

```Go
func evacuate(t *maptype, h *hmap, oldbucket uintptr)
```

### <a id="advanceEvacuationMark" href="#advanceEvacuationMark">func advanceEvacuationMark(h *hmap, t *maptype, newbit uintptr)</a>

```
searchKey: runtime.advanceEvacuationMark
```

```Go
func advanceEvacuationMark(h *hmap, t *maptype, newbit uintptr)
```

### <a id="reflect_mapaccess" href="#reflect_mapaccess">func reflect_mapaccess(t *maptype, h *hmap, key unsafe.Pointer) unsafe.Pointer</a>

```
searchKey: runtime.reflect_mapaccess
```

```Go
func reflect_mapaccess(t *maptype, h *hmap, key unsafe.Pointer) unsafe.Pointer
```

### <a id="reflect_mapassign" href="#reflect_mapassign">func reflect_mapassign(t *maptype, h *hmap, key unsafe.Pointer, elem unsafe.Pointer)</a>

```
searchKey: runtime.reflect_mapassign
```

```Go
func reflect_mapassign(t *maptype, h *hmap, key unsafe.Pointer, elem unsafe.Pointer)
```

### <a id="reflect_mapdelete" href="#reflect_mapdelete">func reflect_mapdelete(t *maptype, h *hmap, key unsafe.Pointer)</a>

```
searchKey: runtime.reflect_mapdelete
```

```Go
func reflect_mapdelete(t *maptype, h *hmap, key unsafe.Pointer)
```

### <a id="reflect_mapiternext" href="#reflect_mapiternext">func reflect_mapiternext(it *hiter)</a>

```
searchKey: runtime.reflect_mapiternext
```

```Go
func reflect_mapiternext(it *hiter)
```

### <a id="reflect_mapiterkey" href="#reflect_mapiterkey">func reflect_mapiterkey(it *hiter) unsafe.Pointer</a>

```
searchKey: runtime.reflect_mapiterkey
```

```Go
func reflect_mapiterkey(it *hiter) unsafe.Pointer
```

### <a id="reflect_mapiterelem" href="#reflect_mapiterelem">func reflect_mapiterelem(it *hiter) unsafe.Pointer</a>

```
searchKey: runtime.reflect_mapiterelem
```

```Go
func reflect_mapiterelem(it *hiter) unsafe.Pointer
```

### <a id="reflect_maplen" href="#reflect_maplen">func reflect_maplen(h *hmap) int</a>

```
searchKey: runtime.reflect_maplen
```

```Go
func reflect_maplen(h *hmap) int
```

### <a id="reflectlite_maplen" href="#reflectlite_maplen">func reflectlite_maplen(h *hmap) int</a>

```
searchKey: runtime.reflectlite_maplen
```

```Go
func reflectlite_maplen(h *hmap) int
```

### <a id="mapaccess1_fast32" href="#mapaccess1_fast32">func mapaccess1_fast32(t *maptype, h *hmap, key uint32) unsafe.Pointer</a>

```
searchKey: runtime.mapaccess1_fast32
```

```Go
func mapaccess1_fast32(t *maptype, h *hmap, key uint32) unsafe.Pointer
```

### <a id="mapaccess2_fast32" href="#mapaccess2_fast32">func mapaccess2_fast32(t *maptype, h *hmap, key uint32) (unsafe.Pointer, bool)</a>

```
searchKey: runtime.mapaccess2_fast32
```

```Go
func mapaccess2_fast32(t *maptype, h *hmap, key uint32) (unsafe.Pointer, bool)
```

### <a id="mapassign_fast32" href="#mapassign_fast32">func mapassign_fast32(t *maptype, h *hmap, key uint32) unsafe.Pointer</a>

```
searchKey: runtime.mapassign_fast32
```

```Go
func mapassign_fast32(t *maptype, h *hmap, key uint32) unsafe.Pointer
```

### <a id="mapassign_fast32ptr" href="#mapassign_fast32ptr">func mapassign_fast32ptr(t *maptype, h *hmap, key unsafe.Pointer) unsafe.Pointer</a>

```
searchKey: runtime.mapassign_fast32ptr
```

```Go
func mapassign_fast32ptr(t *maptype, h *hmap, key unsafe.Pointer) unsafe.Pointer
```

### <a id="mapdelete_fast32" href="#mapdelete_fast32">func mapdelete_fast32(t *maptype, h *hmap, key uint32)</a>

```
searchKey: runtime.mapdelete_fast32
```

```Go
func mapdelete_fast32(t *maptype, h *hmap, key uint32)
```

### <a id="growWork_fast32" href="#growWork_fast32">func growWork_fast32(t *maptype, h *hmap, bucket uintptr)</a>

```
searchKey: runtime.growWork_fast32
```

```Go
func growWork_fast32(t *maptype, h *hmap, bucket uintptr)
```

### <a id="evacuate_fast32" href="#evacuate_fast32">func evacuate_fast32(t *maptype, h *hmap, oldbucket uintptr)</a>

```
searchKey: runtime.evacuate_fast32
```

```Go
func evacuate_fast32(t *maptype, h *hmap, oldbucket uintptr)
```

### <a id="mapaccess1_fast64" href="#mapaccess1_fast64">func mapaccess1_fast64(t *maptype, h *hmap, key uint64) unsafe.Pointer</a>

```
searchKey: runtime.mapaccess1_fast64
```

```Go
func mapaccess1_fast64(t *maptype, h *hmap, key uint64) unsafe.Pointer
```

### <a id="mapaccess2_fast64" href="#mapaccess2_fast64">func mapaccess2_fast64(t *maptype, h *hmap, key uint64) (unsafe.Pointer, bool)</a>

```
searchKey: runtime.mapaccess2_fast64
```

```Go
func mapaccess2_fast64(t *maptype, h *hmap, key uint64) (unsafe.Pointer, bool)
```

### <a id="mapassign_fast64" href="#mapassign_fast64">func mapassign_fast64(t *maptype, h *hmap, key uint64) unsafe.Pointer</a>

```
searchKey: runtime.mapassign_fast64
```

```Go
func mapassign_fast64(t *maptype, h *hmap, key uint64) unsafe.Pointer
```

### <a id="mapassign_fast64ptr" href="#mapassign_fast64ptr">func mapassign_fast64ptr(t *maptype, h *hmap, key unsafe.Pointer) unsafe.Pointer</a>

```
searchKey: runtime.mapassign_fast64ptr
```

```Go
func mapassign_fast64ptr(t *maptype, h *hmap, key unsafe.Pointer) unsafe.Pointer
```

### <a id="mapdelete_fast64" href="#mapdelete_fast64">func mapdelete_fast64(t *maptype, h *hmap, key uint64)</a>

```
searchKey: runtime.mapdelete_fast64
```

```Go
func mapdelete_fast64(t *maptype, h *hmap, key uint64)
```

### <a id="growWork_fast64" href="#growWork_fast64">func growWork_fast64(t *maptype, h *hmap, bucket uintptr)</a>

```
searchKey: runtime.growWork_fast64
```

```Go
func growWork_fast64(t *maptype, h *hmap, bucket uintptr)
```

### <a id="evacuate_fast64" href="#evacuate_fast64">func evacuate_fast64(t *maptype, h *hmap, oldbucket uintptr)</a>

```
searchKey: runtime.evacuate_fast64
```

```Go
func evacuate_fast64(t *maptype, h *hmap, oldbucket uintptr)
```

### <a id="mapaccess1_faststr" href="#mapaccess1_faststr">func mapaccess1_faststr(t *maptype, h *hmap, ky string) unsafe.Pointer</a>

```
searchKey: runtime.mapaccess1_faststr
```

```Go
func mapaccess1_faststr(t *maptype, h *hmap, ky string) unsafe.Pointer
```

### <a id="mapaccess2_faststr" href="#mapaccess2_faststr">func mapaccess2_faststr(t *maptype, h *hmap, ky string) (unsafe.Pointer, bool)</a>

```
searchKey: runtime.mapaccess2_faststr
```

```Go
func mapaccess2_faststr(t *maptype, h *hmap, ky string) (unsafe.Pointer, bool)
```

### <a id="mapassign_faststr" href="#mapassign_faststr">func mapassign_faststr(t *maptype, h *hmap, s string) unsafe.Pointer</a>

```
searchKey: runtime.mapassign_faststr
```

```Go
func mapassign_faststr(t *maptype, h *hmap, s string) unsafe.Pointer
```

### <a id="mapdelete_faststr" href="#mapdelete_faststr">func mapdelete_faststr(t *maptype, h *hmap, ky string)</a>

```
searchKey: runtime.mapdelete_faststr
```

```Go
func mapdelete_faststr(t *maptype, h *hmap, ky string)
```

### <a id="growWork_faststr" href="#growWork_faststr">func growWork_faststr(t *maptype, h *hmap, bucket uintptr)</a>

```
searchKey: runtime.growWork_faststr
```

```Go
func growWork_faststr(t *maptype, h *hmap, bucket uintptr)
```

### <a id="evacuate_faststr" href="#evacuate_faststr">func evacuate_faststr(t *maptype, h *hmap, oldbucket uintptr)</a>

```
searchKey: runtime.evacuate_faststr
```

```Go
func evacuate_faststr(t *maptype, h *hmap, oldbucket uintptr)
```

### <a id="typedmemmove" href="#typedmemmove">func typedmemmove(typ *_type, dst, src unsafe.Pointer)</a>

```
searchKey: runtime.typedmemmove
```

```Go
func typedmemmove(typ *_type, dst, src unsafe.Pointer)
```

typedmemmove copies a value of type t to dst from src. Must be nosplit, see #16026. 

TODO: Perfect for go:nosplitrec since we can't have a safe point anywhere in the bulk barrier or memmove. 

### <a id="reflect_typedmemmove" href="#reflect_typedmemmove">func reflect_typedmemmove(typ *_type, dst, src unsafe.Pointer)</a>

```
searchKey: runtime.reflect_typedmemmove
```

```Go
func reflect_typedmemmove(typ *_type, dst, src unsafe.Pointer)
```

### <a id="reflectlite_typedmemmove" href="#reflectlite_typedmemmove">func reflectlite_typedmemmove(typ *_type, dst, src unsafe.Pointer)</a>

```
searchKey: runtime.reflectlite_typedmemmove
```

```Go
func reflectlite_typedmemmove(typ *_type, dst, src unsafe.Pointer)
```

### <a id="reflect_typedmemmovepartial" href="#reflect_typedmemmovepartial">func reflect_typedmemmovepartial(typ *_type, dst, src unsafe.Pointer, off, size uintptr)</a>

```
searchKey: runtime.reflect_typedmemmovepartial
```

```Go
func reflect_typedmemmovepartial(typ *_type, dst, src unsafe.Pointer, off, size uintptr)
```

typedmemmovepartial is like typedmemmove but assumes that dst and src point off bytes into the value and only copies size bytes. off must be a multiple of sys.PtrSize. 

### <a id="reflectcallmove" href="#reflectcallmove">func reflectcallmove(typ *_type, dst, src unsafe.Pointer, size uintptr, regs *abi.RegArgs)</a>

```
searchKey: runtime.reflectcallmove
```

```Go
func reflectcallmove(typ *_type, dst, src unsafe.Pointer, size uintptr, regs *abi.RegArgs)
```

reflectcallmove is invoked by reflectcall to copy the return values out of the stack and into the heap, invoking the necessary write barriers. dst, src, and size describe the return value area to copy. typ describes the entire frame (not just the return values). typ may be nil, which indicates write barriers are not needed. 

It must be nosplit and must only call nosplit functions because the stack map of reflectcall is wrong. 

### <a id="typedslicecopy" href="#typedslicecopy">func typedslicecopy(typ *_type, dstPtr unsafe.Pointer, dstLen int, srcPtr unsafe.Pointer, srcLen int) int</a>

```
searchKey: runtime.typedslicecopy
```

```Go
func typedslicecopy(typ *_type, dstPtr unsafe.Pointer, dstLen int, srcPtr unsafe.Pointer, srcLen int) int
```

### <a id="reflect_typedslicecopy" href="#reflect_typedslicecopy">func reflect_typedslicecopy(elemType *_type, dst, src slice) int</a>

```
searchKey: runtime.reflect_typedslicecopy
```

```Go
func reflect_typedslicecopy(elemType *_type, dst, src slice) int
```

### <a id="typedmemclr" href="#typedmemclr">func typedmemclr(typ *_type, ptr unsafe.Pointer)</a>

```
searchKey: runtime.typedmemclr
```

```Go
func typedmemclr(typ *_type, ptr unsafe.Pointer)
```

typedmemclr clears the typed memory at ptr with type typ. The memory at ptr must already be initialized (and hence in type-safe state). If the memory is being initialized for the first time, see memclrNoHeapPointers. 

If the caller knows that typ has pointers, it can alternatively call memclrHasPointers. 

### <a id="reflect_typedmemclr" href="#reflect_typedmemclr">func reflect_typedmemclr(typ *_type, ptr unsafe.Pointer)</a>

```
searchKey: runtime.reflect_typedmemclr
```

```Go
func reflect_typedmemclr(typ *_type, ptr unsafe.Pointer)
```

### <a id="reflect_typedmemclrpartial" href="#reflect_typedmemclrpartial">func reflect_typedmemclrpartial(typ *_type, ptr unsafe.Pointer, off, size uintptr)</a>

```
searchKey: runtime.reflect_typedmemclrpartial
```

```Go
func reflect_typedmemclrpartial(typ *_type, ptr unsafe.Pointer, off, size uintptr)
```

### <a id="memclrHasPointers" href="#memclrHasPointers">func memclrHasPointers(ptr unsafe.Pointer, n uintptr)</a>

```
searchKey: runtime.memclrHasPointers
```

```Go
func memclrHasPointers(ptr unsafe.Pointer, n uintptr)
```

memclrHasPointers clears n bytes of typed memory starting at ptr. The caller must ensure that the type of the object at ptr has pointers, usually by checking typ.ptrdata. However, ptr does not have to point to the start of the allocation. 

### <a id="addb" href="#addb">func addb(p *byte, n uintptr) *byte</a>

```
searchKey: runtime.addb
```

```Go
func addb(p *byte, n uintptr) *byte
```

addb returns the byte pointer p+n. 

### <a id="subtractb" href="#subtractb">func subtractb(p *byte, n uintptr) *byte</a>

```
searchKey: runtime.subtractb
```

```Go
func subtractb(p *byte, n uintptr) *byte
```

subtractb returns the byte pointer p-n. 

### <a id="add1" href="#add1">func add1(p *byte) *byte</a>

```
searchKey: runtime.add1
```

```Go
func add1(p *byte) *byte
```

add1 returns the byte pointer p+1. 

### <a id="subtract1" href="#subtract1">func subtract1(p *byte) *byte</a>

```
searchKey: runtime.subtract1
```

```Go
func subtract1(p *byte) *byte
```

subtract1 returns the byte pointer p-1. 

nosplit because it is used during write barriers and must not be preempted. 

### <a id="badPointer" href="#badPointer">func badPointer(s *mspan, p, refBase, refOff uintptr)</a>

```
searchKey: runtime.badPointer
```

```Go
func badPointer(s *mspan, p, refBase, refOff uintptr)
```

badPointer throws bad pointer in heap panic. 

### <a id="bulkBarrierPreWrite" href="#bulkBarrierPreWrite">func bulkBarrierPreWrite(dst, src, size uintptr)</a>

```
searchKey: runtime.bulkBarrierPreWrite
```

```Go
func bulkBarrierPreWrite(dst, src, size uintptr)
```

bulkBarrierPreWrite executes a write barrier for every pointer slot in the memory range [src, src+size), using pointer/scalar information from [dst, dst+size). This executes the write barriers necessary before a memmove. src, dst, and size must be pointer-aligned. The range [dst, dst+size) must lie within a single object. It does not perform the actual writes. 

As a special case, src == 0 indicates that this is being used for a memclr. bulkBarrierPreWrite will pass 0 for the src of each write barrier. 

Callers should call bulkBarrierPreWrite immediately before calling memmove(dst, src, size). This function is marked nosplit to avoid being preempted; the GC must not stop the goroutine between the memmove and the execution of the barriers. The caller is also responsible for cgo pointer checks if this may be writing Go pointers into non-Go memory. 

The pointer bitmap is not maintained for allocations containing no pointers at all; any caller of bulkBarrierPreWrite must first make sure the underlying allocation contains pointers, usually by checking typ.ptrdata. 

Callers must perform cgo checks if writeBarrier.cgo. 

### <a id="bulkBarrierPreWriteSrcOnly" href="#bulkBarrierPreWriteSrcOnly">func bulkBarrierPreWriteSrcOnly(dst, src, size uintptr)</a>

```
searchKey: runtime.bulkBarrierPreWriteSrcOnly
```

```Go
func bulkBarrierPreWriteSrcOnly(dst, src, size uintptr)
```

bulkBarrierPreWriteSrcOnly is like bulkBarrierPreWrite but does not execute write barriers for [dst, dst+size). 

In addition to the requirements of bulkBarrierPreWrite callers need to ensure [dst, dst+size) is zeroed. 

This is used for special cases where e.g. dst was just created and zeroed with malloc. 

### <a id="bulkBarrierBitmap" href="#bulkBarrierBitmap">func bulkBarrierBitmap(dst, src, size, maskOffset uintptr, bits *uint8)</a>

```
searchKey: runtime.bulkBarrierBitmap
```

```Go
func bulkBarrierBitmap(dst, src, size, maskOffset uintptr, bits *uint8)
```

bulkBarrierBitmap executes write barriers for copying from [src, src+size) to [dst, dst+size) using a 1-bit pointer bitmap. src is assumed to start maskOffset bytes into the data covered by the bitmap in bits (which may not be a multiple of 8). 

This is used by bulkBarrierPreWrite for writes to data and BSS. 

### <a id="typeBitsBulkBarrier" href="#typeBitsBulkBarrier">func typeBitsBulkBarrier(typ *_type, dst, src, size uintptr)</a>

```
searchKey: runtime.typeBitsBulkBarrier
```

```Go
func typeBitsBulkBarrier(typ *_type, dst, src, size uintptr)
```

typeBitsBulkBarrier executes a write barrier for every pointer that would be copied from [src, src+size) to [dst, dst+size) by a memmove using the type bitmap to locate those pointer slots. 

The type typ must correspond exactly to [src, src+size) and [dst, dst+size). dst, src, and size must be pointer-aligned. The type typ must have a plain bitmap, not a GC program. The only use of this function is in channel sends, and the 64 kB channel element limit takes care of this for us. 

Must not be preempted because it typically runs right before memmove, and the GC must observe them as an atomic action. 

Callers must perform cgo checks if writeBarrier.cgo. 

### <a id="heapBitsSetType" href="#heapBitsSetType">func heapBitsSetType(x, size, dataSize uintptr, typ *_type)</a>

```
searchKey: runtime.heapBitsSetType
```

```Go
func heapBitsSetType(x, size, dataSize uintptr, typ *_type)
```

heapBitsSetType records that the new allocation [x, x+size) holds in [x, x+dataSize) one or more values of type typ. (The number of values is given by dataSize / typ.size.) If dataSize < size, the fragment [x+dataSize, x+size) is recorded as non-pointer data. It is known that the type has pointers somewhere; malloc does not call heapBitsSetType when there are no pointers, because all free objects are marked as noscan during heapBitsSweepSpan. 

There can only be one allocation from a given span active at a time, and the bitmap for a span always falls on byte boundaries, so there are no write-write races for access to the heap bitmap. Hence, heapBitsSetType can access the bitmap without atomics. 

There can be read-write races between heapBitsSetType and things that read the heap bitmap like scanobject. However, since heapBitsSetType is only used for objects that have not yet been made reachable, readers will ignore bits being modified by this function. This does mean this function cannot transiently modify bits that belong to neighboring objects. Also, on weakly-ordered machines, callers must execute a store/store (publication) barrier between calling this function and making the object reachable. 

### <a id="heapBitsSetTypeGCProg" href="#heapBitsSetTypeGCProg">func heapBitsSetTypeGCProg(h heapBits, progSize, elemSize, dataSize, allocSize uintptr, prog *byte)</a>

```
searchKey: runtime.heapBitsSetTypeGCProg
```

```Go
func heapBitsSetTypeGCProg(h heapBits, progSize, elemSize, dataSize, allocSize uintptr, prog *byte)
```

heapBitsSetTypeGCProg implements heapBitsSetType using a GC program. progSize is the size of the memory described by the program. elemSize is the size of the element that the GC program describes (a prefix of). dataSize is the total size of the intended data, a multiple of elemSize. allocSize is the total size of the allocated memory. 

GC programs are only used for large allocations. heapBitsSetType requires that allocSize is a multiple of 4 words, so that the relevant bitmap bytes are not shared with surrounding objects. 

### <a id="runGCProg" href="#runGCProg">func runGCProg(prog, trailer, dst *byte, size int) uintptr</a>

```
searchKey: runtime.runGCProg
```

```Go
func runGCProg(prog, trailer, dst *byte, size int) uintptr
```

runGCProg executes the GC program prog, and then trailer if non-nil, writing to dst with entries of the given size. If size == 1, dst is a 1-bit pointer mask laid out moving forward from dst. If size == 2, dst is the 2-bit heap bitmap, and writes move backward starting at dst (because the heap bitmap does). In this case, the caller guarantees that only whole bytes in dst need to be written. 

runGCProg returns the number of 1- or 2-bit entries written to memory. 

### <a id="dematerializeGCProg" href="#dematerializeGCProg">func dematerializeGCProg(s *mspan)</a>

```
searchKey: runtime.dematerializeGCProg
```

```Go
func dematerializeGCProg(s *mspan)
```

### <a id="dumpGCProg" href="#dumpGCProg">func dumpGCProg(p *byte)</a>

```
searchKey: runtime.dumpGCProg
```

```Go
func dumpGCProg(p *byte)
```

### <a id="getgcmaskcb" href="#getgcmaskcb">func getgcmaskcb(frame *stkframe, ctxt unsafe.Pointer) bool</a>

```
searchKey: runtime.getgcmaskcb
```

```Go
func getgcmaskcb(frame *stkframe, ctxt unsafe.Pointer) bool
```

### <a id="reflect_gcbits" href="#reflect_gcbits">func reflect_gcbits(x interface{}) []byte</a>

```
searchKey: runtime.reflect_gcbits
```

```Go
func reflect_gcbits(x interface{}) []byte
```

gcbits returns the GC type info for x, for testing. The result is the bitmap entries (0 or 1), one entry per byte. 

### <a id="getgcmask" href="#getgcmask">func getgcmask(ep interface{}) (mask []byte)</a>

```
searchKey: runtime.getgcmask
```

```Go
func getgcmask(ep interface{}) (mask []byte)
```

Returns GC type info for the pointer stored in ep for testing. If ep points to the stack, only static live information will be returned (i.e. not for objects which are only dynamically live stack objects). 

### <a id="freemcache" href="#freemcache">func freemcache(c *mcache)</a>

```
searchKey: runtime.freemcache
```

```Go
func freemcache(c *mcache)
```

freemcache releases resources associated with this mcache and puts the object onto a free list. 

In some cases there is no way to simply release resources, such as statistics, so donate them to a different mcache (the recipient). 

### <a id="startCheckmarks" href="#startCheckmarks">func startCheckmarks()</a>

```
searchKey: runtime.startCheckmarks
```

```Go
func startCheckmarks()
```

startCheckmarks prepares for the checkmarks phase. 

The world must be stopped. 

### <a id="endCheckmarks" href="#endCheckmarks">func endCheckmarks()</a>

```
searchKey: runtime.endCheckmarks
```

```Go
func endCheckmarks()
```

endCheckmarks ends the checkmarks phase. 

### <a id="setCheckmark" href="#setCheckmark">func setCheckmark(obj, base, off uintptr, mbits markBits) bool</a>

```
searchKey: runtime.setCheckmark
```

```Go
func setCheckmark(obj, base, off uintptr, mbits markBits) bool
```

setCheckmark throws if marking object is a checkmarks violation, and otherwise sets obj's checkmark. It returns true if obj was already checkmarked. 

### <a id="sysAlloc" href="#sysAlloc">func sysAlloc(n uintptr, sysStat *sysMemStat) unsafe.Pointer</a>

```
searchKey: runtime.sysAlloc
```

```Go
func sysAlloc(n uintptr, sysStat *sysMemStat) unsafe.Pointer
```

Don't split the stack as this function may be invoked without a valid G, which prevents us from allocating more stack. 

### <a id="sysUnused" href="#sysUnused">func sysUnused(v unsafe.Pointer, n uintptr)</a>

```
searchKey: runtime.sysUnused
```

```Go
func sysUnused(v unsafe.Pointer, n uintptr)
```

### <a id="sysUsed" href="#sysUsed">func sysUsed(v unsafe.Pointer, n uintptr)</a>

```
searchKey: runtime.sysUsed
```

```Go
func sysUsed(v unsafe.Pointer, n uintptr)
```

### <a id="sysHugePage" href="#sysHugePage">func sysHugePage(v unsafe.Pointer, n uintptr)</a>

```
searchKey: runtime.sysHugePage
```

```Go
func sysHugePage(v unsafe.Pointer, n uintptr)
```

### <a id="sysFree" href="#sysFree">func sysFree(v unsafe.Pointer, n uintptr, sysStat *sysMemStat)</a>

```
searchKey: runtime.sysFree
```

```Go
func sysFree(v unsafe.Pointer, n uintptr, sysStat *sysMemStat)
```

Don't split the stack as this function may be invoked without a valid G, which prevents us from allocating more stack. 

### <a id="sysFault" href="#sysFault">func sysFault(v unsafe.Pointer, n uintptr)</a>

```
searchKey: runtime.sysFault
```

```Go
func sysFault(v unsafe.Pointer, n uintptr)
```

### <a id="sysReserve" href="#sysReserve">func sysReserve(v unsafe.Pointer, n uintptr) unsafe.Pointer</a>

```
searchKey: runtime.sysReserve
```

```Go
func sysReserve(v unsafe.Pointer, n uintptr) unsafe.Pointer
```

### <a id="sysMap" href="#sysMap">func sysMap(v unsafe.Pointer, n uintptr, sysStat *sysMemStat)</a>

```
searchKey: runtime.sysMap
```

```Go
func sysMap(v unsafe.Pointer, n uintptr, sysStat *sysMemStat)
```

### <a id="initMetrics" href="#initMetrics">func initMetrics()</a>

```
searchKey: runtime.initMetrics
```

```Go
func initMetrics()
```

initMetrics initializes the metrics map if it hasn't been yet. 

metricsSema must be held. 

### <a id="readMetrics" href="#readMetrics">func readMetrics(samplesp unsafe.Pointer, len int, cap int)</a>

```
searchKey: runtime.readMetrics
```

```Go
func readMetrics(samplesp unsafe.Pointer, len int, cap int)
```

readMetrics is the implementation of runtime/metrics.Read. 

### <a id="queuefinalizer" href="#queuefinalizer">func queuefinalizer(p unsafe.Pointer, fn *funcval, nret uintptr, fint *_type, ot *ptrtype)</a>

```
searchKey: runtime.queuefinalizer
```

```Go
func queuefinalizer(p unsafe.Pointer, fn *funcval, nret uintptr, fint *_type, ot *ptrtype)
```

### <a id="iterate_finq" href="#iterate_finq">func iterate_finq(callback func(*funcval, unsafe.Pointer, uintptr, *_type, *ptrtype))</a>

```
searchKey: runtime.iterate_finq
```

```Go
func iterate_finq(callback func(*funcval, unsafe.Pointer, uintptr, *_type, *ptrtype))
```

### <a id="createfing" href="#createfing">func createfing()</a>

```
searchKey: runtime.createfing
```

```Go
func createfing()
```

### <a id="runfinq" href="#runfinq">func runfinq()</a>

```
searchKey: runtime.runfinq
```

```Go
func runfinq()
```

This is the goroutine that runs all of the finalizers 

### <a id="SetFinalizer" href="#SetFinalizer">func SetFinalizer(obj interface{}, finalizer interface{})</a>

```
searchKey: runtime.SetFinalizer
tags: [exported]
```

```Go
func SetFinalizer(obj interface{}, finalizer interface{})
```

SetFinalizer sets the finalizer associated with obj to the provided finalizer function. When the garbage collector finds an unreachable block with an associated finalizer, it clears the association and runs finalizer(obj) in a separate goroutine. This makes obj reachable again, but now without an associated finalizer. Assuming that SetFinalizer is not called again, the next time the garbage collector sees that obj is unreachable, it will free obj. 

SetFinalizer(obj, nil) clears any finalizer associated with obj. 

The argument obj must be a pointer to an object allocated by calling new, by taking the address of a composite literal, or by taking the address of a local variable. The argument finalizer must be a function that takes a single argument to which obj's type can be assigned, and can have arbitrary ignored return values. If either of these is not true, SetFinalizer may abort the program. 

Finalizers are run in dependency order: if A points at B, both have finalizers, and they are otherwise unreachable, only the finalizer for A runs; once A is freed, the finalizer for B can run. If a cyclic structure includes a block with a finalizer, that cycle is not guaranteed to be garbage collected and the finalizer is not guaranteed to run, because there is no ordering that respects the dependencies. 

The finalizer is scheduled to run at some arbitrary time after the program can no longer reach the object to which obj points. There is no guarantee that finalizers will run before a program exits, so typically they are useful only for releasing non-memory resources associated with an object during a long-running program. For example, an os.File object could use a finalizer to close the associated operating system file descriptor when a program discards an os.File without calling Close, but it would be a mistake to depend on a finalizer to flush an in-memory I/O buffer such as a bufio.Writer, because the buffer would not be flushed at program exit. 

It is not guaranteed that a finalizer will run if the size of *obj is zero bytes. 

It is not guaranteed that a finalizer will run for objects allocated in initializers for package-level variables. Such objects may be linker-allocated, not heap-allocated. 

A finalizer may run as soon as an object becomes unreachable. In order to use finalizers correctly, the program must ensure that the object is reachable until it is no longer required. Objects stored in global variables, or that can be found by tracing pointers from a global variable, are reachable. For other objects, pass the object to a call of the KeepAlive function to mark the last point in the function where the object must be reachable. 

For example, if p points to a struct, such as os.File, that contains a file descriptor d, and p has a finalizer that closes that file descriptor, and if the last use of p in a function is a call to syscall.Write(p.d, buf, size), then p may be unreachable as soon as the program enters syscall.Write. The finalizer may run at that moment, closing p.d, causing syscall.Write to fail because it is writing to a closed file descriptor (or, worse, to an entirely different file descriptor opened by a different goroutine). To avoid this problem, call runtime.KeepAlive(p) after the call to syscall.Write. 

A single goroutine runs all finalizers for a program, sequentially. If a finalizer must run for a long time, it should do so by starting a new goroutine. 

### <a id="KeepAlive" href="#KeepAlive">func KeepAlive(x interface{})</a>

```
searchKey: runtime.KeepAlive
tags: [exported]
```

```Go
func KeepAlive(x interface{})
```

KeepAlive marks its argument as currently reachable. This ensures that the object is not freed, and its finalizer is not run, before the point in the program where KeepAlive is called. 

A very simplified example showing where KeepAlive is required: 

```
type File struct { d int }
d, err := syscall.Open("/file/path", syscall.O_RDONLY, 0)
// ... do something if err != nil ...
p := &File{d}
runtime.SetFinalizer(p, func(p *File) { syscall.Close(p.d) })
var buf [10]byte
n, err := syscall.Read(p.d, buf[:])
// Ensure p is not finalized until Read returns.
runtime.KeepAlive(p)
// No more uses of p after this point.

```
Without the KeepAlive call, the finalizer could run at the start of syscall.Read, closing the file descriptor before syscall.Read makes the actual system call. 

### <a id="gcinit" href="#gcinit">func gcinit()</a>

```
searchKey: runtime.gcinit
```

```Go
func gcinit()
```

### <a id="gcenable" href="#gcenable">func gcenable()</a>

```
searchKey: runtime.gcenable
```

```Go
func gcenable()
```

gcenable is called after the bulk of the runtime initialization, just before we're about to start letting user code run. It kicks off the background sweeper goroutine, the background scavenger goroutine, and enables GC. 

### <a id="setGCPhase" href="#setGCPhase">func setGCPhase(x uint32)</a>

```
searchKey: runtime.setGCPhase
```

```Go
func setGCPhase(x uint32)
```

### <a id="pollFractionalWorkerExit" href="#pollFractionalWorkerExit">func pollFractionalWorkerExit() bool</a>

```
searchKey: runtime.pollFractionalWorkerExit
```

```Go
func pollFractionalWorkerExit() bool
```

pollFractionalWorkerExit reports whether a fractional mark worker should self-preempt. It assumes it is called from the fractional worker. 

### <a id="GC" href="#GC">func GC()</a>

```
searchKey: runtime.GC
tags: [exported]
```

```Go
func GC()
```

GC runs a garbage collection and blocks the caller until the garbage collection is complete. It may also block the entire program. 

### <a id="gcWaitOnMark" href="#gcWaitOnMark">func gcWaitOnMark(n uint32)</a>

```
searchKey: runtime.gcWaitOnMark
```

```Go
func gcWaitOnMark(n uint32)
```

gcWaitOnMark blocks until GC finishes the Nth mark phase. If GC has already completed this mark phase, it returns immediately. 

### <a id="gcStart" href="#gcStart">func gcStart(trigger gcTrigger)</a>

```
searchKey: runtime.gcStart
```

```Go
func gcStart(trigger gcTrigger)
```

gcStart starts the GC. It transitions from _GCoff to _GCmark (if debug.gcstoptheworld == 0) or performs all of GC (if debug.gcstoptheworld != 0). 

This may return without performing this transition in some cases, such as when called on a system stack or with locks held. 

### <a id="gcMarkDone" href="#gcMarkDone">func gcMarkDone()</a>

```
searchKey: runtime.gcMarkDone
```

```Go
func gcMarkDone()
```

gcMarkDone transitions the GC from mark to mark termination if all reachable objects have been marked (that is, there are no grey objects and can be no more in the future). Otherwise, it flushes all local work to the global queues where it can be discovered by other workers. 

This should be called when all local mark work has been drained and there are no remaining workers. Specifically, when 

```
work.nwait == work.nproc && !gcMarkWorkAvailable(p)

```
The calling context must be preemptible. 

Flushing local work is important because idle Ps may have local work queued. This is the only way to make that work visible and drive GC to completion. 

It is explicitly okay to have write barriers in this function. If it does transition to mark termination, then all reachable objects have been marked, so the write barrier cannot shade any more objects. 

### <a id="gcMarkTermination" href="#gcMarkTermination">func gcMarkTermination(nextTriggerRatio float64)</a>

```
searchKey: runtime.gcMarkTermination
```

```Go
func gcMarkTermination(nextTriggerRatio float64)
```

World must be stopped and mark assists and background workers must be disabled. 

### <a id="gcBgMarkStartWorkers" href="#gcBgMarkStartWorkers">func gcBgMarkStartWorkers()</a>

```
searchKey: runtime.gcBgMarkStartWorkers
```

```Go
func gcBgMarkStartWorkers()
```

gcBgMarkStartWorkers prepares background mark worker goroutines. These goroutines will not run until the mark phase, but they must be started while the work is not stopped and from a regular G stack. The caller must hold worldsema. 

### <a id="gcBgMarkPrepare" href="#gcBgMarkPrepare">func gcBgMarkPrepare()</a>

```
searchKey: runtime.gcBgMarkPrepare
```

```Go
func gcBgMarkPrepare()
```

gcBgMarkPrepare sets up state for background marking. Mutator assists must not yet be enabled. 

### <a id="gcBgMarkWorker" href="#gcBgMarkWorker">func gcBgMarkWorker()</a>

```
searchKey: runtime.gcBgMarkWorker
```

```Go
func gcBgMarkWorker()
```

### <a id="gcMarkWorkAvailable" href="#gcMarkWorkAvailable">func gcMarkWorkAvailable(p *p) bool</a>

```
searchKey: runtime.gcMarkWorkAvailable
```

```Go
func gcMarkWorkAvailable(p *p) bool
```

gcMarkWorkAvailable reports whether executing a mark worker on p is potentially useful. p may be nil, in which case it only checks the global sources of work. 

### <a id="gcMark" href="#gcMark">func gcMark(startTime int64)</a>

```
searchKey: runtime.gcMark
```

```Go
func gcMark(startTime int64)
```

gcMark runs the mark (or, for concurrent GC, mark termination) All gcWork caches must be empty. STW is in effect at this point. 

### <a id="gcSweep" href="#gcSweep">func gcSweep(mode gcMode)</a>

```
searchKey: runtime.gcSweep
```

```Go
func gcSweep(mode gcMode)
```

gcSweep must be called on the system stack because it acquires the heap lock. See mheap for details. 

The world must be stopped. 

### <a id="gcResetMarkState" href="#gcResetMarkState">func gcResetMarkState()</a>

```
searchKey: runtime.gcResetMarkState
```

```Go
func gcResetMarkState()
```

gcResetMarkState resets global state prior to marking (concurrent or STW) and resets the stack scan state of all Gs. 

This is safe to do without the world stopped because any Gs created during or after this will start out in the reset state. 

gcResetMarkState must be called on the system stack because it acquires the heap lock. See mheap for details. 

### <a id="sync_runtime_registerPoolCleanup" href="#sync_runtime_registerPoolCleanup">func sync_runtime_registerPoolCleanup(f func())</a>

```
searchKey: runtime.sync_runtime_registerPoolCleanup
```

```Go
func sync_runtime_registerPoolCleanup(f func())
```

### <a id="clearpools" href="#clearpools">func clearpools()</a>

```
searchKey: runtime.clearpools
```

```Go
func clearpools()
```

### <a id="itoaDiv" href="#itoaDiv">func itoaDiv(buf []byte, val uint64, dec int) []byte</a>

```
searchKey: runtime.itoaDiv
```

```Go
func itoaDiv(buf []byte, val uint64, dec int) []byte
```

itoaDiv formats val/(10**dec) into buf. 

### <a id="fmtNSAsMS" href="#fmtNSAsMS">func fmtNSAsMS(buf []byte, ns uint64) []byte</a>

```
searchKey: runtime.fmtNSAsMS
```

```Go
func fmtNSAsMS(buf []byte, ns uint64) []byte
```

fmtNSAsMS nicely formats ns nanoseconds as milliseconds. 

### <a id="gcTestMoveStackOnNextCall" href="#gcTestMoveStackOnNextCall">func gcTestMoveStackOnNextCall()</a>

```
searchKey: runtime.gcTestMoveStackOnNextCall
```

```Go
func gcTestMoveStackOnNextCall()
```

gcTestMoveStackOnNextCall causes the stack to be moved on a call immediately following the call to this. It may not work correctly if any other work appears after this call (such as returning). Typically the following call should be marked go:noinline so it performs a stack check. 

In rare cases this may not cause the stack to move, specifically if there's a preemption between this call and the next. 

### <a id="gcTestIsReachable" href="#gcTestIsReachable">func gcTestIsReachable(ptrs ...unsafe.Pointer) (mask uint64)</a>

```
searchKey: runtime.gcTestIsReachable
```

```Go
func gcTestIsReachable(ptrs ...unsafe.Pointer) (mask uint64)
```

gcTestIsReachable performs a GC and returns a bit set where bit i is set if ptrs[i] is reachable. 

### <a id="gcTestPointerClass" href="#gcTestPointerClass">func gcTestPointerClass(p unsafe.Pointer) string</a>

```
searchKey: runtime.gcTestPointerClass
```

```Go
func gcTestPointerClass(p unsafe.Pointer) string
```

gcTestPointerClass returns the category of what p points to, one of: "heap", "stack", "data", "bss", "other". This is useful for checking that a test is doing what it's intended to do. 

This is nosplit simply to avoid extra pointer shuffling that may complicate a test. 

### <a id="gcMarkRootPrepare" href="#gcMarkRootPrepare">func gcMarkRootPrepare()</a>

```
searchKey: runtime.gcMarkRootPrepare
```

```Go
func gcMarkRootPrepare()
```

gcMarkRootPrepare queues root scanning jobs (stacks, globals, and some miscellany) and initializes scanning-related state. 

The world must be stopped. 

### <a id="gcMarkRootCheck" href="#gcMarkRootCheck">func gcMarkRootCheck()</a>

```
searchKey: runtime.gcMarkRootCheck
```

```Go
func gcMarkRootCheck()
```

gcMarkRootCheck checks that all roots have been scanned. It is purely for debugging. 

### <a id="markroot" href="#markroot">func markroot(gcw *gcWork, i uint32)</a>

```
searchKey: runtime.markroot
```

```Go
func markroot(gcw *gcWork, i uint32)
```

markroot scans the i'th root. 

Preemption must be disabled (because this uses a gcWork). 

nowritebarrier is only advisory here. 

### <a id="markrootBlock" href="#markrootBlock">func markrootBlock(b0, n0 uintptr, ptrmask0 *uint8, gcw *gcWork, shard int)</a>

```
searchKey: runtime.markrootBlock
```

```Go
func markrootBlock(b0, n0 uintptr, ptrmask0 *uint8, gcw *gcWork, shard int)
```

markrootBlock scans the shard'th shard of the block of memory [b0, b0+n0), with the given pointer mask. 

### <a id="markrootFreeGStacks" href="#markrootFreeGStacks">func markrootFreeGStacks()</a>

```
searchKey: runtime.markrootFreeGStacks
```

```Go
func markrootFreeGStacks()
```

markrootFreeGStacks frees stacks of dead Gs. 

This does not free stacks of dead Gs cached on Ps, but having a few cached stacks around isn't a problem. 

### <a id="markrootSpans" href="#markrootSpans">func markrootSpans(gcw *gcWork, shard int)</a>

```
searchKey: runtime.markrootSpans
```

```Go
func markrootSpans(gcw *gcWork, shard int)
```

markrootSpans marks roots for one shard of markArenas. 

### <a id="gcAssistAlloc" href="#gcAssistAlloc">func gcAssistAlloc(gp *g)</a>

```
searchKey: runtime.gcAssistAlloc
```

```Go
func gcAssistAlloc(gp *g)
```

gcAssistAlloc performs GC work to make gp's assist debt positive. gp must be the calling user gorountine. 

This must be called with preemption enabled. 

### <a id="gcAssistAlloc1" href="#gcAssistAlloc1">func gcAssistAlloc1(gp *g, scanWork int64)</a>

```
searchKey: runtime.gcAssistAlloc1
```

```Go
func gcAssistAlloc1(gp *g, scanWork int64)
```

gcAssistAlloc1 is the part of gcAssistAlloc that runs on the system stack. This is a separate function to make it easier to see that we're not capturing anything from the user stack, since the user stack may move while we're in this function. 

gcAssistAlloc1 indicates whether this assist completed the mark phase by setting gp.param to non-nil. This can't be communicated on the stack since it may move. 

### <a id="gcWakeAllAssists" href="#gcWakeAllAssists">func gcWakeAllAssists()</a>

```
searchKey: runtime.gcWakeAllAssists
```

```Go
func gcWakeAllAssists()
```

gcWakeAllAssists wakes all currently blocked assists. This is used at the end of a GC cycle. gcBlackenEnabled must be false to prevent new assists from going to sleep after this point. 

### <a id="gcParkAssist" href="#gcParkAssist">func gcParkAssist() bool</a>

```
searchKey: runtime.gcParkAssist
```

```Go
func gcParkAssist() bool
```

gcParkAssist puts the current goroutine on the assist queue and parks. 

gcParkAssist reports whether the assist is now satisfied. If it returns false, the caller must retry the assist. 

### <a id="gcFlushBgCredit" href="#gcFlushBgCredit">func gcFlushBgCredit(scanWork int64)</a>

```
searchKey: runtime.gcFlushBgCredit
```

```Go
func gcFlushBgCredit(scanWork int64)
```

gcFlushBgCredit flushes scanWork units of background scan work credit. This first satisfies blocked assists on the work.assistQueue and then flushes any remaining credit to gcController.bgScanCredit. 

Write barriers are disallowed because this is used by gcDrain after it has ensured that all work is drained and this must preserve that condition. 

### <a id="scanstack" href="#scanstack">func scanstack(gp *g, gcw *gcWork)</a>

```
searchKey: runtime.scanstack
```

```Go
func scanstack(gp *g, gcw *gcWork)
```

scanstack scans gp's stack, greying all pointers found on the stack. 

scanstack will also shrink the stack if it is safe to do so. If it is not, it schedules a stack shrink for the next synchronous safe point. 

scanstack is marked go:systemstack because it must not be preempted while using a workbuf. 

### <a id="scanframeworker" href="#scanframeworker">func scanframeworker(frame *stkframe, state *stackScanState, gcw *gcWork)</a>

```
searchKey: runtime.scanframeworker
```

```Go
func scanframeworker(frame *stkframe, state *stackScanState, gcw *gcWork)
```

Scan a stack frame: local variables and function arguments/results. 

### <a id="gcDrain" href="#gcDrain">func gcDrain(gcw *gcWork, flags gcDrainFlags)</a>

```
searchKey: runtime.gcDrain
```

```Go
func gcDrain(gcw *gcWork, flags gcDrainFlags)
```

gcDrain scans roots and objects in work buffers, blackening grey objects until it is unable to get more work. It may return before GC is done; it's the caller's responsibility to balance work from other Ps. 

If flags&gcDrainUntilPreempt != 0, gcDrain returns when g.preempt is set. 

If flags&gcDrainIdle != 0, gcDrain returns when there is other work to do. 

If flags&gcDrainFractional != 0, gcDrain self-preempts when pollFractionalWorkerExit() returns true. This implies gcDrainNoBlock. 

If flags&gcDrainFlushBgCredit != 0, gcDrain flushes scan work credit to gcController.bgScanCredit every gcCreditSlack units of scan work. 

gcDrain will always return if there is a pending STW. 

### <a id="gcDrainN" href="#gcDrainN">func gcDrainN(gcw *gcWork, scanWork int64) int64</a>

```
searchKey: runtime.gcDrainN
```

```Go
func gcDrainN(gcw *gcWork, scanWork int64) int64
```

gcDrainN blackens grey objects until it has performed roughly scanWork units of scan work or the G is preempted. This is best-effort, so it may perform less work if it fails to get a work buffer. Otherwise, it will perform at least n units of work, but may perform more because scanning is always done in whole object increments. It returns the amount of scan work performed. 

The caller goroutine must be in a preemptible state (e.g., _Gwaiting) to prevent deadlocks during stack scanning. As a consequence, this must be called on the system stack. 

### <a id="scanblock" href="#scanblock">func scanblock(b0, n0 uintptr, ptrmask *uint8, gcw *gcWork, stk *stackScanState)</a>

```
searchKey: runtime.scanblock
```

```Go
func scanblock(b0, n0 uintptr, ptrmask *uint8, gcw *gcWork, stk *stackScanState)
```

scanblock scans b as scanobject would, but using an explicit pointer bitmap instead of the heap bitmap. 

This is used to scan non-heap roots, so it does not update gcw.bytesMarked or gcw.scanWork. 

If stk != nil, possible stack pointers are also reported to stk.putPtr. 

### <a id="scanobject" href="#scanobject">func scanobject(b uintptr, gcw *gcWork)</a>

```
searchKey: runtime.scanobject
```

```Go
func scanobject(b uintptr, gcw *gcWork)
```

scanobject scans the object starting at b, adding pointers to gcw. b must point to the beginning of a heap object or an oblet. scanobject consults the GC bitmap for the pointer mask and the spans for the size of the object. 

### <a id="scanConservative" href="#scanConservative">func scanConservative(b, n uintptr, ptrmask *uint8, gcw *gcWork, state *stackScanState)</a>

```
searchKey: runtime.scanConservative
```

```Go
func scanConservative(b, n uintptr, ptrmask *uint8, gcw *gcWork, state *stackScanState)
```

scanConservative scans block [b, b+n) conservatively, treating any pointer-like value in the block as a pointer. 

If ptrmask != nil, only words that are marked in ptrmask are considered as potential pointers. 

If state != nil, it's assumed that [b, b+n) is a block in the stack and may contain pointers to stack objects. 

### <a id="shade" href="#shade">func shade(b uintptr)</a>

```
searchKey: runtime.shade
```

```Go
func shade(b uintptr)
```

Shade the object if it isn't already. The object is not nil and known to be in the heap. Preemption must be disabled. 

### <a id="greyobject" href="#greyobject">func greyobject(obj, base, off uintptr, span *mspan, gcw *gcWork, objIndex uintptr)</a>

```
searchKey: runtime.greyobject
```

```Go
func greyobject(obj, base, off uintptr, span *mspan, gcw *gcWork, objIndex uintptr)
```

obj is the start of an object with mark mbits. If it isn't already marked, mark it and enqueue into gcw. base and off are for debugging only and could be removed. 

See also wbBufFlush1, which partially duplicates this logic. 

### <a id="gcDumpObject" href="#gcDumpObject">func gcDumpObject(label string, obj, off uintptr)</a>

```
searchKey: runtime.gcDumpObject
```

```Go
func gcDumpObject(label string, obj, off uintptr)
```

gcDumpObject dumps the contents of obj for debugging and marks the field at byte offset off in obj. 

### <a id="gcmarknewobject" href="#gcmarknewobject">func gcmarknewobject(span *mspan, obj, size, scanSize uintptr)</a>

```
searchKey: runtime.gcmarknewobject
```

```Go
func gcmarknewobject(span *mspan, obj, size, scanSize uintptr)
```

gcmarknewobject marks a newly allocated object black. obj must not contain any non-nil pointers. 

This is nosplit so it can manipulate a gcWork without preemption. 

### <a id="gcMarkTinyAllocs" href="#gcMarkTinyAllocs">func gcMarkTinyAllocs()</a>

```
searchKey: runtime.gcMarkTinyAllocs
```

```Go
func gcMarkTinyAllocs()
```

gcMarkTinyAllocs greys all active tiny alloc blocks. 

The world must be stopped. 

### <a id="init" href="#init">func init()</a>

```
searchKey: runtime.init
```

```Go
func init()
```

### <a id="setGCPercent" href="#setGCPercent">func setGCPercent(in int32) (out int32)</a>

```
searchKey: runtime.setGCPercent
```

```Go
func setGCPercent(in int32) (out int32)
```

### <a id="readGOGC" href="#readGOGC">func readGOGC() int32</a>

```
searchKey: runtime.readGOGC
```

```Go
func readGOGC() int32
```

### <a id="heapRetained" href="#heapRetained">func heapRetained() uint64</a>

```
searchKey: runtime.heapRetained
```

```Go
func heapRetained() uint64
```

heapRetained returns an estimate of the current heap RSS. 

### <a id="gcPaceScavenger" href="#gcPaceScavenger">func gcPaceScavenger()</a>

```
searchKey: runtime.gcPaceScavenger
```

```Go
func gcPaceScavenger()
```

gcPaceScavenger updates the scavenger's pacing, particularly its rate and RSS goal. 

The RSS goal is based on the current heap goal with a small overhead to accommodate non-determinism in the allocator. 

The pacing is based on scavengePageRate, which applies to both regular and huge pages. See that constant for more information. 

mheap_.lock must be held or the world must be stopped. 

### <a id="readyForScavenger" href="#readyForScavenger">func readyForScavenger()</a>

```
searchKey: runtime.readyForScavenger
```

```Go
func readyForScavenger()
```

readyForScavenger signals sysmon to wake the scavenger because there may be new work to do. 

There may be a significant delay between when this function runs and when the scavenger is kicked awake, but it may be safely invoked in contexts where wakeScavenger is unsafe to call directly. 

### <a id="wakeScavenger" href="#wakeScavenger">func wakeScavenger()</a>

```
searchKey: runtime.wakeScavenger
```

```Go
func wakeScavenger()
```

wakeScavenger immediately unparks the scavenger if necessary. 

May run without a P, but it may allocate, so it must not be called on any allocation path. 

mheap_.lock, scavenge.lock, and sched.lock must not be held. 

### <a id="scavengeSleep" href="#scavengeSleep">func scavengeSleep(ns int64) int64</a>

```
searchKey: runtime.scavengeSleep
```

```Go
func scavengeSleep(ns int64) int64
```

scavengeSleep attempts to put the scavenger to sleep for ns. 

Note that this function should only be called by the scavenger. 

The scavenger may be woken up earlier by a pacing change, and it may not go to sleep at all if there's a pending pacing change. 

Returns the amount of time actually slept. 

### <a id="bgscavenge" href="#bgscavenge">func bgscavenge()</a>

```
searchKey: runtime.bgscavenge
```

```Go
func bgscavenge()
```

Background scavenger. 

The background scavenger maintains the RSS of the application below the line described by the proportional scavenging statistics in the mheap struct. 

### <a id="printScavTrace" href="#printScavTrace">func printScavTrace(gen uint32, released uintptr, forced bool)</a>

```
searchKey: runtime.printScavTrace
```

```Go
func printScavTrace(gen uint32, released uintptr, forced bool)
```

printScavTrace prints a scavenge trace line to standard error. 

released should be the amount of memory released since the last time this was called, and forced indicates whether the scavenge was forced by the application. 

### <a id="fillAligned" href="#fillAligned">func fillAligned(x uint64, m uint) uint64</a>

```
searchKey: runtime.fillAligned
```

```Go
func fillAligned(x uint64, m uint) uint64
```

fillAligned returns x but with all zeroes in m-aligned groups of m bits set to 1 if any bit in the group is non-zero. 

For example, fillAligned(0x0100a3, 8) == 0xff00ff. 

Note that if m == 1, this is a no-op. 

m must be a power of 2 <= maxPagesPerPhysPage. 

### <a id="init" href="#init">func init()</a>

```
searchKey: runtime.init
```

```Go
func init()
```

### <a id="finishsweep_m" href="#finishsweep_m">func finishsweep_m()</a>

```
searchKey: runtime.finishsweep_m
```

```Go
func finishsweep_m()
```

finishsweep_m ensures that all spans are swept. 

The world must be stopped. This ensures there are no sweeps in progress. 

### <a id="bgsweep" href="#bgsweep">func bgsweep()</a>

```
searchKey: runtime.bgsweep
```

```Go
func bgsweep()
```

### <a id="sweepone" href="#sweepone">func sweepone() uintptr</a>

```
searchKey: runtime.sweepone
```

```Go
func sweepone() uintptr
```

sweepone sweeps some unswept heap span and returns the number of pages returned to the heap, or ^uintptr(0) if there was nothing to sweep. 

### <a id="isSweepDone" href="#isSweepDone">func isSweepDone() bool</a>

```
searchKey: runtime.isSweepDone
```

```Go
func isSweepDone() bool
```

isSweepDone reports whether all spans are swept. 

Note that this condition may transition from false to true at any time as the sweeper runs. It may transition from true to false if a GC runs; to prevent that the caller must be non-preemptible or must somehow block GC progress. 

### <a id="deductSweepCredit" href="#deductSweepCredit">func deductSweepCredit(spanBytes uintptr, callerSweepPages uintptr)</a>

```
searchKey: runtime.deductSweepCredit
```

```Go
func deductSweepCredit(spanBytes uintptr, callerSweepPages uintptr)
```

deductSweepCredit deducts sweep credit for allocating a span of size spanBytes. This must be performed *before* the span is allocated to ensure the system has enough credit. If necessary, it performs sweeping to prevent going in to debt. If the caller will also sweep pages (e.g., for a large allocation), it can pass a non-zero callerSweepPages to leave that many pages unswept. 

deductSweepCredit makes a worst-case assumption that all spanBytes bytes of the ultimately allocated span will be available for object allocation. 

deductSweepCredit is the core of the "proportional sweep" system. It uses statistics gathered by the garbage collector to perform enough sweeping so that all pages are swept during the concurrent sweep phase between GC cycles. 

mheap_ must NOT be locked. 

### <a id="clobberfree" href="#clobberfree">func clobberfree(x unsafe.Pointer, size uintptr)</a>

```
searchKey: runtime.clobberfree
```

```Go
func clobberfree(x unsafe.Pointer, size uintptr)
```

clobberfree sets the memory content at x to bad content, for debugging purposes. 

### <a id="init" href="#init">func init()</a>

```
searchKey: runtime.init
```

```Go
func init()
```

### <a id="putempty" href="#putempty">func putempty(b *workbuf)</a>

```
searchKey: runtime.putempty
```

```Go
func putempty(b *workbuf)
```

putempty puts a workbuf onto the work.empty list. Upon entry this goroutine owns b. The lfstack.push relinquishes ownership. 

### <a id="putfull" href="#putfull">func putfull(b *workbuf)</a>

```
searchKey: runtime.putfull
```

```Go
func putfull(b *workbuf)
```

putfull puts the workbuf on the work.full list for the GC. putfull accepts partially full buffers so the GC can avoid competing with the mutators for ownership of partially full buffers. 

### <a id="prepareFreeWorkbufs" href="#prepareFreeWorkbufs">func prepareFreeWorkbufs()</a>

```
searchKey: runtime.prepareFreeWorkbufs
```

```Go
func prepareFreeWorkbufs()
```

prepareFreeWorkbufs moves busy workbuf spans to free list so they can be freed to the heap. This must only be called when all workbufs are on the empty list. 

### <a id="freeSomeWbufs" href="#freeSomeWbufs">func freeSomeWbufs(preemptible bool) bool</a>

```
searchKey: runtime.freeSomeWbufs
```

```Go
func freeSomeWbufs(preemptible bool) bool
```

freeSomeWbufs frees some workbufs back to the heap and returns true if it should be called again to free more. 

### <a id="recordspan" href="#recordspan">func recordspan(vh unsafe.Pointer, p unsafe.Pointer)</a>

```
searchKey: runtime.recordspan
```

```Go
func recordspan(vh unsafe.Pointer, p unsafe.Pointer)
```

recordspan adds a newly allocated span to h.allspans. 

This only happens the first time a span is allocated from mheap.spanalloc (it is not called when a span is reused). 

Write barriers are disallowed here because it can be called from gcWork when allocating new workbufs. However, because it's an indirect call from the fixalloc initializer, the compiler can't see this. 

The heap lock must be held. 

### <a id="arenaBase" href="#arenaBase">func arenaBase(i arenaIdx) uintptr</a>

```
searchKey: runtime.arenaBase
```

```Go
func arenaBase(i arenaIdx) uintptr
```

arenaBase returns the low address of the region covered by heap arena i. 

### <a id="inheap" href="#inheap">func inheap(b uintptr) bool</a>

```
searchKey: runtime.inheap
```

```Go
func inheap(b uintptr) bool
```

inheap reports whether b is a pointer into a (potentially dead) heap object. It returns false for pointers into mSpanManual spans. Non-preemptible because it is used by write barriers. 

### <a id="inHeapOrStack" href="#inHeapOrStack">func inHeapOrStack(b uintptr) bool</a>

```
searchKey: runtime.inHeapOrStack
```

```Go
func inHeapOrStack(b uintptr) bool
```

inHeapOrStack is a variant of inheap that returns true for pointers into any allocated heap span. 

### <a id="runtime_debug_freeOSMemory" href="#runtime_debug_freeOSMemory">func runtime_debug_freeOSMemory()</a>

```
searchKey: runtime.runtime_debug_freeOSMemory
```

```Go
func runtime_debug_freeOSMemory()
```

### <a id="spanHasSpecials" href="#spanHasSpecials">func spanHasSpecials(s *mspan)</a>

```
searchKey: runtime.spanHasSpecials
```

```Go
func spanHasSpecials(s *mspan)
```

spanHasSpecials marks a span as having specials in the arena bitmap. 

### <a id="spanHasNoSpecials" href="#spanHasNoSpecials">func spanHasNoSpecials(s *mspan)</a>

```
searchKey: runtime.spanHasNoSpecials
```

```Go
func spanHasNoSpecials(s *mspan)
```

spanHasNoSpecials marks a span as having no specials in the arena bitmap. 

### <a id="addspecial" href="#addspecial">func addspecial(p unsafe.Pointer, s *special) bool</a>

```
searchKey: runtime.addspecial
```

```Go
func addspecial(p unsafe.Pointer, s *special) bool
```

Adds the special record s to the list of special records for the object p. All fields of s should be filled in except for offset & next, which this routine will fill in. Returns true if the special was successfully added, false otherwise. (The add will fail only if a record with the same p and s->kind 

```
already exists.)

```
### <a id="addfinalizer" href="#addfinalizer">func addfinalizer(p unsafe.Pointer, f *funcval, nret uintptr, fint *_type, ot *ptrtype) bool</a>

```
searchKey: runtime.addfinalizer
```

```Go
func addfinalizer(p unsafe.Pointer, f *funcval, nret uintptr, fint *_type, ot *ptrtype) bool
```

Adds a finalizer to the object p. Returns true if it succeeded. 

### <a id="removefinalizer" href="#removefinalizer">func removefinalizer(p unsafe.Pointer)</a>

```
searchKey: runtime.removefinalizer
```

```Go
func removefinalizer(p unsafe.Pointer)
```

Removes the finalizer (if any) from the object p. 

### <a id="setprofilebucket" href="#setprofilebucket">func setprofilebucket(p unsafe.Pointer, b *bucket)</a>

```
searchKey: runtime.setprofilebucket
```

```Go
func setprofilebucket(p unsafe.Pointer, b *bucket)
```

Set the heap profile bucket associated with addr to b. 

### <a id="freeSpecial" href="#freeSpecial">func freeSpecial(s *special, p unsafe.Pointer, size uintptr)</a>

```
searchKey: runtime.freeSpecial
```

```Go
func freeSpecial(s *special, p unsafe.Pointer, size uintptr)
```

freeSpecial performs any cleanup on special s and deallocates it. s must already be unlinked from the specials list. 

### <a id="nextMarkBitArenaEpoch" href="#nextMarkBitArenaEpoch">func nextMarkBitArenaEpoch()</a>

```
searchKey: runtime.nextMarkBitArenaEpoch
```

```Go
func nextMarkBitArenaEpoch()
```

nextMarkBitArenaEpoch establishes a new epoch for the arenas holding the mark bits. The arenas are named relative to the current GC cycle which is demarcated by the call to finishweep_m. 

All current spans have been swept. During that sweep each span allocated room for its gcmarkBits in gcBitsArenas.next block. gcBitsArenas.next becomes the gcBitsArenas.current where the GC will mark objects and after each span is swept these bits will be used to allocate objects. gcBitsArenas.current becomes gcBitsArenas.previous where the span's gcAllocBits live until all the spans have been swept during this GC cycle. The span's sweep extinguishes all the references to gcBitsArenas.previous by pointing gcAllocBits into the gcBitsArenas.current. The gcBitsArenas.previous is released to the gcBitsArenas.free list. 

### <a id="chunkBase" href="#chunkBase">func chunkBase(ci chunkIdx) uintptr</a>

```
searchKey: runtime.chunkBase
```

```Go
func chunkBase(ci chunkIdx) uintptr
```

chunkIndex returns the base address of the palloc chunk at index ci. 

### <a id="chunkPageIndex" href="#chunkPageIndex">func chunkPageIndex(p uintptr) uint</a>

```
searchKey: runtime.chunkPageIndex
```

```Go
func chunkPageIndex(p uintptr) uint
```

chunkPageIndex computes the index of the page that contains p, relative to the chunk which contains p. 

### <a id="offAddrToLevelIndex" href="#offAddrToLevelIndex">func offAddrToLevelIndex(level int, addr offAddr) int</a>

```
searchKey: runtime.offAddrToLevelIndex
```

```Go
func offAddrToLevelIndex(level int, addr offAddr) int
```

offAddrToLevelIndex converts an address in the offset address space to the index into summary[level] containing addr. 

### <a id="addrsToSummaryRange" href="#addrsToSummaryRange">func addrsToSummaryRange(level int, base, limit uintptr) (lo int, hi int)</a>

```
searchKey: runtime.addrsToSummaryRange
```

```Go
func addrsToSummaryRange(level int, base, limit uintptr) (lo int, hi int)
```

addrsToSummaryRange converts base and limit pointers into a range of entries for the given summary level. 

The returned range is inclusive on the lower bound and exclusive on the upper bound. 

### <a id="blockAlignSummaryRange" href="#blockAlignSummaryRange">func blockAlignSummaryRange(level int, lo, hi int) (int, int)</a>

```
searchKey: runtime.blockAlignSummaryRange
```

```Go
func blockAlignSummaryRange(level int, lo, hi int) (int, int)
```

blockAlignSummaryRange aligns indices into the given level to that level's block width (1 << levelBits[level]). It assumes lo is inclusive and hi is exclusive, and so aligns them down and up respectively. 

### <a id="findBitRange64" href="#findBitRange64">func findBitRange64(c uint64, n uint) uint</a>

```
searchKey: runtime.findBitRange64
```

```Go
func findBitRange64(c uint64, n uint) uint
```

findBitRange64 returns the bit index of the first set of n consecutive 1 bits. If no consecutive set of 1 bits of size n may be found in c, then it returns an integer >= 64. n must be > 0. 

### <a id="eqslice" href="#eqslice">func eqslice(x, y []uintptr) bool</a>

```
searchKey: runtime.eqslice
```

```Go
func eqslice(x, y []uintptr) bool
```

### <a id="mProf_NextCycle" href="#mProf_NextCycle">func mProf_NextCycle()</a>

```
searchKey: runtime.mProf_NextCycle
```

```Go
func mProf_NextCycle()
```

mProf_NextCycle publishes the next heap profile cycle and creates a fresh heap profile cycle. This operation is fast and can be done during STW. The caller must call mProf_Flush before calling mProf_NextCycle again. 

This is called by mark termination during STW so allocations and frees after the world is started again count towards a new heap profiling cycle. 

### <a id="mProf_Flush" href="#mProf_Flush">func mProf_Flush()</a>

```
searchKey: runtime.mProf_Flush
```

```Go
func mProf_Flush()
```

mProf_Flush flushes the events from the current heap profiling cycle into the active profile. After this it is safe to start a new heap profiling cycle with mProf_NextCycle. 

This is called by GC after mark termination starts the world. In contrast with mProf_NextCycle, this is somewhat expensive, but safe to do concurrently. 

### <a id="mProf_FlushLocked" href="#mProf_FlushLocked">func mProf_FlushLocked()</a>

```
searchKey: runtime.mProf_FlushLocked
```

```Go
func mProf_FlushLocked()
```

### <a id="mProf_PostSweep" href="#mProf_PostSweep">func mProf_PostSweep()</a>

```
searchKey: runtime.mProf_PostSweep
```

```Go
func mProf_PostSweep()
```

mProf_PostSweep records that all sweep frees for this GC cycle have completed. This has the effect of publishing the heap profile snapshot as of the last mark termination without advancing the heap profile cycle. 

### <a id="mProf_Malloc" href="#mProf_Malloc">func mProf_Malloc(p unsafe.Pointer, size uintptr)</a>

```
searchKey: runtime.mProf_Malloc
```

```Go
func mProf_Malloc(p unsafe.Pointer, size uintptr)
```

Called by malloc to record a profiled block. 

### <a id="mProf_Free" href="#mProf_Free">func mProf_Free(b *bucket, size uintptr)</a>

```
searchKey: runtime.mProf_Free
```

```Go
func mProf_Free(b *bucket, size uintptr)
```

Called when freeing a profiled block. 

### <a id="SetBlockProfileRate" href="#SetBlockProfileRate">func SetBlockProfileRate(rate int)</a>

```
searchKey: runtime.SetBlockProfileRate
tags: [exported]
```

```Go
func SetBlockProfileRate(rate int)
```

SetBlockProfileRate controls the fraction of goroutine blocking events that are reported in the blocking profile. The profiler aims to sample an average of one blocking event per rate nanoseconds spent blocked. 

To include every blocking event in the profile, pass rate = 1. To turn off profiling entirely, pass rate <= 0. 

### <a id="blockevent" href="#blockevent">func blockevent(cycles int64, skip int)</a>

```
searchKey: runtime.blockevent
```

```Go
func blockevent(cycles int64, skip int)
```

### <a id="blocksampled" href="#blocksampled">func blocksampled(cycles, rate int64) bool</a>

```
searchKey: runtime.blocksampled
```

```Go
func blocksampled(cycles, rate int64) bool
```

blocksampled returns true for all events where cycles >= rate. Shorter events have a cycles/rate random chance of returning true. 

### <a id="saveblockevent" href="#saveblockevent">func saveblockevent(cycles, rate int64, skip int, which bucketType)</a>

```
searchKey: runtime.saveblockevent
```

```Go
func saveblockevent(cycles, rate int64, skip int, which bucketType)
```

### <a id="SetMutexProfileFraction" href="#SetMutexProfileFraction">func SetMutexProfileFraction(rate int) int</a>

```
searchKey: runtime.SetMutexProfileFraction
tags: [exported]
```

```Go
func SetMutexProfileFraction(rate int) int
```

SetMutexProfileFraction controls the fraction of mutex contention events that are reported in the mutex profile. On average 1/rate events are reported. The previous rate is returned. 

To turn off profiling entirely, pass rate 0. To just read the current rate, pass rate < 0. (For n>1 the details of sampling may change.) 

### <a id="mutexevent" href="#mutexevent">func mutexevent(cycles int64, skip int)</a>

```
searchKey: runtime.mutexevent
```

```Go
func mutexevent(cycles int64, skip int)
```

### <a id="defaultMemProfileRate" href="#defaultMemProfileRate">func defaultMemProfileRate(v int) int</a>

```
searchKey: runtime.defaultMemProfileRate
```

```Go
func defaultMemProfileRate(v int) int
```

defaultMemProfileRate returns 0 if disableMemoryProfiling is set. It exists primarily for the godoc rendering of MemProfileRate above. 

### <a id="MemProfile" href="#MemProfile">func MemProfile(p []MemProfileRecord, inuseZero bool) (n int, ok bool)</a>

```
searchKey: runtime.MemProfile
tags: [exported]
```

```Go
func MemProfile(p []MemProfileRecord, inuseZero bool) (n int, ok bool)
```

MemProfile returns a profile of memory allocated and freed per allocation site. 

MemProfile returns n, the number of records in the current memory profile. If len(p) >= n, MemProfile copies the profile into p and returns n, true. If len(p) < n, MemProfile does not change p and returns n, false. 

If inuseZero is true, the profile includes allocation records where r.AllocBytes > 0 but r.AllocBytes == r.FreeBytes. These are sites where memory was allocated, but it has all been released back to the runtime. 

The returned profile may be up to two garbage collection cycles old. This is to avoid skewing the profile toward allocations; because allocations happen in real time but frees are delayed until the garbage collector performs sweeping, the profile only accounts for allocations that have had a chance to be freed by the garbage collector. 

Most clients should use the runtime/pprof package or the testing package's -test.memprofile flag instead of calling MemProfile directly. 

### <a id="record" href="#record">func record(r *MemProfileRecord, b *bucket)</a>

```
searchKey: runtime.record
```

```Go
func record(r *MemProfileRecord, b *bucket)
```

Write b's data to r. 

### <a id="iterate_memprof" href="#iterate_memprof">func iterate_memprof(fn func(*bucket, uintptr, *uintptr, uintptr, uintptr, uintptr))</a>

```
searchKey: runtime.iterate_memprof
```

```Go
func iterate_memprof(fn func(*bucket, uintptr, *uintptr, uintptr, uintptr, uintptr))
```

### <a id="BlockProfile" href="#BlockProfile">func BlockProfile(p []BlockProfileRecord) (n int, ok bool)</a>

```
searchKey: runtime.BlockProfile
tags: [exported]
```

```Go
func BlockProfile(p []BlockProfileRecord) (n int, ok bool)
```

BlockProfile returns n, the number of records in the current blocking profile. If len(p) >= n, BlockProfile copies the profile into p and returns n, true. If len(p) < n, BlockProfile does not change p and returns n, false. 

Most clients should use the runtime/pprof package or the testing package's -test.blockprofile flag instead of calling BlockProfile directly. 

### <a id="MutexProfile" href="#MutexProfile">func MutexProfile(p []BlockProfileRecord) (n int, ok bool)</a>

```
searchKey: runtime.MutexProfile
tags: [exported]
```

```Go
func MutexProfile(p []BlockProfileRecord) (n int, ok bool)
```

MutexProfile returns n, the number of records in the current mutex profile. If len(p) >= n, MutexProfile copies the profile into p and returns n, true. Otherwise, MutexProfile does not change p, and returns n, false. 

Most clients should use the runtime/pprof package instead of calling MutexProfile directly. 

### <a id="ThreadCreateProfile" href="#ThreadCreateProfile">func ThreadCreateProfile(p []StackRecord) (n int, ok bool)</a>

```
searchKey: runtime.ThreadCreateProfile
tags: [exported]
```

```Go
func ThreadCreateProfile(p []StackRecord) (n int, ok bool)
```

ThreadCreateProfile returns n, the number of records in the thread creation profile. If len(p) >= n, ThreadCreateProfile copies the profile into p and returns n, true. If len(p) < n, ThreadCreateProfile does not change p and returns n, false. 

Most clients should use the runtime/pprof package instead of calling ThreadCreateProfile directly. 

### <a id="runtime_goroutineProfileWithLabels" href="#runtime_goroutineProfileWithLabels">func runtime_goroutineProfileWithLabels(p []StackRecord, labels []unsafe.Pointer) (n int, ok bool)</a>

```
searchKey: runtime.runtime_goroutineProfileWithLabels
```

```Go
func runtime_goroutineProfileWithLabels(p []StackRecord, labels []unsafe.Pointer) (n int, ok bool)
```

### <a id="goroutineProfileWithLabels" href="#goroutineProfileWithLabels">func goroutineProfileWithLabels(p []StackRecord, labels []unsafe.Pointer) (n int, ok bool)</a>

```
searchKey: runtime.goroutineProfileWithLabels
```

```Go
func goroutineProfileWithLabels(p []StackRecord, labels []unsafe.Pointer) (n int, ok bool)
```

labels may be nil. If labels is non-nil, it must have the same length as p. 

### <a id="GoroutineProfile" href="#GoroutineProfile">func GoroutineProfile(p []StackRecord) (n int, ok bool)</a>

```
searchKey: runtime.GoroutineProfile
tags: [exported]
```

```Go
func GoroutineProfile(p []StackRecord) (n int, ok bool)
```

GoroutineProfile returns n, the number of records in the active goroutine stack profile. If len(p) >= n, GoroutineProfile copies the profile into p and returns n, true. If len(p) < n, GoroutineProfile does not change p and returns n, false. 

Most clients should use the runtime/pprof package instead of calling GoroutineProfile directly. 

### <a id="saveg" href="#saveg">func saveg(pc, sp uintptr, gp *g, r *StackRecord)</a>

```
searchKey: runtime.saveg
```

```Go
func saveg(pc, sp uintptr, gp *g, r *StackRecord)
```

### <a id="Stack" href="#Stack">func Stack(buf []byte, all bool) int</a>

```
searchKey: runtime.Stack
tags: [exported]
```

```Go
func Stack(buf []byte, all bool) int
```

Stack formats a stack trace of the calling goroutine into buf and returns the number of bytes written to buf. If all is true, Stack formats stack traces of all other goroutines into buf after the trace for the current goroutine. 

### <a id="tracealloc" href="#tracealloc">func tracealloc(p unsafe.Pointer, size uintptr, typ *_type)</a>

```
searchKey: runtime.tracealloc
```

```Go
func tracealloc(p unsafe.Pointer, size uintptr, typ *_type)
```

### <a id="tracefree" href="#tracefree">func tracefree(p unsafe.Pointer, size uintptr)</a>

```
searchKey: runtime.tracefree
```

```Go
func tracefree(p unsafe.Pointer, size uintptr)
```

### <a id="tracegc" href="#tracegc">func tracegc()</a>

```
searchKey: runtime.tracegc
```

```Go
func tracegc()
```

### <a id="msanread" href="#msanread">func msanread(addr unsafe.Pointer, sz uintptr)</a>

```
searchKey: runtime.msanread
```

```Go
func msanread(addr unsafe.Pointer, sz uintptr)
```

### <a id="msanwrite" href="#msanwrite">func msanwrite(addr unsafe.Pointer, sz uintptr)</a>

```
searchKey: runtime.msanwrite
```

```Go
func msanwrite(addr unsafe.Pointer, sz uintptr)
```

### <a id="msanmalloc" href="#msanmalloc">func msanmalloc(addr unsafe.Pointer, sz uintptr)</a>

```
searchKey: runtime.msanmalloc
```

```Go
func msanmalloc(addr unsafe.Pointer, sz uintptr)
```

### <a id="msanfree" href="#msanfree">func msanfree(addr unsafe.Pointer, sz uintptr)</a>

```
searchKey: runtime.msanfree
```

```Go
func msanfree(addr unsafe.Pointer, sz uintptr)
```

### <a id="msanmove" href="#msanmove">func msanmove(dst, src unsafe.Pointer, sz uintptr)</a>

```
searchKey: runtime.msanmove
```

```Go
func msanmove(dst, src unsafe.Pointer, sz uintptr)
```

### <a id="roundupsize" href="#roundupsize">func roundupsize(size uintptr) uintptr</a>

```
searchKey: runtime.roundupsize
```

```Go
func roundupsize(size uintptr) uintptr
```

Returns size of the memory block that mallocgc will allocate if you ask for the size. 

### <a id="init" href="#init">func init()</a>

```
searchKey: runtime.init
```

```Go
func init()
```

### <a id="ReadMemStats" href="#ReadMemStats">func ReadMemStats(m *MemStats)</a>

```
searchKey: runtime.ReadMemStats
tags: [exported]
```

```Go
func ReadMemStats(m *MemStats)
```

ReadMemStats populates m with memory allocator statistics. 

The returned memory allocator statistics are up to date as of the call to ReadMemStats. This is in contrast with a heap profile, which is a snapshot as of the most recently completed garbage collection cycle. 

### <a id="readmemstats_m" href="#readmemstats_m">func readmemstats_m(stats *MemStats)</a>

```
searchKey: runtime.readmemstats_m
```

```Go
func readmemstats_m(stats *MemStats)
```

### <a id="readGCStats" href="#readGCStats">func readGCStats(pauses *[]uint64)</a>

```
searchKey: runtime.readGCStats
```

```Go
func readGCStats(pauses *[]uint64)
```

### <a id="readGCStats_m" href="#readGCStats_m">func readGCStats_m(pauses *[]uint64)</a>

```
searchKey: runtime.readGCStats_m
```

```Go
func readGCStats_m(pauses *[]uint64)
```

readGCStats_m must be called on the system stack because it acquires the heap lock. See mheap for details. 

### <a id="updatememstats" href="#updatememstats">func updatememstats()</a>

```
searchKey: runtime.updatememstats
```

```Go
func updatememstats()
```

Updates the memstats structure. 

The world must be stopped. 

### <a id="flushmcache" href="#flushmcache">func flushmcache(i int)</a>

```
searchKey: runtime.flushmcache
```

```Go
func flushmcache(i int)
```

flushmcache flushes the mcache of allp[i]. 

The world must be stopped. 

### <a id="flushallmcaches" href="#flushallmcaches">func flushallmcaches()</a>

```
searchKey: runtime.flushallmcaches
```

```Go
func flushallmcaches()
```

flushallmcaches flushes the mcaches of all Ps. 

The world must be stopped. 

### <a id="wbBufFlush" href="#wbBufFlush">func wbBufFlush(dst *uintptr, src uintptr)</a>

```
searchKey: runtime.wbBufFlush
```

```Go
func wbBufFlush(dst *uintptr, src uintptr)
```

wbBufFlush flushes the current P's write barrier buffer to the GC workbufs. It is passed the slot and value of the write barrier that caused the flush so that it can implement cgocheck. 

This must not have write barriers because it is part of the write barrier implementation. 

This and everything it calls must be nosplit because 1) the stack contains untyped slots from gcWriteBarrier and 2) there must not be a GC safe point between the write barrier test in the caller and flushing the buffer. 

TODO: A "go:nosplitrec" annotation would be perfect for this. 

### <a id="wbBufFlush1" href="#wbBufFlush1">func wbBufFlush1(_p_ *p)</a>

```
searchKey: runtime.wbBufFlush1
```

```Go
func wbBufFlush1(_p_ *p)
```

wbBufFlush1 flushes p's write barrier buffer to the GC work queue. 

This must not have write barriers because it is part of the write barrier implementation, so this may lead to infinite loops or buffer corruption. 

This must be non-preemptible because it uses the P's workbuf. 

### <a id="nonblockingPipe" href="#nonblockingPipe">func nonblockingPipe() (r, w int32, errno int32)</a>

```
searchKey: runtime.nonblockingPipe
```

```Go
func nonblockingPipe() (r, w int32, errno int32)
```

### <a id="poll_runtime_pollServerInit" href="#poll_runtime_pollServerInit">func poll_runtime_pollServerInit()</a>

```
searchKey: runtime.poll_runtime_pollServerInit
```

```Go
func poll_runtime_pollServerInit()
```

### <a id="netpollGenericInit" href="#netpollGenericInit">func netpollGenericInit()</a>

```
searchKey: runtime.netpollGenericInit
```

```Go
func netpollGenericInit()
```

### <a id="netpollinited" href="#netpollinited">func netpollinited() bool</a>

```
searchKey: runtime.netpollinited
```

```Go
func netpollinited() bool
```

### <a id="poll_runtime_isPollServerDescriptor" href="#poll_runtime_isPollServerDescriptor">func poll_runtime_isPollServerDescriptor(fd uintptr) bool</a>

```
searchKey: runtime.poll_runtime_isPollServerDescriptor
```

```Go
func poll_runtime_isPollServerDescriptor(fd uintptr) bool
```

poll_runtime_isPollServerDescriptor reports whether fd is a descriptor being used by netpoll. 

### <a id="poll_runtime_pollClose" href="#poll_runtime_pollClose">func poll_runtime_pollClose(pd *pollDesc)</a>

```
searchKey: runtime.poll_runtime_pollClose
```

```Go
func poll_runtime_pollClose(pd *pollDesc)
```

### <a id="poll_runtime_pollReset" href="#poll_runtime_pollReset">func poll_runtime_pollReset(pd *pollDesc, mode int) int</a>

```
searchKey: runtime.poll_runtime_pollReset
```

```Go
func poll_runtime_pollReset(pd *pollDesc, mode int) int
```

poll_runtime_pollReset, which is internal/poll.runtime_pollReset, prepares a descriptor for polling in mode, which is 'r' or 'w'. This returns an error code; the codes are defined above. 

### <a id="poll_runtime_pollWait" href="#poll_runtime_pollWait">func poll_runtime_pollWait(pd *pollDesc, mode int) int</a>

```
searchKey: runtime.poll_runtime_pollWait
```

```Go
func poll_runtime_pollWait(pd *pollDesc, mode int) int
```

poll_runtime_pollWait, which is internal/poll.runtime_pollWait, waits for a descriptor to be ready for reading or writing, according to mode, which is 'r' or 'w'. This returns an error code; the codes are defined above. 

### <a id="poll_runtime_pollWaitCanceled" href="#poll_runtime_pollWaitCanceled">func poll_runtime_pollWaitCanceled(pd *pollDesc, mode int)</a>

```
searchKey: runtime.poll_runtime_pollWaitCanceled
```

```Go
func poll_runtime_pollWaitCanceled(pd *pollDesc, mode int)
```

### <a id="poll_runtime_pollSetDeadline" href="#poll_runtime_pollSetDeadline">func poll_runtime_pollSetDeadline(pd *pollDesc, d int64, mode int)</a>

```
searchKey: runtime.poll_runtime_pollSetDeadline
```

```Go
func poll_runtime_pollSetDeadline(pd *pollDesc, d int64, mode int)
```

### <a id="poll_runtime_pollUnblock" href="#poll_runtime_pollUnblock">func poll_runtime_pollUnblock(pd *pollDesc)</a>

```
searchKey: runtime.poll_runtime_pollUnblock
```

```Go
func poll_runtime_pollUnblock(pd *pollDesc)
```

### <a id="netpollready" href="#netpollready">func netpollready(toRun *gList, pd *pollDesc, mode int32)</a>

```
searchKey: runtime.netpollready
```

```Go
func netpollready(toRun *gList, pd *pollDesc, mode int32)
```

netpollready is called by the platform-specific netpoll function. It declares that the fd associated with pd is ready for I/O. The toRun argument is used to build a list of goroutines to return from netpoll. The mode argument is 'r', 'w', or 'r'+'w' to indicate whether the fd is ready for reading or writing or both. 

This may run while the world is stopped, so write barriers are not allowed. 

### <a id="netpollcheckerr" href="#netpollcheckerr">func netpollcheckerr(pd *pollDesc, mode int32) int</a>

```
searchKey: runtime.netpollcheckerr
```

```Go
func netpollcheckerr(pd *pollDesc, mode int32) int
```

### <a id="netpollblockcommit" href="#netpollblockcommit">func netpollblockcommit(gp *g, gpp unsafe.Pointer) bool</a>

```
searchKey: runtime.netpollblockcommit
```

```Go
func netpollblockcommit(gp *g, gpp unsafe.Pointer) bool
```

### <a id="netpollgoready" href="#netpollgoready">func netpollgoready(gp *g, traceskip int)</a>

```
searchKey: runtime.netpollgoready
```

```Go
func netpollgoready(gp *g, traceskip int)
```

### <a id="netpollblock" href="#netpollblock">func netpollblock(pd *pollDesc, mode int32, waitio bool) bool</a>

```
searchKey: runtime.netpollblock
```

```Go
func netpollblock(pd *pollDesc, mode int32, waitio bool) bool
```

returns true if IO is ready, or false if timedout or closed waitio - wait only for completed IO, ignore errors 

### <a id="netpolldeadlineimpl" href="#netpolldeadlineimpl">func netpolldeadlineimpl(pd *pollDesc, seq uintptr, read, write bool)</a>

```
searchKey: runtime.netpolldeadlineimpl
```

```Go
func netpolldeadlineimpl(pd *pollDesc, seq uintptr, read, write bool)
```

### <a id="netpollDeadline" href="#netpollDeadline">func netpollDeadline(arg interface{}, seq uintptr)</a>

```
searchKey: runtime.netpollDeadline
```

```Go
func netpollDeadline(arg interface{}, seq uintptr)
```

### <a id="netpollReadDeadline" href="#netpollReadDeadline">func netpollReadDeadline(arg interface{}, seq uintptr)</a>

```
searchKey: runtime.netpollReadDeadline
```

```Go
func netpollReadDeadline(arg interface{}, seq uintptr)
```

### <a id="netpollWriteDeadline" href="#netpollWriteDeadline">func netpollWriteDeadline(arg interface{}, seq uintptr)</a>

```
searchKey: runtime.netpollWriteDeadline
```

```Go
func netpollWriteDeadline(arg interface{}, seq uintptr)
```

### <a id="netpollinit" href="#netpollinit">func netpollinit()</a>

```
searchKey: runtime.netpollinit
```

```Go
func netpollinit()
```

### <a id="netpollIsPollDescriptor" href="#netpollIsPollDescriptor">func netpollIsPollDescriptor(fd uintptr) bool</a>

```
searchKey: runtime.netpollIsPollDescriptor
```

```Go
func netpollIsPollDescriptor(fd uintptr) bool
```

### <a id="netpollopen" href="#netpollopen">func netpollopen(fd uintptr, pd *pollDesc) int32</a>

```
searchKey: runtime.netpollopen
```

```Go
func netpollopen(fd uintptr, pd *pollDesc) int32
```

### <a id="netpollclose" href="#netpollclose">func netpollclose(fd uintptr) int32</a>

```
searchKey: runtime.netpollclose
```

```Go
func netpollclose(fd uintptr) int32
```

### <a id="netpollarm" href="#netpollarm">func netpollarm(pd *pollDesc, mode int)</a>

```
searchKey: runtime.netpollarm
```

```Go
func netpollarm(pd *pollDesc, mode int)
```

### <a id="netpollBreak" href="#netpollBreak">func netpollBreak()</a>

```
searchKey: runtime.netpollBreak
```

```Go
func netpollBreak()
```

netpollBreak interrupts a kevent. 

### <a id="unimplemented" href="#unimplemented">func unimplemented(name string)</a>

```
searchKey: runtime.unimplemented
```

```Go
func unimplemented(name string)
```

### <a id="semacreate" href="#semacreate">func semacreate(mp *m)</a>

```
searchKey: runtime.semacreate
```

```Go
func semacreate(mp *m)
```

### <a id="semasleep" href="#semasleep">func semasleep(ns int64) int32</a>

```
searchKey: runtime.semasleep
```

```Go
func semasleep(ns int64) int32
```

### <a id="semawakeup" href="#semawakeup">func semawakeup(mp *m)</a>

```
searchKey: runtime.semawakeup
```

```Go
func semawakeup(mp *m)
```

### <a id="sigNoteSetup" href="#sigNoteSetup">func sigNoteSetup(*note)</a>

```
searchKey: runtime.sigNoteSetup
```

```Go
func sigNoteSetup(*note)
```

sigNoteSetup initializes an async-signal-safe note. 

The current implementation of notes on Darwin is not async-signal-safe, because the functions pthread_mutex_lock, pthread_cond_signal, and pthread_mutex_unlock, called by semawakeup, are not async-signal-safe. There is only one case where we need to wake up a note from a signal handler: the sigsend function. The signal handler code does not require all the features of notes: it does not need to do a timed wait. This is a separate implementation of notes, based on a pipe, that does not support timed waits but is async-signal-safe. 

### <a id="sigNoteWakeup" href="#sigNoteWakeup">func sigNoteWakeup(*note)</a>

```
searchKey: runtime.sigNoteWakeup
```

```Go
func sigNoteWakeup(*note)
```

sigNoteWakeup wakes up a thread sleeping on a note created by sigNoteSetup. 

### <a id="sigNoteSleep" href="#sigNoteSleep">func sigNoteSleep(*note)</a>

```
searchKey: runtime.sigNoteSleep
```

```Go
func sigNoteSleep(*note)
```

sigNoteSleep waits for a note created by sigNoteSetup to be woken. 

### <a id="osinit" href="#osinit">func osinit()</a>

```
searchKey: runtime.osinit
```

```Go
func osinit()
```

BSD interface for threading. 

### <a id="sysctlbynameInt32" href="#sysctlbynameInt32">func sysctlbynameInt32(name []byte) (int32, int32)</a>

```
searchKey: runtime.sysctlbynameInt32
```

```Go
func sysctlbynameInt32(name []byte) (int32, int32)
```

### <a id="internal_cpu_getsysctlbyname" href="#internal_cpu_getsysctlbyname">func internal_cpu_getsysctlbyname(name []byte) (int32, int32)</a>

```
searchKey: runtime.internal_cpu_getsysctlbyname
```

```Go
func internal_cpu_getsysctlbyname(name []byte) (int32, int32)
```

### <a id="getncpu" href="#getncpu">func getncpu() int32</a>

```
searchKey: runtime.getncpu
```

```Go
func getncpu() int32
```

### <a id="getPageSize" href="#getPageSize">func getPageSize() uintptr</a>

```
searchKey: runtime.getPageSize
```

```Go
func getPageSize() uintptr
```

### <a id="getRandomData" href="#getRandomData">func getRandomData(r []byte)</a>

```
searchKey: runtime.getRandomData
```

```Go
func getRandomData(r []byte)
```

### <a id="goenvs" href="#goenvs">func goenvs()</a>

```
searchKey: runtime.goenvs
```

```Go
func goenvs()
```

### <a id="newosproc" href="#newosproc">func newosproc(mp *m)</a>

```
searchKey: runtime.newosproc
```

```Go
func newosproc(mp *m)
```

May run with m.p==nil, so write barriers are not allowed. 

### <a id="mstart_stub" href="#mstart_stub">func mstart_stub()</a>

```
searchKey: runtime.mstart_stub
```

```Go
func mstart_stub()
```

glue code to call mstart from pthread_create. 

### <a id="newosproc0" href="#newosproc0">func newosproc0(stacksize uintptr, fn uintptr)</a>

```
searchKey: runtime.newosproc0
```

```Go
func newosproc0(stacksize uintptr, fn uintptr)
```

newosproc0 is a version of newosproc that can be called before the runtime is initialized. 

This function is not safe to use after initialization as it does not pass an M as fnarg. 

### <a id="libpreinit" href="#libpreinit">func libpreinit()</a>

```
searchKey: runtime.libpreinit
```

```Go
func libpreinit()
```

Called to do synchronous initialization of Go code built with -buildmode=c-archive or -buildmode=c-shared. None of the Go runtime is initialized. 

### <a id="mpreinit" href="#mpreinit">func mpreinit(mp *m)</a>

```
searchKey: runtime.mpreinit
```

```Go
func mpreinit(mp *m)
```

Called to initialize a new m (including the bootstrap m). Called on the parent thread (main thread in case of bootstrap), can allocate memory. 

### <a id="minit" href="#minit">func minit()</a>

```
searchKey: runtime.minit
```

```Go
func minit()
```

Called to initialize a new m (including the bootstrap m). Called on the new thread, cannot allocate memory. 

### <a id="unminit" href="#unminit">func unminit()</a>

```
searchKey: runtime.unminit
```

```Go
func unminit()
```

Called from dropm to undo the effect of an minit. 

### <a id="mdestroy" href="#mdestroy">func mdestroy(mp *m)</a>

```
searchKey: runtime.mdestroy
```

```Go
func mdestroy(mp *m)
```

Called from exitm, but not from drop, to undo the effect of thread-owned resources in minit, semacreate, or elsewhere. Do not take locks after calling this. 

### <a id="osyield_no_g" href="#osyield_no_g">func osyield_no_g()</a>

```
searchKey: runtime.osyield_no_g
```

```Go
func osyield_no_g()
```

### <a id="osyield" href="#osyield">func osyield()</a>

```
searchKey: runtime.osyield
```

```Go
func osyield()
```

### <a id="setsig" href="#setsig">func setsig(i uint32, fn uintptr)</a>

```
searchKey: runtime.setsig
```

```Go
func setsig(i uint32, fn uintptr)
```

### <a id="sigtramp" href="#sigtramp">func sigtramp()</a>

```
searchKey: runtime.sigtramp
```

```Go
func sigtramp()
```

sigtramp is the callback from libc when a signal is received. It is called with the C calling convention. 

### <a id="cgoSigtramp" href="#cgoSigtramp">func cgoSigtramp()</a>

```
searchKey: runtime.cgoSigtramp
```

```Go
func cgoSigtramp()
```

### <a id="setsigstack" href="#setsigstack">func setsigstack(i uint32)</a>

```
searchKey: runtime.setsigstack
```

```Go
func setsigstack(i uint32)
```

### <a id="getsig" href="#getsig">func getsig(i uint32) uintptr</a>

```
searchKey: runtime.getsig
```

```Go
func getsig(i uint32) uintptr
```

### <a id="setSignalstackSP" href="#setSignalstackSP">func setSignalstackSP(s *stackt, sp uintptr)</a>

```
searchKey: runtime.setSignalstackSP
```

```Go
func setSignalstackSP(s *stackt, sp uintptr)
```

setSignaltstackSP sets the ss_sp field of a stackt. 

### <a id="sigaddset" href="#sigaddset">func sigaddset(mask *sigset, i int)</a>

```
searchKey: runtime.sigaddset
```

```Go
func sigaddset(mask *sigset, i int)
```

### <a id="sigdelset" href="#sigdelset">func sigdelset(mask *sigset, i int)</a>

```
searchKey: runtime.sigdelset
```

```Go
func sigdelset(mask *sigset, i int)
```

### <a id="sysargs" href="#sysargs">func sysargs(argc int32, argv **byte)</a>

```
searchKey: runtime.sysargs
```

```Go
func sysargs(argc int32, argv **byte)
```

### <a id="signalM" href="#signalM">func signalM(mp *m, sig int)</a>

```
searchKey: runtime.signalM
```

```Go
func signalM(mp *m, sig int)
```

### <a id="osStackAlloc" href="#osStackAlloc">func osStackAlloc(s *mspan)</a>

```
searchKey: runtime.osStackAlloc
```

```Go
func osStackAlloc(s *mspan)
```

osStackAlloc performs OS-specific initialization before s is used as stack memory. 

### <a id="osStackFree" href="#osStackFree">func osStackFree(s *mspan)</a>

```
searchKey: runtime.osStackFree
```

```Go
func osStackFree(s *mspan)
```

osStackFree undoes the effect of osStackAlloc before s is returned to the heap. 

### <a id="panicCheck1" href="#panicCheck1">func panicCheck1(pc uintptr, msg string)</a>

```
searchKey: runtime.panicCheck1
```

```Go
func panicCheck1(pc uintptr, msg string)
```

Check to make sure we can really generate a panic. If the panic was generated from the runtime, or from inside malloc, then convert to a throw of msg. pc should be the program counter of the compiler-generated code that triggered this panic. 

### <a id="panicCheck2" href="#panicCheck2">func panicCheck2(err string)</a>

```
searchKey: runtime.panicCheck2
```

```Go
func panicCheck2(err string)
```

Same as above, but calling from the runtime is allowed. 

Using this function is necessary for any panic that may be generated by runtime.sigpanic, since those are always called by the runtime. 

### <a id="goPanicIndex" href="#goPanicIndex">func goPanicIndex(x int, y int)</a>

```
searchKey: runtime.goPanicIndex
```

```Go
func goPanicIndex(x int, y int)
```

failures in the comparisons for s[x], 0 <= x < y (y == len(s)) 

### <a id="goPanicIndexU" href="#goPanicIndexU">func goPanicIndexU(x uint, y int)</a>

```
searchKey: runtime.goPanicIndexU
```

```Go
func goPanicIndexU(x uint, y int)
```

### <a id="goPanicSliceAlen" href="#goPanicSliceAlen">func goPanicSliceAlen(x int, y int)</a>

```
searchKey: runtime.goPanicSliceAlen
```

```Go
func goPanicSliceAlen(x int, y int)
```

failures in the comparisons for s[:x], 0 <= x <= y (y == len(s) or cap(s)) 

### <a id="goPanicSliceAlenU" href="#goPanicSliceAlenU">func goPanicSliceAlenU(x uint, y int)</a>

```
searchKey: runtime.goPanicSliceAlenU
```

```Go
func goPanicSliceAlenU(x uint, y int)
```

### <a id="goPanicSliceAcap" href="#goPanicSliceAcap">func goPanicSliceAcap(x int, y int)</a>

```
searchKey: runtime.goPanicSliceAcap
```

```Go
func goPanicSliceAcap(x int, y int)
```

### <a id="goPanicSliceAcapU" href="#goPanicSliceAcapU">func goPanicSliceAcapU(x uint, y int)</a>

```
searchKey: runtime.goPanicSliceAcapU
```

```Go
func goPanicSliceAcapU(x uint, y int)
```

### <a id="goPanicSliceB" href="#goPanicSliceB">func goPanicSliceB(x int, y int)</a>

```
searchKey: runtime.goPanicSliceB
```

```Go
func goPanicSliceB(x int, y int)
```

failures in the comparisons for s[x:y], 0 <= x <= y 

### <a id="goPanicSliceBU" href="#goPanicSliceBU">func goPanicSliceBU(x uint, y int)</a>

```
searchKey: runtime.goPanicSliceBU
```

```Go
func goPanicSliceBU(x uint, y int)
```

### <a id="goPanicSlice3Alen" href="#goPanicSlice3Alen">func goPanicSlice3Alen(x int, y int)</a>

```
searchKey: runtime.goPanicSlice3Alen
```

```Go
func goPanicSlice3Alen(x int, y int)
```

failures in the comparisons for s[::x], 0 <= x <= y (y == len(s) or cap(s)) 

### <a id="goPanicSlice3AlenU" href="#goPanicSlice3AlenU">func goPanicSlice3AlenU(x uint, y int)</a>

```
searchKey: runtime.goPanicSlice3AlenU
```

```Go
func goPanicSlice3AlenU(x uint, y int)
```

### <a id="goPanicSlice3Acap" href="#goPanicSlice3Acap">func goPanicSlice3Acap(x int, y int)</a>

```
searchKey: runtime.goPanicSlice3Acap
```

```Go
func goPanicSlice3Acap(x int, y int)
```

### <a id="goPanicSlice3AcapU" href="#goPanicSlice3AcapU">func goPanicSlice3AcapU(x uint, y int)</a>

```
searchKey: runtime.goPanicSlice3AcapU
```

```Go
func goPanicSlice3AcapU(x uint, y int)
```

### <a id="goPanicSlice3B" href="#goPanicSlice3B">func goPanicSlice3B(x int, y int)</a>

```
searchKey: runtime.goPanicSlice3B
```

```Go
func goPanicSlice3B(x int, y int)
```

failures in the comparisons for s[:x:y], 0 <= x <= y 

### <a id="goPanicSlice3BU" href="#goPanicSlice3BU">func goPanicSlice3BU(x uint, y int)</a>

```
searchKey: runtime.goPanicSlice3BU
```

```Go
func goPanicSlice3BU(x uint, y int)
```

### <a id="goPanicSlice3C" href="#goPanicSlice3C">func goPanicSlice3C(x int, y int)</a>

```
searchKey: runtime.goPanicSlice3C
```

```Go
func goPanicSlice3C(x int, y int)
```

failures in the comparisons for s[x:y:], 0 <= x <= y 

### <a id="goPanicSlice3CU" href="#goPanicSlice3CU">func goPanicSlice3CU(x uint, y int)</a>

```
searchKey: runtime.goPanicSlice3CU
```

```Go
func goPanicSlice3CU(x uint, y int)
```

### <a id="goPanicSliceConvert" href="#goPanicSliceConvert">func goPanicSliceConvert(x int, y int)</a>

```
searchKey: runtime.goPanicSliceConvert
```

```Go
func goPanicSliceConvert(x int, y int)
```

failures in the conversion (*[x]T)s, 0 <= x <= y, x == cap(s) 

### <a id="panicIndex" href="#panicIndex">func panicIndex(x int, y int)</a>

```
searchKey: runtime.panicIndex
```

```Go
func panicIndex(x int, y int)
```

Implemented in assembly, as they take arguments in registers. Declared here to mark them as ABIInternal. 

### <a id="panicIndexU" href="#panicIndexU">func panicIndexU(x uint, y int)</a>

```
searchKey: runtime.panicIndexU
```

```Go
func panicIndexU(x uint, y int)
```

### <a id="panicSliceAlen" href="#panicSliceAlen">func panicSliceAlen(x int, y int)</a>

```
searchKey: runtime.panicSliceAlen
```

```Go
func panicSliceAlen(x int, y int)
```

### <a id="panicSliceAlenU" href="#panicSliceAlenU">func panicSliceAlenU(x uint, y int)</a>

```
searchKey: runtime.panicSliceAlenU
```

```Go
func panicSliceAlenU(x uint, y int)
```

### <a id="panicSliceAcap" href="#panicSliceAcap">func panicSliceAcap(x int, y int)</a>

```
searchKey: runtime.panicSliceAcap
```

```Go
func panicSliceAcap(x int, y int)
```

### <a id="panicSliceAcapU" href="#panicSliceAcapU">func panicSliceAcapU(x uint, y int)</a>

```
searchKey: runtime.panicSliceAcapU
```

```Go
func panicSliceAcapU(x uint, y int)
```

### <a id="panicSliceB" href="#panicSliceB">func panicSliceB(x int, y int)</a>

```
searchKey: runtime.panicSliceB
```

```Go
func panicSliceB(x int, y int)
```

### <a id="panicSliceBU" href="#panicSliceBU">func panicSliceBU(x uint, y int)</a>

```
searchKey: runtime.panicSliceBU
```

```Go
func panicSliceBU(x uint, y int)
```

### <a id="panicSlice3Alen" href="#panicSlice3Alen">func panicSlice3Alen(x int, y int)</a>

```
searchKey: runtime.panicSlice3Alen
```

```Go
func panicSlice3Alen(x int, y int)
```

### <a id="panicSlice3AlenU" href="#panicSlice3AlenU">func panicSlice3AlenU(x uint, y int)</a>

```
searchKey: runtime.panicSlice3AlenU
```

```Go
func panicSlice3AlenU(x uint, y int)
```

### <a id="panicSlice3Acap" href="#panicSlice3Acap">func panicSlice3Acap(x int, y int)</a>

```
searchKey: runtime.panicSlice3Acap
```

```Go
func panicSlice3Acap(x int, y int)
```

### <a id="panicSlice3AcapU" href="#panicSlice3AcapU">func panicSlice3AcapU(x uint, y int)</a>

```
searchKey: runtime.panicSlice3AcapU
```

```Go
func panicSlice3AcapU(x uint, y int)
```

### <a id="panicSlice3B" href="#panicSlice3B">func panicSlice3B(x int, y int)</a>

```
searchKey: runtime.panicSlice3B
```

```Go
func panicSlice3B(x int, y int)
```

### <a id="panicSlice3BU" href="#panicSlice3BU">func panicSlice3BU(x uint, y int)</a>

```
searchKey: runtime.panicSlice3BU
```

```Go
func panicSlice3BU(x uint, y int)
```

### <a id="panicSlice3C" href="#panicSlice3C">func panicSlice3C(x int, y int)</a>

```
searchKey: runtime.panicSlice3C
```

```Go
func panicSlice3C(x int, y int)
```

### <a id="panicSlice3CU" href="#panicSlice3CU">func panicSlice3CU(x uint, y int)</a>

```
searchKey: runtime.panicSlice3CU
```

```Go
func panicSlice3CU(x uint, y int)
```

### <a id="panicSliceConvert" href="#panicSliceConvert">func panicSliceConvert(x int, y int)</a>

```
searchKey: runtime.panicSliceConvert
```

```Go
func panicSliceConvert(x int, y int)
```

### <a id="panicshift" href="#panicshift">func panicshift()</a>

```
searchKey: runtime.panicshift
```

```Go
func panicshift()
```

### <a id="panicdivide" href="#panicdivide">func panicdivide()</a>

```
searchKey: runtime.panicdivide
```

```Go
func panicdivide()
```

### <a id="panicoverflow" href="#panicoverflow">func panicoverflow()</a>

```
searchKey: runtime.panicoverflow
```

```Go
func panicoverflow()
```

### <a id="panicfloat" href="#panicfloat">func panicfloat()</a>

```
searchKey: runtime.panicfloat
```

```Go
func panicfloat()
```

### <a id="panicmem" href="#panicmem">func panicmem()</a>

```
searchKey: runtime.panicmem
```

```Go
func panicmem()
```

### <a id="panicmemAddr" href="#panicmemAddr">func panicmemAddr(addr uintptr)</a>

```
searchKey: runtime.panicmemAddr
```

```Go
func panicmemAddr(addr uintptr)
```

### <a id="deferproc" href="#deferproc">func deferproc(siz int32, fn *funcval)</a>

```
searchKey: runtime.deferproc
```

```Go
func deferproc(siz int32, fn *funcval)
```

Create a new deferred function fn with siz bytes of arguments. The compiler turns a defer statement into a call to this. 

### <a id="deferprocStack" href="#deferprocStack">func deferprocStack(d *_defer)</a>

```
searchKey: runtime.deferprocStack
```

```Go
func deferprocStack(d *_defer)
```

deferprocStack queues a new deferred function with a defer record on the stack. The defer record must have its siz and fn fields initialized. All other fields can contain junk. The defer record must be immediately followed in memory by the arguments of the defer. Nosplit because the arguments on the stack won't be scanned until the defer record is spliced into the gp._defer list. 

### <a id="deferclass" href="#deferclass">func deferclass(siz uintptr) uintptr</a>

```
searchKey: runtime.deferclass
```

```Go
func deferclass(siz uintptr) uintptr
```

defer size class for arg size sz 

### <a id="totaldefersize" href="#totaldefersize">func totaldefersize(siz uintptr) uintptr</a>

```
searchKey: runtime.totaldefersize
```

```Go
func totaldefersize(siz uintptr) uintptr
```

total size of memory block for defer with arg size sz 

### <a id="testdefersizes" href="#testdefersizes">func testdefersizes()</a>

```
searchKey: runtime.testdefersizes
```

```Go
func testdefersizes()
```

Ensure that defer arg sizes that map to the same defer size class also map to the same malloc size class. 

### <a id="deferArgs" href="#deferArgs">func deferArgs(d *_defer) unsafe.Pointer</a>

```
searchKey: runtime.deferArgs
```

```Go
func deferArgs(d *_defer) unsafe.Pointer
```

The arguments associated with a deferred call are stored immediately after the _defer header in memory. 

### <a id="deferFunc" href="#deferFunc">func deferFunc(d *_defer) func()</a>

```
searchKey: runtime.deferFunc
```

```Go
func deferFunc(d *_defer) func()
```

deferFunc returns d's deferred function. This is temporary while we support both modes of GOEXPERIMENT=regabidefer. Once we commit to that experiment, we should change the type of d.fn. 

### <a id="init" href="#init">func init()</a>

```
searchKey: runtime.init
```

```Go
func init()
```

### <a id="freedefer" href="#freedefer">func freedefer(d *_defer)</a>

```
searchKey: runtime.freedefer
```

```Go
func freedefer(d *_defer)
```

Free the given defer. The defer cannot be used after this call. 

This must not grow the stack because there may be a frame without a stack map when this is called. 

### <a id="freedeferpanic" href="#freedeferpanic">func freedeferpanic()</a>

```
searchKey: runtime.freedeferpanic
```

```Go
func freedeferpanic()
```

Separate function so that it can split stack. Windows otherwise runs out of stack space. 

### <a id="freedeferfn" href="#freedeferfn">func freedeferfn()</a>

```
searchKey: runtime.freedeferfn
```

```Go
func freedeferfn()
```

### <a id="deferreturn" href="#deferreturn">func deferreturn()</a>

```
searchKey: runtime.deferreturn
```

```Go
func deferreturn()
```

Run a deferred function if there is one. The compiler inserts a call to this at the end of any function which calls defer. If there is a deferred function, this will call runtime·jmpdefer, which will jump to the deferred function such that it appears to have been called by the caller of deferreturn at the point just before deferreturn was called. The effect is that deferreturn is called again and again until there are no more deferred functions. 

Declared as nosplit, because the function should not be preempted once we start modifying the caller's frame in order to reuse the frame to call the deferred function. 

### <a id="Goexit" href="#Goexit">func Goexit()</a>

```
searchKey: runtime.Goexit
tags: [exported]
```

```Go
func Goexit()
```

Goexit terminates the goroutine that calls it. No other goroutine is affected. Goexit runs all deferred calls before terminating the goroutine. Because Goexit is not a panic, any recover calls in those deferred functions will return nil. 

Calling Goexit from the main goroutine terminates that goroutine without func main returning. Since func main has not returned, the program continues execution of other goroutines. If all other goroutines exit, the program crashes. 

### <a id="preprintpanics" href="#preprintpanics">func preprintpanics(p *_panic)</a>

```
searchKey: runtime.preprintpanics
```

```Go
func preprintpanics(p *_panic)
```

Call all Error and String methods before freezing the world. Used when crashing with panicking. 

### <a id="printpanics" href="#printpanics">func printpanics(p *_panic)</a>

```
searchKey: runtime.printpanics
```

```Go
func printpanics(p *_panic)
```

Print all currently active panics. Used when crashing. Should only be called after preprintpanics. 

### <a id="addOneOpenDeferFrame" href="#addOneOpenDeferFrame">func addOneOpenDeferFrame(gp *g, pc uintptr, sp unsafe.Pointer)</a>

```
searchKey: runtime.addOneOpenDeferFrame
```

```Go
func addOneOpenDeferFrame(gp *g, pc uintptr, sp unsafe.Pointer)
```

addOneOpenDeferFrame scans the stack for the first frame (if any) with open-coded defers and if it finds one, adds a single record to the defer chain for that frame. If sp is non-nil, it starts the stack scan from the frame specified by sp. If sp is nil, it uses the sp from the current defer record (which has just been finished). Hence, it continues the stack scan from the frame of the defer that just finished. It skips any frame that already has an open-coded _defer record, which would have been created from a previous (unrecovered) panic. 

Note: All entries of the defer chain (including this new open-coded entry) have their pointers (including sp) adjusted properly if the stack moves while running deferred functions. Also, it is safe to pass in the sp arg (which is the direct result of calling getcallersp()), because all pointer variables (including arguments) are adjusted as needed during stack copies. 

### <a id="readvarintUnsafe" href="#readvarintUnsafe">func readvarintUnsafe(fd unsafe.Pointer) (uint32, unsafe.Pointer)</a>

```
searchKey: runtime.readvarintUnsafe
```

```Go
func readvarintUnsafe(fd unsafe.Pointer) (uint32, unsafe.Pointer)
```

readvarintUnsafe reads the uint32 in varint format starting at fd, and returns the uint32 and a pointer to the byte following the varint. 

There is a similar function runtime.readvarint, which takes a slice of bytes, rather than an unsafe pointer. These functions are duplicated, because one of the two use cases for the functions would get slower if the functions were combined. 

### <a id="runOpenDeferFrame" href="#runOpenDeferFrame">func runOpenDeferFrame(gp *g, d *_defer) bool</a>

```
searchKey: runtime.runOpenDeferFrame
```

```Go
func runOpenDeferFrame(gp *g, d *_defer) bool
```

runOpenDeferFrame runs the active open-coded defers in the frame specified by d. It normally processes all active defers in the frame, but stops immediately if a defer does a successful recover. It returns true if there are no remaining defers to run in the frame. 

### <a id="reflectcallSave" href="#reflectcallSave">func reflectcallSave(p *_panic, fn, arg unsafe.Pointer, argsize uint32)</a>

```
searchKey: runtime.reflectcallSave
```

```Go
func reflectcallSave(p *_panic, fn, arg unsafe.Pointer, argsize uint32)
```

reflectcallSave calls reflectcall after saving the caller's pc and sp in the panic record. This allows the runtime to return to the Goexit defer processing loop, in the unusual case where the Goexit may be bypassed by a successful recover. 

This is marked as a wrapper by the compiler so it doesn't appear in tracebacks. 

### <a id="deferCallSave" href="#deferCallSave">func deferCallSave(p *_panic, fn func())</a>

```
searchKey: runtime.deferCallSave
```

```Go
func deferCallSave(p *_panic, fn func())
```

deferCallSave calls fn() after saving the caller's pc and sp in the panic record. This allows the runtime to return to the Goexit defer processing loop, in the unusual case where the Goexit may be bypassed by a successful recover. 

This is marked as a wrapper by the compiler so it doesn't appear in tracebacks. 

### <a id="gopanic" href="#gopanic">func gopanic(e interface{})</a>

```
searchKey: runtime.gopanic
```

```Go
func gopanic(e interface{})
```

The implementation of the predeclared function panic. 

### <a id="getargp" href="#getargp">func getargp() uintptr</a>

```
searchKey: runtime.getargp
```

```Go
func getargp() uintptr
```

getargp returns the location where the caller writes outgoing function call arguments. 

### <a id="gorecover" href="#gorecover">func gorecover(argp uintptr) interface{}</a>

```
searchKey: runtime.gorecover
```

```Go
func gorecover(argp uintptr) interface{}
```

The implementation of the predeclared function recover. Cannot split the stack because it needs to reliably find the stack segment of its caller. 

TODO(rsc): Once we commit to CopyStackAlways, this doesn't need to be nosplit. 

### <a id="sync_throw" href="#sync_throw">func sync_throw(s string)</a>

```
searchKey: runtime.sync_throw
```

```Go
func sync_throw(s string)
```

### <a id="throw" href="#throw">func throw(s string)</a>

```
searchKey: runtime.throw
```

```Go
func throw(s string)
```

### <a id="recovery" href="#recovery">func recovery(gp *g)</a>

```
searchKey: runtime.recovery
```

```Go
func recovery(gp *g)
```

Unwind the stack after a deferred function calls recover after a panic. Then arrange to continue running as though the caller of the deferred function returned normally. 

### <a id="fatalthrow" href="#fatalthrow">func fatalthrow()</a>

```
searchKey: runtime.fatalthrow
```

```Go
func fatalthrow()
```

fatalthrow implements an unrecoverable runtime throw. It freezes the system, prints stack traces starting from its caller, and terminates the process. 

### <a id="fatalpanic" href="#fatalpanic">func fatalpanic(msgs *_panic)</a>

```
searchKey: runtime.fatalpanic
```

```Go
func fatalpanic(msgs *_panic)
```

fatalpanic implements an unrecoverable panic. It is like fatalthrow, except that if msgs != nil, fatalpanic also prints panic messages and decrements runningPanicDefers once main is blocked from exiting. 

### <a id="startpanic_m" href="#startpanic_m">func startpanic_m() bool</a>

```
searchKey: runtime.startpanic_m
```

```Go
func startpanic_m() bool
```

startpanic_m prepares for an unrecoverable panic. 

It returns true if panic messages should be printed, or false if the runtime is in bad shape and should just print stacks. 

It must not have write barriers even though the write barrier explicitly ignores writes once dying > 0. Write barriers still assume that g.m.p != nil, and this function may not have P in some contexts (e.g. a panic in a signal handler for a signal sent to an M with no P). 

### <a id="dopanic_m" href="#dopanic_m">func dopanic_m(gp *g, pc, sp uintptr) bool</a>

```
searchKey: runtime.dopanic_m
```

```Go
func dopanic_m(gp *g, pc, sp uintptr) bool
```

### <a id="canpanic" href="#canpanic">func canpanic(gp *g) bool</a>

```
searchKey: runtime.canpanic
```

```Go
func canpanic(gp *g) bool
```

canpanic returns false if a signal should throw instead of panicking. 

### <a id="shouldPushSigpanic" href="#shouldPushSigpanic">func shouldPushSigpanic(gp *g, pc, lr uintptr) bool</a>

```
searchKey: runtime.shouldPushSigpanic
```

```Go
func shouldPushSigpanic(gp *g, pc, lr uintptr) bool
```

shouldPushSigpanic reports whether pc should be used as sigpanic's return PC (pushing a frame for the call). Otherwise, it should be left alone so that LR is used as sigpanic's return PC, effectively replacing the top-most frame with sigpanic. This is used by preparePanic. 

### <a id="isAbortPC" href="#isAbortPC">func isAbortPC(pc uintptr) bool</a>

```
searchKey: runtime.isAbortPC
```

```Go
func isAbortPC(pc uintptr) bool
```

isAbortPC reports whether pc is the program counter at which runtime.abort raises a signal. 

It is nosplit because it's part of the isgoexception implementation. 

### <a id="plugin_lastmoduleinit" href="#plugin_lastmoduleinit">func plugin_lastmoduleinit() (path string, syms map[string]interface{}, errstr string)</a>

```
searchKey: runtime.plugin_lastmoduleinit
```

```Go
func plugin_lastmoduleinit() (path string, syms map[string]interface{}, errstr string)
```

### <a id="pluginftabverify" href="#pluginftabverify">func pluginftabverify(md *moduledata)</a>

```
searchKey: runtime.pluginftabverify
```

```Go
func pluginftabverify(md *moduledata)
```

### <a id="inRange" href="#inRange">func inRange(r0, r1, v0, v1 uintptr) bool</a>

```
searchKey: runtime.inRange
```

```Go
func inRange(r0, r1, v0, v1 uintptr) bool
```

inRange reports whether v0 or v1 are in the range [r0, r1]. 

### <a id="resumeG" href="#resumeG">func resumeG(state suspendGState)</a>

```
searchKey: runtime.resumeG
```

```Go
func resumeG(state suspendGState)
```

resumeG undoes the effects of suspendG, allowing the suspended goroutine to continue from its current safe-point. 

### <a id="canPreemptM" href="#canPreemptM">func canPreemptM(mp *m) bool</a>

```
searchKey: runtime.canPreemptM
```

```Go
func canPreemptM(mp *m) bool
```

canPreemptM reports whether mp is in a state that is safe to preempt. 

It is nosplit because it has nosplit callers. 

### <a id="asyncPreempt" href="#asyncPreempt">func asyncPreempt()</a>

```
searchKey: runtime.asyncPreempt
```

```Go
func asyncPreempt()
```

asyncPreempt saves all user registers and calls asyncPreempt2. 

When stack scanning encounters an asyncPreempt frame, it scans that frame and its parent frame conservatively. 

asyncPreempt is implemented in assembly. 

### <a id="asyncPreempt2" href="#asyncPreempt2">func asyncPreempt2()</a>

```
searchKey: runtime.asyncPreempt2
```

```Go
func asyncPreempt2()
```

### <a id="init" href="#init">func init()</a>

```
searchKey: runtime.init
```

```Go
func init()
```

### <a id="wantAsyncPreempt" href="#wantAsyncPreempt">func wantAsyncPreempt(gp *g) bool</a>

```
searchKey: runtime.wantAsyncPreempt
```

```Go
func wantAsyncPreempt(gp *g) bool
```

wantAsyncPreempt returns whether an asynchronous preemption is queued for gp. 

### <a id="isAsyncSafePoint" href="#isAsyncSafePoint">func isAsyncSafePoint(gp *g, pc, sp, lr uintptr) (bool, uintptr)</a>

```
searchKey: runtime.isAsyncSafePoint
```

```Go
func isAsyncSafePoint(gp *g, pc, sp, lr uintptr) (bool, uintptr)
```

isAsyncSafePoint reports whether gp at instruction PC is an asynchronous safe point. This indicates that: 

1. It's safe to suspend gp and conservatively scan its stack and registers. There are no potentially hidden pointer values and it's not in the middle of an atomic sequence like a write barrier. 

2. gp has enough stack space to inject the asyncPreempt call. 

3. It's generally safe to interact with the runtime, even if we're in a signal handler stopped here. For example, there are no runtime locks held, so acquiring a runtime lock won't self-deadlock. 

In some cases the PC is safe for asynchronous preemption but it also needs to adjust the resumption PC. The new PC is returned in the second result. 

### <a id="osPreemptExtEnter" href="#osPreemptExtEnter">func osPreemptExtEnter(mp *m)</a>

```
searchKey: runtime.osPreemptExtEnter
```

```Go
func osPreemptExtEnter(mp *m)
```

### <a id="osPreemptExtExit" href="#osPreemptExtExit">func osPreemptExtExit(mp *m)</a>

```
searchKey: runtime.osPreemptExtExit
```

```Go
func osPreemptExtExit(mp *m)
```

### <a id="bytes" href="#bytes">func bytes(s string) (ret []byte)</a>

```
searchKey: runtime.bytes
```

```Go
func bytes(s string) (ret []byte)
```

### <a id="recordForPanic" href="#recordForPanic">func recordForPanic(b []byte)</a>

```
searchKey: runtime.recordForPanic
```

```Go
func recordForPanic(b []byte)
```

recordForPanic maintains a circular buffer of messages written by the runtime leading up to a process crash, allowing the messages to be extracted from a core dump. 

The text written during a process crash (following "panic" or "fatal error") is not saved, since the goroutine stacks will generally be readable from the runtime datastructures in the core file. 

### <a id="printlock" href="#printlock">func printlock()</a>

```
searchKey: runtime.printlock
```

```Go
func printlock()
```

### <a id="printunlock" href="#printunlock">func printunlock()</a>

```
searchKey: runtime.printunlock
```

```Go
func printunlock()
```

### <a id="gwrite" href="#gwrite">func gwrite(b []byte)</a>

```
searchKey: runtime.gwrite
```

```Go
func gwrite(b []byte)
```

write to goroutine-local buffer if diverting output, or else standard error. 

### <a id="printsp" href="#printsp">func printsp()</a>

```
searchKey: runtime.printsp
```

```Go
func printsp()
```

### <a id="printnl" href="#printnl">func printnl()</a>

```
searchKey: runtime.printnl
```

```Go
func printnl()
```

### <a id="printbool" href="#printbool">func printbool(v bool)</a>

```
searchKey: runtime.printbool
```

```Go
func printbool(v bool)
```

### <a id="printfloat" href="#printfloat">func printfloat(v float64)</a>

```
searchKey: runtime.printfloat
```

```Go
func printfloat(v float64)
```

### <a id="printcomplex" href="#printcomplex">func printcomplex(c complex128)</a>

```
searchKey: runtime.printcomplex
```

```Go
func printcomplex(c complex128)
```

### <a id="printuint" href="#printuint">func printuint(v uint64)</a>

```
searchKey: runtime.printuint
```

```Go
func printuint(v uint64)
```

### <a id="printint" href="#printint">func printint(v int64)</a>

```
searchKey: runtime.printint
```

```Go
func printint(v int64)
```

### <a id="printhex" href="#printhex">func printhex(v uint64)</a>

```
searchKey: runtime.printhex
```

```Go
func printhex(v uint64)
```

### <a id="printpointer" href="#printpointer">func printpointer(p unsafe.Pointer)</a>

```
searchKey: runtime.printpointer
```

```Go
func printpointer(p unsafe.Pointer)
```

### <a id="printuintptr" href="#printuintptr">func printuintptr(p uintptr)</a>

```
searchKey: runtime.printuintptr
```

```Go
func printuintptr(p uintptr)
```

### <a id="printstring" href="#printstring">func printstring(s string)</a>

```
searchKey: runtime.printstring
```

```Go
func printstring(s string)
```

### <a id="printslice" href="#printslice">func printslice(s []byte)</a>

```
searchKey: runtime.printslice
```

```Go
func printslice(s []byte)
```

### <a id="printeface" href="#printeface">func printeface(e eface)</a>

```
searchKey: runtime.printeface
```

```Go
func printeface(e eface)
```

### <a id="printiface" href="#printiface">func printiface(i iface)</a>

```
searchKey: runtime.printiface
```

```Go
func printiface(i iface)
```

### <a id="hexdumpWords" href="#hexdumpWords">func hexdumpWords(p, end uintptr, mark func(uintptr) byte)</a>

```
searchKey: runtime.hexdumpWords
```

```Go
func hexdumpWords(p, end uintptr, mark func(uintptr) byte)
```

hexdumpWords prints a word-oriented hex dump of [p, end). 

If mark != nil, it will be called with each printed word's address and should return a character mark to appear just before that word's value. It can return 0 to indicate no mark. 

### <a id="main_main" href="#main_main">func main_main()</a>

```
searchKey: runtime.main_main
```

```Go
func main_main()
```

### <a id="main" href="#main">func main()</a>

```
searchKey: runtime.main
```

```Go
func main()
```

The main goroutine. 

### <a id="os_beforeExit" href="#os_beforeExit">func os_beforeExit()</a>

```
searchKey: runtime.os_beforeExit
```

```Go
func os_beforeExit()
```

os_beforeExit is called from os.Exit(0). 

### <a id="init" href="#init">func init()</a>

```
searchKey: runtime.init
```

```Go
func init()
```

start forcegc helper goroutine 

### <a id="forcegchelper" href="#forcegchelper">func forcegchelper()</a>

```
searchKey: runtime.forcegchelper
```

```Go
func forcegchelper()
```

### <a id="Gosched" href="#Gosched">func Gosched()</a>

```
searchKey: runtime.Gosched
tags: [exported]
```

```Go
func Gosched()
```

Gosched yields the processor, allowing other goroutines to run. It does not suspend the current goroutine, so execution resumes automatically. 

### <a id="goschedguarded" href="#goschedguarded">func goschedguarded()</a>

```
searchKey: runtime.goschedguarded
```

```Go
func goschedguarded()
```

goschedguarded yields the processor like gosched, but also checks for forbidden states and opts out of the yield in those cases. 

### <a id="gopark" href="#gopark">func gopark(unlockf func(*g, unsafe.Pointer) bool, lock unsafe.Pointer, reason waitReason, traceEv byte, traceskip int)</a>

```
searchKey: runtime.gopark
```

```Go
func gopark(unlockf func(*g, unsafe.Pointer) bool, lock unsafe.Pointer, reason waitReason, traceEv byte, traceskip int)
```

Puts the current goroutine into a waiting state and calls unlockf on the system stack. 

If unlockf returns false, the goroutine is resumed. 

unlockf must not access this G's stack, as it may be moved between the call to gopark and the call to unlockf. 

Note that because unlockf is called after putting the G into a waiting state, the G may have already been readied by the time unlockf is called unless there is external synchronization preventing the G from being readied. If unlockf returns false, it must guarantee that the G cannot be externally readied. 

Reason explains why the goroutine has been parked. It is displayed in stack traces and heap dumps. Reasons should be unique and descriptive. Do not re-use reasons, add new ones. 

### <a id="goparkunlock" href="#goparkunlock">func goparkunlock(lock *mutex, reason waitReason, traceEv byte, traceskip int)</a>

```
searchKey: runtime.goparkunlock
```

```Go
func goparkunlock(lock *mutex, reason waitReason, traceEv byte, traceskip int)
```

Puts the current goroutine into a waiting state and unlocks the lock. The goroutine can be made runnable again by calling goready(gp). 

### <a id="goready" href="#goready">func goready(gp *g, traceskip int)</a>

```
searchKey: runtime.goready
```

```Go
func goready(gp *g, traceskip int)
```

### <a id="releaseSudog" href="#releaseSudog">func releaseSudog(s *sudog)</a>

```
searchKey: runtime.releaseSudog
```

```Go
func releaseSudog(s *sudog)
```

### <a id="funcPC" href="#funcPC">func funcPC(f interface{}) uintptr</a>

```
searchKey: runtime.funcPC
```

```Go
func funcPC(f interface{}) uintptr
```

funcPC returns the entry PC of the function f. It assumes that f is a func value. Otherwise the behavior is undefined. CAREFUL: In programs with plugins, funcPC can return different values for the same function (because there are actually multiple copies of the same function in the address space). To be safe, don't use the results of this function in any == expression. It is only safe to use the result as an address at which to start executing code. 

### <a id="badmcall" href="#badmcall">func badmcall(fn func(*g))</a>

```
searchKey: runtime.badmcall
```

```Go
func badmcall(fn func(*g))
```

called from assembly 

### <a id="badmcall2" href="#badmcall2">func badmcall2(fn func(*g))</a>

```
searchKey: runtime.badmcall2
```

```Go
func badmcall2(fn func(*g))
```

### <a id="badreflectcall" href="#badreflectcall">func badreflectcall()</a>

```
searchKey: runtime.badreflectcall
```

```Go
func badreflectcall()
```

### <a id="badmorestackg0" href="#badmorestackg0">func badmorestackg0()</a>

```
searchKey: runtime.badmorestackg0
```

```Go
func badmorestackg0()
```

### <a id="badmorestackgsignal" href="#badmorestackgsignal">func badmorestackgsignal()</a>

```
searchKey: runtime.badmorestackgsignal
```

```Go
func badmorestackgsignal()
```

### <a id="badctxt" href="#badctxt">func badctxt()</a>

```
searchKey: runtime.badctxt
```

```Go
func badctxt()
```

### <a id="lockedOSThread" href="#lockedOSThread">func lockedOSThread() bool</a>

```
searchKey: runtime.lockedOSThread
```

```Go
func lockedOSThread() bool
```

### <a id="allgadd" href="#allgadd">func allgadd(gp *g)</a>

```
searchKey: runtime.allgadd
```

```Go
func allgadd(gp *g)
```

### <a id="forEachG" href="#forEachG">func forEachG(fn func(gp *g))</a>

```
searchKey: runtime.forEachG
```

```Go
func forEachG(fn func(gp *g))
```

forEachG calls fn on every G from allgs. 

forEachG takes a lock to exclude concurrent addition of new Gs. 

### <a id="forEachGRace" href="#forEachGRace">func forEachGRace(fn func(gp *g))</a>

```
searchKey: runtime.forEachGRace
```

```Go
func forEachGRace(fn func(gp *g))
```

forEachGRace calls fn on every G from allgs. 

forEachGRace avoids locking, but does not exclude addition of new Gs during execution, which may be missed. 

### <a id="cpuinit" href="#cpuinit">func cpuinit()</a>

```
searchKey: runtime.cpuinit
```

```Go
func cpuinit()
```

cpuinit extracts the environment variable GODEBUG from the environment on Unix-like operating systems and calls internal/cpu.Initialize. 

### <a id="schedinit" href="#schedinit">func schedinit()</a>

```
searchKey: runtime.schedinit
```

```Go
func schedinit()
```

The bootstrap sequence is: 

```
call osinit
call schedinit
make & queue new G
call runtime·mstart

```
The new G calls runtime·main. 

### <a id="dumpgstatus" href="#dumpgstatus">func dumpgstatus(gp *g)</a>

```
searchKey: runtime.dumpgstatus
```

```Go
func dumpgstatus(gp *g)
```

### <a id="checkmcount" href="#checkmcount">func checkmcount()</a>

```
searchKey: runtime.checkmcount
```

```Go
func checkmcount()
```

sched.lock must be held. 

### <a id="mReserveID" href="#mReserveID">func mReserveID() int64</a>

```
searchKey: runtime.mReserveID
```

```Go
func mReserveID() int64
```

mReserveID returns the next ID to use for a new m. This new m is immediately considered 'running' by checkdead. 

sched.lock must be held. 

### <a id="mcommoninit" href="#mcommoninit">func mcommoninit(mp *m, id int64)</a>

```
searchKey: runtime.mcommoninit
```

```Go
func mcommoninit(mp *m, id int64)
```

Pre-allocated ID may be passed as 'id', or omitted by passing -1. 

### <a id="fastrandinit" href="#fastrandinit">func fastrandinit()</a>

```
searchKey: runtime.fastrandinit
```

```Go
func fastrandinit()
```

### <a id="ready" href="#ready">func ready(gp *g, traceskip int, next bool)</a>

```
searchKey: runtime.ready
```

```Go
func ready(gp *g, traceskip int, next bool)
```

Mark gp ready to run. 

### <a id="freezetheworld" href="#freezetheworld">func freezetheworld()</a>

```
searchKey: runtime.freezetheworld
```

```Go
func freezetheworld()
```

Similar to stopTheWorld but best-effort and can be called several times. There is no reverse operation, used during crashing. This function must not lock any mutexes. 

### <a id="readgstatus" href="#readgstatus">func readgstatus(gp *g) uint32</a>

```
searchKey: runtime.readgstatus
```

```Go
func readgstatus(gp *g) uint32
```

All reads and writes of g's status go through readgstatus, casgstatus castogscanstatus, casfrom_Gscanstatus. 

### <a id="casfrom_Gscanstatus" href="#casfrom_Gscanstatus">func casfrom_Gscanstatus(gp *g, oldval, newval uint32)</a>

```
searchKey: runtime.casfrom_Gscanstatus
```

```Go
func casfrom_Gscanstatus(gp *g, oldval, newval uint32)
```

The Gscanstatuses are acting like locks and this releases them. If it proves to be a performance hit we should be able to make these simple atomic stores but for now we are going to throw if we see an inconsistent state. 

### <a id="castogscanstatus" href="#castogscanstatus">func castogscanstatus(gp *g, oldval, newval uint32) bool</a>

```
searchKey: runtime.castogscanstatus
```

```Go
func castogscanstatus(gp *g, oldval, newval uint32) bool
```

This will return false if the gp is not in the expected status and the cas fails. This acts like a lock acquire while the casfromgstatus acts like a lock release. 

### <a id="casgstatus" href="#casgstatus">func casgstatus(gp *g, oldval, newval uint32)</a>

```
searchKey: runtime.casgstatus
```

```Go
func casgstatus(gp *g, oldval, newval uint32)
```

If asked to move to or from a Gscanstatus this will throw. Use the castogscanstatus and casfrom_Gscanstatus instead. casgstatus will loop if the g->atomicstatus is in a Gscan status until the routine that put it in the Gscan state is finished. 

### <a id="casgcopystack" href="#casgcopystack">func casgcopystack(gp *g) uint32</a>

```
searchKey: runtime.casgcopystack
```

```Go
func casgcopystack(gp *g) uint32
```

casgstatus(gp, oldstatus, Gcopystack), assuming oldstatus is Gwaiting or Grunnable. Returns old status. Cannot call casgstatus directly, because we are racing with an async wakeup that might come in from netpoll. If we see Gwaiting from the readgstatus, it might have become Grunnable by the time we get to the cas. If we called casgstatus, it would loop waiting for the status to go back to Gwaiting, which it never will. 

### <a id="casGToPreemptScan" href="#casGToPreemptScan">func casGToPreemptScan(gp *g, old, new uint32)</a>

```
searchKey: runtime.casGToPreemptScan
```

```Go
func casGToPreemptScan(gp *g, old, new uint32)
```

casGToPreemptScan transitions gp from _Grunning to _Gscan|_Gpreempted. 

TODO(austin): This is the only status operation that both changes the status and locks the _Gscan bit. Rethink this. 

### <a id="casGFromPreempted" href="#casGFromPreempted">func casGFromPreempted(gp *g, old, new uint32) bool</a>

```
searchKey: runtime.casGFromPreempted
```

```Go
func casGFromPreempted(gp *g, old, new uint32) bool
```

casGFromPreempted attempts to transition gp from _Gpreempted to _Gwaiting. If successful, the caller is responsible for re-scheduling gp. 

### <a id="stopTheWorld" href="#stopTheWorld">func stopTheWorld(reason string)</a>

```
searchKey: runtime.stopTheWorld
```

```Go
func stopTheWorld(reason string)
```

stopTheWorld stops all P's from executing goroutines, interrupting all goroutines at GC safe points and records reason as the reason for the stop. On return, only the current goroutine's P is running. stopTheWorld must not be called from a system stack and the caller must not hold worldsema. The caller must call startTheWorld when other P's should resume execution. 

stopTheWorld is safe for multiple goroutines to call at the same time. Each will execute its own stop, and the stops will be serialized. 

This is also used by routines that do stack dumps. If the system is in panic or being exited, this may not reliably stop all goroutines. 

### <a id="startTheWorld" href="#startTheWorld">func startTheWorld()</a>

```
searchKey: runtime.startTheWorld
```

```Go
func startTheWorld()
```

startTheWorld undoes the effects of stopTheWorld. 

### <a id="stopTheWorldGC" href="#stopTheWorldGC">func stopTheWorldGC(reason string)</a>

```
searchKey: runtime.stopTheWorldGC
```

```Go
func stopTheWorldGC(reason string)
```

stopTheWorldGC has the same effect as stopTheWorld, but blocks until the GC is not running. It also blocks a GC from starting until startTheWorldGC is called. 

### <a id="startTheWorldGC" href="#startTheWorldGC">func startTheWorldGC()</a>

```
searchKey: runtime.startTheWorldGC
```

```Go
func startTheWorldGC()
```

startTheWorldGC undoes the effects of stopTheWorldGC. 

### <a id="stopTheWorldWithSema" href="#stopTheWorldWithSema">func stopTheWorldWithSema()</a>

```
searchKey: runtime.stopTheWorldWithSema
```

```Go
func stopTheWorldWithSema()
```

stopTheWorldWithSema is the core implementation of stopTheWorld. The caller is responsible for acquiring worldsema and disabling preemption first and then should stopTheWorldWithSema on the system stack: 

```
semacquire(&worldsema, 0)
m.preemptoff = "reason"
systemstack(stopTheWorldWithSema)

```
When finished, the caller must either call startTheWorld or undo these three operations separately: 

```
m.preemptoff = ""
systemstack(startTheWorldWithSema)
semrelease(&worldsema)

```
It is allowed to acquire worldsema once and then execute multiple startTheWorldWithSema/stopTheWorldWithSema pairs. Other P's are able to execute between successive calls to startTheWorldWithSema and stopTheWorldWithSema. Holding worldsema causes any other goroutines invoking stopTheWorld to block. 

### <a id="startTheWorldWithSema" href="#startTheWorldWithSema">func startTheWorldWithSema(emitTraceEvent bool) int64</a>

```
searchKey: runtime.startTheWorldWithSema
```

```Go
func startTheWorldWithSema(emitTraceEvent bool) int64
```

### <a id="usesLibcall" href="#usesLibcall">func usesLibcall() bool</a>

```
searchKey: runtime.usesLibcall
```

```Go
func usesLibcall() bool
```

usesLibcall indicates whether this runtime performs system calls via libcall. 

### <a id="mStackIsSystemAllocated" href="#mStackIsSystemAllocated">func mStackIsSystemAllocated() bool</a>

```
searchKey: runtime.mStackIsSystemAllocated
```

```Go
func mStackIsSystemAllocated() bool
```

mStackIsSystemAllocated indicates whether this runtime starts on a system-allocated stack. 

### <a id="mstart" href="#mstart">func mstart()</a>

```
searchKey: runtime.mstart
```

```Go
func mstart()
```

mstart is the entry-point for new Ms. It is written in assembly, uses ABI0, is marked TOPFRAME, and calls mstart0. 

### <a id="mstart0" href="#mstart0">func mstart0()</a>

```
searchKey: runtime.mstart0
```

```Go
func mstart0()
```

mstart0 is the Go entry-point for new Ms. This must not split the stack because we may not even have stack bounds set up yet. 

May run during STW (because it doesn't have a P yet), so write barriers are not allowed. 

### <a id="mstart1" href="#mstart1">func mstart1()</a>

```
searchKey: runtime.mstart1
```

```Go
func mstart1()
```

The go:noinline is to guarantee the getcallerpc/getcallersp below are safe, so that we can set up g0.sched to return to the call of mstart1 above. 

### <a id="mstartm0" href="#mstartm0">func mstartm0()</a>

```
searchKey: runtime.mstartm0
```

```Go
func mstartm0()
```

mstartm0 implements part of mstart1 that only runs on the m0. 

Write barriers are allowed here because we know the GC can't be running yet, so they'll be no-ops. 

### <a id="mPark" href="#mPark">func mPark()</a>

```
searchKey: runtime.mPark
```

```Go
func mPark()
```

mPark causes a thread to park itself - temporarily waking for fixups but otherwise waiting to be fully woken. This is the only way that m's should park themselves. 

### <a id="mexit" href="#mexit">func mexit(osStack bool)</a>

```
searchKey: runtime.mexit
```

```Go
func mexit(osStack bool)
```

mexit tears down and exits the current thread. 

Don't call this directly to exit the thread, since it must run at the top of the thread stack. Instead, use gogo(&_g_.m.g0.sched) to unwind the stack to the point that exits the thread. 

It is entered with m.p != nil, so write barriers are allowed. It will release the P before exiting. 

### <a id="forEachP" href="#forEachP">func forEachP(fn func(*p))</a>

```
searchKey: runtime.forEachP
```

```Go
func forEachP(fn func(*p))
```

forEachP calls fn(p) for every P p when p reaches a GC safe point. If a P is currently executing code, this will bring the P to a GC safe point and execute fn on that P. If the P is not executing code (it is idle or in a syscall), this will call fn(p) directly while preventing the P from exiting its state. This does not ensure that fn will run on every CPU executing Go code, but it acts as a global memory barrier. GC uses this as a "ragged barrier." 

The caller must hold worldsema. 

### <a id="syscall_runtime_doAllThreadsSyscall" href="#syscall_runtime_doAllThreadsSyscall">func syscall_runtime_doAllThreadsSyscall(fn func(bool) bool)</a>

```
searchKey: runtime.syscall_runtime_doAllThreadsSyscall
```

```Go
func syscall_runtime_doAllThreadsSyscall(fn func(bool) bool)
```

syscall_runtime_doAllThreadsSyscall serializes Go execution and executes a specified fn() call on all m's. 

The boolean argument to fn() indicates whether the function's return value will be consulted or not. That is, fn(true) should return true if fn() succeeds, and fn(true) should return false if it failed. When fn(false) is called, its return status will be ignored. 

syscall_runtime_doAllThreadsSyscall first invokes fn(true) on a single, coordinating, m, and only if it returns true does it go on to invoke fn(false) on all of the other m's known to the process. 

### <a id="runSafePointFn" href="#runSafePointFn">func runSafePointFn()</a>

```
searchKey: runtime.runSafePointFn
```

```Go
func runSafePointFn()
```

runSafePointFn runs the safe point function, if any, for this P. This should be called like 

```
if getg().m.p.runSafePointFn != 0 {
    runSafePointFn()
}

```
runSafePointFn must be checked on any transition in to _Pidle or _Psyscall to avoid a race where forEachP sees that the P is running just before the P goes into _Pidle/_Psyscall and neither forEachP nor the P run the safe-point function. 

### <a id="needm" href="#needm">func needm()</a>

```
searchKey: runtime.needm
```

```Go
func needm()
```

needm is called when a cgo callback happens on a thread without an m (a thread not created by Go). In this case, needm is expected to find an m to use and return with m, g initialized correctly. Since m and g are not set now (likely nil, but see below) needm is limited in what routines it can call. In particular it can only call nosplit functions (textflag 7) and cannot do any scheduling that requires an m. 

In order to avoid needing heavy lifting here, we adopt the following strategy: there is a stack of available m's that can be stolen. Using compare-and-swap to pop from the stack has ABA races, so we simulate a lock by doing an exchange (via Casuintptr) to steal the stack head and replace the top pointer with MLOCKED (1). This serves as a simple spin lock that we can use even without an m. The thread that locks the stack in this way unlocks the stack by storing a valid stack head pointer. 

In order to make sure that there is always an m structure available to be stolen, we maintain the invariant that there is always one more than needed. At the beginning of the program (if cgo is in use) the list is seeded with a single m. If needm finds that it has taken the last m off the list, its job is - once it has installed its own m so that it can do things like allocate memory - to create a spare m and put it on the list. 

Each of these extra m's also has a g0 and a curg that are pressed into service as the scheduling stack and current goroutine for the duration of the cgo callback. 

When the callback is done with the m, it calls dropm to put the m back on the list. 

### <a id="newextram" href="#newextram">func newextram()</a>

```
searchKey: runtime.newextram
```

```Go
func newextram()
```

newextram allocates m's and puts them on the extra list. It is called with a working local m, so that it can do things like call schedlock and allocate. 

### <a id="oneNewExtraM" href="#oneNewExtraM">func oneNewExtraM()</a>

```
searchKey: runtime.oneNewExtraM
```

```Go
func oneNewExtraM()
```

oneNewExtraM allocates an m and puts it on the extra list. 

### <a id="dropm" href="#dropm">func dropm()</a>

```
searchKey: runtime.dropm
```

```Go
func dropm()
```

dropm is called when a cgo callback has called needm but is now done with the callback and returning back into the non-Go thread. It puts the current m back onto the extra list. 

The main expense here is the call to signalstack to release the m's signal stack, and then the call to needm on the next callback from this thread. It is tempting to try to save the m for next time, which would eliminate both these costs, but there might not be a next time: the current thread (which Go does not control) might exit. If we saved the m for that thread, there would be an m leak each time such a thread exited. Instead, we acquire and release an m on each call. These should typically not be scheduling operations, just a few atomics, so the cost should be small. 

TODO(rsc): An alternative would be to allocate a dummy pthread per-thread variable using pthread_key_create. Unlike the pthread keys we already use on OS X, this dummy key would never be read by Go code. It would exist only so that we could register at thread-exit-time destructor. That destructor would put the m back onto the extra list. This is purely a performance optimization. The current version, in which dropm happens on each cgo call, is still correct too. We may have to keep the current version on systems with cgo but without pthreads, like Windows. 

### <a id="getm" href="#getm">func getm() uintptr</a>

```
searchKey: runtime.getm
```

```Go
func getm() uintptr
```

A helper function for EnsureDropM. 

### <a id="unlockextra" href="#unlockextra">func unlockextra(mp *m)</a>

```
searchKey: runtime.unlockextra
```

```Go
func unlockextra(mp *m)
```

### <a id="newm" href="#newm">func newm(fn func(), _p_ *p, id int64)</a>

```
searchKey: runtime.newm
```

```Go
func newm(fn func(), _p_ *p, id int64)
```

Create a new m. It will start off with a call to fn, or else the scheduler. fn needs to be static and not a heap allocated closure. May run with m.p==nil, so write barriers are not allowed. 

id is optional pre-allocated m ID. Omit by passing -1. 

### <a id="newm1" href="#newm1">func newm1(mp *m)</a>

```
searchKey: runtime.newm1
```

```Go
func newm1(mp *m)
```

### <a id="startTemplateThread" href="#startTemplateThread">func startTemplateThread()</a>

```
searchKey: runtime.startTemplateThread
```

```Go
func startTemplateThread()
```

startTemplateThread starts the template thread if it is not already running. 

The calling thread must itself be in a known-good state. 

### <a id="mDoFixup" href="#mDoFixup">func mDoFixup() bool</a>

```
searchKey: runtime.mDoFixup
```

```Go
func mDoFixup() bool
```

mDoFixup runs any outstanding fixup function for the running m. Returns true if a fixup was outstanding and actually executed. 

Note: to avoid deadlocks, and the need for the fixup function itself to be async safe, signals are blocked for the working m while it holds the mFixup lock. (See golang.org/issue/44193) 

### <a id="mDoFixupAndOSYield" href="#mDoFixupAndOSYield">func mDoFixupAndOSYield()</a>

```
searchKey: runtime.mDoFixupAndOSYield
```

```Go
func mDoFixupAndOSYield()
```

mDoFixupAndOSYield is called when an m is unable to send a signal because the allThreadsSyscall mechanism is in progress. That is, an mPark() has been interrupted with this signal handler so we need to ensure the fixup is executed from this context. 

### <a id="templateThread" href="#templateThread">func templateThread()</a>

```
searchKey: runtime.templateThread
```

```Go
func templateThread()
```

templateThread is a thread in a known-good state that exists solely to start new threads in known-good states when the calling thread may not be in a good state. 

Many programs never need this, so templateThread is started lazily when we first enter a state that might lead to running on a thread in an unknown state. 

templateThread runs on an M without a P, so it must not have write barriers. 

### <a id="stopm" href="#stopm">func stopm()</a>

```
searchKey: runtime.stopm
```

```Go
func stopm()
```

Stops execution of the current m until new work is available. Returns with acquired P. 

### <a id="mspinning" href="#mspinning">func mspinning()</a>

```
searchKey: runtime.mspinning
```

```Go
func mspinning()
```

### <a id="startm" href="#startm">func startm(_p_ *p, spinning bool)</a>

```
searchKey: runtime.startm
```

```Go
func startm(_p_ *p, spinning bool)
```

Schedules some M to run the p (creates an M if necessary). If p==nil, tries to get an idle P, if no idle P's does nothing. May run with m.p==nil, so write barriers are not allowed. If spinning is set, the caller has incremented nmspinning and startm will either decrement nmspinning or set m.spinning in the newly started M. 

Callers passing a non-nil P must call from a non-preemptible context. See comment on acquirem below. 

Must not have write barriers because this may be called without a P. 

### <a id="handoffp" href="#handoffp">func handoffp(_p_ *p)</a>

```
searchKey: runtime.handoffp
```

```Go
func handoffp(_p_ *p)
```

Hands off P from syscall or locked M. Always runs without a P, so write barriers are not allowed. 

### <a id="wakep" href="#wakep">func wakep()</a>

```
searchKey: runtime.wakep
```

```Go
func wakep()
```

Tries to add one more P to execute G's. Called when a G is made runnable (newproc, ready). 

### <a id="stoplockedm" href="#stoplockedm">func stoplockedm()</a>

```
searchKey: runtime.stoplockedm
```

```Go
func stoplockedm()
```

Stops execution of the current m that is locked to a g until the g is runnable again. Returns with acquired P. 

### <a id="startlockedm" href="#startlockedm">func startlockedm(gp *g)</a>

```
searchKey: runtime.startlockedm
```

```Go
func startlockedm(gp *g)
```

Schedules the locked m to run the locked gp. May run during STW, so write barriers are not allowed. 

### <a id="gcstopm" href="#gcstopm">func gcstopm()</a>

```
searchKey: runtime.gcstopm
```

```Go
func gcstopm()
```

Stops the current m for stopTheWorld. Returns when the world is restarted. 

### <a id="execute" href="#execute">func execute(gp *g, inheritTime bool)</a>

```
searchKey: runtime.execute
```

```Go
func execute(gp *g, inheritTime bool)
```

Schedules gp to run on the current M. If inheritTime is true, gp inherits the remaining time in the current time slice. Otherwise, it starts a new time slice. Never returns. 

Write barriers are allowed because this is called immediately after acquiring a P in several places. 

### <a id="pollWork" href="#pollWork">func pollWork() bool</a>

```
searchKey: runtime.pollWork
```

```Go
func pollWork() bool
```

pollWork reports whether there is non-background work this P could be doing. This is a fairly lightweight check to be used for background work loops, like idle GC. It checks a subset of the conditions checked by the actual scheduler. 

### <a id="checkTimersNoP" href="#checkTimersNoP">func checkTimersNoP(allpSnapshot []*p, timerpMaskSnapshot pMask, pollUntil int64) int64</a>

```
searchKey: runtime.checkTimersNoP
```

```Go
func checkTimersNoP(allpSnapshot []*p, timerpMaskSnapshot pMask, pollUntil int64) int64
```

Check all Ps for a timer expiring sooner than pollUntil. 

Returns updated pollUntil value. 

### <a id="wakeNetPoller" href="#wakeNetPoller">func wakeNetPoller(when int64)</a>

```
searchKey: runtime.wakeNetPoller
```

```Go
func wakeNetPoller(when int64)
```

wakeNetPoller wakes up the thread sleeping in the network poller if it isn't going to wake up before the when argument; or it wakes an idle P to service timers and the network poller if there isn't one already. 

### <a id="resetspinning" href="#resetspinning">func resetspinning()</a>

```
searchKey: runtime.resetspinning
```

```Go
func resetspinning()
```

### <a id="injectglist" href="#injectglist">func injectglist(glist *gList)</a>

```
searchKey: runtime.injectglist
```

```Go
func injectglist(glist *gList)
```

injectglist adds each runnable G on the list to some run queue, and clears glist. If there is no current P, they are added to the global queue, and up to npidle M's are started to run them. Otherwise, for each idle P, this adds a G to the global queue and starts an M. Any remaining G's are added to the current P's local run queue. This may temporarily acquire sched.lock. Can run concurrently with GC. 

### <a id="schedule" href="#schedule">func schedule()</a>

```
searchKey: runtime.schedule
```

```Go
func schedule()
```

One round of scheduler: find a runnable goroutine and execute it. Never returns. 

### <a id="dropg" href="#dropg">func dropg()</a>

```
searchKey: runtime.dropg
```

```Go
func dropg()
```

dropg removes the association between m and the current goroutine m->curg (gp for short). Typically a caller sets gp's status away from Grunning and then immediately calls dropg to finish the job. The caller is also responsible for arranging that gp will be restarted using ready at an appropriate time. After calling dropg and arranging for gp to be readied later, the caller can do other work but eventually should call schedule to restart the scheduling of goroutines on this m. 

### <a id="checkTimers" href="#checkTimers">func checkTimers(pp *p, now int64) (rnow, pollUntil int64, ran bool)</a>

```
searchKey: runtime.checkTimers
```

```Go
func checkTimers(pp *p, now int64) (rnow, pollUntil int64, ran bool)
```

checkTimers runs any timers for the P that are ready. If now is not 0 it is the current time. It returns the passed time or the current time if now was passed as 0. and the time when the next timer should run or 0 if there is no next timer, and reports whether it ran any timers. If the time when the next timer should run is not 0, it is always larger than the returned time. We pass now in and out to avoid extra calls of nanotime. 

### <a id="parkunlock_c" href="#parkunlock_c">func parkunlock_c(gp *g, lock unsafe.Pointer) bool</a>

```
searchKey: runtime.parkunlock_c
```

```Go
func parkunlock_c(gp *g, lock unsafe.Pointer) bool
```

### <a id="park_m" href="#park_m">func park_m(gp *g)</a>

```
searchKey: runtime.park_m
```

```Go
func park_m(gp *g)
```

park continuation on g0. 

### <a id="goschedImpl" href="#goschedImpl">func goschedImpl(gp *g)</a>

```
searchKey: runtime.goschedImpl
```

```Go
func goschedImpl(gp *g)
```

### <a id="gosched_m" href="#gosched_m">func gosched_m(gp *g)</a>

```
searchKey: runtime.gosched_m
```

```Go
func gosched_m(gp *g)
```

Gosched continuation on g0. 

### <a id="goschedguarded_m" href="#goschedguarded_m">func goschedguarded_m(gp *g)</a>

```
searchKey: runtime.goschedguarded_m
```

```Go
func goschedguarded_m(gp *g)
```

goschedguarded is a forbidden-states-avoided version of gosched_m 

### <a id="gopreempt_m" href="#gopreempt_m">func gopreempt_m(gp *g)</a>

```
searchKey: runtime.gopreempt_m
```

```Go
func gopreempt_m(gp *g)
```

### <a id="preemptPark" href="#preemptPark">func preemptPark(gp *g)</a>

```
searchKey: runtime.preemptPark
```

```Go
func preemptPark(gp *g)
```

preemptPark parks gp and puts it in _Gpreempted. 

### <a id="goyield" href="#goyield">func goyield()</a>

```
searchKey: runtime.goyield
```

```Go
func goyield()
```

goyield is like Gosched, but it: - emits a GoPreempt trace event instead of a GoSched trace event - puts the current G on the runq of the current P instead of the globrunq 

### <a id="goyield_m" href="#goyield_m">func goyield_m(gp *g)</a>

```
searchKey: runtime.goyield_m
```

```Go
func goyield_m(gp *g)
```

### <a id="goexit1" href="#goexit1">func goexit1()</a>

```
searchKey: runtime.goexit1
```

```Go
func goexit1()
```

Finishes execution of the current goroutine. 

### <a id="goexit0" href="#goexit0">func goexit0(gp *g)</a>

```
searchKey: runtime.goexit0
```

```Go
func goexit0(gp *g)
```

goexit continuation on g0. 

### <a id="save" href="#save">func save(pc, sp uintptr)</a>

```
searchKey: runtime.save
```

```Go
func save(pc, sp uintptr)
```

save updates getg().sched to refer to pc and sp so that a following gogo will restore pc and sp. 

save must not have write barriers because invoking a write barrier can clobber getg().sched. 

### <a id="reentersyscall" href="#reentersyscall">func reentersyscall(pc, sp uintptr)</a>

```
searchKey: runtime.reentersyscall
```

```Go
func reentersyscall(pc, sp uintptr)
```

The goroutine g is about to enter a system call. Record that it's not using the cpu anymore. This is called only from the go syscall library and cgocall, not from the low-level system calls used by the runtime. 

Entersyscall cannot split the stack: the save must make g->sched refer to the caller's stack segment, because entersyscall is going to return immediately after. 

Nothing entersyscall calls can split the stack either. We cannot safely move the stack during an active call to syscall, because we do not know which of the uintptr arguments are really pointers (back into the stack). In practice, this means that we make the fast path run through entersyscall doing no-split things, and the slow path has to use systemstack to run bigger things on the system stack. 

reentersyscall is the entry point used by cgo callbacks, where explicitly saved SP and PC are restored. This is needed when exitsyscall will be called from a function further up in the call stack than the parent, as g->syscallsp must always point to a valid stack frame. entersyscall below is the normal entry point for syscalls, which obtains the SP and PC from the caller. 

Syscall tracing: At the start of a syscall we emit traceGoSysCall to capture the stack trace. If the syscall does not block, that is it, we do not emit any other events. If the syscall blocks (that is, P is retaken), retaker emits traceGoSysBlock; when syscall returns we emit traceGoSysExit and when the goroutine starts running (potentially instantly, if exitsyscallfast returns true) we emit traceGoStart. To ensure that traceGoSysExit is emitted strictly after traceGoSysBlock, we remember current value of syscalltick in m (_g_.m.syscalltick = _g_.m.p.ptr().syscalltick), whoever emits traceGoSysBlock increments p.syscalltick afterwards; and we wait for the increment before emitting traceGoSysExit. Note that the increment is done even if tracing is not enabled, because tracing can be enabled in the middle of syscall. We don't want the wait to hang. 

### <a id="entersyscall" href="#entersyscall">func entersyscall()</a>

```
searchKey: runtime.entersyscall
```

```Go
func entersyscall()
```

Standard syscall entry used by the go syscall library and normal cgo calls. 

This is exported via linkname to assembly in the syscall package. 

### <a id="entersyscall_sysmon" href="#entersyscall_sysmon">func entersyscall_sysmon()</a>

```
searchKey: runtime.entersyscall_sysmon
```

```Go
func entersyscall_sysmon()
```

### <a id="entersyscall_gcwait" href="#entersyscall_gcwait">func entersyscall_gcwait()</a>

```
searchKey: runtime.entersyscall_gcwait
```

```Go
func entersyscall_gcwait()
```

### <a id="entersyscallblock" href="#entersyscallblock">func entersyscallblock()</a>

```
searchKey: runtime.entersyscallblock
```

```Go
func entersyscallblock()
```

The same as entersyscall(), but with a hint that the syscall is blocking. 

### <a id="entersyscallblock_handoff" href="#entersyscallblock_handoff">func entersyscallblock_handoff()</a>

```
searchKey: runtime.entersyscallblock_handoff
```

```Go
func entersyscallblock_handoff()
```

### <a id="exitsyscall" href="#exitsyscall">func exitsyscall()</a>

```
searchKey: runtime.exitsyscall
```

```Go
func exitsyscall()
```

The goroutine g exited its system call. Arrange for it to run on a cpu again. This is called only from the go syscall library, not from the low-level system calls used by the runtime. 

Write barriers are not allowed because our P may have been stolen. 

This is exported via linkname to assembly in the syscall package. 

### <a id="exitsyscallfast" href="#exitsyscallfast">func exitsyscallfast(oldp *p) bool</a>

```
searchKey: runtime.exitsyscallfast
```

```Go
func exitsyscallfast(oldp *p) bool
```

### <a id="exitsyscallfast_reacquired" href="#exitsyscallfast_reacquired">func exitsyscallfast_reacquired()</a>

```
searchKey: runtime.exitsyscallfast_reacquired
```

```Go
func exitsyscallfast_reacquired()
```

exitsyscallfast_reacquired is the exitsyscall path on which this G has successfully reacquired the P it was running on before the syscall. 

### <a id="exitsyscallfast_pidle" href="#exitsyscallfast_pidle">func exitsyscallfast_pidle() bool</a>

```
searchKey: runtime.exitsyscallfast_pidle
```

```Go
func exitsyscallfast_pidle() bool
```

### <a id="exitsyscall0" href="#exitsyscall0">func exitsyscall0(gp *g)</a>

```
searchKey: runtime.exitsyscall0
```

```Go
func exitsyscall0(gp *g)
```

exitsyscall slow path on g0. Failed to acquire P, enqueue gp as runnable. 

Called via mcall, so gp is the calling g from this M. 

### <a id="beforefork" href="#beforefork">func beforefork()</a>

```
searchKey: runtime.beforefork
```

```Go
func beforefork()
```

### <a id="syscall_runtime_BeforeFork" href="#syscall_runtime_BeforeFork">func syscall_runtime_BeforeFork()</a>

```
searchKey: runtime.syscall_runtime_BeforeFork
```

```Go
func syscall_runtime_BeforeFork()
```

Called from syscall package before fork. 

### <a id="afterfork" href="#afterfork">func afterfork()</a>

```
searchKey: runtime.afterfork
```

```Go
func afterfork()
```

### <a id="syscall_runtime_AfterFork" href="#syscall_runtime_AfterFork">func syscall_runtime_AfterFork()</a>

```
searchKey: runtime.syscall_runtime_AfterFork
```

```Go
func syscall_runtime_AfterFork()
```

Called from syscall package after fork in parent. 

### <a id="syscall_runtime_AfterForkInChild" href="#syscall_runtime_AfterForkInChild">func syscall_runtime_AfterForkInChild()</a>

```
searchKey: runtime.syscall_runtime_AfterForkInChild
```

```Go
func syscall_runtime_AfterForkInChild()
```

Called from syscall package after fork in child. It resets non-sigignored signals to the default handler, and restores the signal mask in preparation for the exec. 

Because this might be called during a vfork, and therefore may be temporarily sharing address space with the parent process, this must not change any global variables or calling into C code that may do so. 

### <a id="syscall_runtime_BeforeExec" href="#syscall_runtime_BeforeExec">func syscall_runtime_BeforeExec()</a>

```
searchKey: runtime.syscall_runtime_BeforeExec
```

```Go
func syscall_runtime_BeforeExec()
```

Called from syscall package before Exec. 

### <a id="syscall_runtime_AfterExec" href="#syscall_runtime_AfterExec">func syscall_runtime_AfterExec()</a>

```
searchKey: runtime.syscall_runtime_AfterExec
```

```Go
func syscall_runtime_AfterExec()
```

Called from syscall package after Exec. 

### <a id="newproc" href="#newproc">func newproc(siz int32, fn *funcval)</a>

```
searchKey: runtime.newproc
```

```Go
func newproc(siz int32, fn *funcval)
```

Create a new g running fn with siz bytes of arguments. Put it on the queue of g's waiting to run. The compiler turns a go statement into a call to this. 

The stack layout of this call is unusual: it assumes that the arguments to pass to fn are on the stack sequentially immediately after &fn. Hence, they are logically part of newproc's argument frame, even though they don't appear in its signature (and can't because their types differ between call sites). 

This must be nosplit because this stack layout means there are untyped arguments in newproc's argument frame. Stack copies won't be able to adjust them and stack splits won't be able to copy them. 

### <a id="saveAncestors" href="#saveAncestors">func saveAncestors(callergp *g) *[]ancestorInfo</a>

```
searchKey: runtime.saveAncestors
```

```Go
func saveAncestors(callergp *g) *[]ancestorInfo
```

saveAncestors copies previous ancestors of the given caller g and includes infor for the current caller into a new set of tracebacks for a g being created. 

### <a id="gfput" href="#gfput">func gfput(_p_ *p, gp *g)</a>

```
searchKey: runtime.gfput
```

```Go
func gfput(_p_ *p, gp *g)
```

Put on gfree list. If local list is too long, transfer a batch to the global list. 

### <a id="gfpurge" href="#gfpurge">func gfpurge(_p_ *p)</a>

```
searchKey: runtime.gfpurge
```

```Go
func gfpurge(_p_ *p)
```

Purge all cached G's from gfree list to the global list. 

### <a id="Breakpoint" href="#Breakpoint">func Breakpoint()</a>

```
searchKey: runtime.Breakpoint
tags: [exported]
```

```Go
func Breakpoint()
```

Breakpoint executes a breakpoint trap. 

### <a id="dolockOSThread" href="#dolockOSThread">func dolockOSThread()</a>

```
searchKey: runtime.dolockOSThread
```

```Go
func dolockOSThread()
```

dolockOSThread is called by LockOSThread and lockOSThread below after they modify m.locked. Do not allow preemption during this call, or else the m might be different in this function than in the caller. 

### <a id="LockOSThread" href="#LockOSThread">func LockOSThread()</a>

```
searchKey: runtime.LockOSThread
tags: [exported]
```

```Go
func LockOSThread()
```

LockOSThread wires the calling goroutine to its current operating system thread. The calling goroutine will always execute in that thread, and no other goroutine will execute in it, until the calling goroutine has made as many calls to UnlockOSThread as to LockOSThread. If the calling goroutine exits without unlocking the thread, the thread will be terminated. 

All init functions are run on the startup thread. Calling LockOSThread from an init function will cause the main function to be invoked on that thread. 

A goroutine should call LockOSThread before calling OS services or non-Go library functions that depend on per-thread state. 

### <a id="lockOSThread" href="#lockOSThread">func lockOSThread()</a>

```
searchKey: runtime.lockOSThread
```

```Go
func lockOSThread()
```

### <a id="dounlockOSThread" href="#dounlockOSThread">func dounlockOSThread()</a>

```
searchKey: runtime.dounlockOSThread
```

```Go
func dounlockOSThread()
```

dounlockOSThread is called by UnlockOSThread and unlockOSThread below after they update m->locked. Do not allow preemption during this call, or else the m might be in different in this function than in the caller. 

### <a id="UnlockOSThread" href="#UnlockOSThread">func UnlockOSThread()</a>

```
searchKey: runtime.UnlockOSThread
tags: [exported]
```

```Go
func UnlockOSThread()
```

UnlockOSThread undoes an earlier call to LockOSThread. If this drops the number of active LockOSThread calls on the calling goroutine to zero, it unwires the calling goroutine from its fixed operating system thread. If there are no active LockOSThread calls, this is a no-op. 

Before calling UnlockOSThread, the caller must ensure that the OS thread is suitable for running other goroutines. If the caller made any permanent changes to the state of the thread that would affect other goroutines, it should not call this function and thus leave the goroutine locked to the OS thread until the goroutine (and hence the thread) exits. 

### <a id="unlockOSThread" href="#unlockOSThread">func unlockOSThread()</a>

```
searchKey: runtime.unlockOSThread
```

```Go
func unlockOSThread()
```

### <a id="badunlockosthread" href="#badunlockosthread">func badunlockosthread()</a>

```
searchKey: runtime.badunlockosthread
```

```Go
func badunlockosthread()
```

### <a id="gcount" href="#gcount">func gcount() int32</a>

```
searchKey: runtime.gcount
```

```Go
func gcount() int32
```

### <a id="mcount" href="#mcount">func mcount() int32</a>

```
searchKey: runtime.mcount
```

```Go
func mcount() int32
```

### <a id="_System" href="#_System">func _System()</a>

```
searchKey: runtime._System
```

```Go
func _System()
```

### <a id="_ExternalCode" href="#_ExternalCode">func _ExternalCode()</a>

```
searchKey: runtime._ExternalCode
```

```Go
func _ExternalCode()
```

### <a id="_LostExternalCode" href="#_LostExternalCode">func _LostExternalCode()</a>

```
searchKey: runtime._LostExternalCode
```

```Go
func _LostExternalCode()
```

### <a id="_GC" href="#_GC">func _GC()</a>

```
searchKey: runtime._GC
```

```Go
func _GC()
```

### <a id="_LostSIGPROFDuringAtomic64" href="#_LostSIGPROFDuringAtomic64">func _LostSIGPROFDuringAtomic64()</a>

```
searchKey: runtime._LostSIGPROFDuringAtomic64
```

```Go
func _LostSIGPROFDuringAtomic64()
```

### <a id="_VDSO" href="#_VDSO">func _VDSO()</a>

```
searchKey: runtime._VDSO
```

```Go
func _VDSO()
```

### <a id="sigprof" href="#sigprof">func sigprof(pc, sp, lr uintptr, gp *g, mp *m)</a>

```
searchKey: runtime.sigprof
```

```Go
func sigprof(pc, sp, lr uintptr, gp *g, mp *m)
```

Called if we receive a SIGPROF signal. Called by the signal handler, may run during STW. 

### <a id="sigprofNonGo" href="#sigprofNonGo">func sigprofNonGo()</a>

```
searchKey: runtime.sigprofNonGo
```

```Go
func sigprofNonGo()
```

sigprofNonGo is called if we receive a SIGPROF signal on a non-Go thread, and the signal handler collected a stack trace in sigprofCallers. When this is called, sigprofCallersUse will be non-zero. g is nil, and what we can do is very limited. 

### <a id="sigprofNonGoPC" href="#sigprofNonGoPC">func sigprofNonGoPC(pc uintptr)</a>

```
searchKey: runtime.sigprofNonGoPC
```

```Go
func sigprofNonGoPC(pc uintptr)
```

sigprofNonGoPC is called when a profiling signal arrived on a non-Go thread and we have a single PC value, not a stack trace. g is nil, and what we can do is very limited. 

### <a id="setcpuprofilerate" href="#setcpuprofilerate">func setcpuprofilerate(hz int32)</a>

```
searchKey: runtime.setcpuprofilerate
```

```Go
func setcpuprofilerate(hz int32)
```

setcpuprofilerate sets the CPU profiling rate to hz times per second. If hz <= 0, setcpuprofilerate turns off CPU profiling. 

### <a id="acquirep" href="#acquirep">func acquirep(_p_ *p)</a>

```
searchKey: runtime.acquirep
```

```Go
func acquirep(_p_ *p)
```

Associate p and the current m. 

This function is allowed to have write barriers even if the caller isn't because it immediately acquires _p_. 

### <a id="wirep" href="#wirep">func wirep(_p_ *p)</a>

```
searchKey: runtime.wirep
```

```Go
func wirep(_p_ *p)
```

wirep is the first step of acquirep, which actually associates the current M to _p_. This is broken out so we can disallow write barriers for this part, since we don't yet have a P. 

### <a id="incidlelocked" href="#incidlelocked">func incidlelocked(v int32)</a>

```
searchKey: runtime.incidlelocked
```

```Go
func incidlelocked(v int32)
```

### <a id="checkdead" href="#checkdead">func checkdead()</a>

```
searchKey: runtime.checkdead
```

```Go
func checkdead()
```

Check for deadlock situation. The check is based on number of running M's, if 0 -> deadlock. sched.lock must be held. 

### <a id="sysmon" href="#sysmon">func sysmon()</a>

```
searchKey: runtime.sysmon
```

```Go
func sysmon()
```

Always runs without a P, so write barriers are not allowed. 

### <a id="retake" href="#retake">func retake(now int64) uint32</a>

```
searchKey: runtime.retake
```

```Go
func retake(now int64) uint32
```

### <a id="preemptall" href="#preemptall">func preemptall() bool</a>

```
searchKey: runtime.preemptall
```

```Go
func preemptall() bool
```

Tell all goroutines that they have been preempted and they should stop. This function is purely best-effort. It can fail to inform a goroutine if a processor just started running it. No locks need to be held. Returns true if preemption request was issued to at least one goroutine. 

### <a id="preemptone" href="#preemptone">func preemptone(_p_ *p) bool</a>

```
searchKey: runtime.preemptone
```

```Go
func preemptone(_p_ *p) bool
```

Tell the goroutine running on processor P to stop. This function is purely best-effort. It can incorrectly fail to inform the goroutine. It can inform the wrong goroutine. Even if it informs the correct goroutine, that goroutine might ignore the request if it is simultaneously executing newstack. No lock needs to be held. Returns true if preemption request was issued. The actual preemption will happen at some point in the future and will be indicated by the gp->status no longer being Grunning 

### <a id="schedtrace" href="#schedtrace">func schedtrace(detailed bool)</a>

```
searchKey: runtime.schedtrace
```

```Go
func schedtrace(detailed bool)
```

### <a id="schedEnableUser" href="#schedEnableUser">func schedEnableUser(enable bool)</a>

```
searchKey: runtime.schedEnableUser
```

```Go
func schedEnableUser(enable bool)
```

schedEnableUser enables or disables the scheduling of user goroutines. 

This does not stop already running user goroutines, so the caller should first stop the world when disabling user goroutines. 

### <a id="schedEnabled" href="#schedEnabled">func schedEnabled(gp *g) bool</a>

```
searchKey: runtime.schedEnabled
```

```Go
func schedEnabled(gp *g) bool
```

schedEnabled reports whether gp should be scheduled. It returns false is scheduling of gp is disabled. 

sched.lock must be held. 

### <a id="mput" href="#mput">func mput(mp *m)</a>

```
searchKey: runtime.mput
```

```Go
func mput(mp *m)
```

Put mp on midle list. sched.lock must be held. May run during STW, so write barriers are not allowed. 

### <a id="globrunqput" href="#globrunqput">func globrunqput(gp *g)</a>

```
searchKey: runtime.globrunqput
```

```Go
func globrunqput(gp *g)
```

Put gp on the global runnable queue. sched.lock must be held. May run during STW, so write barriers are not allowed. 

### <a id="globrunqputhead" href="#globrunqputhead">func globrunqputhead(gp *g)</a>

```
searchKey: runtime.globrunqputhead
```

```Go
func globrunqputhead(gp *g)
```

Put gp at the head of the global runnable queue. sched.lock must be held. May run during STW, so write barriers are not allowed. 

### <a id="globrunqputbatch" href="#globrunqputbatch">func globrunqputbatch(batch *gQueue, n int32)</a>

```
searchKey: runtime.globrunqputbatch
```

```Go
func globrunqputbatch(batch *gQueue, n int32)
```

Put a batch of runnable goroutines on the global runnable queue. This clears *batch. sched.lock must be held. May run during STW, so write barriers are not allowed. 

### <a id="updateTimerPMask" href="#updateTimerPMask">func updateTimerPMask(pp *p)</a>

```
searchKey: runtime.updateTimerPMask
```

```Go
func updateTimerPMask(pp *p)
```

updateTimerPMask clears pp's timer mask if it has no timers on its heap. 

Ideally, the timer mask would be kept immediately consistent on any timer operations. Unfortunately, updating a shared global data structure in the timer hot path adds too much overhead in applications frequently switching between no timers and some timers. 

As a compromise, the timer mask is updated only on pidleget / pidleput. A running P (returned by pidleget) may add a timer at any time, so its mask must be set. An idle P (passed to pidleput) cannot add new timers while idle, so if it has no timers at that time, its mask may be cleared. 

Thus, we get the following effects on timer-stealing in findrunnable: 

* Idle Ps with no timers when they go idle are never checked in findrunnable 

```
(for work- or timer-stealing; this is the ideal case).

```
* Running Ps must always be checked. * Idle Ps whose timers are stolen must continue to be checked until they run 

```
again, even after timer expiration.

```
When the P starts running again, the mask should be set, as a timer may be added at any time. 

TODO(prattmic): Additional targeted updates may improve the above cases. e.g., updating the mask when stealing a timer. 

### <a id="pidleput" href="#pidleput">func pidleput(_p_ *p)</a>

```
searchKey: runtime.pidleput
```

```Go
func pidleput(_p_ *p)
```

pidleput puts p to on the _Pidle list. 

This releases ownership of p. Once sched.lock is released it is no longer safe to use p. 

sched.lock must be held. 

May run during STW, so write barriers are not allowed. 

### <a id="runqempty" href="#runqempty">func runqempty(_p_ *p) bool</a>

```
searchKey: runtime.runqempty
```

```Go
func runqempty(_p_ *p) bool
```

runqempty reports whether _p_ has no Gs on its local run queue. It never returns true spuriously. 

### <a id="runqput" href="#runqput">func runqput(_p_ *p, gp *g, next bool)</a>

```
searchKey: runtime.runqput
```

```Go
func runqput(_p_ *p, gp *g, next bool)
```

runqput tries to put g on the local runnable queue. If next is false, runqput adds g to the tail of the runnable queue. If next is true, runqput puts g in the _p_.runnext slot. If the run queue is full, runnext puts g on the global queue. Executed only by the owner P. 

### <a id="runqputslow" href="#runqputslow">func runqputslow(_p_ *p, gp *g, h, t uint32) bool</a>

```
searchKey: runtime.runqputslow
```

```Go
func runqputslow(_p_ *p, gp *g, h, t uint32) bool
```

Put g and a batch of work from local runnable queue on global queue. Executed only by the owner P. 

### <a id="runqputbatch" href="#runqputbatch">func runqputbatch(pp *p, q *gQueue, qsize int)</a>

```
searchKey: runtime.runqputbatch
```

```Go
func runqputbatch(pp *p, q *gQueue, qsize int)
```

runqputbatch tries to put all the G's on q on the local runnable queue. If the queue is full, they are put on the global queue; in that case this will temporarily acquire the scheduler lock. Executed only by the owner P. 

### <a id="runqgrab" href="#runqgrab">func runqgrab(_p_ *p, batch *[256]guintptr, batchHead uint32, stealRunNextG bool) uint32</a>

```
searchKey: runtime.runqgrab
```

```Go
func runqgrab(_p_ *p, batch *[256]guintptr, batchHead uint32, stealRunNextG bool) uint32
```

Grabs a batch of goroutines from _p_'s runnable queue into batch. Batch is a ring buffer starting at batchHead. Returns number of grabbed goroutines. Can be executed by any P. 

### <a id="setMaxThreads" href="#setMaxThreads">func setMaxThreads(in int) (out int)</a>

```
searchKey: runtime.setMaxThreads
```

```Go
func setMaxThreads(in int) (out int)
```

### <a id="procPin" href="#procPin">func procPin() int</a>

```
searchKey: runtime.procPin
```

```Go
func procPin() int
```

### <a id="procUnpin" href="#procUnpin">func procUnpin()</a>

```
searchKey: runtime.procUnpin
```

```Go
func procUnpin()
```

### <a id="sync_runtime_procPin" href="#sync_runtime_procPin">func sync_runtime_procPin() int</a>

```
searchKey: runtime.sync_runtime_procPin
```

```Go
func sync_runtime_procPin() int
```

### <a id="sync_runtime_procUnpin" href="#sync_runtime_procUnpin">func sync_runtime_procUnpin()</a>

```
searchKey: runtime.sync_runtime_procUnpin
```

```Go
func sync_runtime_procUnpin()
```

### <a id="sync_atomic_runtime_procPin" href="#sync_atomic_runtime_procPin">func sync_atomic_runtime_procPin() int</a>

```
searchKey: runtime.sync_atomic_runtime_procPin
```

```Go
func sync_atomic_runtime_procPin() int
```

### <a id="sync_atomic_runtime_procUnpin" href="#sync_atomic_runtime_procUnpin">func sync_atomic_runtime_procUnpin()</a>

```
searchKey: runtime.sync_atomic_runtime_procUnpin
```

```Go
func sync_atomic_runtime_procUnpin()
```

### <a id="sync_runtime_canSpin" href="#sync_runtime_canSpin">func sync_runtime_canSpin(i int) bool</a>

```
searchKey: runtime.sync_runtime_canSpin
```

```Go
func sync_runtime_canSpin(i int) bool
```

Active spinning for sync.Mutex. 

### <a id="sync_runtime_doSpin" href="#sync_runtime_doSpin">func sync_runtime_doSpin()</a>

```
searchKey: runtime.sync_runtime_doSpin
```

```Go
func sync_runtime_doSpin()
```

### <a id="gcd" href="#gcd">func gcd(a, b uint32) uint32</a>

```
searchKey: runtime.gcd
```

```Go
func gcd(a, b uint32) uint32
```

### <a id="doInit" href="#doInit">func doInit(t *initTask)</a>

```
searchKey: runtime.doInit
```

```Go
func doInit(t *initTask)
```

### <a id="countSub" href="#countSub">func countSub(x, y uint32) int</a>

```
searchKey: runtime.countSub
```

```Go
func countSub(x, y uint32) int
```

countSub subtracts two counts obtained from profIndex.dataCount or profIndex.tagCount, assuming that they are no more than 2^29 apart (guaranteed since they are never more than len(data) or len(tags) apart, respectively). tagCount wraps at 2^30, while dataCount wraps at 2^32. This function works for both. 

### <a id="runtime_setProfLabel" href="#runtime_setProfLabel">func runtime_setProfLabel(labels unsafe.Pointer)</a>

```
searchKey: runtime.runtime_setProfLabel
```

```Go
func runtime_setProfLabel(labels unsafe.Pointer)
```

### <a id="runtime_getProfLabel" href="#runtime_getProfLabel">func runtime_getProfLabel() unsafe.Pointer</a>

```
searchKey: runtime.runtime_getProfLabel
```

```Go
func runtime_getProfLabel() unsafe.Pointer
```

### <a id="raceReadObjectPC" href="#raceReadObjectPC">func raceReadObjectPC(t *_type, addr unsafe.Pointer, callerpc, pc uintptr)</a>

```
searchKey: runtime.raceReadObjectPC
```

```Go
func raceReadObjectPC(t *_type, addr unsafe.Pointer, callerpc, pc uintptr)
```

### <a id="raceWriteObjectPC" href="#raceWriteObjectPC">func raceWriteObjectPC(t *_type, addr unsafe.Pointer, callerpc, pc uintptr)</a>

```
searchKey: runtime.raceWriteObjectPC
```

```Go
func raceWriteObjectPC(t *_type, addr unsafe.Pointer, callerpc, pc uintptr)
```

### <a id="raceinit" href="#raceinit">func raceinit() (uintptr, uintptr)</a>

```
searchKey: runtime.raceinit
```

```Go
func raceinit() (uintptr, uintptr)
```

### <a id="racefini" href="#racefini">func racefini()</a>

```
searchKey: runtime.racefini
```

```Go
func racefini()
```

### <a id="raceproccreate" href="#raceproccreate">func raceproccreate() uintptr</a>

```
searchKey: runtime.raceproccreate
```

```Go
func raceproccreate() uintptr
```

### <a id="raceprocdestroy" href="#raceprocdestroy">func raceprocdestroy(ctx uintptr)</a>

```
searchKey: runtime.raceprocdestroy
```

```Go
func raceprocdestroy(ctx uintptr)
```

### <a id="racemapshadow" href="#racemapshadow">func racemapshadow(addr unsafe.Pointer, size uintptr)</a>

```
searchKey: runtime.racemapshadow
```

```Go
func racemapshadow(addr unsafe.Pointer, size uintptr)
```

### <a id="racewritepc" href="#racewritepc">func racewritepc(addr unsafe.Pointer, callerpc, pc uintptr)</a>

```
searchKey: runtime.racewritepc
```

```Go
func racewritepc(addr unsafe.Pointer, callerpc, pc uintptr)
```

### <a id="racereadpc" href="#racereadpc">func racereadpc(addr unsafe.Pointer, callerpc, pc uintptr)</a>

```
searchKey: runtime.racereadpc
```

```Go
func racereadpc(addr unsafe.Pointer, callerpc, pc uintptr)
```

### <a id="racereadrangepc" href="#racereadrangepc">func racereadrangepc(addr unsafe.Pointer, sz, callerpc, pc uintptr)</a>

```
searchKey: runtime.racereadrangepc
```

```Go
func racereadrangepc(addr unsafe.Pointer, sz, callerpc, pc uintptr)
```

### <a id="racewriterangepc" href="#racewriterangepc">func racewriterangepc(addr unsafe.Pointer, sz, callerpc, pc uintptr)</a>

```
searchKey: runtime.racewriterangepc
```

```Go
func racewriterangepc(addr unsafe.Pointer, sz, callerpc, pc uintptr)
```

### <a id="raceacquire" href="#raceacquire">func raceacquire(addr unsafe.Pointer)</a>

```
searchKey: runtime.raceacquire
```

```Go
func raceacquire(addr unsafe.Pointer)
```

### <a id="raceacquireg" href="#raceacquireg">func raceacquireg(gp *g, addr unsafe.Pointer)</a>

```
searchKey: runtime.raceacquireg
```

```Go
func raceacquireg(gp *g, addr unsafe.Pointer)
```

### <a id="raceacquirectx" href="#raceacquirectx">func raceacquirectx(racectx uintptr, addr unsafe.Pointer)</a>

```
searchKey: runtime.raceacquirectx
```

```Go
func raceacquirectx(racectx uintptr, addr unsafe.Pointer)
```

### <a id="racerelease" href="#racerelease">func racerelease(addr unsafe.Pointer)</a>

```
searchKey: runtime.racerelease
```

```Go
func racerelease(addr unsafe.Pointer)
```

### <a id="racereleaseg" href="#racereleaseg">func racereleaseg(gp *g, addr unsafe.Pointer)</a>

```
searchKey: runtime.racereleaseg
```

```Go
func racereleaseg(gp *g, addr unsafe.Pointer)
```

### <a id="racereleaseacquire" href="#racereleaseacquire">func racereleaseacquire(addr unsafe.Pointer)</a>

```
searchKey: runtime.racereleaseacquire
```

```Go
func racereleaseacquire(addr unsafe.Pointer)
```

### <a id="racereleaseacquireg" href="#racereleaseacquireg">func racereleaseacquireg(gp *g, addr unsafe.Pointer)</a>

```
searchKey: runtime.racereleaseacquireg
```

```Go
func racereleaseacquireg(gp *g, addr unsafe.Pointer)
```

### <a id="racereleasemerge" href="#racereleasemerge">func racereleasemerge(addr unsafe.Pointer)</a>

```
searchKey: runtime.racereleasemerge
```

```Go
func racereleasemerge(addr unsafe.Pointer)
```

### <a id="racereleasemergeg" href="#racereleasemergeg">func racereleasemergeg(gp *g, addr unsafe.Pointer)</a>

```
searchKey: runtime.racereleasemergeg
```

```Go
func racereleasemergeg(gp *g, addr unsafe.Pointer)
```

### <a id="racefingo" href="#racefingo">func racefingo()</a>

```
searchKey: runtime.racefingo
```

```Go
func racefingo()
```

### <a id="racemalloc" href="#racemalloc">func racemalloc(p unsafe.Pointer, sz uintptr)</a>

```
searchKey: runtime.racemalloc
```

```Go
func racemalloc(p unsafe.Pointer, sz uintptr)
```

### <a id="racefree" href="#racefree">func racefree(p unsafe.Pointer, sz uintptr)</a>

```
searchKey: runtime.racefree
```

```Go
func racefree(p unsafe.Pointer, sz uintptr)
```

### <a id="racegostart" href="#racegostart">func racegostart(pc uintptr) uintptr</a>

```
searchKey: runtime.racegostart
```

```Go
func racegostart(pc uintptr) uintptr
```

### <a id="racegoend" href="#racegoend">func racegoend()</a>

```
searchKey: runtime.racegoend
```

```Go
func racegoend()
```

### <a id="racectxend" href="#racectxend">func racectxend(racectx uintptr)</a>

```
searchKey: runtime.racectxend
```

```Go
func racectxend(racectx uintptr)
```

### <a id="setMaxStack" href="#setMaxStack">func setMaxStack(in int) (out int)</a>

```
searchKey: runtime.setMaxStack
```

```Go
func setMaxStack(in int) (out int)
```

### <a id="setPanicOnFault" href="#setPanicOnFault">func setPanicOnFault(new bool) (old bool)</a>

```
searchKey: runtime.setPanicOnFault
```

```Go
func setPanicOnFault(new bool) (old bool)
```

### <a id="osRelax" href="#osRelax">func osRelax(relax bool)</a>

```
searchKey: runtime.osRelax
```

```Go
func osRelax(relax bool)
```

osRelax is called by the scheduler when transitioning to and from all Ps being idle. 

### <a id="tickspersecond" href="#tickspersecond">func tickspersecond() int64</a>

```
searchKey: runtime.tickspersecond
```

```Go
func tickspersecond() int64
```

Note: Called by runtime/pprof in addition to runtime code. 

### <a id="syscall_runtime_envs" href="#syscall_runtime_envs">func syscall_runtime_envs() []string</a>

```
searchKey: runtime.syscall_runtime_envs
```

```Go
func syscall_runtime_envs() []string
```

### <a id="syscall_Getpagesize" href="#syscall_Getpagesize">func syscall_Getpagesize() int</a>

```
searchKey: runtime.syscall_Getpagesize
```

```Go
func syscall_Getpagesize() int
```

### <a id="os_runtime_args" href="#os_runtime_args">func os_runtime_args() []string</a>

```
searchKey: runtime.os_runtime_args
```

```Go
func os_runtime_args() []string
```

### <a id="syscall_Exit" href="#syscall_Exit">func syscall_Exit(code int)</a>

```
searchKey: runtime.syscall_Exit
```

```Go
func syscall_Exit(code int)
```

### <a id="gotraceback" href="#gotraceback">func gotraceback() (level int32, all, crash bool)</a>

```
searchKey: runtime.gotraceback
```

```Go
func gotraceback() (level int32, all, crash bool)
```

gotraceback returns the current traceback settings. 

If level is 0, suppress all tracebacks. If level is 1, show tracebacks, but exclude runtime frames. If level is 2, show tracebacks including runtime frames. If all is set, print all goroutine stacks. Otherwise, print just the current goroutine. If crash is set, crash (core dump, etc) after tracebacking. 

### <a id="argv_index" href="#argv_index">func argv_index(argv **byte, i int32) *byte</a>

```
searchKey: runtime.argv_index
```

```Go
func argv_index(argv **byte, i int32) *byte
```

nosplit for use in linux startup sysargs 

### <a id="args" href="#args">func args(c int32, v **byte)</a>

```
searchKey: runtime.args
```

```Go
func args(c int32, v **byte)
```

### <a id="goargs" href="#goargs">func goargs()</a>

```
searchKey: runtime.goargs
```

```Go
func goargs()
```

### <a id="goenvs_unix" href="#goenvs_unix">func goenvs_unix()</a>

```
searchKey: runtime.goenvs_unix
```

```Go
func goenvs_unix()
```

### <a id="environ" href="#environ">func environ() []string</a>

```
searchKey: runtime.environ
```

```Go
func environ() []string
```

### <a id="testAtomic64" href="#testAtomic64">func testAtomic64()</a>

```
searchKey: runtime.testAtomic64
```

```Go
func testAtomic64()
```

### <a id="check" href="#check">func check()</a>

```
searchKey: runtime.check
```

```Go
func check()
```

### <a id="parsedebugvars" href="#parsedebugvars">func parsedebugvars()</a>

```
searchKey: runtime.parsedebugvars
```

```Go
func parsedebugvars()
```

### <a id="setTraceback" href="#setTraceback">func setTraceback(level string)</a>

```
searchKey: runtime.setTraceback
```

```Go
func setTraceback(level string)
```

### <a id="timediv" href="#timediv">func timediv(v int64, div int32, rem *int32) int32</a>

```
searchKey: runtime.timediv
```

```Go
func timediv(v int64, div int32, rem *int32) int32
```

Poor mans 64-bit division. This is a very special function, do not use it if you are not sure what you are doing. int64 division is lowered into _divv() call on 386, which does not fit into nosplit functions. Handles overflow in a time-specific manner. This keeps us within no-split stack limits on 32-bit processors. 

### <a id="releasem" href="#releasem">func releasem(mp *m)</a>

```
searchKey: runtime.releasem
```

```Go
func releasem(mp *m)
```

### <a id="reflect_typelinks" href="#reflect_typelinks">func reflect_typelinks() ([]unsafe.Pointer, [][]int32)</a>

```
searchKey: runtime.reflect_typelinks
```

```Go
func reflect_typelinks() ([]unsafe.Pointer, [][]int32)
```

### <a id="reflect_resolveNameOff" href="#reflect_resolveNameOff">func reflect_resolveNameOff(ptrInModule unsafe.Pointer, off int32) unsafe.Pointer</a>

```
searchKey: runtime.reflect_resolveNameOff
```

```Go
func reflect_resolveNameOff(ptrInModule unsafe.Pointer, off int32) unsafe.Pointer
```

reflect_resolveNameOff resolves a name offset from a base pointer. 

### <a id="reflect_resolveTypeOff" href="#reflect_resolveTypeOff">func reflect_resolveTypeOff(rtype unsafe.Pointer, off int32) unsafe.Pointer</a>

```
searchKey: runtime.reflect_resolveTypeOff
```

```Go
func reflect_resolveTypeOff(rtype unsafe.Pointer, off int32) unsafe.Pointer
```

reflect_resolveTypeOff resolves an *rtype offset from a base type. 

### <a id="reflect_resolveTextOff" href="#reflect_resolveTextOff">func reflect_resolveTextOff(rtype unsafe.Pointer, off int32) unsafe.Pointer</a>

```
searchKey: runtime.reflect_resolveTextOff
```

```Go
func reflect_resolveTextOff(rtype unsafe.Pointer, off int32) unsafe.Pointer
```

reflect_resolveTextOff resolves a function pointer offset from a base type. 

### <a id="reflectlite_resolveNameOff" href="#reflectlite_resolveNameOff">func reflectlite_resolveNameOff(ptrInModule unsafe.Pointer, off int32) unsafe.Pointer</a>

```
searchKey: runtime.reflectlite_resolveNameOff
```

```Go
func reflectlite_resolveNameOff(ptrInModule unsafe.Pointer, off int32) unsafe.Pointer
```

reflectlite_resolveNameOff resolves a name offset from a base pointer. 

### <a id="reflectlite_resolveTypeOff" href="#reflectlite_resolveTypeOff">func reflectlite_resolveTypeOff(rtype unsafe.Pointer, off int32) unsafe.Pointer</a>

```
searchKey: runtime.reflectlite_resolveTypeOff
```

```Go
func reflectlite_resolveTypeOff(rtype unsafe.Pointer, off int32) unsafe.Pointer
```

reflectlite_resolveTypeOff resolves an *rtype offset from a base type. 

### <a id="reflect_addReflectOff" href="#reflect_addReflectOff">func reflect_addReflectOff(ptr unsafe.Pointer) int32</a>

```
searchKey: runtime.reflect_addReflectOff
```

```Go
func reflect_addReflectOff(ptr unsafe.Pointer) int32
```

reflect_addReflectOff adds a pointer to the reflection offset lookup map. 

### <a id="setGNoWB" href="#setGNoWB">func setGNoWB(gp **g, new *g)</a>

```
searchKey: runtime.setGNoWB
```

```Go
func setGNoWB(gp **g, new *g)
```

setGNoWB performs *gp = new without a write barrier. For times when it's impractical to use a guintptr. 

### <a id="setMNoWB" href="#setMNoWB">func setMNoWB(mp **m, new *m)</a>

```
searchKey: runtime.setMNoWB
```

```Go
func setMNoWB(mp **m, new *m)
```

setMNoWB performs *mp = new without a write barrier. For times when it's impractical to use an muintptr. 

### <a id="extendRandom" href="#extendRandom">func extendRandom(r []byte, n int)</a>

```
searchKey: runtime.extendRandom
```

```Go
func extendRandom(r []byte, n int)
```

extendRandom extends the random numbers in r[:n] to the whole slice r. Treats n<0 as n==0. 

### <a id="selectsetpc" href="#selectsetpc">func selectsetpc(pc *uintptr)</a>

```
searchKey: runtime.selectsetpc
```

```Go
func selectsetpc(pc *uintptr)
```

### <a id="sellock" href="#sellock">func sellock(scases []scase, lockorder []uint16)</a>

```
searchKey: runtime.sellock
```

```Go
func sellock(scases []scase, lockorder []uint16)
```

### <a id="selunlock" href="#selunlock">func selunlock(scases []scase, lockorder []uint16)</a>

```
searchKey: runtime.selunlock
```

```Go
func selunlock(scases []scase, lockorder []uint16)
```

### <a id="selparkcommit" href="#selparkcommit">func selparkcommit(gp *g, _ unsafe.Pointer) bool</a>

```
searchKey: runtime.selparkcommit
```

```Go
func selparkcommit(gp *g, _ unsafe.Pointer) bool
```

### <a id="block" href="#block">func block()</a>

```
searchKey: runtime.block
```

```Go
func block()
```

### <a id="selectgo" href="#selectgo">func selectgo(cas0 *scase, order0 *uint16, pc0 *uintptr, nsends, nrecvs int, block bool) (int, bool)</a>

```
searchKey: runtime.selectgo
```

```Go
func selectgo(cas0 *scase, order0 *uint16, pc0 *uintptr, nsends, nrecvs int, block bool) (int, bool)
```

selectgo implements the select statement. 

cas0 points to an array of type [ncases]scase, and order0 points to an array of type [2*ncases]uint16 where ncases must be <= 65536. Both reside on the goroutine's stack (regardless of any escaping in selectgo). 

For race detector builds, pc0 points to an array of type [ncases]uintptr (also on the stack); for other builds, it's set to nil. 

selectgo returns the index of the chosen scase, which matches the ordinal position of its respective select{recv,send,default} call. Also, if the chosen scase was a receive operation, it reports whether a value was received. 

### <a id="reflect_rselect" href="#reflect_rselect">func reflect_rselect(cases []runtimeSelect) (int, bool)</a>

```
searchKey: runtime.reflect_rselect
```

```Go
func reflect_rselect(cases []runtimeSelect) (int, bool)
```

### <a id="sync_runtime_Semacquire" href="#sync_runtime_Semacquire">func sync_runtime_Semacquire(addr *uint32)</a>

```
searchKey: runtime.sync_runtime_Semacquire
```

```Go
func sync_runtime_Semacquire(addr *uint32)
```

### <a id="poll_runtime_Semacquire" href="#poll_runtime_Semacquire">func poll_runtime_Semacquire(addr *uint32)</a>

```
searchKey: runtime.poll_runtime_Semacquire
```

```Go
func poll_runtime_Semacquire(addr *uint32)
```

### <a id="sync_runtime_Semrelease" href="#sync_runtime_Semrelease">func sync_runtime_Semrelease(addr *uint32, handoff bool, skipframes int)</a>

```
searchKey: runtime.sync_runtime_Semrelease
```

```Go
func sync_runtime_Semrelease(addr *uint32, handoff bool, skipframes int)
```

### <a id="sync_runtime_SemacquireMutex" href="#sync_runtime_SemacquireMutex">func sync_runtime_SemacquireMutex(addr *uint32, lifo bool, skipframes int)</a>

```
searchKey: runtime.sync_runtime_SemacquireMutex
```

```Go
func sync_runtime_SemacquireMutex(addr *uint32, lifo bool, skipframes int)
```

### <a id="poll_runtime_Semrelease" href="#poll_runtime_Semrelease">func poll_runtime_Semrelease(addr *uint32)</a>

```
searchKey: runtime.poll_runtime_Semrelease
```

```Go
func poll_runtime_Semrelease(addr *uint32)
```

### <a id="readyWithTime" href="#readyWithTime">func readyWithTime(s *sudog, traceskip int)</a>

```
searchKey: runtime.readyWithTime
```

```Go
func readyWithTime(s *sudog, traceskip int)
```

### <a id="semacquire" href="#semacquire">func semacquire(addr *uint32)</a>

```
searchKey: runtime.semacquire
```

```Go
func semacquire(addr *uint32)
```

Called from runtime. 

### <a id="semacquire1" href="#semacquire1">func semacquire1(addr *uint32, lifo bool, profile semaProfileFlags, skipframes int)</a>

```
searchKey: runtime.semacquire1
```

```Go
func semacquire1(addr *uint32, lifo bool, profile semaProfileFlags, skipframes int)
```

### <a id="semrelease" href="#semrelease">func semrelease(addr *uint32)</a>

```
searchKey: runtime.semrelease
```

```Go
func semrelease(addr *uint32)
```

### <a id="semrelease1" href="#semrelease1">func semrelease1(addr *uint32, handoff bool, skipframes int)</a>

```
searchKey: runtime.semrelease1
```

```Go
func semrelease1(addr *uint32, handoff bool, skipframes int)
```

### <a id="cansemacquire" href="#cansemacquire">func cansemacquire(addr *uint32) bool</a>

```
searchKey: runtime.cansemacquire
```

```Go
func cansemacquire(addr *uint32) bool
```

### <a id="less" href="#less">func less(a, b uint32) bool</a>

```
searchKey: runtime.less
```

```Go
func less(a, b uint32) bool
```

less checks if a < b, considering a & b running counts that may overflow the 32-bit range, and that their "unwrapped" difference is always less than 2^31. 

### <a id="notifyListAdd" href="#notifyListAdd">func notifyListAdd(l *notifyList) uint32</a>

```
searchKey: runtime.notifyListAdd
```

```Go
func notifyListAdd(l *notifyList) uint32
```

notifyListAdd adds the caller to a notify list such that it can receive notifications. The caller must eventually call notifyListWait to wait for such a notification, passing the returned ticket number. 

### <a id="notifyListWait" href="#notifyListWait">func notifyListWait(l *notifyList, t uint32)</a>

```
searchKey: runtime.notifyListWait
```

```Go
func notifyListWait(l *notifyList, t uint32)
```

notifyListWait waits for a notification. If one has been sent since notifyListAdd was called, it returns immediately. Otherwise, it blocks. 

### <a id="notifyListNotifyAll" href="#notifyListNotifyAll">func notifyListNotifyAll(l *notifyList)</a>

```
searchKey: runtime.notifyListNotifyAll
```

```Go
func notifyListNotifyAll(l *notifyList)
```

notifyListNotifyAll notifies all entries in the list. 

### <a id="notifyListNotifyOne" href="#notifyListNotifyOne">func notifyListNotifyOne(l *notifyList)</a>

```
searchKey: runtime.notifyListNotifyOne
```

```Go
func notifyListNotifyOne(l *notifyList)
```

notifyListNotifyOne notifies one entry in the list. 

### <a id="notifyListCheck" href="#notifyListCheck">func notifyListCheck(sz uintptr)</a>

```
searchKey: runtime.notifyListCheck
```

```Go
func notifyListCheck(sz uintptr)
```

### <a id="sync_nanotime" href="#sync_nanotime">func sync_nanotime() int64</a>

```
searchKey: runtime.sync_nanotime
```

```Go
func sync_nanotime() int64
```

### <a id="dumpregs" href="#dumpregs">func dumpregs(c *sigctxt)</a>

```
searchKey: runtime.dumpregs
```

```Go
func dumpregs(c *sigctxt)
```

### <a id="os_sigpipe" href="#os_sigpipe">func os_sigpipe()</a>

```
searchKey: runtime.os_sigpipe
```

```Go
func os_sigpipe()
```

### <a id="signame" href="#signame">func signame(sig uint32) string</a>

```
searchKey: runtime.signame
```

```Go
func signame(sig uint32) string
```

### <a id="init" href="#init">func init()</a>

```
searchKey: runtime.init
```

```Go
func init()
```

### <a id="initsig" href="#initsig">func initsig(preinit bool)</a>

```
searchKey: runtime.initsig
```

```Go
func initsig(preinit bool)
```

Initialize signals. Called by libpreinit so runtime may not be initialized. 

### <a id="sigInstallGoHandler" href="#sigInstallGoHandler">func sigInstallGoHandler(sig uint32) bool</a>

```
searchKey: runtime.sigInstallGoHandler
```

```Go
func sigInstallGoHandler(sig uint32) bool
```

### <a id="sigenable" href="#sigenable">func sigenable(sig uint32)</a>

```
searchKey: runtime.sigenable
```

```Go
func sigenable(sig uint32)
```

sigenable enables the Go signal handler to catch the signal sig. It is only called while holding the os/signal.handlers lock, via os/signal.enableSignal and signal_enable. 

### <a id="sigdisable" href="#sigdisable">func sigdisable(sig uint32)</a>

```
searchKey: runtime.sigdisable
```

```Go
func sigdisable(sig uint32)
```

sigdisable disables the Go signal handler for the signal sig. It is only called while holding the os/signal.handlers lock, via os/signal.disableSignal and signal_disable. 

### <a id="sigignore" href="#sigignore">func sigignore(sig uint32)</a>

```
searchKey: runtime.sigignore
```

```Go
func sigignore(sig uint32)
```

sigignore ignores the signal sig. It is only called while holding the os/signal.handlers lock, via os/signal.ignoreSignal and signal_ignore. 

### <a id="clearSignalHandlers" href="#clearSignalHandlers">func clearSignalHandlers()</a>

```
searchKey: runtime.clearSignalHandlers
```

```Go
func clearSignalHandlers()
```

clearSignalHandlers clears all signal handlers that are not ignored back to the default. This is called by the child after a fork, so that we can enable the signal mask for the exec without worrying about running a signal handler in the child. 

### <a id="setProcessCPUProfiler" href="#setProcessCPUProfiler">func setProcessCPUProfiler(hz int32)</a>

```
searchKey: runtime.setProcessCPUProfiler
```

```Go
func setProcessCPUProfiler(hz int32)
```

setProcessCPUProfiler is called when the profiling timer changes. It is called with prof.lock held. hz is the new timer, and is 0 if profiling is being disabled. Enable or disable the signal as required for -buildmode=c-archive. 

### <a id="setThreadCPUProfiler" href="#setThreadCPUProfiler">func setThreadCPUProfiler(hz int32)</a>

```
searchKey: runtime.setThreadCPUProfiler
```

```Go
func setThreadCPUProfiler(hz int32)
```

setThreadCPUProfiler makes any thread-specific changes required to implement profiling at a rate of hz. No changes required on Unix systems. 

### <a id="sigpipe" href="#sigpipe">func sigpipe()</a>

```
searchKey: runtime.sigpipe
```

```Go
func sigpipe()
```

### <a id="doSigPreempt" href="#doSigPreempt">func doSigPreempt(gp *g, ctxt *sigctxt)</a>

```
searchKey: runtime.doSigPreempt
```

```Go
func doSigPreempt(gp *g, ctxt *sigctxt)
```

doSigPreempt handles a preemption signal on gp. 

### <a id="preemptM" href="#preemptM">func preemptM(mp *m)</a>

```
searchKey: runtime.preemptM
```

```Go
func preemptM(mp *m)
```

preemptM sends a preemption request to mp. This request may be handled asynchronously and may be coalesced with other requests to the M. When the request is received, if the running G or P are marked for preemption and the goroutine is at an asynchronous safe-point, it will preempt the goroutine. It always atomically increments mp.preemptGen after handling a preemption request. 

### <a id="sigtrampgo" href="#sigtrampgo">func sigtrampgo(sig uint32, info *siginfo, ctx unsafe.Pointer)</a>

```
searchKey: runtime.sigtrampgo
```

```Go
func sigtrampgo(sig uint32, info *siginfo, ctx unsafe.Pointer)
```

sigtrampgo is called from the signal handler function, sigtramp, written in assembly code. This is called by the signal handler, and the world may be stopped. 

It must be nosplit because getg() is still the G that was running (if any) when the signal was delivered, but it's (usually) called on the gsignal stack. Until this switches the G to gsignal, the stack bounds check won't work. 

### <a id="adjustSignalStack" href="#adjustSignalStack">func adjustSignalStack(sig uint32, mp *m, gsigStack *gsignalStack) bool</a>

```
searchKey: runtime.adjustSignalStack
```

```Go
func adjustSignalStack(sig uint32, mp *m, gsigStack *gsignalStack) bool
```

adjustSignalStack adjusts the current stack guard based on the stack pointer that is actually in use while handling a signal. We do this in case some non-Go code called sigaltstack. This reports whether the stack was adjusted, and if so stores the old signal stack in *gsigstack. 

### <a id="sighandler" href="#sighandler">func sighandler(sig uint32, info *siginfo, ctxt unsafe.Pointer, gp *g)</a>

```
searchKey: runtime.sighandler
```

```Go
func sighandler(sig uint32, info *siginfo, ctxt unsafe.Pointer, gp *g)
```

sighandler is invoked when a signal occurs. The global g will be set to a gsignal goroutine and we will be running on the alternate signal stack. The parameter g will be the value of the global g when the signal occurred. The sig, info, and ctxt parameters are from the system signal handler: they are the parameters passed when the SA is passed to the sigaction system call. 

The garbage collector may have stopped the world, so write barriers are not allowed. 

### <a id="sigpanic" href="#sigpanic">func sigpanic()</a>

```
searchKey: runtime.sigpanic
```

```Go
func sigpanic()
```

sigpanic turns a synchronous signal into a run-time panic. If the signal handler sees a synchronous panic, it arranges the stack to look like the function where the signal occurred called sigpanic, sets the signal's PC value to sigpanic, and returns from the signal handler. The effect is that the program will act as though the function that got the signal simply called sigpanic instead. 

This must NOT be nosplit because the linker doesn't know where sigpanic calls can be injected. 

The signal handler must not inject a call to sigpanic if getg().throwsplit, since sigpanic may need to grow the stack. 

This is exported via linkname to assembly in runtime/cgo. 

### <a id="dieFromSignal" href="#dieFromSignal">func dieFromSignal(sig uint32)</a>

```
searchKey: runtime.dieFromSignal
```

```Go
func dieFromSignal(sig uint32)
```

dieFromSignal kills the program with a signal. This provides the expected exit status for the shell. This is only called with fatal signals expected to kill the process. 

### <a id="raisebadsignal" href="#raisebadsignal">func raisebadsignal(sig uint32, c *sigctxt)</a>

```
searchKey: runtime.raisebadsignal
```

```Go
func raisebadsignal(sig uint32, c *sigctxt)
```

raisebadsignal is called when a signal is received on a non-Go thread, and the Go program does not want to handle it (that is, the program has not called os/signal.Notify for the signal). 

### <a id="crash" href="#crash">func crash()</a>

```
searchKey: runtime.crash
```

```Go
func crash()
```

### <a id="ensureSigM" href="#ensureSigM">func ensureSigM()</a>

```
searchKey: runtime.ensureSigM
```

```Go
func ensureSigM()
```

ensureSigM starts one global, sleeping thread to make sure at least one thread is available to catch signals enabled for os/signal. 

### <a id="noSignalStack" href="#noSignalStack">func noSignalStack(sig uint32)</a>

```
searchKey: runtime.noSignalStack
```

```Go
func noSignalStack(sig uint32)
```

This is called when we receive a signal when there is no signal stack. This can only happen if non-Go code calls sigaltstack to disable the signal stack. 

### <a id="sigNotOnStack" href="#sigNotOnStack">func sigNotOnStack(sig uint32)</a>

```
searchKey: runtime.sigNotOnStack
```

```Go
func sigNotOnStack(sig uint32)
```

This is called if we receive a signal when there is a signal stack but we are not on it. This can only happen if non-Go code called sigaction without setting the SS_ONSTACK flag. 

### <a id="signalDuringFork" href="#signalDuringFork">func signalDuringFork(sig uint32)</a>

```
searchKey: runtime.signalDuringFork
```

```Go
func signalDuringFork(sig uint32)
```

signalDuringFork is called if we receive a signal while doing a fork. We do not want signals at that time, as a signal sent to the process group may be delivered to the child process, causing confusion. This should never be called, because we block signals across the fork; this function is just a safety check. See issue 18600 for background. 

### <a id="badsignal" href="#badsignal">func badsignal(sig uintptr, c *sigctxt)</a>

```
searchKey: runtime.badsignal
```

```Go
func badsignal(sig uintptr, c *sigctxt)
```

This runs on a foreign stack, without an m or a g. No stack split. 

### <a id="sigfwd" href="#sigfwd">func sigfwd(fn uintptr, sig uint32, info *siginfo, ctx unsafe.Pointer)</a>

```
searchKey: runtime.sigfwd
```

```Go
func sigfwd(fn uintptr, sig uint32, info *siginfo, ctx unsafe.Pointer)
```

### <a id="sigfwdgo" href="#sigfwdgo">func sigfwdgo(sig uint32, info *siginfo, ctx unsafe.Pointer) bool</a>

```
searchKey: runtime.sigfwdgo
```

```Go
func sigfwdgo(sig uint32, info *siginfo, ctx unsafe.Pointer) bool
```

Determines if the signal should be handled by Go and if not, forwards the signal to the handler that was installed before Go's. Returns whether the signal was forwarded. This is called by the signal handler, and the world may be stopped. 

### <a id="sigsave" href="#sigsave">func sigsave(p *sigset)</a>

```
searchKey: runtime.sigsave
```

```Go
func sigsave(p *sigset)
```

sigsave saves the current thread's signal mask into *p. This is used to preserve the non-Go signal mask when a non-Go thread calls a Go function. This is nosplit and nowritebarrierrec because it is called by needm which may be called on a non-Go thread with no g available. 

### <a id="msigrestore" href="#msigrestore">func msigrestore(sigmask sigset)</a>

```
searchKey: runtime.msigrestore
```

```Go
func msigrestore(sigmask sigset)
```

msigrestore sets the current thread's signal mask to sigmask. This is used to restore the non-Go signal mask when a non-Go thread calls a Go function. This is nosplit and nowritebarrierrec because it is called by dropm after g has been cleared. 

### <a id="sigblock" href="#sigblock">func sigblock(exiting bool)</a>

```
searchKey: runtime.sigblock
```

```Go
func sigblock(exiting bool)
```

sigblock blocks signals in the current thread's signal mask. This is used to block signals while setting up and tearing down g when a non-Go thread calls a Go function. When a thread is exiting we use the sigsetAllExiting value, otherwise the OS specific definition of sigset_all is used. This is nosplit and nowritebarrierrec because it is called by needm which may be called on a non-Go thread with no g available. 

### <a id="unblocksig" href="#unblocksig">func unblocksig(sig uint32)</a>

```
searchKey: runtime.unblocksig
```

```Go
func unblocksig(sig uint32)
```

unblocksig removes sig from the current thread's signal mask. This is nosplit and nowritebarrierrec because it is called from dieFromSignal, which can be called by sigfwdgo while running in the signal handler, on the signal stack, with no g available. 

### <a id="minitSignals" href="#minitSignals">func minitSignals()</a>

```
searchKey: runtime.minitSignals
```

```Go
func minitSignals()
```

minitSignals is called when initializing a new m to set the thread's alternate signal stack and signal mask. 

### <a id="minitSignalStack" href="#minitSignalStack">func minitSignalStack()</a>

```
searchKey: runtime.minitSignalStack
```

```Go
func minitSignalStack()
```

minitSignalStack is called when initializing a new m to set the alternate signal stack. If the alternate signal stack is not set for the thread (the normal case) then set the alternate signal stack to the gsignal stack. If the alternate signal stack is set for the thread (the case when a non-Go thread sets the alternate signal stack and then calls a Go function) then set the gsignal stack to the alternate signal stack. We also set the alternate signal stack to the gsignal stack if cgo is not used (regardless of whether it is already set). Record which choice was made in newSigstack, so that it can be undone in unminit. 

### <a id="minitSignalMask" href="#minitSignalMask">func minitSignalMask()</a>

```
searchKey: runtime.minitSignalMask
```

```Go
func minitSignalMask()
```

minitSignalMask is called when initializing a new m to set the thread's signal mask. When this is called all signals have been blocked for the thread.  This starts with m.sigmask, which was set either from initSigmask for a newly created thread or by calling sigsave if this is a non-Go thread calling a Go function. It removes all essential signals from the mask, thus causing those signals to not be blocked. Then it sets the thread's signal mask. After this is called the thread can receive signals. 

### <a id="unminitSignals" href="#unminitSignals">func unminitSignals()</a>

```
searchKey: runtime.unminitSignals
```

```Go
func unminitSignals()
```

unminitSignals is called from dropm, via unminit, to undo the effect of calling minit on a non-Go thread. 

### <a id="blockableSig" href="#blockableSig">func blockableSig(sig uint32) bool</a>

```
searchKey: runtime.blockableSig
```

```Go
func blockableSig(sig uint32) bool
```

blockableSig reports whether sig may be blocked by the signal mask. We never want to block the signals marked _SigUnblock; these are the synchronous signals that turn into a Go panic. In a Go program--not a c-archive/c-shared--we never want to block the signals marked _SigKill or _SigThrow, as otherwise it's possible for all running threads to block them and delay their delivery until we start a new thread. When linked into a C program we let the C code decide on the disposition of those signals. 

### <a id="setGsignalStack" href="#setGsignalStack">func setGsignalStack(st *stackt, old *gsignalStack)</a>

```
searchKey: runtime.setGsignalStack
```

```Go
func setGsignalStack(st *stackt, old *gsignalStack)
```

setGsignalStack sets the gsignal stack of the current m to an alternate signal stack returned from the sigaltstack system call. It saves the old values in *old for use by restoreGsignalStack. This is used when handling a signal if non-Go code has set the alternate signal stack. 

### <a id="restoreGsignalStack" href="#restoreGsignalStack">func restoreGsignalStack(st *gsignalStack)</a>

```
searchKey: runtime.restoreGsignalStack
```

```Go
func restoreGsignalStack(st *gsignalStack)
```

restoreGsignalStack restores the gsignal stack to the value it had before entering the signal handler. 

### <a id="signalstack" href="#signalstack">func signalstack(s *stack)</a>

```
searchKey: runtime.signalstack
```

```Go
func signalstack(s *stack)
```

signalstack sets the current thread's alternate signal stack to s. 

### <a id="setsigsegv" href="#setsigsegv">func setsigsegv(pc uintptr)</a>

```
searchKey: runtime.setsigsegv
```

```Go
func setsigsegv(pc uintptr)
```

setsigsegv is used on darwin/arm64 to fake a segmentation fault. 

This is exported via linkname to assembly in runtime/cgo. 

### <a id="sigsend" href="#sigsend">func sigsend(s uint32) bool</a>

```
searchKey: runtime.sigsend
```

```Go
func sigsend(s uint32) bool
```

sigsend delivers a signal from sighandler to the internal signal delivery queue. It reports whether the signal was sent. If not, the caller typically crashes the program. It runs from the signal handler, so it's limited in what it can do. 

### <a id="sigRecvPrepareForFixup" href="#sigRecvPrepareForFixup">func sigRecvPrepareForFixup()</a>

```
searchKey: runtime.sigRecvPrepareForFixup
```

```Go
func sigRecvPrepareForFixup()
```

sigRecvPrepareForFixup is used to temporarily wake up the signal_recv() running thread while it is blocked waiting for the arrival of a signal. If it causes the thread to wake up, the sig.state travels through this sequence: sigReceiving -> sigFixup -> sigIdle -> sigReceiving and resumes. (This is only called while GC is disabled.) 

### <a id="signal_recv" href="#signal_recv">func signal_recv() uint32</a>

```
searchKey: runtime.signal_recv
```

```Go
func signal_recv() uint32
```

Called to receive the next queued signal. Must only be called from a single goroutine at a time. 

### <a id="signalWaitUntilIdle" href="#signalWaitUntilIdle">func signalWaitUntilIdle()</a>

```
searchKey: runtime.signalWaitUntilIdle
```

```Go
func signalWaitUntilIdle()
```

signalWaitUntilIdle waits until the signal delivery mechanism is idle. This is used to ensure that we do not drop a signal notification due to a race between disabling a signal and receiving a signal. This assumes that signal delivery has already been disabled for the signal(s) in question, and here we are just waiting to make sure that all the signals have been delivered to the user channels by the os/signal package. 

### <a id="signal_enable" href="#signal_enable">func signal_enable(s uint32)</a>

```
searchKey: runtime.signal_enable
```

```Go
func signal_enable(s uint32)
```

Must only be called from a single goroutine at a time. 

### <a id="signal_disable" href="#signal_disable">func signal_disable(s uint32)</a>

```
searchKey: runtime.signal_disable
```

```Go
func signal_disable(s uint32)
```

Must only be called from a single goroutine at a time. 

### <a id="signal_ignore" href="#signal_ignore">func signal_ignore(s uint32)</a>

```
searchKey: runtime.signal_ignore
```

```Go
func signal_ignore(s uint32)
```

Must only be called from a single goroutine at a time. 

### <a id="sigInitIgnored" href="#sigInitIgnored">func sigInitIgnored(s uint32)</a>

```
searchKey: runtime.sigInitIgnored
```

```Go
func sigInitIgnored(s uint32)
```

sigInitIgnored marks the signal as already ignored. This is called at program start by initsig. In a shared library initsig is called by libpreinit, so the runtime may not be initialized yet. 

### <a id="signal_ignored" href="#signal_ignored">func signal_ignored(s uint32) bool</a>

```
searchKey: runtime.signal_ignored
```

```Go
func signal_ignored(s uint32) bool
```

Checked by signal handlers. 

### <a id="panicmakeslicelen" href="#panicmakeslicelen">func panicmakeslicelen()</a>

```
searchKey: runtime.panicmakeslicelen
```

```Go
func panicmakeslicelen()
```

### <a id="panicmakeslicecap" href="#panicmakeslicecap">func panicmakeslicecap()</a>

```
searchKey: runtime.panicmakeslicecap
```

```Go
func panicmakeslicecap()
```

### <a id="makeslicecopy" href="#makeslicecopy">func makeslicecopy(et *_type, tolen int, fromlen int, from unsafe.Pointer) unsafe.Pointer</a>

```
searchKey: runtime.makeslicecopy
```

```Go
func makeslicecopy(et *_type, tolen int, fromlen int, from unsafe.Pointer) unsafe.Pointer
```

makeslicecopy allocates a slice of "tolen" elements of type "et", then copies "fromlen" elements of type "et" into that new allocation from "from". 

### <a id="makeslice" href="#makeslice">func makeslice(et *_type, len, cap int) unsafe.Pointer</a>

```
searchKey: runtime.makeslice
```

```Go
func makeslice(et *_type, len, cap int) unsafe.Pointer
```

### <a id="makeslice64" href="#makeslice64">func makeslice64(et *_type, len64, cap64 int64) unsafe.Pointer</a>

```
searchKey: runtime.makeslice64
```

```Go
func makeslice64(et *_type, len64, cap64 int64) unsafe.Pointer
```

### <a id="unsafeslice" href="#unsafeslice">func unsafeslice(et *_type, len int)</a>

```
searchKey: runtime.unsafeslice
```

```Go
func unsafeslice(et *_type, len int)
```

### <a id="unsafeslice64" href="#unsafeslice64">func unsafeslice64(et *_type, len64 int64)</a>

```
searchKey: runtime.unsafeslice64
```

```Go
func unsafeslice64(et *_type, len64 int64)
```

### <a id="panicunsafeslicelen" href="#panicunsafeslicelen">func panicunsafeslicelen()</a>

```
searchKey: runtime.panicunsafeslicelen
```

```Go
func panicunsafeslicelen()
```

### <a id="isPowerOfTwo" href="#isPowerOfTwo">func isPowerOfTwo(x uintptr) bool</a>

```
searchKey: runtime.isPowerOfTwo
```

```Go
func isPowerOfTwo(x uintptr) bool
```

### <a id="slicecopy" href="#slicecopy">func slicecopy(toPtr unsafe.Pointer, toLen int, fromPtr unsafe.Pointer, fromLen int, width uintptr) int</a>

```
searchKey: runtime.slicecopy
```

```Go
func slicecopy(toPtr unsafe.Pointer, toLen int, fromPtr unsafe.Pointer, fromLen int, width uintptr) int
```

slicecopy is used to copy from a string or slice of pointerless elements into a slice. 

### <a id="funpack64" href="#funpack64">func funpack64(f uint64) (sign, mant uint64, exp int, inf, nan bool)</a>

```
searchKey: runtime.funpack64
```

```Go
func funpack64(f uint64) (sign, mant uint64, exp int, inf, nan bool)
```

### <a id="funpack32" href="#funpack32">func funpack32(f uint32) (sign, mant uint32, exp int, inf, nan bool)</a>

```
searchKey: runtime.funpack32
```

```Go
func funpack32(f uint32) (sign, mant uint32, exp int, inf, nan bool)
```

### <a id="fpack64" href="#fpack64">func fpack64(sign, mant uint64, exp int, trunc uint64) uint64</a>

```
searchKey: runtime.fpack64
```

```Go
func fpack64(sign, mant uint64, exp int, trunc uint64) uint64
```

### <a id="fpack32" href="#fpack32">func fpack32(sign, mant uint32, exp int, trunc uint32) uint32</a>

```
searchKey: runtime.fpack32
```

```Go
func fpack32(sign, mant uint32, exp int, trunc uint32) uint32
```

### <a id="fadd64" href="#fadd64">func fadd64(f, g uint64) uint64</a>

```
searchKey: runtime.fadd64
```

```Go
func fadd64(f, g uint64) uint64
```

### <a id="fsub64" href="#fsub64">func fsub64(f, g uint64) uint64</a>

```
searchKey: runtime.fsub64
```

```Go
func fsub64(f, g uint64) uint64
```

### <a id="fneg64" href="#fneg64">func fneg64(f uint64) uint64</a>

```
searchKey: runtime.fneg64
```

```Go
func fneg64(f uint64) uint64
```

### <a id="fmul64" href="#fmul64">func fmul64(f, g uint64) uint64</a>

```
searchKey: runtime.fmul64
```

```Go
func fmul64(f, g uint64) uint64
```

### <a id="fdiv64" href="#fdiv64">func fdiv64(f, g uint64) uint64</a>

```
searchKey: runtime.fdiv64
```

```Go
func fdiv64(f, g uint64) uint64
```

### <a id="f64to32" href="#f64to32">func f64to32(f uint64) uint32</a>

```
searchKey: runtime.f64to32
```

```Go
func f64to32(f uint64) uint32
```

### <a id="f32to64" href="#f32to64">func f32to64(f uint32) uint64</a>

```
searchKey: runtime.f32to64
```

```Go
func f32to64(f uint32) uint64
```

### <a id="fcmp64" href="#fcmp64">func fcmp64(f, g uint64) (cmp int32, isnan bool)</a>

```
searchKey: runtime.fcmp64
```

```Go
func fcmp64(f, g uint64) (cmp int32, isnan bool)
```

### <a id="f64toint" href="#f64toint">func f64toint(f uint64) (val int64, ok bool)</a>

```
searchKey: runtime.f64toint
```

```Go
func f64toint(f uint64) (val int64, ok bool)
```

### <a id="fintto64" href="#fintto64">func fintto64(val int64) (f uint64)</a>

```
searchKey: runtime.fintto64
```

```Go
func fintto64(val int64) (f uint64)
```

### <a id="mullu" href="#mullu">func mullu(u, v uint64) (lo, hi uint64)</a>

```
searchKey: runtime.mullu
```

```Go
func mullu(u, v uint64) (lo, hi uint64)
```

64x64 -> 128 multiply. adapted from hacker's delight. 

### <a id="divlu" href="#divlu">func divlu(u1, u0, v uint64) (q, r uint64)</a>

```
searchKey: runtime.divlu
```

```Go
func divlu(u1, u0, v uint64) (q, r uint64)
```

128/64 -> 64 quotient, 64 remainder. adapted from hacker's delight 

### <a id="fadd32" href="#fadd32">func fadd32(x, y uint32) uint32</a>

```
searchKey: runtime.fadd32
```

```Go
func fadd32(x, y uint32) uint32
```

### <a id="fmul32" href="#fmul32">func fmul32(x, y uint32) uint32</a>

```
searchKey: runtime.fmul32
```

```Go
func fmul32(x, y uint32) uint32
```

### <a id="fdiv32" href="#fdiv32">func fdiv32(x, y uint32) uint32</a>

```
searchKey: runtime.fdiv32
```

```Go
func fdiv32(x, y uint32) uint32
```

### <a id="feq32" href="#feq32">func feq32(x, y uint32) bool</a>

```
searchKey: runtime.feq32
```

```Go
func feq32(x, y uint32) bool
```

### <a id="fgt32" href="#fgt32">func fgt32(x, y uint32) bool</a>

```
searchKey: runtime.fgt32
```

```Go
func fgt32(x, y uint32) bool
```

### <a id="fge32" href="#fge32">func fge32(x, y uint32) bool</a>

```
searchKey: runtime.fge32
```

```Go
func fge32(x, y uint32) bool
```

### <a id="feq64" href="#feq64">func feq64(x, y uint64) bool</a>

```
searchKey: runtime.feq64
```

```Go
func feq64(x, y uint64) bool
```

### <a id="fgt64" href="#fgt64">func fgt64(x, y uint64) bool</a>

```
searchKey: runtime.fgt64
```

```Go
func fgt64(x, y uint64) bool
```

### <a id="fge64" href="#fge64">func fge64(x, y uint64) bool</a>

```
searchKey: runtime.fge64
```

```Go
func fge64(x, y uint64) bool
```

### <a id="fint32to32" href="#fint32to32">func fint32to32(x int32) uint32</a>

```
searchKey: runtime.fint32to32
```

```Go
func fint32to32(x int32) uint32
```

### <a id="fint32to64" href="#fint32to64">func fint32to64(x int32) uint64</a>

```
searchKey: runtime.fint32to64
```

```Go
func fint32to64(x int32) uint64
```

### <a id="fint64to32" href="#fint64to32">func fint64to32(x int64) uint32</a>

```
searchKey: runtime.fint64to32
```

```Go
func fint64to32(x int64) uint32
```

### <a id="fint64to64" href="#fint64to64">func fint64to64(x int64) uint64</a>

```
searchKey: runtime.fint64to64
```

```Go
func fint64to64(x int64) uint64
```

### <a id="f32toint32" href="#f32toint32">func f32toint32(x uint32) int32</a>

```
searchKey: runtime.f32toint32
```

```Go
func f32toint32(x uint32) int32
```

### <a id="f32toint64" href="#f32toint64">func f32toint64(x uint32) int64</a>

```
searchKey: runtime.f32toint64
```

```Go
func f32toint64(x uint32) int64
```

### <a id="f64toint32" href="#f64toint32">func f64toint32(x uint64) int32</a>

```
searchKey: runtime.f64toint32
```

```Go
func f64toint32(x uint64) int32
```

### <a id="f64toint64" href="#f64toint64">func f64toint64(x uint64) int64</a>

```
searchKey: runtime.f64toint64
```

```Go
func f64toint64(x uint64) int64
```

### <a id="f64touint64" href="#f64touint64">func f64touint64(x float64) uint64</a>

```
searchKey: runtime.f64touint64
```

```Go
func f64touint64(x float64) uint64
```

### <a id="f32touint64" href="#f32touint64">func f32touint64(x float32) uint64</a>

```
searchKey: runtime.f32touint64
```

```Go
func f32touint64(x float32) uint64
```

### <a id="fuint64to64" href="#fuint64to64">func fuint64to64(x uint64) float64</a>

```
searchKey: runtime.fuint64to64
```

```Go
func fuint64to64(x uint64) float64
```

### <a id="fuint64to32" href="#fuint64to32">func fuint64to32(x uint64) float32</a>

```
searchKey: runtime.fuint64to32
```

```Go
func fuint64to32(x uint64) float32
```

### <a id="stackinit" href="#stackinit">func stackinit()</a>

```
searchKey: runtime.stackinit
```

```Go
func stackinit()
```

### <a id="stacklog2" href="#stacklog2">func stacklog2(n uintptr) int</a>

```
searchKey: runtime.stacklog2
```

```Go
func stacklog2(n uintptr) int
```

stacklog2 returns ⌊log_2(n)⌋. 

### <a id="stackpoolfree" href="#stackpoolfree">func stackpoolfree(x gclinkptr, order uint8)</a>

```
searchKey: runtime.stackpoolfree
```

```Go
func stackpoolfree(x gclinkptr, order uint8)
```

Adds stack x to the free pool. Must be called with stackpool[order].item.mu held. 

### <a id="stackcacherefill" href="#stackcacherefill">func stackcacherefill(c *mcache, order uint8)</a>

```
searchKey: runtime.stackcacherefill
```

```Go
func stackcacherefill(c *mcache, order uint8)
```

stackcacherefill/stackcacherelease implement a global pool of stack segments. The pool is required to prevent unlimited growth of per-thread caches. 

### <a id="stackcacherelease" href="#stackcacherelease">func stackcacherelease(c *mcache, order uint8)</a>

```
searchKey: runtime.stackcacherelease
```

```Go
func stackcacherelease(c *mcache, order uint8)
```

### <a id="stackcache_clear" href="#stackcache_clear">func stackcache_clear(c *mcache)</a>

```
searchKey: runtime.stackcache_clear
```

```Go
func stackcache_clear(c *mcache)
```

### <a id="stackfree" href="#stackfree">func stackfree(stk stack)</a>

```
searchKey: runtime.stackfree
```

```Go
func stackfree(stk stack)
```

stackfree frees an n byte stack allocation at stk. 

stackfree must run on the system stack because it uses per-P resources and must not split the stack. 

### <a id="adjustpointer" href="#adjustpointer">func adjustpointer(adjinfo *adjustinfo, vpp unsafe.Pointer)</a>

```
searchKey: runtime.adjustpointer
```

```Go
func adjustpointer(adjinfo *adjustinfo, vpp unsafe.Pointer)
```

Adjustpointer checks whether *vpp is in the old stack described by adjinfo. If so, it rewrites *vpp to point into the new stack. 

### <a id="adjustpointers" href="#adjustpointers">func adjustpointers(scanp unsafe.Pointer, bv *bitvector, adjinfo *adjustinfo, f funcInfo)</a>

```
searchKey: runtime.adjustpointers
```

```Go
func adjustpointers(scanp unsafe.Pointer, bv *bitvector, adjinfo *adjustinfo, f funcInfo)
```

bv describes the memory starting at address scanp. Adjust any pointers contained therein. 

### <a id="adjustframe" href="#adjustframe">func adjustframe(frame *stkframe, arg unsafe.Pointer) bool</a>

```
searchKey: runtime.adjustframe
```

```Go
func adjustframe(frame *stkframe, arg unsafe.Pointer) bool
```

Note: the argument/return area is adjusted by the callee. 

### <a id="adjustctxt" href="#adjustctxt">func adjustctxt(gp *g, adjinfo *adjustinfo)</a>

```
searchKey: runtime.adjustctxt
```

```Go
func adjustctxt(gp *g, adjinfo *adjustinfo)
```

### <a id="adjustdefers" href="#adjustdefers">func adjustdefers(gp *g, adjinfo *adjustinfo)</a>

```
searchKey: runtime.adjustdefers
```

```Go
func adjustdefers(gp *g, adjinfo *adjustinfo)
```

### <a id="adjustpanics" href="#adjustpanics">func adjustpanics(gp *g, adjinfo *adjustinfo)</a>

```
searchKey: runtime.adjustpanics
```

```Go
func adjustpanics(gp *g, adjinfo *adjustinfo)
```

### <a id="adjustsudogs" href="#adjustsudogs">func adjustsudogs(gp *g, adjinfo *adjustinfo)</a>

```
searchKey: runtime.adjustsudogs
```

```Go
func adjustsudogs(gp *g, adjinfo *adjustinfo)
```

### <a id="fillstack" href="#fillstack">func fillstack(stk stack, b byte)</a>

```
searchKey: runtime.fillstack
```

```Go
func fillstack(stk stack, b byte)
```

### <a id="findsghi" href="#findsghi">func findsghi(gp *g, stk stack) uintptr</a>

```
searchKey: runtime.findsghi
```

```Go
func findsghi(gp *g, stk stack) uintptr
```

### <a id="syncadjustsudogs" href="#syncadjustsudogs">func syncadjustsudogs(gp *g, used uintptr, adjinfo *adjustinfo) uintptr</a>

```
searchKey: runtime.syncadjustsudogs
```

```Go
func syncadjustsudogs(gp *g, used uintptr, adjinfo *adjustinfo) uintptr
```

syncadjustsudogs adjusts gp's sudogs and copies the part of gp's stack they refer to while synchronizing with concurrent channel operations. It returns the number of bytes of stack copied. 

### <a id="copystack" href="#copystack">func copystack(gp *g, newsize uintptr)</a>

```
searchKey: runtime.copystack
```

```Go
func copystack(gp *g, newsize uintptr)
```

Copies gp's stack to a new stack of a different size. Caller must have changed gp status to Gcopystack. 

### <a id="round2" href="#round2">func round2(x int32) int32</a>

```
searchKey: runtime.round2
```

```Go
func round2(x int32) int32
```

round x up to a power of 2. 

### <a id="newstack" href="#newstack">func newstack()</a>

```
searchKey: runtime.newstack
```

```Go
func newstack()
```

Called from runtime·morestack when more stack is needed. Allocate larger stack and relocate to new stack. Stack growth is multiplicative, for constant amortized cost. 

g->atomicstatus will be Grunning or Gscanrunning upon entry. If the scheduler is trying to stop this g, then it will set preemptStop. 

This must be nowritebarrierrec because it can be called as part of stack growth from other nowritebarrierrec functions, but the compiler doesn't check this. 

### <a id="nilfunc" href="#nilfunc">func nilfunc()</a>

```
searchKey: runtime.nilfunc
```

```Go
func nilfunc()
```

### <a id="gostartcallfn" href="#gostartcallfn">func gostartcallfn(gobuf *gobuf, fv *funcval)</a>

```
searchKey: runtime.gostartcallfn
```

```Go
func gostartcallfn(gobuf *gobuf, fv *funcval)
```

adjust Gobuf as if it executed a call to fn and then stopped before the first instruction in fn. 

### <a id="isShrinkStackSafe" href="#isShrinkStackSafe">func isShrinkStackSafe(gp *g) bool</a>

```
searchKey: runtime.isShrinkStackSafe
```

```Go
func isShrinkStackSafe(gp *g) bool
```

isShrinkStackSafe returns whether it's safe to attempt to shrink gp's stack. Shrinking the stack is only safe when we have precise pointer maps for all frames on the stack. 

### <a id="shrinkstack" href="#shrinkstack">func shrinkstack(gp *g)</a>

```
searchKey: runtime.shrinkstack
```

```Go
func shrinkstack(gp *g)
```

Maybe shrink the stack being used by gp. 

gp must be stopped and we must own its stack. It may be in _Grunning, but only if this is our own user G. 

### <a id="freeStackSpans" href="#freeStackSpans">func freeStackSpans()</a>

```
searchKey: runtime.freeStackSpans
```

```Go
func freeStackSpans()
```

freeStackSpans frees unused stack spans at the end of GC. 

### <a id="init" href="#init">func init()</a>

```
searchKey: runtime.init
```

```Go
func init()
```

### <a id="morestackc" href="#morestackc">func morestackc()</a>

```
searchKey: runtime.morestackc
```

```Go
func morestackc()
```

This is exported as ABI0 via linkname so obj can call it. 

### <a id="concatstrings" href="#concatstrings">func concatstrings(buf *tmpBuf, a []string) string</a>

```
searchKey: runtime.concatstrings
```

```Go
func concatstrings(buf *tmpBuf, a []string) string
```

concatstrings implements a Go string concatenation x+y+z+... The operands are passed in the slice a. If buf != nil, the compiler has determined that the result does not escape the calling function, so the string data can be stored in buf if small enough. 

### <a id="concatstring2" href="#concatstring2">func concatstring2(buf *tmpBuf, a0, a1 string) string</a>

```
searchKey: runtime.concatstring2
```

```Go
func concatstring2(buf *tmpBuf, a0, a1 string) string
```

### <a id="concatstring3" href="#concatstring3">func concatstring3(buf *tmpBuf, a0, a1, a2 string) string</a>

```
searchKey: runtime.concatstring3
```

```Go
func concatstring3(buf *tmpBuf, a0, a1, a2 string) string
```

### <a id="concatstring4" href="#concatstring4">func concatstring4(buf *tmpBuf, a0, a1, a2, a3 string) string</a>

```
searchKey: runtime.concatstring4
```

```Go
func concatstring4(buf *tmpBuf, a0, a1, a2, a3 string) string
```

### <a id="concatstring5" href="#concatstring5">func concatstring5(buf *tmpBuf, a0, a1, a2, a3, a4 string) string</a>

```
searchKey: runtime.concatstring5
```

```Go
func concatstring5(buf *tmpBuf, a0, a1, a2, a3, a4 string) string
```

### <a id="slicebytetostring" href="#slicebytetostring">func slicebytetostring(buf *tmpBuf, ptr *byte, n int) (str string)</a>

```
searchKey: runtime.slicebytetostring
```

```Go
func slicebytetostring(buf *tmpBuf, ptr *byte, n int) (str string)
```

slicebytetostring converts a byte slice to a string. It is inserted by the compiler into generated code. ptr is a pointer to the first element of the slice; n is the length of the slice. Buf is a fixed-size buffer for the result, it is not nil if the result does not escape. 

### <a id="stringDataOnStack" href="#stringDataOnStack">func stringDataOnStack(s string) bool</a>

```
searchKey: runtime.stringDataOnStack
```

```Go
func stringDataOnStack(s string) bool
```

stringDataOnStack reports whether the string's data is stored on the current goroutine's stack. 

### <a id="rawstringtmp" href="#rawstringtmp">func rawstringtmp(buf *tmpBuf, l int) (s string, b []byte)</a>

```
searchKey: runtime.rawstringtmp
```

```Go
func rawstringtmp(buf *tmpBuf, l int) (s string, b []byte)
```

### <a id="slicebytetostringtmp" href="#slicebytetostringtmp">func slicebytetostringtmp(ptr *byte, n int) (str string)</a>

```
searchKey: runtime.slicebytetostringtmp
```

```Go
func slicebytetostringtmp(ptr *byte, n int) (str string)
```

slicebytetostringtmp returns a "string" referring to the actual []byte bytes. 

Callers need to ensure that the returned string will not be used after the calling goroutine modifies the original slice or synchronizes with another goroutine. 

The function is only called when instrumenting and otherwise intrinsified by the compiler. 

Some internal compiler optimizations use this function. - Used for m[T1{... Tn{..., string(k), ...} ...}] and m[string(k)] 

```
where k is []byte, T1 to Tn is a nesting of struct and array literals.

```
- Used for "<"+string(b)+">" concatenation where b is []byte. - Used for string(b)=="foo" comparison where b is []byte. 

### <a id="stringtoslicebyte" href="#stringtoslicebyte">func stringtoslicebyte(buf *tmpBuf, s string) []byte</a>

```
searchKey: runtime.stringtoslicebyte
```

```Go
func stringtoslicebyte(buf *tmpBuf, s string) []byte
```

### <a id="stringtoslicerune" href="#stringtoslicerune">func stringtoslicerune(buf *[tmpStringBufSize]rune, s string) []rune</a>

```
searchKey: runtime.stringtoslicerune
```

```Go
func stringtoslicerune(buf *[tmpStringBufSize]rune, s string) []rune
```

### <a id="slicerunetostring" href="#slicerunetostring">func slicerunetostring(buf *tmpBuf, a []rune) string</a>

```
searchKey: runtime.slicerunetostring
```

```Go
func slicerunetostring(buf *tmpBuf, a []rune) string
```

### <a id="intstring" href="#intstring">func intstring(buf *[4]byte, v int64) (s string)</a>

```
searchKey: runtime.intstring
```

```Go
func intstring(buf *[4]byte, v int64) (s string)
```

### <a id="rawstring" href="#rawstring">func rawstring(size int) (s string, b []byte)</a>

```
searchKey: runtime.rawstring
```

```Go
func rawstring(size int) (s string, b []byte)
```

rawstring allocates storage for a new string. The returned string and byte slice both refer to the same storage. The storage is not zeroed. Callers should use b to set the string contents and then drop b. 

### <a id="rawbyteslice" href="#rawbyteslice">func rawbyteslice(size int) (b []byte)</a>

```
searchKey: runtime.rawbyteslice
```

```Go
func rawbyteslice(size int) (b []byte)
```

rawbyteslice allocates a new byte slice. The byte slice is not zeroed. 

### <a id="rawruneslice" href="#rawruneslice">func rawruneslice(size int) (b []rune)</a>

```
searchKey: runtime.rawruneslice
```

```Go
func rawruneslice(size int) (b []rune)
```

rawruneslice allocates a new rune slice. The rune slice is not zeroed. 

### <a id="gobytes" href="#gobytes">func gobytes(p *byte, n int) (b []byte)</a>

```
searchKey: runtime.gobytes
```

```Go
func gobytes(p *byte, n int) (b []byte)
```

used by cmd/cgo 

### <a id="gostring" href="#gostring">func gostring(p *byte) string</a>

```
searchKey: runtime.gostring
```

```Go
func gostring(p *byte) string
```

This is exported via linkname to assembly in syscall (for Plan9). 

### <a id="gostringn" href="#gostringn">func gostringn(p *byte, l int) string</a>

```
searchKey: runtime.gostringn
```

```Go
func gostringn(p *byte, l int) string
```

### <a id="hasPrefix" href="#hasPrefix">func hasPrefix(s, prefix string) bool</a>

```
searchKey: runtime.hasPrefix
```

```Go
func hasPrefix(s, prefix string) bool
```

### <a id="atoi" href="#atoi">func atoi(s string) (int, bool)</a>

```
searchKey: runtime.atoi
```

```Go
func atoi(s string) (int, bool)
```

atoi parses an int from a string s. The bool result reports whether s is a number representable by a value of type int. 

### <a id="atoi32" href="#atoi32">func atoi32(s string) (int32, bool)</a>

```
searchKey: runtime.atoi32
```

```Go
func atoi32(s string) (int32, bool)
```

atoi32 is like atoi but for integers that fit into an int32. 

### <a id="findnull" href="#findnull">func findnull(s *byte) int</a>

```
searchKey: runtime.findnull
```

```Go
func findnull(s *byte) int
```

### <a id="findnullw" href="#findnullw">func findnullw(s *uint16) int</a>

```
searchKey: runtime.findnullw
```

```Go
func findnullw(s *uint16) int
```

### <a id="gostringnocopy" href="#gostringnocopy">func gostringnocopy(str *byte) string</a>

```
searchKey: runtime.gostringnocopy
```

```Go
func gostringnocopy(str *byte) string
```

### <a id="gostringw" href="#gostringw">func gostringw(strw *uint16) string</a>

```
searchKey: runtime.gostringw
```

```Go
func gostringw(strw *uint16) string
```

### <a id="add" href="#add">func add(p unsafe.Pointer, x uintptr) unsafe.Pointer</a>

```
searchKey: runtime.add
```

```Go
func add(p unsafe.Pointer, x uintptr) unsafe.Pointer
```

Should be a built-in for unsafe.Pointer? 

### <a id="mcall" href="#mcall">func mcall(fn func(*g))</a>

```
searchKey: runtime.mcall
```

```Go
func mcall(fn func(*g))
```

mcall switches from the g to the g0 stack and invokes fn(g), where g is the goroutine that made the call. mcall saves g's current PC/SP in g->sched so that it can be restored later. It is up to fn to arrange for that later execution, typically by recording g in a data structure, causing something to call ready(g) later. mcall returns to the original goroutine g later, when g has been rescheduled. fn must not return at all; typically it ends by calling schedule, to let the m run other goroutines. 

mcall can only be called from g stacks (not g0, not gsignal). 

This must NOT be go:noescape: if fn is a stack-allocated closure, fn puts g on a run queue, and g executes before fn returns, the closure will be invalidated while it is still executing. 

### <a id="systemstack" href="#systemstack">func systemstack(fn func())</a>

```
searchKey: runtime.systemstack
```

```Go
func systemstack(fn func())
```

systemstack runs fn on a system stack. If systemstack is called from the per-OS-thread (g0) stack, or if systemstack is called from the signal handling (gsignal) stack, systemstack calls fn directly and returns. Otherwise, systemstack is being called from the limited stack of an ordinary goroutine. In this case, systemstack switches to the per-OS-thread stack, calls fn, and switches back. It is common to use a func literal as the argument, in order to share inputs and outputs with the code around the call to system stack: 

```
... set up y ...
systemstack(func() {
	x = bigcall(y)
})
... use x ...

```
### <a id="badsystemstack" href="#badsystemstack">func badsystemstack()</a>

```
searchKey: runtime.badsystemstack
```

```Go
func badsystemstack()
```

### <a id="memclrNoHeapPointers" href="#memclrNoHeapPointers">func memclrNoHeapPointers(ptr unsafe.Pointer, n uintptr)</a>

```
searchKey: runtime.memclrNoHeapPointers
```

```Go
func memclrNoHeapPointers(ptr unsafe.Pointer, n uintptr)
```

memclrNoHeapPointers clears n bytes starting at ptr. 

Usually you should use typedmemclr. memclrNoHeapPointers should be used only when the caller knows that *ptr contains no heap pointers because either: 

*ptr is initialized memory and its type is pointer-free, or 

*ptr is uninitialized memory (e.g., memory that's being reused for a new allocation) and hence contains only "junk". 

memclrNoHeapPointers ensures that if ptr is pointer-aligned, and n is a multiple of the pointer size, then any pointer-aligned, pointer-sized portion is cleared atomically. Despite the function name, this is necessary because this function is the underlying implementation of typedmemclr and memclrHasPointers. See the doc of memmove for more details. 

The (CPU-specific) implementations of this function are in memclr_*.s. 

### <a id="reflect_memclrNoHeapPointers" href="#reflect_memclrNoHeapPointers">func reflect_memclrNoHeapPointers(ptr unsafe.Pointer, n uintptr)</a>

```
searchKey: runtime.reflect_memclrNoHeapPointers
```

```Go
func reflect_memclrNoHeapPointers(ptr unsafe.Pointer, n uintptr)
```

### <a id="memmove" href="#memmove">func memmove(to, from unsafe.Pointer, n uintptr)</a>

```
searchKey: runtime.memmove
```

```Go
func memmove(to, from unsafe.Pointer, n uintptr)
```

memmove copies n bytes from "from" to "to". 

memmove ensures that any pointer in "from" is written to "to" with an indivisible write, so that racy reads cannot observe a half-written pointer. This is necessary to prevent the garbage collector from observing invalid pointers, and differs from memmove in unmanaged languages. However, memmove is only required to do this if "from" and "to" may contain pointers, which can only be the case if "from", "to", and "n" are all be word-aligned. 

Implementations are in memmove_*.s. 

### <a id="reflect_memmove" href="#reflect_memmove">func reflect_memmove(to, from unsafe.Pointer, n uintptr)</a>

```
searchKey: runtime.reflect_memmove
```

```Go
func reflect_memmove(to, from unsafe.Pointer, n uintptr)
```

### <a id="fastrand" href="#fastrand">func fastrand() uint32</a>

```
searchKey: runtime.fastrand
```

```Go
func fastrand() uint32
```

### <a id="fastrandn" href="#fastrandn">func fastrandn(n uint32) uint32</a>

```
searchKey: runtime.fastrandn
```

```Go
func fastrandn(n uint32) uint32
```

### <a id="sync_fastrand" href="#sync_fastrand">func sync_fastrand() uint32</a>

```
searchKey: runtime.sync_fastrand
```

```Go
func sync_fastrand() uint32
```

### <a id="net_fastrand" href="#net_fastrand">func net_fastrand() uint32</a>

```
searchKey: runtime.net_fastrand
```

```Go
func net_fastrand() uint32
```

### <a id="os_fastrand" href="#os_fastrand">func os_fastrand() uint32</a>

```
searchKey: runtime.os_fastrand
```

```Go
func os_fastrand() uint32
```

### <a id="memequal" href="#memequal">func memequal(a, b unsafe.Pointer, size uintptr) bool</a>

```
searchKey: runtime.memequal
```

```Go
func memequal(a, b unsafe.Pointer, size uintptr) bool
```

in internal/bytealg/equal_*.s 

### <a id="noescape" href="#noescape">func noescape(p unsafe.Pointer) unsafe.Pointer</a>

```
searchKey: runtime.noescape
```

```Go
func noescape(p unsafe.Pointer) unsafe.Pointer
```

noescape hides a pointer from escape analysis.  noescape is the identity function but escape analysis doesn't think the output depends on the input.  noescape is inlined and currently compiles down to zero instructions. USE CAREFULLY! 

### <a id="cgocallback" href="#cgocallback">func cgocallback(fn, frame, ctxt uintptr)</a>

```
searchKey: runtime.cgocallback
```

```Go
func cgocallback(fn, frame, ctxt uintptr)
```

Not all cgocallback frames are actually cgocallback, so not all have these arguments. Mark them uintptr so that the GC does not misinterpret memory when the arguments are not present. cgocallback is not called from Go, only from crosscall2. This in turn calls cgocallbackg, which is where we'll find pointer-declared arguments. 

### <a id="gogo" href="#gogo">func gogo(buf *gobuf)</a>

```
searchKey: runtime.gogo
```

```Go
func gogo(buf *gobuf)
```

### <a id="jmpdefer" href="#jmpdefer">func jmpdefer(fv *funcval, argp uintptr)</a>

```
searchKey: runtime.jmpdefer
```

```Go
func jmpdefer(fv *funcval, argp uintptr)
```

### <a id="asminit" href="#asminit">func asminit()</a>

```
searchKey: runtime.asminit
```

```Go
func asminit()
```

### <a id="setg" href="#setg">func setg(gg *g)</a>

```
searchKey: runtime.setg
```

```Go
func setg(gg *g)
```

### <a id="breakpoint" href="#breakpoint">func breakpoint()</a>

```
searchKey: runtime.breakpoint
```

```Go
func breakpoint()
```

### <a id="reflectcall" href="#reflectcall">func reflectcall(stackArgsType *_type, fn, stackArgs unsafe.Pointer, stackArgsSize, stackRetOffset, frameSize uint32, regArgs *abi.RegArgs)</a>

```
searchKey: runtime.reflectcall
```

```Go
func reflectcall(stackArgsType *_type, fn, stackArgs unsafe.Pointer, stackArgsSize, stackRetOffset, frameSize uint32, regArgs *abi.RegArgs)
```

reflectcall calls fn with arguments described by stackArgs, stackArgsSize, frameSize, and regArgs. 

Arguments passed on the stack and space for return values passed on the stack must be laid out at the space pointed to by stackArgs (with total length stackArgsSize) according to the ABI. 

stackRetOffset must be some value <= stackArgsSize that indicates the offset within stackArgs where the return value space begins. 

frameSize is the total size of the argument frame at stackArgs and must therefore be >= stackArgsSize. It must include additional space for spilling register arguments for stack growth and preemption. 

TODO(mknyszek): Once we don't need the additional spill space, remove frameSize, since frameSize will be redundant with stackArgsSize. 

Arguments passed in registers must be laid out in regArgs according to the ABI. regArgs will hold any return values passed in registers after the call. 

reflectcall copies stack arguments from stackArgs to the goroutine stack, and then copies back stackArgsSize-stackRetOffset bytes back to the return space in stackArgs once fn has completed. It also "unspills" argument registers from regArgs before calling fn, and spills them back into regArgs immediately following the call to fn. If there are results being returned on the stack, the caller should pass the argument frame type as stackArgsType so that reflectcall can execute appropriate write barriers during the copy. 

reflectcall expects regArgs.ReturnIsPtr to be populated indicating which registers on the return path will contain Go pointers. It will then store these pointers in regArgs.Ptrs such that they are visible to the GC. 

Package reflect passes a frame type. In package runtime, there is only one call that copies results back, in callbackWrap in syscall_windows.go, and it does NOT pass a frame type, meaning there are no write barriers invoked. See that call site for justification. 

Package reflect accesses this symbol through a linkname. 

Arguments passed through to reflectcall do not escape. The type is used only in a very limited callee of reflectcall, the stackArgs are copied, and regArgs is only used in the reflectcall frame. 

### <a id="procyield" href="#procyield">func procyield(cycles uint32)</a>

```
searchKey: runtime.procyield
```

```Go
func procyield(cycles uint32)
```

### <a id="goexit" href="#goexit">func goexit(neverCallThisFunction)</a>

```
searchKey: runtime.goexit
```

```Go
func goexit(neverCallThisFunction)
```

goexit is the return stub at the top of every goroutine call stack. Each goroutine stack is constructed as if goexit called the goroutine's entry point function, so that when the entry point function returns, it will return to goexit, which will call goexit1 to perform the actual exit. 

This function must never be called directly. Call goexit1 instead. gentraceback assumes that goexit terminates the stack. A direct call on the stack will cause gentraceback to stop walking the stack prematurely and if there is leftover state it may panic. 

### <a id="publicationBarrier" href="#publicationBarrier">func publicationBarrier()</a>

```
searchKey: runtime.publicationBarrier
```

```Go
func publicationBarrier()
```

publicationBarrier performs a store/store barrier (a "publication" or "export" barrier). Some form of synchronization is required between initializing an object and making that object accessible to another processor. Without synchronization, the initialization writes and the "publication" write may be reordered, allowing the other processor to follow the pointer and observe an uninitialized object. In general, higher-level synchronization should be used, such as locking or an atomic pointer write. publicationBarrier is for when those aren't an option, such as in the implementation of the memory manager. 

There's no corresponding barrier for the read side because the read side naturally has a data dependency order. All architectures that Go supports or seems likely to ever support automatically enforce data dependency ordering. 

### <a id="getcallerpc" href="#getcallerpc">func getcallerpc() uintptr</a>

```
searchKey: runtime.getcallerpc
```

```Go
func getcallerpc() uintptr
```

### <a id="getcallersp" href="#getcallersp">func getcallersp() uintptr</a>

```
searchKey: runtime.getcallersp
```

```Go
func getcallersp() uintptr
```

### <a id="getclosureptr" href="#getclosureptr">func getclosureptr() uintptr</a>

```
searchKey: runtime.getclosureptr
```

```Go
func getclosureptr() uintptr
```

getclosureptr returns the pointer to the current closure. getclosureptr can only be used in an assignment statement at the entry of a function. Moreover, go:nosplit directive must be specified at the declaration of caller function, so that the function prolog does not clobber the closure register. for example: 

```
//go:nosplit
func f(arg1, arg2, arg3 int) {
	dx := getclosureptr()
}

```
The compiler rewrites calls to this function into instructions that fetch the pointer from a well-known register (DX on x86 architecture, etc.) directly. 

### <a id="asmcgocall" href="#asmcgocall">func asmcgocall(fn, arg unsafe.Pointer) int32</a>

```
searchKey: runtime.asmcgocall
```

```Go
func asmcgocall(fn, arg unsafe.Pointer) int32
```

### <a id="morestack" href="#morestack">func morestack()</a>

```
searchKey: runtime.morestack
```

```Go
func morestack()
```

### <a id="morestack_noctxt" href="#morestack_noctxt">func morestack_noctxt()</a>

```
searchKey: runtime.morestack_noctxt
```

```Go
func morestack_noctxt()
```

### <a id="rt0_go" href="#rt0_go">func rt0_go()</a>

```
searchKey: runtime.rt0_go
```

```Go
func rt0_go()
```

### <a id="return0" href="#return0">func return0()</a>

```
searchKey: runtime.return0
```

```Go
func return0()
```

return0 is a stub used to return 0 from deferproc. It is called at the very end of deferproc to signal the calling Go function that it should not jump to deferreturn. in asm_*.s 

### <a id="call16" href="#call16">func call16(typ, fn, stackArgs unsafe.Pointer, stackArgsSize, stackRetOffset, frameSize uint32, regArgs *abi.RegArgs)</a>

```
searchKey: runtime.call16
```

```Go
func call16(typ, fn, stackArgs unsafe.Pointer, stackArgsSize, stackRetOffset, frameSize uint32, regArgs *abi.RegArgs)
```

in asm_*.s not called directly; definitions here supply type information for traceback. These must have the same signature (arg pointer map) as reflectcall. 

### <a id="call32" href="#call32">func call32(typ, fn, stackArgs unsafe.Pointer, stackArgsSize, stackRetOffset, frameSize uint32, regArgs *abi.RegArgs)</a>

```
searchKey: runtime.call32
```

```Go
func call32(typ, fn, stackArgs unsafe.Pointer, stackArgsSize, stackRetOffset, frameSize uint32, regArgs *abi.RegArgs)
```

### <a id="call64" href="#call64">func call64(typ, fn, stackArgs unsafe.Pointer, stackArgsSize, stackRetOffset, frameSize uint32, regArgs *abi.RegArgs)</a>

```
searchKey: runtime.call64
```

```Go
func call64(typ, fn, stackArgs unsafe.Pointer, stackArgsSize, stackRetOffset, frameSize uint32, regArgs *abi.RegArgs)
```

### <a id="call128" href="#call128">func call128(typ, fn, stackArgs unsafe.Pointer, stackArgsSize, stackRetOffset, frameSize uint32, regArgs *abi.RegArgs)</a>

```
searchKey: runtime.call128
```

```Go
func call128(typ, fn, stackArgs unsafe.Pointer, stackArgsSize, stackRetOffset, frameSize uint32, regArgs *abi.RegArgs)
```

### <a id="call256" href="#call256">func call256(typ, fn, stackArgs unsafe.Pointer, stackArgsSize, stackRetOffset, frameSize uint32, regArgs *abi.RegArgs)</a>

```
searchKey: runtime.call256
```

```Go
func call256(typ, fn, stackArgs unsafe.Pointer, stackArgsSize, stackRetOffset, frameSize uint32, regArgs *abi.RegArgs)
```

### <a id="call512" href="#call512">func call512(typ, fn, stackArgs unsafe.Pointer, stackArgsSize, stackRetOffset, frameSize uint32, regArgs *abi.RegArgs)</a>

```
searchKey: runtime.call512
```

```Go
func call512(typ, fn, stackArgs unsafe.Pointer, stackArgsSize, stackRetOffset, frameSize uint32, regArgs *abi.RegArgs)
```

### <a id="call1024" href="#call1024">func call1024(typ, fn, stackArgs unsafe.Pointer, stackArgsSize, stackRetOffset, frameSize uint32, regArgs *abi.RegArgs)</a>

```
searchKey: runtime.call1024
```

```Go
func call1024(typ, fn, stackArgs unsafe.Pointer, stackArgsSize, stackRetOffset, frameSize uint32, regArgs *abi.RegArgs)
```

### <a id="call2048" href="#call2048">func call2048(typ, fn, stackArgs unsafe.Pointer, stackArgsSize, stackRetOffset, frameSize uint32, regArgs *abi.RegArgs)</a>

```
searchKey: runtime.call2048
```

```Go
func call2048(typ, fn, stackArgs unsafe.Pointer, stackArgsSize, stackRetOffset, frameSize uint32, regArgs *abi.RegArgs)
```

### <a id="call4096" href="#call4096">func call4096(typ, fn, stackArgs unsafe.Pointer, stackArgsSize, stackRetOffset, frameSize uint32, regArgs *abi.RegArgs)</a>

```
searchKey: runtime.call4096
```

```Go
func call4096(typ, fn, stackArgs unsafe.Pointer, stackArgsSize, stackRetOffset, frameSize uint32, regArgs *abi.RegArgs)
```

### <a id="call8192" href="#call8192">func call8192(typ, fn, stackArgs unsafe.Pointer, stackArgsSize, stackRetOffset, frameSize uint32, regArgs *abi.RegArgs)</a>

```
searchKey: runtime.call8192
```

```Go
func call8192(typ, fn, stackArgs unsafe.Pointer, stackArgsSize, stackRetOffset, frameSize uint32, regArgs *abi.RegArgs)
```

### <a id="call16384" href="#call16384">func call16384(typ, fn, stackArgs unsafe.Pointer, stackArgsSize, stackRetOffset, frameSize uint32, regArgs *abi.RegArgs)</a>

```
searchKey: runtime.call16384
```

```Go
func call16384(typ, fn, stackArgs unsafe.Pointer, stackArgsSize, stackRetOffset, frameSize uint32, regArgs *abi.RegArgs)
```

### <a id="call32768" href="#call32768">func call32768(typ, fn, stackArgs unsafe.Pointer, stackArgsSize, stackRetOffset, frameSize uint32, regArgs *abi.RegArgs)</a>

```
searchKey: runtime.call32768
```

```Go
func call32768(typ, fn, stackArgs unsafe.Pointer, stackArgsSize, stackRetOffset, frameSize uint32, regArgs *abi.RegArgs)
```

### <a id="call65536" href="#call65536">func call65536(typ, fn, stackArgs unsafe.Pointer, stackArgsSize, stackRetOffset, frameSize uint32, regArgs *abi.RegArgs)</a>

```
searchKey: runtime.call65536
```

```Go
func call65536(typ, fn, stackArgs unsafe.Pointer, stackArgsSize, stackRetOffset, frameSize uint32, regArgs *abi.RegArgs)
```

### <a id="call131072" href="#call131072">func call131072(typ, fn, stackArgs unsafe.Pointer, stackArgsSize, stackRetOffset, frameSize uint32, regArgs *abi.RegArgs)</a>

```
searchKey: runtime.call131072
```

```Go
func call131072(typ, fn, stackArgs unsafe.Pointer, stackArgsSize, stackRetOffset, frameSize uint32, regArgs *abi.RegArgs)
```

### <a id="call262144" href="#call262144">func call262144(typ, fn, stackArgs unsafe.Pointer, stackArgsSize, stackRetOffset, frameSize uint32, regArgs *abi.RegArgs)</a>

```
searchKey: runtime.call262144
```

```Go
func call262144(typ, fn, stackArgs unsafe.Pointer, stackArgsSize, stackRetOffset, frameSize uint32, regArgs *abi.RegArgs)
```

### <a id="call524288" href="#call524288">func call524288(typ, fn, stackArgs unsafe.Pointer, stackArgsSize, stackRetOffset, frameSize uint32, regArgs *abi.RegArgs)</a>

```
searchKey: runtime.call524288
```

```Go
func call524288(typ, fn, stackArgs unsafe.Pointer, stackArgsSize, stackRetOffset, frameSize uint32, regArgs *abi.RegArgs)
```

### <a id="call1048576" href="#call1048576">func call1048576(typ, fn, stackArgs unsafe.Pointer, stackArgsSize, stackRetOffset, frameSize uint32, regArgs *abi.RegArgs)</a>

```
searchKey: runtime.call1048576
```

```Go
func call1048576(typ, fn, stackArgs unsafe.Pointer, stackArgsSize, stackRetOffset, frameSize uint32, regArgs *abi.RegArgs)
```

### <a id="call2097152" href="#call2097152">func call2097152(typ, fn, stackArgs unsafe.Pointer, stackArgsSize, stackRetOffset, frameSize uint32, regArgs *abi.RegArgs)</a>

```
searchKey: runtime.call2097152
```

```Go
func call2097152(typ, fn, stackArgs unsafe.Pointer, stackArgsSize, stackRetOffset, frameSize uint32, regArgs *abi.RegArgs)
```

### <a id="call4194304" href="#call4194304">func call4194304(typ, fn, stackArgs unsafe.Pointer, stackArgsSize, stackRetOffset, frameSize uint32, regArgs *abi.RegArgs)</a>

```
searchKey: runtime.call4194304
```

```Go
func call4194304(typ, fn, stackArgs unsafe.Pointer, stackArgsSize, stackRetOffset, frameSize uint32, regArgs *abi.RegArgs)
```

### <a id="call8388608" href="#call8388608">func call8388608(typ, fn, stackArgs unsafe.Pointer, stackArgsSize, stackRetOffset, frameSize uint32, regArgs *abi.RegArgs)</a>

```
searchKey: runtime.call8388608
```

```Go
func call8388608(typ, fn, stackArgs unsafe.Pointer, stackArgsSize, stackRetOffset, frameSize uint32, regArgs *abi.RegArgs)
```

### <a id="call16777216" href="#call16777216">func call16777216(typ, fn, stackArgs unsafe.Pointer, stackArgsSize, stackRetOffset, frameSize uint32, regArgs *abi.RegArgs)</a>

```
searchKey: runtime.call16777216
```

```Go
func call16777216(typ, fn, stackArgs unsafe.Pointer, stackArgsSize, stackRetOffset, frameSize uint32, regArgs *abi.RegArgs)
```

### <a id="call33554432" href="#call33554432">func call33554432(typ, fn, stackArgs unsafe.Pointer, stackArgsSize, stackRetOffset, frameSize uint32, regArgs *abi.RegArgs)</a>

```
searchKey: runtime.call33554432
```

```Go
func call33554432(typ, fn, stackArgs unsafe.Pointer, stackArgsSize, stackRetOffset, frameSize uint32, regArgs *abi.RegArgs)
```

### <a id="call67108864" href="#call67108864">func call67108864(typ, fn, stackArgs unsafe.Pointer, stackArgsSize, stackRetOffset, frameSize uint32, regArgs *abi.RegArgs)</a>

```
searchKey: runtime.call67108864
```

```Go
func call67108864(typ, fn, stackArgs unsafe.Pointer, stackArgsSize, stackRetOffset, frameSize uint32, regArgs *abi.RegArgs)
```

### <a id="call134217728" href="#call134217728">func call134217728(typ, fn, stackArgs unsafe.Pointer, stackArgsSize, stackRetOffset, frameSize uint32, regArgs *abi.RegArgs)</a>

```
searchKey: runtime.call134217728
```

```Go
func call134217728(typ, fn, stackArgs unsafe.Pointer, stackArgsSize, stackRetOffset, frameSize uint32, regArgs *abi.RegArgs)
```

### <a id="call268435456" href="#call268435456">func call268435456(typ, fn, stackArgs unsafe.Pointer, stackArgsSize, stackRetOffset, frameSize uint32, regArgs *abi.RegArgs)</a>

```
searchKey: runtime.call268435456
```

```Go
func call268435456(typ, fn, stackArgs unsafe.Pointer, stackArgsSize, stackRetOffset, frameSize uint32, regArgs *abi.RegArgs)
```

### <a id="call536870912" href="#call536870912">func call536870912(typ, fn, stackArgs unsafe.Pointer, stackArgsSize, stackRetOffset, frameSize uint32, regArgs *abi.RegArgs)</a>

```
searchKey: runtime.call536870912
```

```Go
func call536870912(typ, fn, stackArgs unsafe.Pointer, stackArgsSize, stackRetOffset, frameSize uint32, regArgs *abi.RegArgs)
```

### <a id="call1073741824" href="#call1073741824">func call1073741824(typ, fn, stackArgs unsafe.Pointer, stackArgsSize, stackRetOffset, frameSize uint32, regArgs *abi.RegArgs)</a>

```
searchKey: runtime.call1073741824
```

```Go
func call1073741824(typ, fn, stackArgs unsafe.Pointer, stackArgsSize, stackRetOffset, frameSize uint32, regArgs *abi.RegArgs)
```

### <a id="systemstack_switch" href="#systemstack_switch">func systemstack_switch()</a>

```
searchKey: runtime.systemstack_switch
```

```Go
func systemstack_switch()
```

### <a id="alignUp" href="#alignUp">func alignUp(n, a uintptr) uintptr</a>

```
searchKey: runtime.alignUp
```

```Go
func alignUp(n, a uintptr) uintptr
```

alignUp rounds n up to a multiple of a. a must be a power of 2. 

### <a id="alignDown" href="#alignDown">func alignDown(n, a uintptr) uintptr</a>

```
searchKey: runtime.alignDown
```

```Go
func alignDown(n, a uintptr) uintptr
```

alignDown rounds n down to a multiple of a. a must be a power of 2. 

### <a id="divRoundUp" href="#divRoundUp">func divRoundUp(n, a uintptr) uintptr</a>

```
searchKey: runtime.divRoundUp
```

```Go
func divRoundUp(n, a uintptr) uintptr
```

divRoundUp returns ceil(n / a). 

### <a id="checkASM" href="#checkASM">func checkASM() bool</a>

```
searchKey: runtime.checkASM
```

```Go
func checkASM() bool
```

checkASM reports whether assembly runtime checks have passed. 

### <a id="memequal_varlen" href="#memequal_varlen">func memequal_varlen(a, b unsafe.Pointer) bool</a>

```
searchKey: runtime.memequal_varlen
```

```Go
func memequal_varlen(a, b unsafe.Pointer) bool
```

### <a id="bool2int" href="#bool2int">func bool2int(x bool) int</a>

```
searchKey: runtime.bool2int
```

```Go
func bool2int(x bool) int
```

bool2int returns 0 if x is false or 1 if x is true. 

### <a id="abort" href="#abort">func abort()</a>

```
searchKey: runtime.abort
```

```Go
func abort()
```

abort crashes the runtime in situations where even throw might not work. In general it should do something a debugger will recognize (e.g., an INT3 on x86). A crash in abort is recognized by the signal handler, which will attempt to tear down the runtime immediately. 

### <a id="gcWriteBarrier" href="#gcWriteBarrier">func gcWriteBarrier()</a>

```
searchKey: runtime.gcWriteBarrier
```

```Go
func gcWriteBarrier()
```

Called from compiled code; declared for vet; do NOT call from Go. 

### <a id="duffzero" href="#duffzero">func duffzero()</a>

```
searchKey: runtime.duffzero
```

```Go
func duffzero()
```

### <a id="duffcopy" href="#duffcopy">func duffcopy()</a>

```
searchKey: runtime.duffcopy
```

```Go
func duffcopy()
```

### <a id="addmoduledata" href="#addmoduledata">func addmoduledata()</a>

```
searchKey: runtime.addmoduledata
```

```Go
func addmoduledata()
```

Called from linker-generated .initarray; declared for go vet; do NOT call from Go. 

### <a id="sigpanic0" href="#sigpanic0">func sigpanic0()</a>

```
searchKey: runtime.sigpanic0
```

```Go
func sigpanic0()
```

Injected by the signal handler for panicking signals. Initializes any registers that have fixed meaning at calls but are scratch in bodies and calls sigpanic. On many platforms it just jumps to sigpanic. 

### <a id="gcWriteBarrierCX" href="#gcWriteBarrierCX">func gcWriteBarrierCX()</a>

```
searchKey: runtime.gcWriteBarrierCX
```

```Go
func gcWriteBarrierCX()
```

Called from compiled code; declared for vet; do NOT call from Go. 

### <a id="gcWriteBarrierDX" href="#gcWriteBarrierDX">func gcWriteBarrierDX()</a>

```
searchKey: runtime.gcWriteBarrierDX
```

```Go
func gcWriteBarrierDX()
```

### <a id="gcWriteBarrierBX" href="#gcWriteBarrierBX">func gcWriteBarrierBX()</a>

```
searchKey: runtime.gcWriteBarrierBX
```

```Go
func gcWriteBarrierBX()
```

### <a id="gcWriteBarrierBP" href="#gcWriteBarrierBP">func gcWriteBarrierBP()</a>

```
searchKey: runtime.gcWriteBarrierBP
```

```Go
func gcWriteBarrierBP()
```

### <a id="gcWriteBarrierSI" href="#gcWriteBarrierSI">func gcWriteBarrierSI()</a>

```
searchKey: runtime.gcWriteBarrierSI
```

```Go
func gcWriteBarrierSI()
```

### <a id="gcWriteBarrierR8" href="#gcWriteBarrierR8">func gcWriteBarrierR8()</a>

```
searchKey: runtime.gcWriteBarrierR8
```

```Go
func gcWriteBarrierR8()
```

### <a id="gcWriteBarrierR9" href="#gcWriteBarrierR9">func gcWriteBarrierR9()</a>

```
searchKey: runtime.gcWriteBarrierR9
```

```Go
func gcWriteBarrierR9()
```

### <a id="stackcheck" href="#stackcheck">func stackcheck()</a>

```
searchKey: runtime.stackcheck
```

```Go
func stackcheck()
```

stackcheck checks that SP is in range [g->stack.lo, g->stack.hi). 

### <a id="settls" href="#settls">func settls()</a>

```
searchKey: runtime.settls
```

```Go
func settls()
```

Called from assembly only; declared for go vet. 

### <a id="retpolineAX" href="#retpolineAX">func retpolineAX()</a>

```
searchKey: runtime.retpolineAX
```

```Go
func retpolineAX()
```

Retpolines, used by -spectre=ret flag in cmd/asm, cmd/compile. 

### <a id="retpolineCX" href="#retpolineCX">func retpolineCX()</a>

```
searchKey: runtime.retpolineCX
```

```Go
func retpolineCX()
```

### <a id="retpolineDX" href="#retpolineDX">func retpolineDX()</a>

```
searchKey: runtime.retpolineDX
```

```Go
func retpolineDX()
```

### <a id="retpolineBX" href="#retpolineBX">func retpolineBX()</a>

```
searchKey: runtime.retpolineBX
```

```Go
func retpolineBX()
```

### <a id="retpolineBP" href="#retpolineBP">func retpolineBP()</a>

```
searchKey: runtime.retpolineBP
```

```Go
func retpolineBP()
```

### <a id="retpolineSI" href="#retpolineSI">func retpolineSI()</a>

```
searchKey: runtime.retpolineSI
```

```Go
func retpolineSI()
```

### <a id="retpolineDI" href="#retpolineDI">func retpolineDI()</a>

```
searchKey: runtime.retpolineDI
```

```Go
func retpolineDI()
```

### <a id="retpolineR8" href="#retpolineR8">func retpolineR8()</a>

```
searchKey: runtime.retpolineR8
```

```Go
func retpolineR8()
```

### <a id="retpolineR9" href="#retpolineR9">func retpolineR9()</a>

```
searchKey: runtime.retpolineR9
```

```Go
func retpolineR9()
```

### <a id="retpolineR10" href="#retpolineR10">func retpolineR10()</a>

```
searchKey: runtime.retpolineR10
```

```Go
func retpolineR10()
```

### <a id="retpolineR11" href="#retpolineR11">func retpolineR11()</a>

```
searchKey: runtime.retpolineR11
```

```Go
func retpolineR11()
```

### <a id="retpolineR12" href="#retpolineR12">func retpolineR12()</a>

```
searchKey: runtime.retpolineR12
```

```Go
func retpolineR12()
```

### <a id="retpolineR13" href="#retpolineR13">func retpolineR13()</a>

```
searchKey: runtime.retpolineR13
```

```Go
func retpolineR13()
```

### <a id="retpolineR14" href="#retpolineR14">func retpolineR14()</a>

```
searchKey: runtime.retpolineR14
```

```Go
func retpolineR14()
```

### <a id="retpolineR15" href="#retpolineR15">func retpolineR15()</a>

```
searchKey: runtime.retpolineR15
```

```Go
func retpolineR15()
```

### <a id="asmcgocall_no_g" href="#asmcgocall_no_g">func asmcgocall_no_g(fn, arg unsafe.Pointer)</a>

```
searchKey: runtime.asmcgocall_no_g
```

```Go
func asmcgocall_no_g(fn, arg unsafe.Pointer)
```

### <a id="spillArgs" href="#spillArgs">func spillArgs()</a>

```
searchKey: runtime.spillArgs
```

```Go
func spillArgs()
```

Used by reflectcall and the reflect package. 

Spills/loads arguments in registers to/from an internal/abi.RegArgs respectively. Does not follow the Go ABI. 

### <a id="unspillArgs" href="#unspillArgs">func unspillArgs()</a>

```
searchKey: runtime.unspillArgs
```

```Go
func unspillArgs()
```

### <a id="sbrk0" href="#sbrk0">func sbrk0() uintptr</a>

```
searchKey: runtime.sbrk0
```

```Go
func sbrk0() uintptr
```

sbrk0 returns the current process brk, or 0 if not implemented. 

### <a id="runtime_expandFinalInlineFrame" href="#runtime_expandFinalInlineFrame">func runtime_expandFinalInlineFrame(stk []uintptr) []uintptr</a>

```
searchKey: runtime.runtime_expandFinalInlineFrame
```

```Go
func runtime_expandFinalInlineFrame(stk []uintptr) []uintptr
```

runtime_expandFinalInlineFrame expands the final pc in stk to include all "callers" if pc is inline. 

### <a id="expandCgoFrames" href="#expandCgoFrames">func expandCgoFrames(pc uintptr) []Frame</a>

```
searchKey: runtime.expandCgoFrames
```

```Go
func expandCgoFrames(pc uintptr) []Frame
```

expandCgoFrames expands frame information for pc, known to be a non-Go function, using the cgoSymbolizer hook. expandCgoFrames returns nil if pc could not be expanded. 

### <a id="activeModules" href="#activeModules">func activeModules() []*moduledata</a>

```
searchKey: runtime.activeModules
```

```Go
func activeModules() []*moduledata
```

activeModules returns a slice of active modules. 

A module is active once its gcdatamask and gcbssmask have been assembled and it is usable by the GC. 

This is nosplit/nowritebarrier because it is called by the cgo pointer checking code. 

### <a id="modulesinit" href="#modulesinit">func modulesinit()</a>

```
searchKey: runtime.modulesinit
```

```Go
func modulesinit()
```

modulesinit creates the active modules slice out of all loaded modules. 

When a module is first loaded by the dynamic linker, an .init_array function (written by cmd/link) is invoked to call addmoduledata, appending to the module to the linked list that starts with firstmoduledata. 

There are two times this can happen in the lifecycle of a Go program. First, if compiled with -linkshared, a number of modules built with -buildmode=shared can be loaded at program initialization. Second, a Go program can load a module while running that was built with -buildmode=plugin. 

After loading, this function is called which initializes the moduledata so it is usable by the GC and creates a new activeModules list. 

Only one goroutine may call modulesinit at a time. 

### <a id="moduledataverify" href="#moduledataverify">func moduledataverify()</a>

```
searchKey: runtime.moduledataverify
```

```Go
func moduledataverify()
```

### <a id="moduledataverify1" href="#moduledataverify1">func moduledataverify1(datap *moduledata)</a>

```
searchKey: runtime.moduledataverify1
```

```Go
func moduledataverify1(datap *moduledata)
```

### <a id="pcvalueCacheKey" href="#pcvalueCacheKey">func pcvalueCacheKey(targetpc uintptr) uintptr</a>

```
searchKey: runtime.pcvalueCacheKey
```

```Go
func pcvalueCacheKey(targetpc uintptr) uintptr
```

pcvalueCacheKey returns the outermost index in a pcvalueCache to use for targetpc. It must be very cheap to calculate. For now, align to sys.PtrSize and reduce mod the number of entries. In practice, this appears to be fairly randomly and evenly distributed. 

### <a id="pcvalue" href="#pcvalue">func pcvalue(f funcInfo, off uint32, targetpc uintptr, cache *pcvalueCache, strict bool) (int32, uintptr)</a>

```
searchKey: runtime.pcvalue
```

```Go
func pcvalue(f funcInfo, off uint32, targetpc uintptr, cache *pcvalueCache, strict bool) (int32, uintptr)
```

Returns the PCData value, and the PC where this value starts. TODO: the start PC is returned only when cache is nil. 

### <a id="cfuncname" href="#cfuncname">func cfuncname(f funcInfo) *byte</a>

```
searchKey: runtime.cfuncname
```

```Go
func cfuncname(f funcInfo) *byte
```

### <a id="funcname" href="#funcname">func funcname(f funcInfo) string</a>

```
searchKey: runtime.funcname
```

```Go
func funcname(f funcInfo) string
```

### <a id="funcpkgpath" href="#funcpkgpath">func funcpkgpath(f funcInfo) string</a>

```
searchKey: runtime.funcpkgpath
```

```Go
func funcpkgpath(f funcInfo) string
```

### <a id="cfuncnameFromNameoff" href="#cfuncnameFromNameoff">func cfuncnameFromNameoff(f funcInfo, nameoff int32) *byte</a>

```
searchKey: runtime.cfuncnameFromNameoff
```

```Go
func cfuncnameFromNameoff(f funcInfo, nameoff int32) *byte
```

### <a id="funcnameFromNameoff" href="#funcnameFromNameoff">func funcnameFromNameoff(f funcInfo, nameoff int32) string</a>

```
searchKey: runtime.funcnameFromNameoff
```

```Go
func funcnameFromNameoff(f funcInfo, nameoff int32) string
```

### <a id="funcfile" href="#funcfile">func funcfile(f funcInfo, fileno int32) string</a>

```
searchKey: runtime.funcfile
```

```Go
func funcfile(f funcInfo, fileno int32) string
```

### <a id="funcline1" href="#funcline1">func funcline1(f funcInfo, targetpc uintptr, strict bool) (file string, line int32)</a>

```
searchKey: runtime.funcline1
```

```Go
func funcline1(f funcInfo, targetpc uintptr, strict bool) (file string, line int32)
```

### <a id="funcline" href="#funcline">func funcline(f funcInfo, targetpc uintptr) (file string, line int32)</a>

```
searchKey: runtime.funcline
```

```Go
func funcline(f funcInfo, targetpc uintptr) (file string, line int32)
```

### <a id="funcspdelta" href="#funcspdelta">func funcspdelta(f funcInfo, targetpc uintptr, cache *pcvalueCache) int32</a>

```
searchKey: runtime.funcspdelta
```

```Go
func funcspdelta(f funcInfo, targetpc uintptr, cache *pcvalueCache) int32
```

### <a id="funcMaxSPDelta" href="#funcMaxSPDelta">func funcMaxSPDelta(f funcInfo) int32</a>

```
searchKey: runtime.funcMaxSPDelta
```

```Go
func funcMaxSPDelta(f funcInfo) int32
```

funcMaxSPDelta returns the maximum spdelta at any point in f. 

### <a id="pcdatastart" href="#pcdatastart">func pcdatastart(f funcInfo, table uint32) uint32</a>

```
searchKey: runtime.pcdatastart
```

```Go
func pcdatastart(f funcInfo, table uint32) uint32
```

### <a id="pcdatavalue" href="#pcdatavalue">func pcdatavalue(f funcInfo, table uint32, targetpc uintptr, cache *pcvalueCache) int32</a>

```
searchKey: runtime.pcdatavalue
```

```Go
func pcdatavalue(f funcInfo, table uint32, targetpc uintptr, cache *pcvalueCache) int32
```

### <a id="pcdatavalue1" href="#pcdatavalue1">func pcdatavalue1(f funcInfo, table uint32, targetpc uintptr, cache *pcvalueCache, strict bool) int32</a>

```
searchKey: runtime.pcdatavalue1
```

```Go
func pcdatavalue1(f funcInfo, table uint32, targetpc uintptr, cache *pcvalueCache, strict bool) int32
```

### <a id="pcdatavalue2" href="#pcdatavalue2">func pcdatavalue2(f funcInfo, table uint32, targetpc uintptr) (int32, uintptr)</a>

```
searchKey: runtime.pcdatavalue2
```

```Go
func pcdatavalue2(f funcInfo, table uint32, targetpc uintptr) (int32, uintptr)
```

Like pcdatavalue, but also return the start PC of this PCData value. It doesn't take a cache. 

### <a id="funcdata" href="#funcdata">func funcdata(f funcInfo, i uint8) unsafe.Pointer</a>

```
searchKey: runtime.funcdata
```

```Go
func funcdata(f funcInfo, i uint8) unsafe.Pointer
```

### <a id="step" href="#step">func step(p []byte, pc *uintptr, val *int32, first bool) (newp []byte, ok bool)</a>

```
searchKey: runtime.step
```

```Go
func step(p []byte, pc *uintptr, val *int32, first bool) (newp []byte, ok bool)
```

step advances to the next pc, value pair in the encoded table. 

### <a id="readvarint" href="#readvarint">func readvarint(p []byte) (read uint32, val uint32)</a>

```
searchKey: runtime.readvarint
```

```Go
func readvarint(p []byte) (read uint32, val uint32)
```

readvarint reads a varint from p. 

### <a id="syscall_syscall" href="#syscall_syscall">func syscall_syscall(fn, a1, a2, a3 uintptr) (r1, r2, err uintptr)</a>

```
searchKey: runtime.syscall_syscall
```

```Go
func syscall_syscall(fn, a1, a2, a3 uintptr) (r1, r2, err uintptr)
```

### <a id="syscall" href="#syscall">func syscall()</a>

```
searchKey: runtime.syscall
```

```Go
func syscall()
```

### <a id="syscall_syscallX" href="#syscall_syscallX">func syscall_syscallX(fn, a1, a2, a3 uintptr) (r1, r2, err uintptr)</a>

```
searchKey: runtime.syscall_syscallX
```

```Go
func syscall_syscallX(fn, a1, a2, a3 uintptr) (r1, r2, err uintptr)
```

### <a id="syscallX" href="#syscallX">func syscallX()</a>

```
searchKey: runtime.syscallX
```

```Go
func syscallX()
```

### <a id="syscall_syscall6" href="#syscall_syscall6">func syscall_syscall6(fn, a1, a2, a3, a4, a5, a6 uintptr) (r1, r2, err uintptr)</a>

```
searchKey: runtime.syscall_syscall6
```

```Go
func syscall_syscall6(fn, a1, a2, a3, a4, a5, a6 uintptr) (r1, r2, err uintptr)
```

### <a id="syscall6" href="#syscall6">func syscall6()</a>

```
searchKey: runtime.syscall6
```

```Go
func syscall6()
```

### <a id="syscall_syscall6X" href="#syscall_syscall6X">func syscall_syscall6X(fn, a1, a2, a3, a4, a5, a6 uintptr) (r1, r2, err uintptr)</a>

```
searchKey: runtime.syscall_syscall6X
```

```Go
func syscall_syscall6X(fn, a1, a2, a3, a4, a5, a6 uintptr) (r1, r2, err uintptr)
```

### <a id="syscall6X" href="#syscall6X">func syscall6X()</a>

```
searchKey: runtime.syscall6X
```

```Go
func syscall6X()
```

### <a id="syscall_syscallPtr" href="#syscall_syscallPtr">func syscall_syscallPtr(fn, a1, a2, a3 uintptr) (r1, r2, err uintptr)</a>

```
searchKey: runtime.syscall_syscallPtr
```

```Go
func syscall_syscallPtr(fn, a1, a2, a3 uintptr) (r1, r2, err uintptr)
```

### <a id="syscallPtr" href="#syscallPtr">func syscallPtr()</a>

```
searchKey: runtime.syscallPtr
```

```Go
func syscallPtr()
```

### <a id="syscall_rawSyscall" href="#syscall_rawSyscall">func syscall_rawSyscall(fn, a1, a2, a3 uintptr) (r1, r2, err uintptr)</a>

```
searchKey: runtime.syscall_rawSyscall
```

```Go
func syscall_rawSyscall(fn, a1, a2, a3 uintptr) (r1, r2, err uintptr)
```

### <a id="syscall_rawSyscall6" href="#syscall_rawSyscall6">func syscall_rawSyscall6(fn, a1, a2, a3, a4, a5, a6 uintptr) (r1, r2, err uintptr)</a>

```
searchKey: runtime.syscall_rawSyscall6
```

```Go
func syscall_rawSyscall6(fn, a1, a2, a3, a4, a5, a6 uintptr) (r1, r2, err uintptr)
```

### <a id="crypto_x509_syscall" href="#crypto_x509_syscall">func crypto_x509_syscall(fn, a1, a2, a3, a4, a5, a6 uintptr) (r1 uintptr)</a>

```
searchKey: runtime.crypto_x509_syscall
```

```Go
func crypto_x509_syscall(fn, a1, a2, a3, a4, a5, a6 uintptr) (r1 uintptr)
```

### <a id="syscallNoErr" href="#syscallNoErr">func syscallNoErr()</a>

```
searchKey: runtime.syscallNoErr
```

```Go
func syscallNoErr()
```

### <a id="pthread_attr_init" href="#pthread_attr_init">func pthread_attr_init(attr *pthreadattr) int32</a>

```
searchKey: runtime.pthread_attr_init
```

```Go
func pthread_attr_init(attr *pthreadattr) int32
```

### <a id="pthread_attr_init_trampoline" href="#pthread_attr_init_trampoline">func pthread_attr_init_trampoline()</a>

```
searchKey: runtime.pthread_attr_init_trampoline
```

```Go
func pthread_attr_init_trampoline()
```

### <a id="pthread_attr_getstacksize" href="#pthread_attr_getstacksize">func pthread_attr_getstacksize(attr *pthreadattr, size *uintptr) int32</a>

```
searchKey: runtime.pthread_attr_getstacksize
```

```Go
func pthread_attr_getstacksize(attr *pthreadattr, size *uintptr) int32
```

### <a id="pthread_attr_getstacksize_trampoline" href="#pthread_attr_getstacksize_trampoline">func pthread_attr_getstacksize_trampoline()</a>

```
searchKey: runtime.pthread_attr_getstacksize_trampoline
```

```Go
func pthread_attr_getstacksize_trampoline()
```

### <a id="pthread_attr_setdetachstate" href="#pthread_attr_setdetachstate">func pthread_attr_setdetachstate(attr *pthreadattr, state int) int32</a>

```
searchKey: runtime.pthread_attr_setdetachstate
```

```Go
func pthread_attr_setdetachstate(attr *pthreadattr, state int) int32
```

### <a id="pthread_attr_setdetachstate_trampoline" href="#pthread_attr_setdetachstate_trampoline">func pthread_attr_setdetachstate_trampoline()</a>

```
searchKey: runtime.pthread_attr_setdetachstate_trampoline
```

```Go
func pthread_attr_setdetachstate_trampoline()
```

### <a id="pthread_create" href="#pthread_create">func pthread_create(attr *pthreadattr, start uintptr, arg unsafe.Pointer) int32</a>

```
searchKey: runtime.pthread_create
```

```Go
func pthread_create(attr *pthreadattr, start uintptr, arg unsafe.Pointer) int32
```

### <a id="pthread_create_trampoline" href="#pthread_create_trampoline">func pthread_create_trampoline()</a>

```
searchKey: runtime.pthread_create_trampoline
```

```Go
func pthread_create_trampoline()
```

### <a id="raise" href="#raise">func raise(sig uint32)</a>

```
searchKey: runtime.raise
```

```Go
func raise(sig uint32)
```

### <a id="raise_trampoline" href="#raise_trampoline">func raise_trampoline()</a>

```
searchKey: runtime.raise_trampoline
```

```Go
func raise_trampoline()
```

### <a id="pthread_self_trampoline" href="#pthread_self_trampoline">func pthread_self_trampoline()</a>

```
searchKey: runtime.pthread_self_trampoline
```

```Go
func pthread_self_trampoline()
```

### <a id="pthread_kill" href="#pthread_kill">func pthread_kill(t pthread, sig uint32)</a>

```
searchKey: runtime.pthread_kill
```

```Go
func pthread_kill(t pthread, sig uint32)
```

### <a id="pthread_kill_trampoline" href="#pthread_kill_trampoline">func pthread_kill_trampoline()</a>

```
searchKey: runtime.pthread_kill_trampoline
```

```Go
func pthread_kill_trampoline()
```

### <a id="mmap" href="#mmap">func mmap(addr unsafe.Pointer, n uintptr, prot, flags, fd int32, off uint32) (unsafe.Pointer, int)</a>

```
searchKey: runtime.mmap
```

```Go
func mmap(addr unsafe.Pointer, n uintptr, prot, flags, fd int32, off uint32) (unsafe.Pointer, int)
```

mmap is used to do low-level memory allocation via mmap. Don't allow stack splits, since this function (used by sysAlloc) is called in a lot of low-level parts of the runtime and callers often assume it won't acquire any locks. go:nosplit 

### <a id="mmap_trampoline" href="#mmap_trampoline">func mmap_trampoline()</a>

```
searchKey: runtime.mmap_trampoline
```

```Go
func mmap_trampoline()
```

### <a id="munmap" href="#munmap">func munmap(addr unsafe.Pointer, n uintptr)</a>

```
searchKey: runtime.munmap
```

```Go
func munmap(addr unsafe.Pointer, n uintptr)
```

### <a id="munmap_trampoline" href="#munmap_trampoline">func munmap_trampoline()</a>

```
searchKey: runtime.munmap_trampoline
```

```Go
func munmap_trampoline()
```

### <a id="madvise" href="#madvise">func madvise(addr unsafe.Pointer, n uintptr, flags int32)</a>

```
searchKey: runtime.madvise
```

```Go
func madvise(addr unsafe.Pointer, n uintptr, flags int32)
```

### <a id="madvise_trampoline" href="#madvise_trampoline">func madvise_trampoline()</a>

```
searchKey: runtime.madvise_trampoline
```

```Go
func madvise_trampoline()
```

### <a id="mlock" href="#mlock">func mlock(addr unsafe.Pointer, n uintptr)</a>

```
searchKey: runtime.mlock
```

```Go
func mlock(addr unsafe.Pointer, n uintptr)
```

### <a id="mlock_trampoline" href="#mlock_trampoline">func mlock_trampoline()</a>

```
searchKey: runtime.mlock_trampoline
```

```Go
func mlock_trampoline()
```

### <a id="read" href="#read">func read(fd int32, p unsafe.Pointer, n int32) int32</a>

```
searchKey: runtime.read
```

```Go
func read(fd int32, p unsafe.Pointer, n int32) int32
```

### <a id="read_trampoline" href="#read_trampoline">func read_trampoline()</a>

```
searchKey: runtime.read_trampoline
```

```Go
func read_trampoline()
```

### <a id="pipe" href="#pipe">func pipe() (r, w int32, errno int32)</a>

```
searchKey: runtime.pipe
```

```Go
func pipe() (r, w int32, errno int32)
```

### <a id="pipe_trampoline" href="#pipe_trampoline">func pipe_trampoline()</a>

```
searchKey: runtime.pipe_trampoline
```

```Go
func pipe_trampoline()
```

### <a id="closefd" href="#closefd">func closefd(fd int32) int32</a>

```
searchKey: runtime.closefd
```

```Go
func closefd(fd int32) int32
```

### <a id="close_trampoline" href="#close_trampoline">func close_trampoline()</a>

```
searchKey: runtime.close_trampoline
```

```Go
func close_trampoline()
```

### <a id="exit" href="#exit">func exit(code int32)</a>

```
searchKey: runtime.exit
```

```Go
func exit(code int32)
```

This is exported via linkname to assembly in runtime/cgo. 

### <a id="exit_trampoline" href="#exit_trampoline">func exit_trampoline()</a>

```
searchKey: runtime.exit_trampoline
```

```Go
func exit_trampoline()
```

### <a id="usleep" href="#usleep">func usleep(usec uint32)</a>

```
searchKey: runtime.usleep
```

```Go
func usleep(usec uint32)
```

### <a id="usleep_trampoline" href="#usleep_trampoline">func usleep_trampoline()</a>

```
searchKey: runtime.usleep_trampoline
```

```Go
func usleep_trampoline()
```

### <a id="usleep_no_g" href="#usleep_no_g">func usleep_no_g(usec uint32)</a>

```
searchKey: runtime.usleep_no_g
```

```Go
func usleep_no_g(usec uint32)
```

### <a id="write1" href="#write1">func write1(fd uintptr, p unsafe.Pointer, n int32) int32</a>

```
searchKey: runtime.write1
```

```Go
func write1(fd uintptr, p unsafe.Pointer, n int32) int32
```

### <a id="write_trampoline" href="#write_trampoline">func write_trampoline()</a>

```
searchKey: runtime.write_trampoline
```

```Go
func write_trampoline()
```

### <a id="open" href="#open">func open(name *byte, mode, perm int32) (ret int32)</a>

```
searchKey: runtime.open
```

```Go
func open(name *byte, mode, perm int32) (ret int32)
```

### <a id="open_trampoline" href="#open_trampoline">func open_trampoline()</a>

```
searchKey: runtime.open_trampoline
```

```Go
func open_trampoline()
```

### <a id="nanotime1" href="#nanotime1">func nanotime1() int64</a>

```
searchKey: runtime.nanotime1
```

```Go
func nanotime1() int64
```

### <a id="nanotime_trampoline" href="#nanotime_trampoline">func nanotime_trampoline()</a>

```
searchKey: runtime.nanotime_trampoline
```

```Go
func nanotime_trampoline()
```

### <a id="walltime" href="#walltime">func walltime() (int64, int32)</a>

```
searchKey: runtime.walltime
```

```Go
func walltime() (int64, int32)
```

### <a id="walltime_trampoline" href="#walltime_trampoline">func walltime_trampoline()</a>

```
searchKey: runtime.walltime_trampoline
```

```Go
func walltime_trampoline()
```

### <a id="sigaction" href="#sigaction">func sigaction(sig uint32, new *usigactiont, old *usigactiont)</a>

```
searchKey: runtime.sigaction
```

```Go
func sigaction(sig uint32, new *usigactiont, old *usigactiont)
```

### <a id="sigaction_trampoline" href="#sigaction_trampoline">func sigaction_trampoline()</a>

```
searchKey: runtime.sigaction_trampoline
```

```Go
func sigaction_trampoline()
```

### <a id="sigprocmask" href="#sigprocmask">func sigprocmask(how uint32, new *sigset, old *sigset)</a>

```
searchKey: runtime.sigprocmask
```

```Go
func sigprocmask(how uint32, new *sigset, old *sigset)
```

### <a id="sigprocmask_trampoline" href="#sigprocmask_trampoline">func sigprocmask_trampoline()</a>

```
searchKey: runtime.sigprocmask_trampoline
```

```Go
func sigprocmask_trampoline()
```

### <a id="sigaltstack" href="#sigaltstack">func sigaltstack(new *stackt, old *stackt)</a>

```
searchKey: runtime.sigaltstack
```

```Go
func sigaltstack(new *stackt, old *stackt)
```

### <a id="sigaltstack_trampoline" href="#sigaltstack_trampoline">func sigaltstack_trampoline()</a>

```
searchKey: runtime.sigaltstack_trampoline
```

```Go
func sigaltstack_trampoline()
```

### <a id="raiseproc" href="#raiseproc">func raiseproc(sig uint32)</a>

```
searchKey: runtime.raiseproc
```

```Go
func raiseproc(sig uint32)
```

### <a id="raiseproc_trampoline" href="#raiseproc_trampoline">func raiseproc_trampoline()</a>

```
searchKey: runtime.raiseproc_trampoline
```

```Go
func raiseproc_trampoline()
```

### <a id="setitimer" href="#setitimer">func setitimer(mode int32, new, old *itimerval)</a>

```
searchKey: runtime.setitimer
```

```Go
func setitimer(mode int32, new, old *itimerval)
```

### <a id="setitimer_trampoline" href="#setitimer_trampoline">func setitimer_trampoline()</a>

```
searchKey: runtime.setitimer_trampoline
```

```Go
func setitimer_trampoline()
```

### <a id="sysctl" href="#sysctl">func sysctl(mib *uint32, miblen uint32, oldp *byte, oldlenp *uintptr, newp *byte, newlen uintptr) int32</a>

```
searchKey: runtime.sysctl
```

```Go
func sysctl(mib *uint32, miblen uint32, oldp *byte, oldlenp *uintptr, newp *byte, newlen uintptr) int32
```

### <a id="sysctl_trampoline" href="#sysctl_trampoline">func sysctl_trampoline()</a>

```
searchKey: runtime.sysctl_trampoline
```

```Go
func sysctl_trampoline()
```

### <a id="sysctlbyname" href="#sysctlbyname">func sysctlbyname(name *byte, oldp *byte, oldlenp *uintptr, newp *byte, newlen uintptr) int32</a>

```
searchKey: runtime.sysctlbyname
```

```Go
func sysctlbyname(name *byte, oldp *byte, oldlenp *uintptr, newp *byte, newlen uintptr) int32
```

### <a id="sysctlbyname_trampoline" href="#sysctlbyname_trampoline">func sysctlbyname_trampoline()</a>

```
searchKey: runtime.sysctlbyname_trampoline
```

```Go
func sysctlbyname_trampoline()
```

### <a id="fcntl" href="#fcntl">func fcntl(fd, cmd, arg int32) int32</a>

```
searchKey: runtime.fcntl
```

```Go
func fcntl(fd, cmd, arg int32) int32
```

### <a id="fcntl_trampoline" href="#fcntl_trampoline">func fcntl_trampoline()</a>

```
searchKey: runtime.fcntl_trampoline
```

```Go
func fcntl_trampoline()
```

### <a id="kqueue" href="#kqueue">func kqueue() int32</a>

```
searchKey: runtime.kqueue
```

```Go
func kqueue() int32
```

### <a id="kqueue_trampoline" href="#kqueue_trampoline">func kqueue_trampoline()</a>

```
searchKey: runtime.kqueue_trampoline
```

```Go
func kqueue_trampoline()
```

### <a id="kevent" href="#kevent">func kevent(kq int32, ch *keventt, nch int32, ev *keventt, nev int32, ts *timespec) int32</a>

```
searchKey: runtime.kevent
```

```Go
func kevent(kq int32, ch *keventt, nch int32, ev *keventt, nev int32, ts *timespec) int32
```

### <a id="kevent_trampoline" href="#kevent_trampoline">func kevent_trampoline()</a>

```
searchKey: runtime.kevent_trampoline
```

```Go
func kevent_trampoline()
```

### <a id="pthread_mutex_init" href="#pthread_mutex_init">func pthread_mutex_init(m *pthreadmutex, attr *pthreadmutexattr) int32</a>

```
searchKey: runtime.pthread_mutex_init
```

```Go
func pthread_mutex_init(m *pthreadmutex, attr *pthreadmutexattr) int32
```

### <a id="pthread_mutex_init_trampoline" href="#pthread_mutex_init_trampoline">func pthread_mutex_init_trampoline()</a>

```
searchKey: runtime.pthread_mutex_init_trampoline
```

```Go
func pthread_mutex_init_trampoline()
```

### <a id="pthread_mutex_lock" href="#pthread_mutex_lock">func pthread_mutex_lock(m *pthreadmutex) int32</a>

```
searchKey: runtime.pthread_mutex_lock
```

```Go
func pthread_mutex_lock(m *pthreadmutex) int32
```

### <a id="pthread_mutex_lock_trampoline" href="#pthread_mutex_lock_trampoline">func pthread_mutex_lock_trampoline()</a>

```
searchKey: runtime.pthread_mutex_lock_trampoline
```

```Go
func pthread_mutex_lock_trampoline()
```

### <a id="pthread_mutex_unlock" href="#pthread_mutex_unlock">func pthread_mutex_unlock(m *pthreadmutex) int32</a>

```
searchKey: runtime.pthread_mutex_unlock
```

```Go
func pthread_mutex_unlock(m *pthreadmutex) int32
```

### <a id="pthread_mutex_unlock_trampoline" href="#pthread_mutex_unlock_trampoline">func pthread_mutex_unlock_trampoline()</a>

```
searchKey: runtime.pthread_mutex_unlock_trampoline
```

```Go
func pthread_mutex_unlock_trampoline()
```

### <a id="pthread_cond_init" href="#pthread_cond_init">func pthread_cond_init(c *pthreadcond, attr *pthreadcondattr) int32</a>

```
searchKey: runtime.pthread_cond_init
```

```Go
func pthread_cond_init(c *pthreadcond, attr *pthreadcondattr) int32
```

### <a id="pthread_cond_init_trampoline" href="#pthread_cond_init_trampoline">func pthread_cond_init_trampoline()</a>

```
searchKey: runtime.pthread_cond_init_trampoline
```

```Go
func pthread_cond_init_trampoline()
```

### <a id="pthread_cond_wait" href="#pthread_cond_wait">func pthread_cond_wait(c *pthreadcond, m *pthreadmutex) int32</a>

```
searchKey: runtime.pthread_cond_wait
```

```Go
func pthread_cond_wait(c *pthreadcond, m *pthreadmutex) int32
```

### <a id="pthread_cond_wait_trampoline" href="#pthread_cond_wait_trampoline">func pthread_cond_wait_trampoline()</a>

```
searchKey: runtime.pthread_cond_wait_trampoline
```

```Go
func pthread_cond_wait_trampoline()
```

### <a id="pthread_cond_timedwait_relative_np" href="#pthread_cond_timedwait_relative_np">func pthread_cond_timedwait_relative_np(c *pthreadcond, m *pthreadmutex, t *timespec) int32</a>

```
searchKey: runtime.pthread_cond_timedwait_relative_np
```

```Go
func pthread_cond_timedwait_relative_np(c *pthreadcond, m *pthreadmutex, t *timespec) int32
```

### <a id="pthread_cond_timedwait_relative_np_trampoline" href="#pthread_cond_timedwait_relative_np_trampoline">func pthread_cond_timedwait_relative_np_trampoline()</a>

```
searchKey: runtime.pthread_cond_timedwait_relative_np_trampoline
```

```Go
func pthread_cond_timedwait_relative_np_trampoline()
```

### <a id="pthread_cond_signal" href="#pthread_cond_signal">func pthread_cond_signal(c *pthreadcond) int32</a>

```
searchKey: runtime.pthread_cond_signal
```

```Go
func pthread_cond_signal(c *pthreadcond) int32
```

### <a id="pthread_cond_signal_trampoline" href="#pthread_cond_signal_trampoline">func pthread_cond_signal_trampoline()</a>

```
searchKey: runtime.pthread_cond_signal_trampoline
```

```Go
func pthread_cond_signal_trampoline()
```

### <a id="exitThread" href="#exitThread">func exitThread(wait *uint32)</a>

```
searchKey: runtime.exitThread
```

```Go
func exitThread(wait *uint32)
```

Not used on Darwin, but must be defined. 

### <a id="closeonexec" href="#closeonexec">func closeonexec(fd int32)</a>

```
searchKey: runtime.closeonexec
```

```Go
func closeonexec(fd int32)
```

### <a id="setNonblock" href="#setNonblock">func setNonblock(fd int32)</a>

```
searchKey: runtime.setNonblock
```

```Go
func setNonblock(fd int32)
```

### <a id="libcCall" href="#libcCall">func libcCall(fn, arg unsafe.Pointer) int32</a>

```
searchKey: runtime.libcCall
```

```Go
func libcCall(fn, arg unsafe.Pointer) int32
```

Call fn with arg as its argument. Return what fn returns. fn is the raw pc value of the entry point of the desired function. Switches to the system stack, if not already there. Preserves the calling point as the location where a profiler traceback will begin. 

### <a id="prepGoExitFrame" href="#prepGoExitFrame">func prepGoExitFrame(sp uintptr)</a>

```
searchKey: runtime.prepGoExitFrame
```

```Go
func prepGoExitFrame(sp uintptr)
```

### <a id="gostartcall" href="#gostartcall">func gostartcall(buf *gobuf, fn, ctxt unsafe.Pointer)</a>

```
searchKey: runtime.gostartcall
```

```Go
func gostartcall(buf *gobuf, fn, ctxt unsafe.Pointer)
```

adjust Gobuf as if it executed a call to fn with context ctxt and then stopped before the first instruction in fn. 

### <a id="timeSleep" href="#timeSleep">func timeSleep(ns int64)</a>

```
searchKey: runtime.timeSleep
```

```Go
func timeSleep(ns int64)
```

timeSleep puts the current goroutine to sleep for at least ns nanoseconds. 

### <a id="resetForSleep" href="#resetForSleep">func resetForSleep(gp *g, ut unsafe.Pointer) bool</a>

```
searchKey: runtime.resetForSleep
```

```Go
func resetForSleep(gp *g, ut unsafe.Pointer) bool
```

resetForSleep is called after the goroutine is parked for timeSleep. We can't call resettimer in timeSleep itself because if this is a short sleep and there are many goroutines then the P can wind up running the timer function, goroutineReady, before the goroutine has been parked. 

### <a id="startTimer" href="#startTimer">func startTimer(t *timer)</a>

```
searchKey: runtime.startTimer
```

```Go
func startTimer(t *timer)
```

startTimer adds t to the timer heap. 

### <a id="stopTimer" href="#stopTimer">func stopTimer(t *timer) bool</a>

```
searchKey: runtime.stopTimer
```

```Go
func stopTimer(t *timer) bool
```

stopTimer stops a timer. It reports whether t was stopped before being run. 

### <a id="resetTimer" href="#resetTimer">func resetTimer(t *timer, when int64) bool</a>

```
searchKey: runtime.resetTimer
```

```Go
func resetTimer(t *timer, when int64) bool
```

resetTimer resets an inactive timer, adding it to the heap. Reports whether the timer was modified before it was run. 

### <a id="modTimer" href="#modTimer">func modTimer(t *timer, when, period int64, f func(interface{}, uintptr), arg interface{}, seq uintptr)</a>

```
searchKey: runtime.modTimer
```

```Go
func modTimer(t *timer, when, period int64, f func(interface{}, uintptr), arg interface{}, seq uintptr)
```

modTimer modifies an existing timer. 

### <a id="goroutineReady" href="#goroutineReady">func goroutineReady(arg interface{}, seq uintptr)</a>

```
searchKey: runtime.goroutineReady
```

```Go
func goroutineReady(arg interface{}, seq uintptr)
```

Ready the goroutine arg. 

### <a id="addtimer" href="#addtimer">func addtimer(t *timer)</a>

```
searchKey: runtime.addtimer
```

```Go
func addtimer(t *timer)
```

addtimer adds a timer to the current P. This should only be called with a newly created timer. That avoids the risk of changing the when field of a timer in some P's heap, which could cause the heap to become unsorted. 

### <a id="doaddtimer" href="#doaddtimer">func doaddtimer(pp *p, t *timer)</a>

```
searchKey: runtime.doaddtimer
```

```Go
func doaddtimer(pp *p, t *timer)
```

doaddtimer adds t to the current P's heap. The caller must have locked the timers for pp. 

### <a id="deltimer" href="#deltimer">func deltimer(t *timer) bool</a>

```
searchKey: runtime.deltimer
```

```Go
func deltimer(t *timer) bool
```

deltimer deletes the timer t. It may be on some other P, so we can't actually remove it from the timers heap. We can only mark it as deleted. It will be removed in due course by the P whose heap it is on. Reports whether the timer was removed before it was run. 

### <a id="dodeltimer" href="#dodeltimer">func dodeltimer(pp *p, i int)</a>

```
searchKey: runtime.dodeltimer
```

```Go
func dodeltimer(pp *p, i int)
```

dodeltimer removes timer i from the current P's heap. We are locked on the P when this is called. It reports whether it saw no problems due to races. The caller must have locked the timers for pp. 

### <a id="dodeltimer0" href="#dodeltimer0">func dodeltimer0(pp *p)</a>

```
searchKey: runtime.dodeltimer0
```

```Go
func dodeltimer0(pp *p)
```

dodeltimer0 removes timer 0 from the current P's heap. We are locked on the P when this is called. It reports whether it saw no problems due to races. The caller must have locked the timers for pp. 

### <a id="modtimer" href="#modtimer">func modtimer(t *timer, when, period int64, f func(interface{}, uintptr), arg interface{}, seq uintptr) bool</a>

```
searchKey: runtime.modtimer
```

```Go
func modtimer(t *timer, when, period int64, f func(interface{}, uintptr), arg interface{}, seq uintptr) bool
```

modtimer modifies an existing timer. This is called by the netpoll code or time.Ticker.Reset or time.Timer.Reset. Reports whether the timer was modified before it was run. 

### <a id="resettimer" href="#resettimer">func resettimer(t *timer, when int64) bool</a>

```
searchKey: runtime.resettimer
```

```Go
func resettimer(t *timer, when int64) bool
```

resettimer resets the time when a timer should fire. If used for an inactive timer, the timer will become active. This should be called instead of addtimer if the timer value has been, or may have been, used previously. Reports whether the timer was modified before it was run. 

### <a id="cleantimers" href="#cleantimers">func cleantimers(pp *p)</a>

```
searchKey: runtime.cleantimers
```

```Go
func cleantimers(pp *p)
```

cleantimers cleans up the head of the timer queue. This speeds up programs that create and delete timers; leaving them in the heap slows down addtimer. Reports whether no timer problems were found. The caller must have locked the timers for pp. 

### <a id="moveTimers" href="#moveTimers">func moveTimers(pp *p, timers []*timer)</a>

```
searchKey: runtime.moveTimers
```

```Go
func moveTimers(pp *p, timers []*timer)
```

moveTimers moves a slice of timers to pp. The slice has been taken from a different P. This is currently called when the world is stopped, but the caller is expected to have locked the timers for pp. 

### <a id="adjusttimers" href="#adjusttimers">func adjusttimers(pp *p, now int64)</a>

```
searchKey: runtime.adjusttimers
```

```Go
func adjusttimers(pp *p, now int64)
```

adjusttimers looks through the timers in the current P's heap for any timers that have been modified to run earlier, and puts them in the correct place in the heap. While looking for those timers, it also moves timers that have been modified to run later, and removes deleted timers. The caller must have locked the timers for pp. 

### <a id="addAdjustedTimers" href="#addAdjustedTimers">func addAdjustedTimers(pp *p, moved []*timer)</a>

```
searchKey: runtime.addAdjustedTimers
```

```Go
func addAdjustedTimers(pp *p, moved []*timer)
```

addAdjustedTimers adds any timers we adjusted in adjusttimers back to the timer heap. 

### <a id="nobarrierWakeTime" href="#nobarrierWakeTime">func nobarrierWakeTime(pp *p) int64</a>

```
searchKey: runtime.nobarrierWakeTime
```

```Go
func nobarrierWakeTime(pp *p) int64
```

nobarrierWakeTime looks at P's timers and returns the time when we should wake up the netpoller. It returns 0 if there are no timers. This function is invoked when dropping a P, and must run without any write barriers. 

### <a id="runtimer" href="#runtimer">func runtimer(pp *p, now int64) int64</a>

```
searchKey: runtime.runtimer
```

```Go
func runtimer(pp *p, now int64) int64
```

runtimer examines the first timer in timers. If it is ready based on now, it runs the timer and removes or updates it. Returns 0 if it ran a timer, -1 if there are no more timers, or the time when the first timer should run. The caller must have locked the timers for pp. If a timer is run, this will temporarily unlock the timers. 

### <a id="runOneTimer" href="#runOneTimer">func runOneTimer(pp *p, t *timer, now int64)</a>

```
searchKey: runtime.runOneTimer
```

```Go
func runOneTimer(pp *p, t *timer, now int64)
```

runOneTimer runs a single timer. The caller must have locked the timers for pp. This will temporarily unlock the timers while running the timer function. 

### <a id="clearDeletedTimers" href="#clearDeletedTimers">func clearDeletedTimers(pp *p)</a>

```
searchKey: runtime.clearDeletedTimers
```

```Go
func clearDeletedTimers(pp *p)
```

clearDeletedTimers removes all deleted timers from the P's timer heap. This is used to avoid clogging up the heap if the program starts a lot of long-running timers and then stops them. For example, this can happen via context.WithTimeout. 

This is the only function that walks through the entire timer heap, other than moveTimers which only runs when the world is stopped. 

The caller must have locked the timers for pp. 

### <a id="verifyTimerHeap" href="#verifyTimerHeap">func verifyTimerHeap(pp *p)</a>

```
searchKey: runtime.verifyTimerHeap
```

```Go
func verifyTimerHeap(pp *p)
```

verifyTimerHeap verifies that the timer heap is in a valid state. This is only for debugging, and is only called if verifyTimers is true. The caller must have locked the timers. 

### <a id="updateTimer0When" href="#updateTimer0When">func updateTimer0When(pp *p)</a>

```
searchKey: runtime.updateTimer0When
```

```Go
func updateTimer0When(pp *p)
```

updateTimer0When sets the P's timer0When field. The caller must have locked the timers for pp. 

### <a id="updateTimerModifiedEarliest" href="#updateTimerModifiedEarliest">func updateTimerModifiedEarliest(pp *p, nextwhen int64)</a>

```
searchKey: runtime.updateTimerModifiedEarliest
```

```Go
func updateTimerModifiedEarliest(pp *p, nextwhen int64)
```

updateTimerModifiedEarliest updates the recorded nextwhen field of the earlier timerModifiedEarier value. The timers for pp will not be locked. 

### <a id="siftupTimer" href="#siftupTimer">func siftupTimer(t []*timer, i int)</a>

```
searchKey: runtime.siftupTimer
```

```Go
func siftupTimer(t []*timer, i int)
```

### <a id="siftdownTimer" href="#siftdownTimer">func siftdownTimer(t []*timer, i int)</a>

```
searchKey: runtime.siftdownTimer
```

```Go
func siftdownTimer(t []*timer, i int)
```

### <a id="badTimer" href="#badTimer">func badTimer()</a>

```
searchKey: runtime.badTimer
```

```Go
func badTimer()
```

badTimer is called if the timer data structures have been corrupted, presumably due to racy use by the program. We panic here rather than panicing due to invalid slice access while holding locks. See issue #25686. 

### <a id="nanotime" href="#nanotime">func nanotime() int64</a>

```
searchKey: runtime.nanotime
```

```Go
func nanotime() int64
```

### <a id="write" href="#write">func write(fd uintptr, p unsafe.Pointer, n int32) int32</a>

```
searchKey: runtime.write
```

```Go
func write(fd uintptr, p unsafe.Pointer, n int32) int32
```

write must be nosplit on Windows (see write1) 

### <a id="time_now" href="#time_now">func time_now() (sec int64, nsec int32, mono int64)</a>

```
searchKey: runtime.time_now
```

```Go
func time_now() (sec int64, nsec int32, mono int64)
```

### <a id="osSetupTLS" href="#osSetupTLS">func osSetupTLS(mp *m)</a>

```
searchKey: runtime.osSetupTLS
```

```Go
func osSetupTLS(mp *m)
```

### <a id="StartTrace" href="#StartTrace">func StartTrace() error</a>

```
searchKey: runtime.StartTrace
tags: [exported]
```

```Go
func StartTrace() error
```

StartTrace enables tracing for the current process. While tracing, the data will be buffered and available via ReadTrace. StartTrace returns an error if tracing is already enabled. Most clients should use the runtime/trace package or the testing package's -test.trace flag instead of calling StartTrace directly. 

### <a id="StopTrace" href="#StopTrace">func StopTrace()</a>

```
searchKey: runtime.StopTrace
tags: [exported]
```

```Go
func StopTrace()
```

StopTrace stops tracing, if it was previously enabled. StopTrace only returns after all the reads for the trace have completed. 

### <a id="ReadTrace" href="#ReadTrace">func ReadTrace() []byte</a>

```
searchKey: runtime.ReadTrace
tags: [exported]
```

```Go
func ReadTrace() []byte
```

ReadTrace returns the next chunk of binary tracing data, blocking until data is available. If tracing is turned off and all the data accumulated while it was on has been returned, ReadTrace returns nil. The caller must copy the returned data before calling ReadTrace again. ReadTrace must be called from one goroutine at a time. 

### <a id="traceProcFree" href="#traceProcFree">func traceProcFree(pp *p)</a>

```
searchKey: runtime.traceProcFree
```

```Go
func traceProcFree(pp *p)
```

traceProcFree frees trace buffer associated with pp. 

### <a id="traceFullQueue" href="#traceFullQueue">func traceFullQueue(buf traceBufPtr)</a>

```
searchKey: runtime.traceFullQueue
```

```Go
func traceFullQueue(buf traceBufPtr)
```

traceFullQueue queues buf into queue of full buffers. 

### <a id="traceEvent" href="#traceEvent">func traceEvent(ev byte, skip int, args ...uint64)</a>

```
searchKey: runtime.traceEvent
```

```Go
func traceEvent(ev byte, skip int, args ...uint64)
```

traceEvent writes a single event to trace buffer, flushing the buffer if necessary. ev is event type. If skip > 0, write current stack id as the last argument (skipping skip top frames). If skip = 0, this event type should contain a stack, but we don't want to collect and remember it for this particular call. 

### <a id="traceEventLocked" href="#traceEventLocked">func traceEventLocked(extraBytes int, mp *m, pid int32, bufp *traceBufPtr, ev byte, skip int, args ...uint64)</a>

```
searchKey: runtime.traceEventLocked
```

```Go
func traceEventLocked(extraBytes int, mp *m, pid int32, bufp *traceBufPtr, ev byte, skip int, args ...uint64)
```

### <a id="traceStackID" href="#traceStackID">func traceStackID(mp *m, buf []uintptr, skip int) uint64</a>

```
searchKey: runtime.traceStackID
```

```Go
func traceStackID(mp *m, buf []uintptr, skip int) uint64
```

### <a id="traceReleaseBuffer" href="#traceReleaseBuffer">func traceReleaseBuffer(pid int32)</a>

```
searchKey: runtime.traceReleaseBuffer
```

```Go
func traceReleaseBuffer(pid int32)
```

traceReleaseBuffer releases a buffer previously acquired with traceAcquireBuffer. 

### <a id="traceAppend" href="#traceAppend">func traceAppend(buf []byte, v uint64) []byte</a>

```
searchKey: runtime.traceAppend
```

```Go
func traceAppend(buf []byte, v uint64) []byte
```

traceAppend appends v to buf in little-endian-base-128 encoding. 

### <a id="allFrames" href="#allFrames">func allFrames(pcs []uintptr) []Frame</a>

```
searchKey: runtime.allFrames
```

```Go
func allFrames(pcs []uintptr) []Frame
```

allFrames returns all of the Frames corresponding to pcs. 

### <a id="traceGomaxprocs" href="#traceGomaxprocs">func traceGomaxprocs(procs int32)</a>

```
searchKey: runtime.traceGomaxprocs
```

```Go
func traceGomaxprocs(procs int32)
```

### <a id="traceProcStart" href="#traceProcStart">func traceProcStart()</a>

```
searchKey: runtime.traceProcStart
```

```Go
func traceProcStart()
```

### <a id="traceProcStop" href="#traceProcStop">func traceProcStop(pp *p)</a>

```
searchKey: runtime.traceProcStop
```

```Go
func traceProcStop(pp *p)
```

### <a id="traceGCStart" href="#traceGCStart">func traceGCStart()</a>

```
searchKey: runtime.traceGCStart
```

```Go
func traceGCStart()
```

### <a id="traceGCDone" href="#traceGCDone">func traceGCDone()</a>

```
searchKey: runtime.traceGCDone
```

```Go
func traceGCDone()
```

### <a id="traceGCSTWStart" href="#traceGCSTWStart">func traceGCSTWStart(kind int)</a>

```
searchKey: runtime.traceGCSTWStart
```

```Go
func traceGCSTWStart(kind int)
```

### <a id="traceGCSTWDone" href="#traceGCSTWDone">func traceGCSTWDone()</a>

```
searchKey: runtime.traceGCSTWDone
```

```Go
func traceGCSTWDone()
```

### <a id="traceGCSweepStart" href="#traceGCSweepStart">func traceGCSweepStart()</a>

```
searchKey: runtime.traceGCSweepStart
```

```Go
func traceGCSweepStart()
```

traceGCSweepStart prepares to trace a sweep loop. This does not emit any events until traceGCSweepSpan is called. 

traceGCSweepStart must be paired with traceGCSweepDone and there must be no preemption points between these two calls. 

### <a id="traceGCSweepSpan" href="#traceGCSweepSpan">func traceGCSweepSpan(bytesSwept uintptr)</a>

```
searchKey: runtime.traceGCSweepSpan
```

```Go
func traceGCSweepSpan(bytesSwept uintptr)
```

traceGCSweepSpan traces the sweep of a single page. 

This may be called outside a traceGCSweepStart/traceGCSweepDone pair; however, it will not emit any trace events in this case. 

### <a id="traceGCSweepDone" href="#traceGCSweepDone">func traceGCSweepDone()</a>

```
searchKey: runtime.traceGCSweepDone
```

```Go
func traceGCSweepDone()
```

### <a id="traceGCMarkAssistStart" href="#traceGCMarkAssistStart">func traceGCMarkAssistStart()</a>

```
searchKey: runtime.traceGCMarkAssistStart
```

```Go
func traceGCMarkAssistStart()
```

### <a id="traceGCMarkAssistDone" href="#traceGCMarkAssistDone">func traceGCMarkAssistDone()</a>

```
searchKey: runtime.traceGCMarkAssistDone
```

```Go
func traceGCMarkAssistDone()
```

### <a id="traceGoCreate" href="#traceGoCreate">func traceGoCreate(newg *g, pc uintptr)</a>

```
searchKey: runtime.traceGoCreate
```

```Go
func traceGoCreate(newg *g, pc uintptr)
```

### <a id="traceGoStart" href="#traceGoStart">func traceGoStart()</a>

```
searchKey: runtime.traceGoStart
```

```Go
func traceGoStart()
```

### <a id="traceGoEnd" href="#traceGoEnd">func traceGoEnd()</a>

```
searchKey: runtime.traceGoEnd
```

```Go
func traceGoEnd()
```

### <a id="traceGoSched" href="#traceGoSched">func traceGoSched()</a>

```
searchKey: runtime.traceGoSched
```

```Go
func traceGoSched()
```

### <a id="traceGoPreempt" href="#traceGoPreempt">func traceGoPreempt()</a>

```
searchKey: runtime.traceGoPreempt
```

```Go
func traceGoPreempt()
```

### <a id="traceGoPark" href="#traceGoPark">func traceGoPark(traceEv byte, skip int)</a>

```
searchKey: runtime.traceGoPark
```

```Go
func traceGoPark(traceEv byte, skip int)
```

### <a id="traceGoUnpark" href="#traceGoUnpark">func traceGoUnpark(gp *g, skip int)</a>

```
searchKey: runtime.traceGoUnpark
```

```Go
func traceGoUnpark(gp *g, skip int)
```

### <a id="traceGoSysCall" href="#traceGoSysCall">func traceGoSysCall()</a>

```
searchKey: runtime.traceGoSysCall
```

```Go
func traceGoSysCall()
```

### <a id="traceGoSysExit" href="#traceGoSysExit">func traceGoSysExit(ts int64)</a>

```
searchKey: runtime.traceGoSysExit
```

```Go
func traceGoSysExit(ts int64)
```

### <a id="traceGoSysBlock" href="#traceGoSysBlock">func traceGoSysBlock(pp *p)</a>

```
searchKey: runtime.traceGoSysBlock
```

```Go
func traceGoSysBlock(pp *p)
```

### <a id="traceHeapAlloc" href="#traceHeapAlloc">func traceHeapAlloc()</a>

```
searchKey: runtime.traceHeapAlloc
```

```Go
func traceHeapAlloc()
```

### <a id="traceHeapGoal" href="#traceHeapGoal">func traceHeapGoal()</a>

```
searchKey: runtime.traceHeapGoal
```

```Go
func traceHeapGoal()
```

### <a id="trace_userTaskCreate" href="#trace_userTaskCreate">func trace_userTaskCreate(id, parentID uint64, taskType string)</a>

```
searchKey: runtime.trace_userTaskCreate
```

```Go
func trace_userTaskCreate(id, parentID uint64, taskType string)
```

### <a id="trace_userTaskEnd" href="#trace_userTaskEnd">func trace_userTaskEnd(id uint64)</a>

```
searchKey: runtime.trace_userTaskEnd
```

```Go
func trace_userTaskEnd(id uint64)
```

### <a id="trace_userRegion" href="#trace_userRegion">func trace_userRegion(id, mode uint64, name string)</a>

```
searchKey: runtime.trace_userRegion
```

```Go
func trace_userRegion(id, mode uint64, name string)
```

### <a id="trace_userLog" href="#trace_userLog">func trace_userLog(id uint64, category, message string)</a>

```
searchKey: runtime.trace_userLog
```

```Go
func trace_userLog(id uint64, category, message string)
```

### <a id="tracebackdefers" href="#tracebackdefers">func tracebackdefers(gp *g, callback func(*stkframe, unsafe.Pointer) bool, v unsafe.Pointer)</a>

```
searchKey: runtime.tracebackdefers
```

```Go
func tracebackdefers(gp *g, callback func(*stkframe, unsafe.Pointer) bool, v unsafe.Pointer)
```

Traceback over the deferred function calls. Report them like calls that have been invoked but not started executing yet. 

### <a id="gentraceback" href="#gentraceback">func gentraceback(pc0, sp0, lr0 uintptr, gp *g, skip int, pcbuf *uintptr, max int, callback func(*stkframe, unsafe.Pointer) bool, v unsafe.Pointer, flags uint) int</a>

```
searchKey: runtime.gentraceback
```

```Go
func gentraceback(pc0, sp0, lr0 uintptr, gp *g, skip int, pcbuf *uintptr, max int, callback func(*stkframe, unsafe.Pointer) bool, v unsafe.Pointer, flags uint) int
```

Generic traceback. Handles runtime stack prints (pcbuf == nil), the runtime.Callers function (pcbuf != nil), as well as the garbage collector (callback != nil).  A little clunky to merge these, but avoids duplicating the code and all its subtlety. 

The skip argument is only valid with pcbuf != nil and counts the number of logical frames to skip rather than physical frames (with inlining, a PC in pcbuf can represent multiple calls). 

### <a id="printArgs" href="#printArgs">func printArgs(f funcInfo, argp unsafe.Pointer)</a>

```
searchKey: runtime.printArgs
```

```Go
func printArgs(f funcInfo, argp unsafe.Pointer)
```

printArgs prints function arguments in traceback. 

### <a id="tracebackCgoContext" href="#tracebackCgoContext">func tracebackCgoContext(pcbuf *uintptr, printing bool, ctxt uintptr, n, max int) int</a>

```
searchKey: runtime.tracebackCgoContext
```

```Go
func tracebackCgoContext(pcbuf *uintptr, printing bool, ctxt uintptr, n, max int) int
```

tracebackCgoContext handles tracing back a cgo context value, from the context argument to setCgoTraceback, for the gentraceback function. It returns the new value of n. 

### <a id="printcreatedby" href="#printcreatedby">func printcreatedby(gp *g)</a>

```
searchKey: runtime.printcreatedby
```

```Go
func printcreatedby(gp *g)
```

### <a id="printcreatedby1" href="#printcreatedby1">func printcreatedby1(f funcInfo, pc uintptr)</a>

```
searchKey: runtime.printcreatedby1
```

```Go
func printcreatedby1(f funcInfo, pc uintptr)
```

### <a id="traceback" href="#traceback">func traceback(pc, sp, lr uintptr, gp *g)</a>

```
searchKey: runtime.traceback
```

```Go
func traceback(pc, sp, lr uintptr, gp *g)
```

### <a id="tracebacktrap" href="#tracebacktrap">func tracebacktrap(pc, sp, lr uintptr, gp *g)</a>

```
searchKey: runtime.tracebacktrap
```

```Go
func tracebacktrap(pc, sp, lr uintptr, gp *g)
```

tracebacktrap is like traceback but expects that the PC and SP were obtained from a trap, not from gp->sched or gp->syscallpc/gp->syscallsp or getcallerpc/getcallersp. Because they are from a trap instead of from a saved pair, the initial PC must not be rewound to the previous instruction. (All the saved pairs record a PC that is a return address, so we rewind it into the CALL instruction.) If gp.m.libcall{g,pc,sp} information is available, it uses that information in preference to the pc/sp/lr passed in. 

### <a id="traceback1" href="#traceback1">func traceback1(pc, sp, lr uintptr, gp *g, flags uint)</a>

```
searchKey: runtime.traceback1
```

```Go
func traceback1(pc, sp, lr uintptr, gp *g, flags uint)
```

### <a id="printAncestorTraceback" href="#printAncestorTraceback">func printAncestorTraceback(ancestor ancestorInfo)</a>

```
searchKey: runtime.printAncestorTraceback
```

```Go
func printAncestorTraceback(ancestor ancestorInfo)
```

printAncestorTraceback prints the traceback of the given ancestor. TODO: Unify this with gentraceback and CallersFrames. 

### <a id="printAncestorTracebackFuncInfo" href="#printAncestorTracebackFuncInfo">func printAncestorTracebackFuncInfo(f funcInfo, pc uintptr)</a>

```
searchKey: runtime.printAncestorTracebackFuncInfo
```

```Go
func printAncestorTracebackFuncInfo(f funcInfo, pc uintptr)
```

printAncestorTraceback prints the given function info at a given pc within an ancestor traceback. The precision of this info is reduced due to only have access to the pcs at the time of the caller goroutine being created. 

### <a id="callers" href="#callers">func callers(skip int, pcbuf []uintptr) int</a>

```
searchKey: runtime.callers
```

```Go
func callers(skip int, pcbuf []uintptr) int
```

### <a id="gcallers" href="#gcallers">func gcallers(gp *g, skip int, pcbuf []uintptr) int</a>

```
searchKey: runtime.gcallers
```

```Go
func gcallers(gp *g, skip int, pcbuf []uintptr) int
```

### <a id="showframe" href="#showframe">func showframe(f funcInfo, gp *g, firstFrame bool, funcID, childID funcID) bool</a>

```
searchKey: runtime.showframe
```

```Go
func showframe(f funcInfo, gp *g, firstFrame bool, funcID, childID funcID) bool
```

showframe reports whether the frame with the given characteristics should be printed during a traceback. 

### <a id="showfuncinfo" href="#showfuncinfo">func showfuncinfo(f funcInfo, firstFrame bool, funcID, childID funcID) bool</a>

```
searchKey: runtime.showfuncinfo
```

```Go
func showfuncinfo(f funcInfo, firstFrame bool, funcID, childID funcID) bool
```

showfuncinfo reports whether a function with the given characteristics should be printed during a traceback. 

### <a id="isExportedRuntime" href="#isExportedRuntime">func isExportedRuntime(name string) bool</a>

```
searchKey: runtime.isExportedRuntime
```

```Go
func isExportedRuntime(name string) bool
```

isExportedRuntime reports whether name is an exported runtime function. It is only for runtime functions, so ASCII A-Z is fine. 

### <a id="elideWrapperCalling" href="#elideWrapperCalling">func elideWrapperCalling(id funcID) bool</a>

```
searchKey: runtime.elideWrapperCalling
```

```Go
func elideWrapperCalling(id funcID) bool
```

elideWrapperCalling reports whether a wrapper function that called function id should be elided from stack traces. 

### <a id="goroutineheader" href="#goroutineheader">func goroutineheader(gp *g)</a>

```
searchKey: runtime.goroutineheader
```

```Go
func goroutineheader(gp *g)
```

### <a id="tracebackothers" href="#tracebackothers">func tracebackothers(me *g)</a>

```
searchKey: runtime.tracebackothers
```

```Go
func tracebackothers(me *g)
```

### <a id="tracebackHexdump" href="#tracebackHexdump">func tracebackHexdump(stk stack, frame *stkframe, bad uintptr)</a>

```
searchKey: runtime.tracebackHexdump
```

```Go
func tracebackHexdump(stk stack, frame *stkframe, bad uintptr)
```

tracebackHexdump hexdumps part of stk around frame.sp and frame.fp for debugging purposes. If the address bad is included in the hexdumped range, it will mark it as well. 

### <a id="isSystemGoroutine" href="#isSystemGoroutine">func isSystemGoroutine(gp *g, fixed bool) bool</a>

```
searchKey: runtime.isSystemGoroutine
```

```Go
func isSystemGoroutine(gp *g, fixed bool) bool
```

isSystemGoroutine reports whether the goroutine g must be omitted in stack dumps and deadlock detector. This is any goroutine that starts at a runtime.* entry point, except for runtime.main, runtime.handleAsyncEvent (wasm only) and sometimes runtime.runfinq. 

If fixed is true, any goroutine that can vary between user and system (that is, the finalizer goroutine) is considered a user goroutine. 

### <a id="SetCgoTraceback" href="#SetCgoTraceback">func SetCgoTraceback(version int, traceback, context, symbolizer unsafe.Pointer)</a>

```
searchKey: runtime.SetCgoTraceback
tags: [exported]
```

```Go
func SetCgoTraceback(version int, traceback, context, symbolizer unsafe.Pointer)
```

SetCgoTraceback records three C functions to use to gather traceback information from C code and to convert that traceback information into symbolic information. These are used when printing stack traces for a program that uses cgo. 

The traceback and context functions may be called from a signal handler, and must therefore use only async-signal safe functions. The symbolizer function may be called while the program is crashing, and so must be cautious about using memory.  None of the functions may call back into Go. 

The context function will be called with a single argument, a pointer to a struct: 

```
struct {
	Context uintptr
}

```
In C syntax, this struct will be 

```
struct {
	uintptr_t Context;
};

```
If the Context field is 0, the context function is being called to record the current traceback context. It should record in the Context field whatever information is needed about the current point of execution to later produce a stack trace, probably the stack pointer and PC. In this case the context function will be called from C code. 

If the Context field is not 0, then it is a value returned by a previous call to the context function. This case is called when the context is no longer needed; that is, when the Go code is returning to its C code caller. This permits the context function to release any associated resources. 

While it would be correct for the context function to record a complete a stack trace whenever it is called, and simply copy that out in the traceback function, in a typical program the context function will be called many times without ever recording a traceback for that context. Recording a complete stack trace in a call to the context function is likely to be inefficient. 

The traceback function will be called with a single argument, a pointer to a struct: 

```
struct {
	Context    uintptr
	SigContext uintptr
	Buf        *uintptr
	Max        uintptr
}

```
In C syntax, this struct will be 

```
struct {
	uintptr_t  Context;
	uintptr_t  SigContext;
	uintptr_t* Buf;
	uintptr_t  Max;
};

```
The Context field will be zero to gather a traceback from the current program execution point. In this case, the traceback function will be called from C code. 

Otherwise Context will be a value previously returned by a call to the context function. The traceback function should gather a stack trace from that saved point in the program execution. The traceback function may be called from an execution thread other than the one that recorded the context, but only when the context is known to be valid and unchanging. The traceback function may also be called deeper in the call stack on the same thread that recorded the context. The traceback function may be called multiple times with the same Context value; it will usually be appropriate to cache the result, if possible, the first time this is called for a specific context value. 

If the traceback function is called from a signal handler on a Unix system, SigContext will be the signal context argument passed to the signal handler (a C ucontext_t* cast to uintptr_t). This may be used to start tracing at the point where the signal occurred. If the traceback function is not called from a signal handler, SigContext will be zero. 

Buf is where the traceback information should be stored. It should be PC values, such that Buf[0] is the PC of the caller, Buf[1] is the PC of that function's caller, and so on.  Max is the maximum number of entries to store.  The function should store a zero to indicate the top of the stack, or that the caller is on a different stack, presumably a Go stack. 

Unlike runtime.Callers, the PC values returned should, when passed to the symbolizer function, return the file/line of the call instruction.  No additional subtraction is required or appropriate. 

On all platforms, the traceback function is invoked when a call from Go to C to Go requests a stack trace. On linux/amd64, linux/ppc64le, and freebsd/amd64, the traceback function is also invoked when a signal is received by a thread that is executing a cgo call. The traceback function should not make assumptions about when it is called, as future versions of Go may make additional calls. 

The symbolizer function will be called with a single argument, a pointer to a struct: 

```
struct {
	PC      uintptr // program counter to fetch information for
	File    *byte   // file name (NUL terminated)
	Lineno  uintptr // line number
	Func    *byte   // function name (NUL terminated)
	Entry   uintptr // function entry point
	More    uintptr // set non-zero if more info for this PC
	Data    uintptr // unused by runtime, available for function
}

```
In C syntax, this struct will be 

```
struct {
	uintptr_t PC;
	char*     File;
	uintptr_t Lineno;
	char*     Func;
	uintptr_t Entry;
	uintptr_t More;
	uintptr_t Data;
};

```
The PC field will be a value returned by a call to the traceback function. 

The first time the function is called for a particular traceback, all the fields except PC will be 0. The function should fill in the other fields if possible, setting them to 0/nil if the information is not available. The Data field may be used to store any useful information across calls. The More field should be set to non-zero if there is more information for this PC, zero otherwise. If More is set non-zero, the function will be called again with the same PC, and may return different information (this is intended for use with inlined functions). If More is zero, the function will be called with the next PC value in the traceback. When the traceback is complete, the function will be called once more with PC set to zero; this may be used to free any information. Each call will leave the fields of the struct set to the same values they had upon return, except for the PC field when the More field is zero. The function must not keep a copy of the struct pointer between calls. 

When calling SetCgoTraceback, the version argument is the version number of the structs that the functions expect to receive. Currently this must be zero. 

The symbolizer function may be nil, in which case the results of the traceback function will be displayed as numbers. If the traceback function is nil, the symbolizer function will never be called. The context function may be nil, in which case the traceback function will only be called with the context field set to zero.  If the context function is nil, then calls from Go to C to Go will not show a traceback for the C portion of the call stack. 

SetCgoTraceback should be called only once, ideally from an init function. 

### <a id="printCgoTraceback" href="#printCgoTraceback">func printCgoTraceback(callers *cgoCallers)</a>

```
searchKey: runtime.printCgoTraceback
```

```Go
func printCgoTraceback(callers *cgoCallers)
```

cgoTraceback prints a traceback of callers. 

### <a id="printOneCgoTraceback" href="#printOneCgoTraceback">func printOneCgoTraceback(pc uintptr, max int, arg *cgoSymbolizerArg) int</a>

```
searchKey: runtime.printOneCgoTraceback
```

```Go
func printOneCgoTraceback(pc uintptr, max int, arg *cgoSymbolizerArg) int
```

printOneCgoTraceback prints the traceback of a single cgo caller. This can print more than one line because of inlining. Returns the number of frames printed. 

### <a id="callCgoSymbolizer" href="#callCgoSymbolizer">func callCgoSymbolizer(arg *cgoSymbolizerArg)</a>

```
searchKey: runtime.callCgoSymbolizer
```

```Go
func callCgoSymbolizer(arg *cgoSymbolizerArg)
```

callCgoSymbolizer calls the cgoSymbolizer function. 

### <a id="cgoContextPCs" href="#cgoContextPCs">func cgoContextPCs(ctxt uintptr, buf []uintptr)</a>

```
searchKey: runtime.cgoContextPCs
```

```Go
func cgoContextPCs(ctxt uintptr, buf []uintptr)
```

cgoContextPCs gets the PC values from a cgo traceback. 

### <a id="reflectOffsLock" href="#reflectOffsLock">func reflectOffsLock()</a>

```
searchKey: runtime.reflectOffsLock
```

```Go
func reflectOffsLock()
```

### <a id="reflectOffsUnlock" href="#reflectOffsUnlock">func reflectOffsUnlock()</a>

```
searchKey: runtime.reflectOffsUnlock
```

```Go
func reflectOffsUnlock()
```

### <a id="typelinksinit" href="#typelinksinit">func typelinksinit()</a>

```
searchKey: runtime.typelinksinit
```

```Go
func typelinksinit()
```

typelinksinit scans the types from extra modules and builds the moduledata typemap used to de-duplicate type pointers. 

### <a id="typesEqual" href="#typesEqual">func typesEqual(t, v *_type, seen map[_typePair]struct{}) bool</a>

```
searchKey: runtime.typesEqual
```

```Go
func typesEqual(t, v *_type, seen map[_typePair]struct{}) bool
```

typesEqual reports whether two types are equal. 

Everywhere in the runtime and reflect packages, it is assumed that there is exactly one *_type per Go type, so that pointer equality can be used to test if types are equal. There is one place that breaks this assumption: buildmode=shared. In this case a type can appear as two different pieces of memory. This is hidden from the runtime and reflect package by the per-module typemap built in typelinksinit. It uses typesEqual to map types from later modules back into earlier ones. 

Only typelinksinit needs this function. 

### <a id="isDirectIface" href="#isDirectIface">func isDirectIface(t *_type) bool</a>

```
searchKey: runtime.isDirectIface
```

```Go
func isDirectIface(t *_type) bool
```

isDirectIface reports whether t is stored directly in an interface value. 

### <a id="countrunes" href="#countrunes">func countrunes(s string) int</a>

```
searchKey: runtime.countrunes
```

```Go
func countrunes(s string) int
```

countrunes returns the number of runes in s. 

### <a id="decoderune" href="#decoderune">func decoderune(s string, k int) (r rune, pos int)</a>

```
searchKey: runtime.decoderune
```

```Go
func decoderune(s string, k int) (r rune, pos int)
```

decoderune returns the non-ASCII rune at the start of s[k:] and the index after the rune in s. 

decoderune assumes that caller has checked that the to be decoded rune is a non-ASCII rune. 

If the string appears to be incomplete or decoding problems are encountered (runeerror, k + 1) is returned to ensure progress when decoderune is used to iterate over a string. 

### <a id="encoderune" href="#encoderune">func encoderune(p []byte, r rune) int</a>

```
searchKey: runtime.encoderune
```

```Go
func encoderune(p []byte, r rune) int
```

encoderune writes into p (which must be large enough) the UTF-8 encoding of the rune. It returns the number of bytes written. 

### <a id="inVDSOPage" href="#inVDSOPage">func inVDSOPage(pc uintptr) bool</a>

```
searchKey: runtime.inVDSOPage
```

```Go
func inVDSOPage(pc uintptr) bool
```

### <a id="writeErr" href="#writeErr">func writeErr(b []byte)</a>

```
searchKey: runtime.writeErr
```

```Go
func writeErr(b []byte)
```

### <a id="Fcntl" href="#Fcntl">func Fcntl(fd, cmd, arg uintptr) (uintptr, uintptr)</a>

```
searchKey: runtime.Fcntl
```

```Go
func Fcntl(fd, cmd, arg uintptr) (uintptr, uintptr)
```

### <a id="DumpDebugLog" href="#DumpDebugLog">func DumpDebugLog() string</a>

```
searchKey: runtime.DumpDebugLog
```

```Go
func DumpDebugLog() string
```

### <a id="ResetDebugLog" href="#ResetDebugLog">func ResetDebugLog()</a>

```
searchKey: runtime.ResetDebugLog
```

```Go
func ResetDebugLog()
```

### <a id="GetPhysPageSize" href="#GetPhysPageSize">func GetPhysPageSize() uintptr</a>

```
searchKey: runtime.GetPhysPageSize
```

```Go
func GetPhysPageSize() uintptr
```

### <a id="LFStackPush" href="#LFStackPush">func LFStackPush(head *uint64, node *LFNode)</a>

```
searchKey: runtime.LFStackPush
```

```Go
func LFStackPush(head *uint64, node *LFNode)
```

### <a id="Netpoll" href="#Netpoll">func Netpoll(delta int64)</a>

```
searchKey: runtime.Netpoll
```

```Go
func Netpoll(delta int64)
```

### <a id="GCMask" href="#GCMask">func GCMask(x interface{}) (ret []byte)</a>

```
searchKey: runtime.GCMask
```

```Go
func GCMask(x interface{}) (ret []byte)
```

### <a id="RunSchedLocalQueueTest" href="#RunSchedLocalQueueTest">func RunSchedLocalQueueTest()</a>

```
searchKey: runtime.RunSchedLocalQueueTest
```

```Go
func RunSchedLocalQueueTest()
```

### <a id="RunSchedLocalQueueStealTest" href="#RunSchedLocalQueueStealTest">func RunSchedLocalQueueStealTest()</a>

```
searchKey: runtime.RunSchedLocalQueueStealTest
```

```Go
func RunSchedLocalQueueStealTest()
```

### <a id="RunSchedLocalQueueEmptyTest" href="#RunSchedLocalQueueEmptyTest">func RunSchedLocalQueueEmptyTest(iters int)</a>

```
searchKey: runtime.RunSchedLocalQueueEmptyTest
```

```Go
func RunSchedLocalQueueEmptyTest(iters int)
```

### <a id="MemclrBytes" href="#MemclrBytes">func MemclrBytes(b []byte)</a>

```
searchKey: runtime.MemclrBytes
```

```Go
func MemclrBytes(b []byte)
```

### <a id="GostringW" href="#GostringW">func GostringW(w []uint16) (s string)</a>

```
searchKey: runtime.GostringW
```

```Go
func GostringW(w []uint16) (s string)
```

entry point for testing 

### <a id="Envs" href="#Envs">func Envs() []string</a>

```
searchKey: runtime.Envs
```

```Go
func Envs() []string
```

### <a id="SetEnvs" href="#SetEnvs">func SetEnvs(e []string)</a>

```
searchKey: runtime.SetEnvs
```

```Go
func SetEnvs(e []string)
```

### <a id="BenchSetType" href="#BenchSetType">func BenchSetType(n int, x interface{})</a>

```
searchKey: runtime.BenchSetType
```

```Go
func BenchSetType(n int, x interface{})
```

### <a id="SetTracebackEnv" href="#SetTracebackEnv">func SetTracebackEnv(level string)</a>

```
searchKey: runtime.SetTracebackEnv
```

```Go
func SetTracebackEnv(level string)
```

SetTracebackEnv is like runtime/debug.SetTraceback, but it raises the "environment" traceback level, so later calls to debug.SetTraceback (e.g., from testing timeouts) can't lower it. 

### <a id="CountPagesInUse" href="#CountPagesInUse">func CountPagesInUse() (pagesInUse, counted uintptr)</a>

```
searchKey: runtime.CountPagesInUse
```

```Go
func CountPagesInUse() (pagesInUse, counted uintptr)
```

### <a id="Fastrand" href="#Fastrand">func Fastrand() uint32</a>

```
searchKey: runtime.Fastrand
```

```Go
func Fastrand() uint32
```

### <a id="Fastrandn" href="#Fastrandn">func Fastrandn(n uint32) uint32</a>

```
searchKey: runtime.Fastrandn
```

```Go
func Fastrandn(n uint32) uint32
```

### <a id="ReadMetricsSlow" href="#ReadMetricsSlow">func ReadMetricsSlow(memStats *MemStats, samplesp unsafe.Pointer, len, cap int)</a>

```
searchKey: runtime.ReadMetricsSlow
```

```Go
func ReadMetricsSlow(memStats *MemStats, samplesp unsafe.Pointer, len, cap int)
```

### <a id="BlockOnSystemStack" href="#BlockOnSystemStack">func BlockOnSystemStack()</a>

```
searchKey: runtime.BlockOnSystemStack
```

```Go
func BlockOnSystemStack()
```

BlockOnSystemStack switches to the system stack, prints "x\n" to stderr, and blocks in a stack containing "runtime.blockOnSystemStackInternal". 

### <a id="blockOnSystemStackInternal" href="#blockOnSystemStackInternal">func blockOnSystemStackInternal()</a>

```
searchKey: runtime.blockOnSystemStackInternal
```

```Go
func blockOnSystemStackInternal()
```

### <a id="MapBucketsCount" href="#MapBucketsCount">func MapBucketsCount(m map[int]int) int</a>

```
searchKey: runtime.MapBucketsCount
```

```Go
func MapBucketsCount(m map[int]int) int
```

### <a id="MapBucketsPointerIsNil" href="#MapBucketsPointerIsNil">func MapBucketsPointerIsNil(m map[int]int) bool</a>

```
searchKey: runtime.MapBucketsPointerIsNil
```

```Go
func MapBucketsPointerIsNil(m map[int]int) bool
```

### <a id="LockOSCounts" href="#LockOSCounts">func LockOSCounts() (external, internal uint32)</a>

```
searchKey: runtime.LockOSCounts
```

```Go
func LockOSCounts() (external, internal uint32)
```

### <a id="TracebackSystemstack" href="#TracebackSystemstack">func TracebackSystemstack(stk []uintptr, i int) int</a>

```
searchKey: runtime.TracebackSystemstack
```

```Go
func TracebackSystemstack(stk []uintptr, i int) int
```

### <a id="KeepNArenaHints" href="#KeepNArenaHints">func KeepNArenaHints(n int)</a>

```
searchKey: runtime.KeepNArenaHints
```

```Go
func KeepNArenaHints(n int)
```

### <a id="MapNextArenaHint" href="#MapNextArenaHint">func MapNextArenaHint() (start, end uintptr)</a>

```
searchKey: runtime.MapNextArenaHint
```

```Go
func MapNextArenaHint() (start, end uintptr)
```

MapNextArenaHint reserves a page at the next arena growth hint, preventing the arena from growing there, and returns the range of addresses that are no longer viable. 

### <a id="GetNextArenaHint" href="#GetNextArenaHint">func GetNextArenaHint() uintptr</a>

```
searchKey: runtime.GetNextArenaHint
```

```Go
func GetNextArenaHint() uintptr
```

### <a id="PanicForTesting" href="#PanicForTesting">func PanicForTesting(b []byte, i int) byte</a>

```
searchKey: runtime.PanicForTesting
```

```Go
func PanicForTesting(b []byte, i int) byte
```

### <a id="unexportedPanicForTesting" href="#unexportedPanicForTesting">func unexportedPanicForTesting(b []byte, i int) byte</a>

```
searchKey: runtime.unexportedPanicForTesting
```

```Go
func unexportedPanicForTesting(b []byte, i int) byte
```

### <a id="G0StackOverflow" href="#G0StackOverflow">func G0StackOverflow()</a>

```
searchKey: runtime.G0StackOverflow
```

```Go
func G0StackOverflow()
```

### <a id="stackOverflow" href="#stackOverflow">func stackOverflow(x *byte)</a>

```
searchKey: runtime.stackOverflow
```

```Go
func stackOverflow(x *byte)
```

### <a id="MapTombstoneCheck" href="#MapTombstoneCheck">func MapTombstoneCheck(m map[int]int)</a>

```
searchKey: runtime.MapTombstoneCheck
```

```Go
func MapTombstoneCheck(m map[int]int)
```

### <a id="RunGetgThreadSwitchTest" href="#RunGetgThreadSwitchTest">func RunGetgThreadSwitchTest()</a>

```
searchKey: runtime.RunGetgThreadSwitchTest
```

```Go
func RunGetgThreadSwitchTest()
```

### <a id="FindBitRange64" href="#FindBitRange64">func FindBitRange64(c uint64, n uint) uint</a>

```
searchKey: runtime.FindBitRange64
```

```Go
func FindBitRange64(c uint64, n uint) uint
```

Expose non-trivial helpers for testing. 

### <a id="DiffPallocBits" href="#DiffPallocBits">func DiffPallocBits(a, b *PallocBits) []BitRange</a>

```
searchKey: runtime.DiffPallocBits
```

```Go
func DiffPallocBits(a, b *PallocBits) []BitRange
```

Given two PallocBits, returns a set of bit ranges where they differ. 

### <a id="StringifyPallocBits" href="#StringifyPallocBits">func StringifyPallocBits(b *PallocBits, r BitRange) string</a>

```
searchKey: runtime.StringifyPallocBits
```

```Go
func StringifyPallocBits(b *PallocBits, r BitRange) string
```

StringifyPallocBits gets the bits in the bit range r from b, and returns a string containing the bits as ASCII 0 and 1 characters. 

### <a id="FillAligned" href="#FillAligned">func FillAligned(x uint64, m uint) uint64</a>

```
searchKey: runtime.FillAligned
```

```Go
func FillAligned(x uint64, m uint) uint64
```

Expose fillAligned for testing. 

### <a id="FreePageAlloc" href="#FreePageAlloc">func FreePageAlloc(pp *PageAlloc)</a>

```
searchKey: runtime.FreePageAlloc
```

```Go
func FreePageAlloc(pp *PageAlloc)
```

FreePageAlloc releases hard OS resources owned by the pageAlloc. Once this is called the pageAlloc may no longer be used. The object itself will be collected by the garbage collector once it is no longer live. 

### <a id="PageBase" href="#PageBase">func PageBase(c ChunkIdx, pageIdx uint) uintptr</a>

```
searchKey: runtime.PageBase
```

```Go
func PageBase(c ChunkIdx, pageIdx uint) uintptr
```

PageBase returns an address given a chunk index and a page index relative to that chunk. 

### <a id="CheckScavengedBitsCleared" href="#CheckScavengedBitsCleared">func CheckScavengedBitsCleared(mismatches []BitsMismatch) (n int, ok bool)</a>

```
searchKey: runtime.CheckScavengedBitsCleared
```

```Go
func CheckScavengedBitsCleared(mismatches []BitsMismatch) (n int, ok bool)
```

### <a id="PageCachePagesLeaked" href="#PageCachePagesLeaked">func PageCachePagesLeaked() (leaked uintptr)</a>

```
searchKey: runtime.PageCachePagesLeaked
```

```Go
func PageCachePagesLeaked() (leaked uintptr)
```

### <a id="SemNwait" href="#SemNwait">func SemNwait(addr *uint32) uint32</a>

```
searchKey: runtime.SemNwait
```

```Go
func SemNwait(addr *uint32) uint32
```

### <a id="FreeMSpan" href="#FreeMSpan">func FreeMSpan(s *MSpan)</a>

```
searchKey: runtime.FreeMSpan
```

```Go
func FreeMSpan(s *MSpan)
```

Free an allocated mspan. 

### <a id="MSpanCountAlloc" href="#MSpanCountAlloc">func MSpanCountAlloc(ms *MSpan, bits []byte) int</a>

```
searchKey: runtime.MSpanCountAlloc
```

```Go
func MSpanCountAlloc(ms *MSpan, bits []byte) int
```

### <a id="SetIntArgRegs" href="#SetIntArgRegs">func SetIntArgRegs(a int) int</a>

```
searchKey: runtime.SetIntArgRegs
```

```Go
func SetIntArgRegs(a int) int
```

### <a id="FinalizerGAsleep" href="#FinalizerGAsleep">func FinalizerGAsleep() bool</a>

```
searchKey: runtime.FinalizerGAsleep
```

```Go
func FinalizerGAsleep() bool
```

### <a id="GCTestIsReachable" href="#GCTestIsReachable">func GCTestIsReachable(ptrs ...unsafe.Pointer) (mask uint64)</a>

```
searchKey: runtime.GCTestIsReachable
```

```Go
func GCTestIsReachable(ptrs ...unsafe.Pointer) (mask uint64)
```

For GCTestIsReachable, it's important that we do this as a call so escape analysis can see through it. 

### <a id="GCTestPointerClass" href="#GCTestPointerClass">func GCTestPointerClass(p unsafe.Pointer) string</a>

```
searchKey: runtime.GCTestPointerClass
```

```Go
func GCTestPointerClass(p unsafe.Pointer) string
```

For GCTestPointerClass, it's important that we do this as a call so escape analysis can see through it. 

This is nosplit because gcTestPointerClass is. 

### <a id="sigismember" href="#sigismember">func sigismember(mask *sigset, i int) bool</a>

```
searchKey: runtime.sigismember
```

```Go
func sigismember(mask *sigset, i int) bool
```

### <a id="Sigisblocked" href="#Sigisblocked">func Sigisblocked(i int) bool</a>

```
searchKey: runtime.Sigisblocked
```

```Go
func Sigisblocked(i int) bool
```

### <a id="WaitForSigusr1" href="#WaitForSigusr1">func WaitForSigusr1(r, w int32, ready func(mp *M)) (int64, int64)</a>

```
searchKey: runtime.WaitForSigusr1
```

```Go
func WaitForSigusr1(r, w int32, ready func(mp *M)) (int64, int64)
```

WaitForSigusr1 blocks until a SIGUSR1 is received. It calls ready when it is set up to receive SIGUSR1. The ready function should cause a SIGUSR1 to be sent. The r and w arguments are a pipe that the signal handler can use to report when the signal is received. 

Once SIGUSR1 is received, it returns the ID of the current M and the ID of the M the SIGUSR1 was received on. If the caller writes a non-zero byte to w, WaitForSigusr1 returns immediately with -1, -1. 

### <a id="waitForSigusr1Callback" href="#waitForSigusr1Callback">func waitForSigusr1Callback(gp *g) bool</a>

```
searchKey: runtime.waitForSigusr1Callback
```

```Go
func waitForSigusr1Callback(gp *g) bool
```

waitForSigusr1Callback is called from the signal handler during WaitForSigusr1. It must not have write barriers because there may not be a P. 

### <a id="SendSigusr1" href="#SendSigusr1">func SendSigusr1(mp *M)</a>

```
searchKey: runtime.SendSigusr1
```

```Go
func SendSigusr1(mp *M)
```

SendSigusr1 sends SIGUSR1 to mp. 

### <a id="RunStealOrderTest" href="#RunStealOrderTest">func RunStealOrderTest()</a>

```
searchKey: runtime.RunStealOrderTest
```

```Go
func RunStealOrderTest()
```


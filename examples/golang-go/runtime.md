# Package runtime

Package runtime contains operations that interact with Go's runtime system, such as functions to control goroutines. It also includes the low-level type information used by the reflect package; see reflect's documentation for the programmable interface to the run-time type system. 

### hdr-Environment_VariablesEnvironment Variables
The following environment variables ($name or %name%, depending on the host operating system) control the run-time behavior of Go programs. The meanings and use may change from release to release. 

The GOGC variable sets the initial garbage collection target percentage. A collection is triggered when the ratio of freshly allocated data to live data remaining after the previous collection reaches this percentage. The default is GOGC=100. Setting GOGC=off disables the garbage collector entirely. The runtime/debug package's SetGCPercent function allows changing this percentage at run time. See [https://golang.org/pkg/runtime/debug/#SetGCPercent](https://golang.org/pkg/runtime/debug/#SetGCPercent). 

The GODEBUG variable controls debugging variables within the runtime. It is a comma-separated list of name=val pairs setting these named variables: 

```
allocfreetrace: setting allocfreetrace=1 causes every allocation to be
profiled and a stack trace printed on each object's allocation and free.

clobberfree: setting clobberfree=1 causes the garbage collector to
clobber the memory content of an object with bad content when it frees
the object.

cgocheck: setting cgocheck=0 disables all checks for packages
using cgo to incorrectly pass Go pointers to non-Go code.
Setting cgocheck=1 (the default) enables relatively cheap
checks that may miss some errors.  Setting cgocheck=2 enables
expensive checks that should not miss any errors, but will
cause your program to run slower.

efence: setting efence=1 causes the allocator to run in a mode
where each object is allocated on a unique page and addresses are
never recycled.

gccheckmark: setting gccheckmark=1 enables verification of the
garbage collector's concurrent mark phase by performing a
second mark pass while the world is stopped.  If the second
pass finds a reachable object that was not found by concurrent
mark, the garbage collector will panic.

gcpacertrace: setting gcpacertrace=1 causes the garbage collector to
print information about the internal state of the concurrent pacer.

gcshrinkstackoff: setting gcshrinkstackoff=1 disables moving goroutines
onto smaller stacks. In this mode, a goroutine's stack can only grow.

gcstoptheworld: setting gcstoptheworld=1 disables concurrent garbage collection,
making every garbage collection a stop-the-world event. Setting gcstoptheworld=2
also disables concurrent sweeping after the garbage collection finishes.

gctrace: setting gctrace=1 causes the garbage collector to emit a single line to standard
error at each collection, summarizing the amount of memory collected and the
length of the pause. The format of this line is subject to change.
Currently, it is:
	gc # @#s #%: #+#+# ms clock, #+#/#/#+# ms cpu, #->#-># MB, # MB goal, # P
where the fields are as follows:
	gc #        the GC number, incremented at each GC
	@#s         time in seconds since program start
	#%          percentage of time spent in GC since program start
	#+...+#     wall-clock/CPU times for the phases of the GC
	#->#-># MB  heap size at GC start, at GC end, and live heap
	# MB goal   goal heap size
	# P         number of processors used
The phases are stop-the-world (STW) sweep termination, concurrent
mark and scan, and STW mark termination. The CPU times
for mark/scan are broken down in to assist time (GC performed in
line with allocation), background GC time, and idle GC time.
If the line ends with "(forced)", this GC was forced by a
runtime.GC() call.

inittrace: setting inittrace=1 causes the runtime to emit a single line to standard
error for each package with init work, summarizing the execution time and memory
allocation. No information is printed for inits executed as part of plugin loading
and for packages without both user defined and compiler generated init work.
The format of this line is subject to change. Currently, it is:
	init # @#ms, # ms clock, # bytes, # allocs
where the fields are as follows:
	init #      the package name
	@# ms       time in milliseconds when the init started since program start
	# clock     wall-clock time for package initialization work
	# bytes     memory allocated on the heap
	# allocs    number of heap allocations

madvdontneed: setting madvdontneed=0 will use MADV_FREE
instead of MADV_DONTNEED on Linux when returning memory to the
kernel. This is more efficient, but means RSS numbers will
drop only when the OS is under memory pressure.

memprofilerate: setting memprofilerate=X will update the value of runtime.MemProfileRate.
When set to 0 memory profiling is disabled.  Refer to the description of
MemProfileRate for the default value.

invalidptr: invalidptr=1 (the default) causes the garbage collector and stack
copier to crash the program if an invalid pointer value (for example, 1)
is found in a pointer-typed location. Setting invalidptr=0 disables this check.
This should only be used as a temporary workaround to diagnose buggy code.
The real fix is to not store integers in pointer-typed locations.

sbrk: setting sbrk=1 replaces the memory allocator and garbage collector
with a trivial allocator that obtains memory from the operating system and
never reclaims any memory.

scavtrace: setting scavtrace=1 causes the runtime to emit a single line to standard
error, roughly once per GC cycle, summarizing the amount of work done by the
scavenger as well as the total amount of memory returned to the operating system
and an estimate of physical memory utilization. The format of this line is subject
to change, but currently it is:
	scav # # KiB work, # KiB total, #% util
where the fields are as follows:
	scav #       the scavenge cycle number
	# KiB work   the amount of memory returned to the OS since the last line
	# KiB total  the total amount of memory returned to the OS
	#% util      the fraction of all unscavenged memory which is in-use
If the line ends with "(forced)", then scavenging was forced by a
debug.FreeOSMemory() call.

scheddetail: setting schedtrace=X and scheddetail=1 causes the scheduler to emit
detailed multiline info every X milliseconds, describing state of the scheduler,
processors, threads and goroutines.

schedtrace: setting schedtrace=X causes the scheduler to emit a single line to standard
error every X milliseconds, summarizing the scheduler state.

tracebackancestors: setting tracebackancestors=N extends tracebacks with the stacks at
which goroutines were created, where N limits the number of ancestor goroutines to
report. This also extends the information returned by runtime.Stack. Ancestor's goroutine
IDs will refer to the ID of the goroutine at the time of creation; it's possible for this
ID to be reused for another goroutine. Setting N to 0 will report no ancestry information.

asyncpreemptoff: asyncpreemptoff=1 disables signal-based
asynchronous goroutine preemption. This makes some loops
non-preemptible for long periods, which may delay GC and
goroutine scheduling. This is useful for debugging GC issues
because it also disables the conservative stack scanning used
for asynchronously preempted goroutines.

```
The net, net/http, and crypto/tls packages also refer to debugging variables in GODEBUG. See the documentation for those packages for details. 

The GOMAXPROCS variable limits the number of operating system threads that can execute user-level Go code simultaneously. There is no limit to the number of threads that can be blocked in system calls on behalf of Go code; those do not count against the GOMAXPROCS limit. This package's GOMAXPROCS function queries and changes the limit. 

The GORACE variable configures the race detector, for programs built using -race. See [https://golang.org/doc/articles/race_detector.html](https://golang.org/doc/articles/race_detector.html) for details. 

The GOTRACEBACK variable controls the amount of output generated when a Go program fails due to an unrecovered panic or an unexpected runtime condition. By default, a failure prints a stack trace for the current goroutine, eliding functions internal to the run-time system, and then exits with exit code 2. The failure prints stack traces for all goroutines if there is no current goroutine or the failure is internal to the run-time. GOTRACEBACK=none omits the goroutine stack traces entirely. GOTRACEBACK=single (the default) behaves as described above. GOTRACEBACK=all adds stack traces for all user-created goroutines. GOTRACEBACK=system is like `all' but adds stack frames for run-time functions and shows goroutines created internally by the run-time. GOTRACEBACK=crash is like `system' but crashes in an operating system-specific manner instead of exiting. For example, on Unix systems, the crash raises SIGABRT to trigger a core dump. For historical reasons, the GOTRACEBACK settings 0, 1, and 2 are synonyms for none, all, and system, respectively. The runtime/debug package's SetTraceback function allows increasing the amount of output at run time, but it cannot reduce the amount below that specified by the environment variable. See [https://golang.org/pkg/runtime/debug/#SetTraceback](https://golang.org/pkg/runtime/debug/#SetTraceback). 

The GOARCH, GOOS, GOPATH, and GOROOT environment variables complete the set of Go environment variables. They influence the building of Go programs (see [https://golang.org/cmd/go](https://golang.org/cmd/go) and [https://golang.org/pkg/go/build](https://golang.org/pkg/go/build)). GOARCH, GOOS, and GOROOT are recorded at compile time and made available by constants or functions in this package, but they do not influence the execution of the run-time system. 

## Index

* Subpages
  * [runtime/internal](runtime/internal.md)
  * [runtime/cgo](runtime/cgo.md)
  * [runtime/debug](runtime/debug.md)
  * [runtime/debug_test](runtime/debug_test.md)
  * [runtime/metrics](runtime/metrics.md)
  * [runtime/metrics_test](runtime/metrics_test.md)
  * [runtime/pprof](runtime/pprof.md)
  * [runtime/race](runtime/race.md)
  * [runtime/trace](runtime/trace.md)
  * [runtime/trace_test](runtime/trace_test.md)
* [Constants](#const)
    * [const Compiler](#Compiler)
    * [const DebugLogBytes](#DebugLogBytes)
    * [const DebugLogStringLimit](#DebugLogStringLimit)
    * [const DlogEnabled](#DlogEnabled)
    * [const ENOMEM](#ENOMEM)
    * [const GOARCH](#GOARCH)
    * [const GOOS](#GOOS)
    * [const MAP_ANON](#MAP_ANON)
    * [const MAP_FIXED](#MAP_FIXED)
    * [const MAP_PRIVATE](#MAP_PRIVATE)
    * [const PageAlloc64Bit](#PageAlloc64Bit)
    * [const PageCachePages](#PageCachePages)
    * [const PageSize](#PageSize)
    * [const PallocChunkPages](#PallocChunkPages)
    * [const PallocSumBytes](#PallocSumBytes)
    * [const PreemptMSupported](#PreemptMSupported)
    * [const ProfBufBlocking](#ProfBufBlocking)
    * [const ProfBufNonBlocking](#ProfBufNonBlocking)
    * [const PtrSize](#PtrSize)
    * [const Raceenabled](#Raceenabled)
    * [const RuntimeHmapSize](#RuntimeHmapSize)
    * [const TimeHistNumSubBuckets](#TimeHistNumSubBuckets)
    * [const TimeHistNumSuperBuckets](#TimeHistNumSuperBuckets)
    * [const TimeHistSubBucketBits](#TimeHistSubBucketBits)
    * [const active_spin](#active_spin)
    * [const active_spin_cnt](#active_spin_cnt)
    * [const addrBits](#addrBits)
    * [const aixAddrBits](#aixAddrBits)
    * [const aixCntBits](#aixCntBits)
    * [const arenaBaseOffset](#arenaBaseOffset)
    * [const arenaBaseOffsetUintptr](#arenaBaseOffsetUintptr)
    * [const arenaBits](#arenaBits)
    * [const arenaL1Bits](#arenaL1Bits)
    * [const arenaL1Shift](#arenaL1Shift)
    * [const arenaL2Bits](#arenaL2Bits)
    * [const bias32](#bias32)
    * [const bias64](#bias64)
    * [const bitPointer](#bitPointer)
    * [const bitPointerAll](#bitPointerAll)
    * [const bitScan](#bitScan)
    * [const bitScanAll](#bitScanAll)
    * [const blockProfile](#blockProfile)
    * [const boundsConvert](#boundsConvert)
    * [const boundsIndex](#boundsIndex)
    * [const boundsSlice3Acap](#boundsSlice3Acap)
    * [const boundsSlice3Alen](#boundsSlice3Alen)
    * [const boundsSlice3B](#boundsSlice3B)
    * [const boundsSlice3C](#boundsSlice3C)
    * [const boundsSliceAcap](#boundsSliceAcap)
    * [const boundsSliceAlen](#boundsSliceAlen)
    * [const boundsSliceB](#boundsSliceB)
    * [const buckHashSize](#buckHashSize)
    * [const bucketCnt](#bucketCnt)
    * [const bucketCntBits](#bucketCntBits)
    * [const bufSize](#bufSize)
    * [const c0](#c0)
    * [const c1](#c1)
    * [const cgoCheckPointerFail](#cgoCheckPointerFail)
    * [const cgoResultFail](#cgoResultFail)
    * [const cgoWriteBarrierFail](#cgoWriteBarrierFail)
    * [const clobberdeadPtr](#clobberdeadPtr)
    * [const cntBits](#cntBits)
    * [const concurrentSweep](#concurrentSweep)
    * [const dataOffset](#dataOffset)
    * [const debugCallRuntime](#debugCallRuntime)
    * [const debugCallSystemStack](#debugCallSystemStack)
    * [const debugCallUnknownFunc](#debugCallUnknownFunc)
    * [const debugCallUnsafePoint](#debugCallUnsafePoint)
    * [const debugChan](#debugChan)
    * [const debugCheckBP](#debugCheckBP)
    * [const debugLogBoolFalse](#debugLogBoolFalse)
    * [const debugLogBoolTrue](#debugLogBoolTrue)
    * [const debugLogBytes](#debugLogBytes)
    * [const debugLogConstString](#debugLogConstString)
    * [const debugLogHeaderSize](#debugLogHeaderSize)
    * [const debugLogHex](#debugLogHex)
    * [const debugLogInt](#debugLogInt)
    * [const debugLogPC](#debugLogPC)
    * [const debugLogPtr](#debugLogPtr)
    * [const debugLogString](#debugLogString)
    * [const debugLogStringLimit](#debugLogStringLimit)
    * [const debugLogStringOverflow](#debugLogStringOverflow)
    * [const debugLogSyncSize](#debugLogSyncSize)
    * [const debugLogTraceback](#debugLogTraceback)
    * [const debugLogUint](#debugLogUint)
    * [const debugLogUnknown](#debugLogUnknown)
    * [const debugMalloc](#debugMalloc)
    * [const debugPcln](#debugPcln)
    * [const debugScanConservative](#debugScanConservative)
    * [const debugSelect](#debugSelect)
    * [const defaultHeapMinimum](#defaultHeapMinimum)
    * [const deferHeaderSize](#deferHeaderSize)
    * [const dlogEnabled](#dlogEnabled)
    * [const drainCheckThreshold](#drainCheckThreshold)
    * [const emptyOne](#emptyOne)
    * [const emptyRest](#emptyRest)
    * [const evacuatedEmpty](#evacuatedEmpty)
    * [const evacuatedX](#evacuatedX)
    * [const evacuatedY](#evacuatedY)
    * [const expbits32](#expbits32)
    * [const expbits64](#expbits64)
    * [const fInf](#fInf)
    * [const fNegInf](#fNegInf)
    * [const fastlogNumBits](#fastlogNumBits)
    * [const fieldKindEface](#fieldKindEface)
    * [const fieldKindEol](#fieldKindEol)
    * [const fieldKindIface](#fieldKindIface)
    * [const fieldKindPtr](#fieldKindPtr)
    * [const fixedRootCount](#fixedRootCount)
    * [const fixedRootFinalizers](#fixedRootFinalizers)
    * [const fixedRootFreeGStacks](#fixedRootFreeGStacks)
    * [const forcePreemptNS](#forcePreemptNS)
    * [const framepointer_enabled](#framepointer_enabled)
    * [const freeChunkSum](#freeChunkSum)
    * [const freezeStopWait](#freezeStopWait)
    * [const funcFlag_SPWRITE](#funcFlag_SPWRITE)
    * [const funcFlag_TOPFRAME](#funcFlag_TOPFRAME)
    * [const funcID_abort](#funcID_abort)
    * [const funcID_asmcgocall](#funcID_asmcgocall)
    * [const funcID_asyncPreempt](#funcID_asyncPreempt)
    * [const funcID_cgocallback](#funcID_cgocallback)
    * [const funcID_debugCallV2](#funcID_debugCallV2)
    * [const funcID_gcBgMarkWorker](#funcID_gcBgMarkWorker)
    * [const funcID_goexit](#funcID_goexit)
    * [const funcID_gogo](#funcID_gogo)
    * [const funcID_gopanic](#funcID_gopanic)
    * [const funcID_handleAsyncEvent](#funcID_handleAsyncEvent)
    * [const funcID_jmpdefer](#funcID_jmpdefer)
    * [const funcID_mcall](#funcID_mcall)
    * [const funcID_morestack](#funcID_morestack)
    * [const funcID_mstart](#funcID_mstart)
    * [const funcID_normal](#funcID_normal)
    * [const funcID_panicwrap](#funcID_panicwrap)
    * [const funcID_rt0_go](#funcID_rt0_go)
    * [const funcID_runfinq](#funcID_runfinq)
    * [const funcID_runtime_main](#funcID_runtime_main)
    * [const funcID_sigpanic](#funcID_sigpanic)
    * [const funcID_systemstack](#funcID_systemstack)
    * [const funcID_systemstack_switch](#funcID_systemstack_switch)
    * [const funcID_wrapper](#funcID_wrapper)
    * [const gTrackingPeriod](#gTrackingPeriod)
    * [const gcAssistTimeSlack](#gcAssistTimeSlack)
    * [const gcBackgroundMode](#gcBackgroundMode)
    * [const gcBackgroundUtilization](#gcBackgroundUtilization)
    * [const gcBitsChunkBytes](#gcBitsChunkBytes)
    * [const gcBitsHeaderBytes](#gcBitsHeaderBytes)
    * [const gcCreditSlack](#gcCreditSlack)
    * [const gcDrainFlushBgCredit](#gcDrainFlushBgCredit)
    * [const gcDrainFractional](#gcDrainFractional)
    * [const gcDrainIdle](#gcDrainIdle)
    * [const gcDrainUntilPreempt](#gcDrainUntilPreempt)
    * [const gcForceBlockMode](#gcForceBlockMode)
    * [const gcForceMode](#gcForceMode)
    * [const gcGoalUtilization](#gcGoalUtilization)
    * [const gcMarkWorkerDedicatedMode](#gcMarkWorkerDedicatedMode)
    * [const gcMarkWorkerFractionalMode](#gcMarkWorkerFractionalMode)
    * [const gcMarkWorkerIdleMode](#gcMarkWorkerIdleMode)
    * [const gcMarkWorkerNotWorker](#gcMarkWorkerNotWorker)
    * [const gcOverAssistWork](#gcOverAssistWork)
    * [const gcTriggerCycle](#gcTriggerCycle)
    * [const gcTriggerHeap](#gcTriggerHeap)
    * [const gcTriggerTime](#gcTriggerTime)
    * [const hashRandomBytes](#hashRandomBytes)
    * [const hashWriting](#hashWriting)
    * [const hchanSize](#hchanSize)
    * [const heapAddrBits](#heapAddrBits)
    * [const heapArenaBitmapBytes](#heapArenaBitmapBytes)
    * [const heapArenaBytes](#heapArenaBytes)
    * [const heapBitsShift](#heapBitsShift)
    * [const heapStatsDep](#heapStatsDep)
    * [const hicb](#hicb)
    * [const inf32](#inf32)
    * [const inf64](#inf64)
    * [const itabInitSize](#itabInitSize)
    * [const iterator](#iterator)
    * [const kindArray](#kindArray)
    * [const kindBool](#kindBool)
    * [const kindChan](#kindChan)
    * [const kindComplex128](#kindComplex128)
    * [const kindComplex64](#kindComplex64)
    * [const kindDirectIface](#kindDirectIface)
    * [const kindFloat32](#kindFloat32)
    * [const kindFloat64](#kindFloat64)
    * [const kindFunc](#kindFunc)
    * [const kindGCProg](#kindGCProg)
    * [const kindInt](#kindInt)
    * [const kindInt16](#kindInt16)
    * [const kindInt32](#kindInt32)
    * [const kindInt64](#kindInt64)
    * [const kindInt8](#kindInt8)
    * [const kindInterface](#kindInterface)
    * [const kindMap](#kindMap)
    * [const kindMask](#kindMask)
    * [const kindPtr](#kindPtr)
    * [const kindSlice](#kindSlice)
    * [const kindString](#kindString)
    * [const kindStruct](#kindStruct)
    * [const kindUint](#kindUint)
    * [const kindUint16](#kindUint16)
    * [const kindUint32](#kindUint32)
    * [const kindUint64](#kindUint64)
    * [const kindUint8](#kindUint8)
    * [const kindUintptr](#kindUintptr)
    * [const kindUnsafePointer](#kindUnsafePointer)
    * [const largeSizeDiv](#largeSizeDiv)
    * [const loadFactorDen](#loadFactorDen)
    * [const loadFactorNum](#loadFactorNum)
    * [const locb](#locb)
    * [const lockRankAllg](#lockRankAllg)
    * [const lockRankAllp](#lockRankAllp)
    * [const lockRankAssistQueue](#lockRankAssistQueue)
    * [const lockRankCpuprof](#lockRankCpuprof)
    * [const lockRankDeadlock](#lockRankDeadlock)
    * [const lockRankDebug](#lockRankDebug)
    * [const lockRankDebugPtrmask](#lockRankDebugPtrmask)
    * [const lockRankDefer](#lockRankDefer)
    * [const lockRankDummy](#lockRankDummy)
    * [const lockRankFaketimeState](#lockRankFaketimeState)
    * [const lockRankFin](#lockRankFin)
    * [const lockRankForcegc](#lockRankForcegc)
    * [const lockRankGFree](#lockRankGFree)
    * [const lockRankGcBitsArenas](#lockRankGcBitsArenas)
    * [const lockRankGlobalAlloc](#lockRankGlobalAlloc)
    * [const lockRankGscan](#lockRankGscan)
    * [const lockRankHchan](#lockRankHchan)
    * [const lockRankHchanLeaf](#lockRankHchanLeaf)
    * [const lockRankItab](#lockRankItab)
    * [const lockRankLeafRank](#lockRankLeafRank)
    * [const lockRankMheap](#lockRankMheap)
    * [const lockRankMheapSpecial](#lockRankMheapSpecial)
    * [const lockRankMspanSpecial](#lockRankMspanSpecial)
    * [const lockRankNetpollInit](#lockRankNetpollInit)
    * [const lockRankNewmHandoff](#lockRankNewmHandoff)
    * [const lockRankNotifyList](#lockRankNotifyList)
    * [const lockRankPanic](#lockRankPanic)
    * [const lockRankPollCache](#lockRankPollCache)
    * [const lockRankPollDesc](#lockRankPollDesc)
    * [const lockRankProf](#lockRankProf)
    * [const lockRankRaceFini](#lockRankRaceFini)
    * [const lockRankReflectOffs](#lockRankReflectOffs)
    * [const lockRankRoot](#lockRankRoot)
    * [const lockRankRwmutexR](#lockRankRwmutexR)
    * [const lockRankRwmutexW](#lockRankRwmutexW)
    * [const lockRankScavenge](#lockRankScavenge)
    * [const lockRankSched](#lockRankSched)
    * [const lockRankSpanSetSpine](#lockRankSpanSetSpine)
    * [const lockRankStackLarge](#lockRankStackLarge)
    * [const lockRankStackpool](#lockRankStackpool)
    * [const lockRankSudog](#lockRankSudog)
    * [const lockRankSweep](#lockRankSweep)
    * [const lockRankSweepWaiters](#lockRankSweepWaiters)
    * [const lockRankSysmon](#lockRankSysmon)
    * [const lockRankTicks](#lockRankTicks)
    * [const lockRankTimers](#lockRankTimers)
    * [const lockRankTrace](#lockRankTrace)
    * [const lockRankTraceBuf](#lockRankTraceBuf)
    * [const lockRankTraceStackTab](#lockRankTraceStackTab)
    * [const lockRankTraceStrings](#lockRankTraceStrings)
    * [const lockRankWbufSpans](#lockRankWbufSpans)
    * [const locked](#locked)
    * [const logHeapArenaBytes](#logHeapArenaBytes)
    * [const logMaxPackedValue](#logMaxPackedValue)
    * [const logPallocChunkBytes](#logPallocChunkBytes)
    * [const logPallocChunkPages](#logPallocChunkPages)
    * [const m1](#m1)
    * [const m2](#m2)
    * [const m3](#m3)
    * [const m4](#m4)
    * [const m5](#m5)
    * [const mProfCycleWrap](#mProfCycleWrap)
    * [const mSpanDead](#mSpanDead)
    * [const mSpanInUse](#mSpanInUse)
    * [const mSpanManual](#mSpanManual)
    * [const mantbits32](#mantbits32)
    * [const mantbits64](#mantbits64)
    * [const mask2](#mask2)
    * [const mask3](#mask3)
    * [const mask4](#mask4)
    * [const maskx](#maskx)
    * [const maxAlign](#maxAlign)
    * [const maxAlloc](#maxAlloc)
    * [const maxCPUProfStack](#maxCPUProfStack)
    * [const maxElemSize](#maxElemSize)
    * [const maxInt](#maxInt)
    * [const maxKeySize](#maxKeySize)
    * [const maxObjsPerSpan](#maxObjsPerSpan)
    * [const maxObletBytes](#maxObletBytes)
    * [const maxPackedValue](#maxPackedValue)
    * [const maxPagesPerPhysPage](#maxPagesPerPhysPage)
    * [const maxPhysHugePageSize](#maxPhysHugePageSize)
    * [const maxPhysPageSize](#maxPhysPageSize)
    * [const maxRune](#maxRune)
    * [const maxSmallSize](#maxSmallSize)
    * [const maxStack](#maxStack)
    * [const maxTinySize](#maxTinySize)
    * [const maxUint](#maxUint)
    * [const maxWhen](#maxWhen)
    * [const maxZero](#maxZero)
    * [const memProfile](#memProfile)
    * [const metricKindBad](#metricKindBad)
    * [const metricKindFloat64](#metricKindFloat64)
    * [const metricKindFloat64Histogram](#metricKindFloat64Histogram)
    * [const metricKindUint64](#metricKindUint64)
    * [const minDeferAlloc](#minDeferAlloc)
    * [const minDeferArgs](#minDeferArgs)
    * [const minLegalPointer](#minLegalPointer)
    * [const minPhysPageSize](#minPhysPageSize)
    * [const minTopHash](#minTopHash)
    * [const minfunc](#minfunc)
    * [const msanenabled](#msanenabled)
    * [const mutexProfile](#mutexProfile)
    * [const nan32](#nan32)
    * [const nan64](#nan64)
    * [const neg32](#neg32)
    * [const neg64](#neg64)
    * [const noCheck](#noCheck)
    * [const numSpanClasses](#numSpanClasses)
    * [const numStatsDeps](#numStatsDeps)
    * [const numSweepClasses](#numSweepClasses)
    * [const offsetARMHasIDIVA](#offsetARMHasIDIVA)
    * [const offsetMIPS64XHasMSA](#offsetMIPS64XHasMSA)
    * [const offsetX86HasAVX](#offsetX86HasAVX)
    * [const offsetX86HasAVX2](#offsetX86HasAVX2)
    * [const offsetX86HasERMS](#offsetX86HasERMS)
    * [const offsetX86HasSSE2](#offsetX86HasSSE2)
    * [const oldIterator](#oldIterator)
    * [const osRelaxMinNS](#osRelaxMinNS)
    * [const pageAlloc32Bit](#pageAlloc32Bit)
    * [const pageAlloc64Bit](#pageAlloc64Bit)
    * [const pageCachePages](#pageCachePages)
    * [const pageMask](#pageMask)
    * [const pageShift](#pageShift)
    * [const pageSize](#pageSize)
    * [const pagesPerArena](#pagesPerArena)
    * [const pagesPerReclaimerChunk](#pagesPerReclaimerChunk)
    * [const pagesPerSpanRoot](#pagesPerSpanRoot)
    * [const pallocChunkBytes](#pallocChunkBytes)
    * [const pallocChunkPages](#pallocChunkPages)
    * [const pallocChunksL1Bits](#pallocChunksL1Bits)
    * [const pallocChunksL1Shift](#pallocChunksL1Shift)
    * [const pallocChunksL2Bits](#pallocChunksL2Bits)
    * [const pallocSumBytes](#pallocSumBytes)
    * [const passive_spin](#passive_spin)
    * [const pcbucketsize](#pcbucketsize)
    * [const pdReady](#pdReady)
    * [const pdWait](#pdWait)
    * [const persistentChunkSize](#persistentChunkSize)
    * [const physPageAlignedStacks](#physPageAlignedStacks)
    * [const pollBlockSize](#pollBlockSize)
    * [const pollErrClosing](#pollErrClosing)
    * [const pollErrNotPollable](#pollErrNotPollable)
    * [const pollErrTimeout](#pollErrTimeout)
    * [const pollNoError](#pollNoError)
    * [const preemptMSupported](#preemptMSupported)
    * [const profBufBlocking](#profBufBlocking)
    * [const profBufNonBlocking](#profBufNonBlocking)
    * [const profReaderSleeping](#profReaderSleeping)
    * [const profWriteExtra](#profWriteExtra)
    * [const raceenabled](#raceenabled)
    * [const randomizeScheduler](#randomizeScheduler)
    * [const retainExtraPercent](#retainExtraPercent)
    * [const rootBlockBytes](#rootBlockBytes)
    * [const rune1Max](#rune1Max)
    * [const rune2Max](#rune2Max)
    * [const rune3Max](#rune3Max)
    * [const runeError](#runeError)
    * [const runeSelf](#runeSelf)
    * [const rwmutexMaxReaders](#rwmutexMaxReaders)
    * [const sameSizeGrow](#sameSizeGrow)
    * [const scavengeCostRatio](#scavengeCostRatio)
    * [const scavengePercent](#scavengePercent)
    * [const scavengeReservationShards](#scavengeReservationShards)
    * [const selectDefault](#selectDefault)
    * [const selectRecv](#selectRecv)
    * [const selectSend](#selectSend)
    * [const semTabSize](#semTabSize)
    * [const semaBlockProfile](#semaBlockProfile)
    * [const semaMutexProfile](#semaMutexProfile)
    * [const sigFixup](#sigFixup)
    * [const sigIdle](#sigIdle)
    * [const sigPreempt](#sigPreempt)
    * [const sigReceiving](#sigReceiving)
    * [const sigSending](#sigSending)
    * [const smallSizeDiv](#smallSizeDiv)
    * [const smallSizeMax](#smallSizeMax)
    * [const spanAllocHeap](#spanAllocHeap)
    * [const spanAllocPtrScalarBits](#spanAllocPtrScalarBits)
    * [const spanAllocStack](#spanAllocStack)
    * [const spanAllocWorkBuf](#spanAllocWorkBuf)
    * [const spanSetBlockEntries](#spanSetBlockEntries)
    * [const spanSetInitSpineCap](#spanSetInitSpineCap)
    * [const stackDebug](#stackDebug)
    * [const stackFaultOnFree](#stackFaultOnFree)
    * [const stackForceMove](#stackForceMove)
    * [const stackFork](#stackFork)
    * [const stackFromSystem](#stackFromSystem)
    * [const stackNoCache](#stackNoCache)
    * [const stackPoisonCopy](#stackPoisonCopy)
    * [const stackPreempt](#stackPreempt)
    * [const stackTraceDebug](#stackTraceDebug)
    * [const summaryL0Bits](#summaryL0Bits)
    * [const summaryLevelBits](#summaryLevelBits)
    * [const summaryLevels](#summaryLevels)
    * [const surrogateMax](#surrogateMax)
    * [const surrogateMin](#surrogateMin)
    * [const sweepClassDone](#sweepClassDone)
    * [const sweepMinHeapDistance](#sweepMinHeapDistance)
    * [const sysStatsDep](#sysStatsDep)
    * [const t1](#t1)
    * [const t2](#t2)
    * [const t3](#t3)
    * [const t4](#t4)
    * [const t5](#t5)
    * [const tagAllocSample](#tagAllocSample)
    * [const tagBSS](#tagBSS)
    * [const tagData](#tagData)
    * [const tagDefer](#tagDefer)
    * [const tagEOF](#tagEOF)
    * [const tagFinalizer](#tagFinalizer)
    * [const tagGoroutine](#tagGoroutine)
    * [const tagItab](#tagItab)
    * [const tagMemProf](#tagMemProf)
    * [const tagMemStats](#tagMemStats)
    * [const tagOSThread](#tagOSThread)
    * [const tagObject](#tagObject)
    * [const tagOtherRoot](#tagOtherRoot)
    * [const tagPanic](#tagPanic)
    * [const tagParams](#tagParams)
    * [const tagQueuedFinalizer](#tagQueuedFinalizer)
    * [const tagStackFrame](#tagStackFrame)
    * [const tagType](#tagType)
    * [const testSmallBuf](#testSmallBuf)
    * [const tflagExtraStar](#tflagExtraStar)
    * [const tflagNamed](#tflagNamed)
    * [const tflagRegularMemory](#tflagRegularMemory)
    * [const tflagUncommon](#tflagUncommon)
    * [const timeHistNumSubBuckets](#timeHistNumSubBuckets)
    * [const timeHistNumSuperBuckets](#timeHistNumSuperBuckets)
    * [const timeHistSubBucketBits](#timeHistSubBucketBits)
    * [const timeHistTotalBuckets](#timeHistTotalBuckets)
    * [const timerDeleted](#timerDeleted)
    * [const timerModifiedEarlier](#timerModifiedEarlier)
    * [const timerModifiedLater](#timerModifiedLater)
    * [const timerModifying](#timerModifying)
    * [const timerMoving](#timerMoving)
    * [const timerNoStatus](#timerNoStatus)
    * [const timerRemoved](#timerRemoved)
    * [const timerRemoving](#timerRemoving)
    * [const timerRunning](#timerRunning)
    * [const timerWaiting](#timerWaiting)
    * [const tinySizeClass](#tinySizeClass)
    * [const tinySpanClass](#tinySpanClass)
    * [const tlsSize](#tlsSize)
    * [const tlsSlots](#tlsSlots)
    * [const tmpStringBufSize](#tmpStringBufSize)
    * [const traceArgCountShift](#traceArgCountShift)
    * [const traceBytesPerNumber](#traceBytesPerNumber)
    * [const traceEvBatch](#traceEvBatch)
    * [const traceEvCount](#traceEvCount)
    * [const traceEvFrequency](#traceEvFrequency)
    * [const traceEvFutileWakeup](#traceEvFutileWakeup)
    * [const traceEvGCDone](#traceEvGCDone)
    * [const traceEvGCMarkAssistDone](#traceEvGCMarkAssistDone)
    * [const traceEvGCMarkAssistStart](#traceEvGCMarkAssistStart)
    * [const traceEvGCSTWDone](#traceEvGCSTWDone)
    * [const traceEvGCSTWStart](#traceEvGCSTWStart)
    * [const traceEvGCStart](#traceEvGCStart)
    * [const traceEvGCSweepDone](#traceEvGCSweepDone)
    * [const traceEvGCSweepStart](#traceEvGCSweepStart)
    * [const traceEvGoBlock](#traceEvGoBlock)
    * [const traceEvGoBlockCond](#traceEvGoBlockCond)
    * [const traceEvGoBlockGC](#traceEvGoBlockGC)
    * [const traceEvGoBlockNet](#traceEvGoBlockNet)
    * [const traceEvGoBlockRecv](#traceEvGoBlockRecv)
    * [const traceEvGoBlockSelect](#traceEvGoBlockSelect)
    * [const traceEvGoBlockSend](#traceEvGoBlockSend)
    * [const traceEvGoBlockSync](#traceEvGoBlockSync)
    * [const traceEvGoCreate](#traceEvGoCreate)
    * [const traceEvGoEnd](#traceEvGoEnd)
    * [const traceEvGoInSyscall](#traceEvGoInSyscall)
    * [const traceEvGoPreempt](#traceEvGoPreempt)
    * [const traceEvGoSched](#traceEvGoSched)
    * [const traceEvGoSleep](#traceEvGoSleep)
    * [const traceEvGoStart](#traceEvGoStart)
    * [const traceEvGoStartLabel](#traceEvGoStartLabel)
    * [const traceEvGoStartLocal](#traceEvGoStartLocal)
    * [const traceEvGoStop](#traceEvGoStop)
    * [const traceEvGoSysBlock](#traceEvGoSysBlock)
    * [const traceEvGoSysCall](#traceEvGoSysCall)
    * [const traceEvGoSysExit](#traceEvGoSysExit)
    * [const traceEvGoSysExitLocal](#traceEvGoSysExitLocal)
    * [const traceEvGoUnblock](#traceEvGoUnblock)
    * [const traceEvGoUnblockLocal](#traceEvGoUnblockLocal)
    * [const traceEvGoWaiting](#traceEvGoWaiting)
    * [const traceEvGomaxprocs](#traceEvGomaxprocs)
    * [const traceEvHeapAlloc](#traceEvHeapAlloc)
    * [const traceEvHeapGoal](#traceEvHeapGoal)
    * [const traceEvNone](#traceEvNone)
    * [const traceEvProcStart](#traceEvProcStart)
    * [const traceEvProcStop](#traceEvProcStop)
    * [const traceEvStack](#traceEvStack)
    * [const traceEvString](#traceEvString)
    * [const traceEvTimerGoroutine](#traceEvTimerGoroutine)
    * [const traceEvUserLog](#traceEvUserLog)
    * [const traceEvUserRegion](#traceEvUserRegion)
    * [const traceEvUserTaskCreate](#traceEvUserTaskCreate)
    * [const traceEvUserTaskEnd](#traceEvUserTaskEnd)
    * [const traceFutileWakeup](#traceFutileWakeup)
    * [const traceGlobProc](#traceGlobProc)
    * [const traceStackSize](#traceStackSize)
    * [const traceTickDiv](#traceTickDiv)
    * [const tracebackAll](#tracebackAll)
    * [const tracebackCrash](#tracebackCrash)
    * [const tracebackShift](#tracebackShift)
    * [const tx](#tx)
    * [const typeCacheAssoc](#typeCacheAssoc)
    * [const typeCacheBuckets](#typeCacheBuckets)
    * [const uintptrMask](#uintptrMask)
    * [const usesLR](#usesLR)
    * [const verifyTimers](#verifyTimers)
    * [const waitReasonChanReceive](#waitReasonChanReceive)
    * [const waitReasonChanReceiveNilChan](#waitReasonChanReceiveNilChan)
    * [const waitReasonChanSend](#waitReasonChanSend)
    * [const waitReasonChanSendNilChan](#waitReasonChanSendNilChan)
    * [const waitReasonDebugCall](#waitReasonDebugCall)
    * [const waitReasonDumpingHeap](#waitReasonDumpingHeap)
    * [const waitReasonFinalizerWait](#waitReasonFinalizerWait)
    * [const waitReasonForceGCIdle](#waitReasonForceGCIdle)
    * [const waitReasonGCAssistMarking](#waitReasonGCAssistMarking)
    * [const waitReasonGCAssistWait](#waitReasonGCAssistWait)
    * [const waitReasonGCScavengeWait](#waitReasonGCScavengeWait)
    * [const waitReasonGCSweepWait](#waitReasonGCSweepWait)
    * [const waitReasonGCWorkerIdle](#waitReasonGCWorkerIdle)
    * [const waitReasonGarbageCollection](#waitReasonGarbageCollection)
    * [const waitReasonGarbageCollectionScan](#waitReasonGarbageCollectionScan)
    * [const waitReasonIOWait](#waitReasonIOWait)
    * [const waitReasonPanicWait](#waitReasonPanicWait)
    * [const waitReasonPreempted](#waitReasonPreempted)
    * [const waitReasonSelect](#waitReasonSelect)
    * [const waitReasonSelectNoCases](#waitReasonSelectNoCases)
    * [const waitReasonSemacquire](#waitReasonSemacquire)
    * [const waitReasonSleep](#waitReasonSleep)
    * [const waitReasonSyncCondWait](#waitReasonSyncCondWait)
    * [const waitReasonTimerGoroutineIdle](#waitReasonTimerGoroutineIdle)
    * [const waitReasonTraceReaderBlocked](#waitReasonTraceReaderBlocked)
    * [const waitReasonWaitForGCCycle](#waitReasonWaitForGCCycle)
    * [const waitReasonZero](#waitReasonZero)
    * [const wbBufEntries](#wbBufEntries)
    * [const wbBufEntryPointers](#wbBufEntryPointers)
    * [const wordsPerBitmapByte](#wordsPerBitmapByte)
    * [const workbufAlloc](#workbufAlloc)
    * [const _64bit](#_64bit)
    * [const _ArgsSizeUnknown](#_ArgsSizeUnknown)
    * [const _BUS_ADRALN](#_BUS_ADRALN)
    * [const _BUS_ADRERR](#_BUS_ADRERR)
    * [const _BUS_OBJERR](#_BUS_OBJERR)
    * [const _CTL_HW](#_CTL_HW)
    * [const _ConcurrentSweep](#_ConcurrentSweep)
    * [const _DebugGC](#_DebugGC)
    * [const _EAGAIN](#_EAGAIN)
    * [const _EFAULT](#_EFAULT)
    * [const _EINTR](#_EINTR)
    * [const _ENOMEM](#_ENOMEM)
    * [const _ETIMEDOUT](#_ETIMEDOUT)
    * [const _EVFILT_READ](#_EVFILT_READ)
    * [const _EVFILT_WRITE](#_EVFILT_WRITE)
    * [const _EV_ADD](#_EV_ADD)
    * [const _EV_CLEAR](#_EV_CLEAR)
    * [const _EV_DELETE](#_EV_DELETE)
    * [const _EV_EOF](#_EV_EOF)
    * [const _EV_ERROR](#_EV_ERROR)
    * [const _EV_RECEIPT](#_EV_RECEIPT)
    * [const _FD_CLOEXEC](#_FD_CLOEXEC)
    * [const _FPE_FLTDIV](#_FPE_FLTDIV)
    * [const _FPE_FLTINV](#_FPE_FLTINV)
    * [const _FPE_FLTOVF](#_FPE_FLTOVF)
    * [const _FPE_FLTRES](#_FPE_FLTRES)
    * [const _FPE_FLTSUB](#_FPE_FLTSUB)
    * [const _FPE_FLTUND](#_FPE_FLTUND)
    * [const _FPE_INTDIV](#_FPE_INTDIV)
    * [const _FPE_INTOVF](#_FPE_INTOVF)
    * [const _FUNCDATA_ArgInfo](#_FUNCDATA_ArgInfo)
    * [const _FUNCDATA_ArgsPointerMaps](#_FUNCDATA_ArgsPointerMaps)
    * [const _FUNCDATA_InlTree](#_FUNCDATA_InlTree)
    * [const _FUNCDATA_LocalsPointerMaps](#_FUNCDATA_LocalsPointerMaps)
    * [const _FUNCDATA_OpenCodedDeferInfo](#_FUNCDATA_OpenCodedDeferInfo)
    * [const _FUNCDATA_StackObjects](#_FUNCDATA_StackObjects)
    * [const _F_GETFL](#_F_GETFL)
    * [const _F_SETFD](#_F_SETFD)
    * [const _F_SETFL](#_F_SETFL)
    * [const _FinBlockSize](#_FinBlockSize)
    * [const _FixAllocChunk](#_FixAllocChunk)
    * [const _FixedStack](#_FixedStack)
    * [const _FixedStack0](#_FixedStack0)
    * [const _FixedStack1](#_FixedStack1)
    * [const _FixedStack2](#_FixedStack2)
    * [const _FixedStack3](#_FixedStack3)
    * [const _FixedStack4](#_FixedStack4)
    * [const _FixedStack5](#_FixedStack5)
    * [const _FixedStack6](#_FixedStack6)
    * [const _GCmark](#_GCmark)
    * [const _GCmarktermination](#_GCmarktermination)
    * [const _GCoff](#_GCoff)
    * [const _Gcopystack](#_Gcopystack)
    * [const _Gdead](#_Gdead)
    * [const _Genqueue_unused](#_Genqueue_unused)
    * [const _Gidle](#_Gidle)
    * [const _Gmoribund_unused](#_Gmoribund_unused)
    * [const _GoidCacheBatch](#_GoidCacheBatch)
    * [const _Gpreempted](#_Gpreempted)
    * [const _Grunnable](#_Grunnable)
    * [const _Grunning](#_Grunning)
    * [const _Gscan](#_Gscan)
    * [const _Gscanpreempted](#_Gscanpreempted)
    * [const _Gscanrunnable](#_Gscanrunnable)
    * [const _Gscanrunning](#_Gscanrunning)
    * [const _Gscansyscall](#_Gscansyscall)
    * [const _Gscanwaiting](#_Gscanwaiting)
    * [const _Gsyscall](#_Gsyscall)
    * [const _Gwaiting](#_Gwaiting)
    * [const _HW_NCPU](#_HW_NCPU)
    * [const _HW_PAGESIZE](#_HW_PAGESIZE)
    * [const _ITIMER_PROF](#_ITIMER_PROF)
    * [const _ITIMER_REAL](#_ITIMER_REAL)
    * [const _ITIMER_VIRTUAL](#_ITIMER_VIRTUAL)
    * [const _KindSpecialFinalizer](#_KindSpecialFinalizer)
    * [const _KindSpecialProfile](#_KindSpecialProfile)
    * [const _KindSpecialReachable](#_KindSpecialReachable)
    * [const _MADV_DONTNEED](#_MADV_DONTNEED)
    * [const _MADV_FREE](#_MADV_FREE)
    * [const _MADV_FREE_REUSABLE](#_MADV_FREE_REUSABLE)
    * [const _MADV_FREE_REUSE](#_MADV_FREE_REUSE)
    * [const _MAP_ANON](#_MAP_ANON)
    * [const _MAP_FIXED](#_MAP_FIXED)
    * [const _MAP_PRIVATE](#_MAP_PRIVATE)
    * [const _MaxGcproc](#_MaxGcproc)
    * [const _MaxSmallSize](#_MaxSmallSize)
    * [const _NSIG](#_NSIG)
    * [const _NumSizeClasses](#_NumSizeClasses)
    * [const _NumStackOrders](#_NumStackOrders)
    * [const _O_NONBLOCK](#_O_NONBLOCK)
    * [const _PCDATA_InlTreeIndex](#_PCDATA_InlTreeIndex)
    * [const _PCDATA_Restart1](#_PCDATA_Restart1)
    * [const _PCDATA_Restart2](#_PCDATA_Restart2)
    * [const _PCDATA_RestartAtEntry](#_PCDATA_RestartAtEntry)
    * [const _PCDATA_StackMapIndex](#_PCDATA_StackMapIndex)
    * [const _PCDATA_UnsafePoint](#_PCDATA_UnsafePoint)
    * [const _PCDATA_UnsafePointSafe](#_PCDATA_UnsafePointSafe)
    * [const _PCDATA_UnsafePointUnsafe](#_PCDATA_UnsafePointUnsafe)
    * [const _PROT_EXEC](#_PROT_EXEC)
    * [const _PROT_NONE](#_PROT_NONE)
    * [const _PROT_READ](#_PROT_READ)
    * [const _PROT_WRITE](#_PROT_WRITE)
    * [const _PTHREAD_CREATE_DETACHED](#_PTHREAD_CREATE_DETACHED)
    * [const _PageMask](#_PageMask)
    * [const _PageShift](#_PageShift)
    * [const _PageSize](#_PageSize)
    * [const _Pdead](#_Pdead)
    * [const _Pgcstop](#_Pgcstop)
    * [const _Pidle](#_Pidle)
    * [const _Prunning](#_Prunning)
    * [const _Psyscall](#_Psyscall)
    * [const _SA_64REGSET](#_SA_64REGSET)
    * [const _SA_ONSTACK](#_SA_ONSTACK)
    * [const _SA_RESTART](#_SA_RESTART)
    * [const _SA_SIGINFO](#_SA_SIGINFO)
    * [const _SA_USERTRAMP](#_SA_USERTRAMP)
    * [const _SEGV_ACCERR](#_SEGV_ACCERR)
    * [const _SEGV_MAPERR](#_SEGV_MAPERR)
    * [const _SIGABRT](#_SIGABRT)
    * [const _SIGALRM](#_SIGALRM)
    * [const _SIGBUS](#_SIGBUS)
    * [const _SIGCHLD](#_SIGCHLD)
    * [const _SIGCONT](#_SIGCONT)
    * [const _SIGEMT](#_SIGEMT)
    * [const _SIGFPE](#_SIGFPE)
    * [const _SIGHUP](#_SIGHUP)
    * [const _SIGILL](#_SIGILL)
    * [const _SIGINFO](#_SIGINFO)
    * [const _SIGINT](#_SIGINT)
    * [const _SIGIO](#_SIGIO)
    * [const _SIGKILL](#_SIGKILL)
    * [const _SIGPIPE](#_SIGPIPE)
    * [const _SIGPROF](#_SIGPROF)
    * [const _SIGQUIT](#_SIGQUIT)
    * [const _SIGSEGV](#_SIGSEGV)
    * [const _SIGSTOP](#_SIGSTOP)
    * [const _SIGSYS](#_SIGSYS)
    * [const _SIGTERM](#_SIGTERM)
    * [const _SIGTRAP](#_SIGTRAP)
    * [const _SIGTSTP](#_SIGTSTP)
    * [const _SIGTTIN](#_SIGTTIN)
    * [const _SIGTTOU](#_SIGTTOU)
    * [const _SIGURG](#_SIGURG)
    * [const _SIGUSR1](#_SIGUSR1)
    * [const _SIGUSR2](#_SIGUSR2)
    * [const _SIGVTALRM](#_SIGVTALRM)
    * [const _SIGWINCH](#_SIGWINCH)
    * [const _SIGXCPU](#_SIGXCPU)
    * [const _SIGXFSZ](#_SIGXFSZ)
    * [const _SIG_BLOCK](#_SIG_BLOCK)
    * [const _SIG_DFL](#_SIG_DFL)
    * [const _SIG_IGN](#_SIG_IGN)
    * [const _SIG_SETMASK](#_SIG_SETMASK)
    * [const _SIG_UNBLOCK](#_SIG_UNBLOCK)
    * [const _SI_USER](#_SI_USER)
    * [const _SS_DISABLE](#_SS_DISABLE)
    * [const _SigDefault](#_SigDefault)
    * [const _SigGoExit](#_SigGoExit)
    * [const _SigIgn](#_SigIgn)
    * [const _SigKill](#_SigKill)
    * [const _SigNotify](#_SigNotify)
    * [const _SigPanic](#_SigPanic)
    * [const _SigSetStack](#_SigSetStack)
    * [const _SigThrow](#_SigThrow)
    * [const _SigUnblock](#_SigUnblock)
    * [const _StackBig](#_StackBig)
    * [const _StackCacheSize](#_StackCacheSize)
    * [const _StackGuard](#_StackGuard)
    * [const _StackLimit](#_StackLimit)
    * [const _StackMin](#_StackMin)
    * [const _StackSmall](#_StackSmall)
    * [const _StackSystem](#_StackSystem)
    * [const _TinySize](#_TinySize)
    * [const _TinySizeClass](#_TinySizeClass)
    * [const _TraceJumpStack](#_TraceJumpStack)
    * [const _TraceRuntimeFrames](#_TraceRuntimeFrames)
    * [const _TraceTrap](#_TraceTrap)
    * [const _TracebackMaxFrames](#_TracebackMaxFrames)
    * [const _WorkbufSize](#_WorkbufSize)
* [Variables](#var)
    * [var Atoi](#Atoi)
    * [var Atoi32](#Atoi32)
    * [var BaseChunkIdx](#BaseChunkIdx)
    * [var BigEndian](#BigEndian)
    * [var BytesHash](#BytesHash)
    * [var Close](#Close)
    * [var Closeonexec](#Closeonexec)
    * [var Dlog](#Dlog)
    * [var EfaceHash](#EfaceHash)
    * [var Entersyscall](#Entersyscall)
    * [var Exitsyscall](#Exitsyscall)
    * [var F32to64](#F32to64)
    * [var F64to32](#F64to32)
    * [var F64toint](#F64toint)
    * [var Fadd64](#Fadd64)
    * [var Fastlog2](#Fastlog2)
    * [var Fcmp64](#Fcmp64)
    * [var Fdiv64](#Fdiv64)
    * [var Fintto64](#Fintto64)
    * [var Fmul64](#Fmul64)
    * [var ForceGCPeriod](#ForceGCPeriod)
    * [var Fsub64](#Fsub64)
    * [var FuncPC](#FuncPC)
    * [var GCTestMoveStackOnNextCall](#GCTestMoveStackOnNextCall)
    * [var HashLoad](#HashLoad)
    * [var IfaceHash](#IfaceHash)
    * [var Int32Hash](#Int32Hash)
    * [var Int64Hash](#Int64Hash)
    * [var LockPartialOrder](#LockPartialOrder)
    * [var LockedOSThread](#LockedOSThread)
    * [var MemHash](#MemHash)
    * [var MemHash32](#MemHash32)
    * [var MemHash64](#MemHash64)
    * [var MemProfileRate](#MemProfileRate)
    * [var MemclrNoHeapPointers](#MemclrNoHeapPointers)
    * [var Memmove](#Memmove)
    * [var Mmap](#Mmap)
    * [var Munmap](#Munmap)
    * [var Nanotime](#Nanotime)
    * [var NetpollBreak](#NetpollBreak)
    * [var NetpollGenericInit](#NetpollGenericInit)
    * [var NonblockingPipe](#NonblockingPipe)
    * [var Open](#Open)
    * [var PhysHugePageSize](#PhysHugePageSize)
    * [var PhysPageSize](#PhysPageSize)
    * [var Pipe](#Pipe)
    * [var Read](#Read)
    * [var ReadUnaligned32](#ReadUnaligned32)
    * [var ReadUnaligned64](#ReadUnaligned64)
    * [var RunSchedLocalQueueEmptyState](#RunSchedLocalQueueEmptyState)
    * [var Semacquire](#Semacquire)
    * [var Semrelease1](#Semrelease1)
    * [var SetNonblock](#SetNonblock)
    * [var StringHash](#StringHash)
    * [var UseAeshash](#UseAeshash)
    * [var Usleep](#Usleep)
    * [var Write](#Write)
    * [var Xadduintptr](#Xadduintptr)
    * [var abiRegArgsEface](#abiRegArgsEface)
    * [var abiRegArgsType](#abiRegArgsType)
    * [var aeskeysched](#aeskeysched)
    * [var agg](#agg)
    * [var allDloggers](#allDloggers)
    * [var allfin](#allfin)
    * [var allglen](#allglen)
    * [var allglock](#allglock)
    * [var allgptr](#allgptr)
    * [var allgs](#allgs)
    * [var allm](#allm)
    * [var allp](#allp)
    * [var allpLock](#allpLock)
    * [var argc](#argc)
    * [var argslice](#argslice)
    * [var argv](#argv)
    * [var arm64HasATOMICS](#arm64HasATOMICS)
    * [var armHasVFPv4](#armHasVFPv4)
    * [var asyncPreemptStack](#asyncPreemptStack)
    * [var badginsignalMsg](#badginsignalMsg)
    * [var badmorestackg0Msg](#badmorestackg0Msg)
    * [var badmorestackgsignalMsg](#badmorestackgsignalMsg)
    * [var badsystemstackMsg](#badsystemstackMsg)
    * [var bbuckets](#bbuckets)
    * [var blockprofilerate](#blockprofilerate)
    * [var boundsErrorFmts](#boundsErrorFmts)
    * [var boundsNegErrorFmts](#boundsNegErrorFmts)
    * [var bucketmem](#bucketmem)
    * [var buckhash](#buckhash)
    * [var buf](#buf)
    * [var buildVersion](#buildVersion)
    * [var cgoAlwaysFalse](#cgoAlwaysFalse)
    * [var cgoContext](#cgoContext)
    * [var cgoHasExtraM](#cgoHasExtraM)
    * [var cgoSymbolizer](#cgoSymbolizer)
    * [var cgoThreadStart](#cgoThreadStart)
    * [var cgoTraceback](#cgoTraceback)
    * [var cgo_yield](#cgo_yield)
    * [var chanrecvpc](#chanrecvpc)
    * [var chansendpc](#chansendpc)
    * [var class_to_allocnpages](#class_to_allocnpages)
    * [var class_to_divmagic](#class_to_divmagic)
    * [var class_to_size](#class_to_size)
    * [var cpuprof](#cpuprof)
    * [var crashing](#crashing)
    * [var dbgvars](#dbgvars)
    * [var deadlock](#deadlock)
    * [var debug](#debug)
    * [var debugPtrmask](#debugPtrmask)
    * [var debuglock](#debuglock)
    * [var defaultGOROOT](#defaultGOROOT)
    * [var deferType](#deferType)
    * [var didothers](#didothers)
    * [var disableMemoryProfiling](#disableMemoryProfiling)
    * [var disableSigChan](#disableSigChan)
    * [var divideError](#divideError)
    * [var dumpfd](#dumpfd)
    * [var dumphdr](#dumphdr)
    * [var earlycgocallback](#earlycgocallback)
    * [var emptymspan](#emptymspan)
    * [var enableSigChan](#enableSigChan)
    * [var envs](#envs)
    * [var execLock](#execLock)
    * [var executablePath](#executablePath)
    * [var extraMCount](#extraMCount)
    * [var extraMWaiters](#extraMWaiters)
    * [var extram](#extram)
    * [var failallocatestack](#failallocatestack)
    * [var failthreadcreate](#failthreadcreate)
    * [var faketime](#faketime)
    * [var fastlog2Table](#fastlog2Table)
    * [var fastrandseed](#fastrandseed)
    * [var finalizer1](#finalizer1)
    * [var finc](#finc)
    * [var fing](#fing)
    * [var fingCreate](#fingCreate)
    * [var fingRunning](#fingRunning)
    * [var fingwait](#fingwait)
    * [var fingwake](#fingwake)
    * [var finlock](#finlock)
    * [var finptrmask](#finptrmask)
    * [var finq](#finq)
    * [var firstmoduledata](#firstmoduledata)
    * [var floatError](#floatError)
    * [var forcegc](#forcegc)
    * [var forcegcperiod](#forcegcperiod)
    * [var freemark](#freemark)
    * [var freezing](#freezing)
    * [var fwdSig](#fwdSig)
    * [var g0](#g0)
    * [var gStatusStrings](#gStatusStrings)
    * [var gcBgMarkWorkerCount](#gcBgMarkWorkerCount)
    * [var gcBgMarkWorkerPool](#gcBgMarkWorkerPool)
    * [var gcBitsArenas](#gcBitsArenas)
    * [var gcBlackenEnabled](#gcBlackenEnabled)
    * [var gcController](#gcController)
    * [var gcMarkDoneFlushed](#gcMarkDoneFlushed)
    * [var gcMarkWorkerModeStrings](#gcMarkWorkerModeStrings)
    * [var gcenable_setup](#gcenable_setup)
    * [var gcphase](#gcphase)
    * [var gcsema](#gcsema)
    * [var globalAlloc](#globalAlloc)
    * [var goarm](#goarm)
    * [var gomaxprocs](#gomaxprocs)
    * [var handlingSig](#handlingSig)
    * [var hashLoad](#hashLoad)
    * [var hashkey](#hashkey)
    * [var idlepMask](#idlepMask)
    * [var inForkedChild](#inForkedChild)
    * [var inf](#inf)
    * [var initSigmask](#initSigmask)
    * [var inittrace](#inittrace)
    * [var intArgRegs](#intArgRegs)
    * [var isIntel](#isIntel)
    * [var isarchive](#isarchive)
    * [var iscgo](#iscgo)
    * [var islibrary](#islibrary)
    * [var itabLock](#itabLock)
    * [var itabTable](#itabTable)
    * [var itabTableInit](#itabTableInit)
    * [var kq](#kq)
    * [var labelSync](#labelSync)
    * [var lastmoduledatap](#lastmoduledatap)
    * [var levelBits](#levelBits)
    * [var levelLogPages](#levelLogPages)
    * [var levelShift](#levelShift)
    * [var lfenceBeforeRdtsc](#lfenceBeforeRdtsc)
    * [var lockNames](#lockNames)
    * [var lockPartialOrder](#lockPartialOrder)
    * [var m0](#m0)
    * [var mFixupRace](#mFixupRace)
    * [var mProf](#mProf)
    * [var mSpanStateNames](#mSpanStateNames)
    * [var mainStarted](#mainStarted)
    * [var main_init_done](#main_init_done)
    * [var main_inittask](#main_inittask)
    * [var maskUpdatedChan](#maskUpdatedChan)
    * [var maxOffAddr](#maxOffAddr)
    * [var maxSearchAddr](#maxSearchAddr)
    * [var maxstackceiling](#maxstackceiling)
    * [var maxstacksize](#maxstacksize)
    * [var mbuckets](#mbuckets)
    * [var mcache0](#mcache0)
    * [var memoryError](#memoryError)
    * [var memstats](#memstats)
    * [var methodValueCallFrameObjs](#methodValueCallFrameObjs)
    * [var metrics](#metrics)
    * [var metricsInit](#metricsInit)
    * [var metricsSema](#metricsSema)
    * [var mheap_](#mheap_)
    * [var minOffAddr](#minOffAddr)
    * [var minhexdigits](#minhexdigits)
    * [var modinfo](#modinfo)
    * [var modulesSlice](#modulesSlice)
    * [var mutexprofilerate](#mutexprofilerate)
    * [var nbuf](#nbuf)
    * [var ncpu](#ncpu)
    * [var netpollBreakRd](#netpollBreakRd)
    * [var netpollBreakWr](#netpollBreakWr)
    * [var netpollInitLock](#netpollInitLock)
    * [var netpollInited](#netpollInited)
    * [var netpollWaiters](#netpollWaiters)
    * [var netpollWakeSig](#netpollWakeSig)
    * [var newmHandoff](#newmHandoff)
    * [var newprocs](#newprocs)
    * [var no_pointers_stackmap](#no_pointers_stackmap)
    * [var oneptrmask](#oneptrmask)
    * [var overflowError](#overflowError)
    * [var overflowTag](#overflowTag)
    * [var panicking](#panicking)
    * [var paniclk](#paniclk)
    * [var pdEface](#pdEface)
    * [var pdType](#pdType)
    * [var pendingPreemptSignals](#pendingPreemptSignals)
    * [var persistentChunks](#persistentChunks)
    * [var physHugePageShift](#physHugePageShift)
    * [var physHugePageSize](#physHugePageSize)
    * [var physPageSize](#physPageSize)
    * [var pinnedTypemaps](#pinnedTypemaps)
    * [var pollcache](#pollcache)
    * [var poolcleanup](#poolcleanup)
    * [var printBacklog](#printBacklog)
    * [var printBacklogIndex](#printBacklogIndex)
    * [var processorVersionInfo](#processorVersionInfo)
    * [var prof](#prof)
    * [var proflock](#proflock)
    * [var ptrnames](#ptrnames)
    * [var racecgosync](#racecgosync)
    * [var raceprocctx0](#raceprocctx0)
    * [var reflectOffs](#reflectOffs)
    * [var runningPanicDefers](#runningPanicDefers)
    * [var runtimeInitTime](#runtimeInitTime)
    * [var runtime_inittask](#runtime_inittask)
    * [var scavenge](#scavenge)
    * [var sched](#sched)
    * [var semtable](#semtable)
    * [var shiftError](#shiftError)
    * [var sig](#sig)
    * [var sigNoteRead](#sigNoteRead)
    * [var sigNoteWrite](#sigNoteWrite)
    * [var signalsOK](#signalsOK)
    * [var sigprofCallers](#sigprofCallers)
    * [var sigprofCallersUse](#sigprofCallersUse)
    * [var sigsetAllExiting](#sigsetAllExiting)
    * [var sigset_all](#sigset_all)
    * [var sigtable](#sigtable)
    * [var sizeClassBuckets](#sizeClassBuckets)
    * [var size_to_class128](#size_to_class128)
    * [var size_to_class8](#size_to_class8)
    * [var sliceEface](#sliceEface)
    * [var sliceType](#sliceType)
    * [var spanSetBlockPool](#spanSetBlockPool)
    * [var stackLarge](#stackLarge)
    * [var stackpool](#stackpool)
    * [var starttime](#starttime)
    * [var staticuint64s](#staticuint64s)
    * [var stealOrder](#stealOrder)
    * [var stringEface](#stringEface)
    * [var stringType](#stringType)
    * [var sweep](#sweep)
    * [var testSigtrap](#testSigtrap)
    * [var testSigusr1](#testSigusr1)
    * [var test_x64](#test_x64)
    * [var test_z64](#test_z64)
    * [var ticks](#ticks)
    * [var timeHistBuckets](#timeHistBuckets)
    * [var timerpMask](#timerpMask)
    * [var tmpbuf](#tmpbuf)
    * [var trace](#trace)
    * [var traceback_cache](#traceback_cache)
    * [var traceback_env](#traceback_env)
    * [var tracelock](#tracelock)
    * [var typecache](#typecache)
    * [var uint16Eface](#uint16Eface)
    * [var uint16Type](#uint16Type)
    * [var uint32Eface](#uint32Eface)
    * [var uint32Type](#uint32Type)
    * [var uint64Eface](#uint64Eface)
    * [var uint64Type](#uint64Type)
    * [var urandom_dev](#urandom_dev)
    * [var useAVXmemmove](#useAVXmemmove)
    * [var useAeshash](#useAeshash)
    * [var useCheckmark](#useCheckmark)
    * [var waitForSigusr1](#waitForSigusr1)
    * [var waitReasonStrings](#waitReasonStrings)
    * [var work](#work)
    * [var worldsema](#worldsema)
    * [var writeBarrier](#writeBarrier)
    * [var x86HasFMA](#x86HasFMA)
    * [var x86HasPOPCNT](#x86HasPOPCNT)
    * [var x86HasSSE41](#x86HasSSE41)
    * [var xbuckets](#xbuckets)
    * [var zeroVal](#zeroVal)
    * [var zerobase](#zerobase)
    * [var _cgo_callers](#_cgo_callers)
    * [var _cgo_init](#_cgo_init)
    * [var _cgo_notify_runtime_init_done](#_cgo_notify_runtime_init_done)
    * [var _cgo_set_context_function](#_cgo_set_context_function)
    * [var _cgo_setenv](#_cgo_setenv)
    * [var _cgo_sys_thread_create](#_cgo_sys_thread_create)
    * [var _cgo_thread_start](#_cgo_thread_start)
    * [var _cgo_unsetenv](#_cgo_unsetenv)
    * [var _cgo_yield](#_cgo_yield)
* [Types](#type)
    * [type AddrRange struct](#AddrRange)
        * [func MakeAddrRange(base, limit uintptr) AddrRange](#MakeAddrRange)
        * [func (a AddrRange) Base() uintptr](#AddrRange.Base)
        * [func (a AddrRange) Equals(b AddrRange) bool](#AddrRange.Equals)
        * [func (a AddrRange) Limit() uintptr](#AddrRange.Limit)
        * [func (a AddrRange) Size() uintptr](#AddrRange.Size)
    * [type AddrRanges struct](#AddrRanges)
        * [func MakeAddrRanges(a ...AddrRange) AddrRanges](#MakeAddrRanges)
        * [func NewAddrRanges() AddrRanges](#NewAddrRanges)
        * [func (a *AddrRanges) Add(r AddrRange)](#AddrRanges.Add)
        * [func (a *AddrRanges) FindSucc(base uintptr) int](#AddrRanges.FindSucc)
        * [func (a *AddrRanges) Ranges() []AddrRange](#AddrRanges.Ranges)
        * [func (a *AddrRanges) TotalBytes() uintptr](#AddrRanges.TotalBytes)
    * [type BitRange struct](#BitRange)
    * [type BitsMismatch struct](#BitsMismatch)
    * [type BlockProfileRecord struct](#BlockProfileRecord)
    * [type ChunkIdx runtime.chunkIdx](#ChunkIdx)
    * [type Error interface](#Error)
    * [type Frame struct](#Frame)
    * [type Frames struct](#Frames)
        * [func CallersFrames(callers []uintptr) *Frames](#CallersFrames)
        * [func (ci *Frames) Next() (frame Frame, more bool)](#Frames.Next)
    * [type Func struct](#Func)
        * [func FuncForPC(pc uintptr) *Func](#FuncForPC)
        * [func (f *Func) Entry() uintptr](#Func.Entry)
        * [func (f *Func) FileLine(pc uintptr) (file string, line int)](#Func.FileLine)
        * [func (f *Func) Name() string](#Func.Name)
        * [func (f *Func) funcInfo() funcInfo](#Func.funcInfo)
        * [func (f *Func) raw() *_func](#Func.raw)
    * [type G runtime.g](#G)
        * [func Getg() *G](#Getg)
        * [func atomicAllG() (**g, uintptr)](#atomicAllG)
        * [func atomicAllGIndex(ptr **g, i uintptr) *g](#atomicAllGIndex)
        * [func beforeIdle(int64, int64) (*g, bool)](#beforeIdle)
        * [func checkIdleGCNoP() (*p, *g)](#checkIdleGCNoP)
        * [func findrunnable() (gp *g, inheritTime bool)](#findrunnable)
        * [func getg() *g](#getg)
        * [func gfget(_p_ *p) *g](#gfget)
        * [func globrunqget(_p_ *p, max int32) *g](#globrunqget)
        * [func malg(stacksize int32) *g](#malg)
        * [func netpollunblock(pd *pollDesc, mode int32, ioready bool) *g](#netpollunblock)
        * [func newproc1(fn *funcval, argp unsafe.Pointer, narg int32, callergp *g, callerpc uintptr) *g](#newproc1)
        * [func runqget(_p_ *p) (gp *g, inheritTime bool)](#runqget)
        * [func runqsteal(_p_, p2 *p, stealRunNextG bool) *g](#runqsteal)
        * [func sigFetchG(c *sigctxt) *g](#sigFetchG)
        * [func stealWork(now int64) (gp *g, inheritTime bool, rnow, pollUntil int64, newWork bool)](#stealWork)
        * [func traceReader() *g](#traceReader)
        * [func wakefing() *g](#wakefing)
    * [type LFNode struct](#LFNode)
        * [func LFStackPop(head *uint64) *LFNode](#LFStackPop)
    * [type LockRank runtime.lockRank](#LockRank)
        * [func (l LockRank) String() string](#LockRank.String)
    * [type M runtime.m](#M)
        * [func acquirem() *m](#acquirem)
        * [func allocm(_p_ *p, fn func(), id int64) *m](#allocm)
        * [func lockextra(nilokay bool) *m](#lockextra)
        * [func mget() *m](#mget)
        * [func traceAcquireBuffer() (mp *m, pid int32, bufp *traceBufPtr)](#traceAcquireBuffer)
    * [type MSpan runtime.mspan](#MSpan)
        * [func AllocMSpan() *MSpan](#AllocMSpan)
    * [type MemProfileRecord struct](#MemProfileRecord)
        * [func (r *MemProfileRecord) InUseBytes() int64](#MemProfileRecord.InUseBytes)
        * [func (r *MemProfileRecord) InUseObjects() int64](#MemProfileRecord.InUseObjects)
        * [func (r *MemProfileRecord) Stack() []uintptr](#MemProfileRecord.Stack)
    * [type MemStats struct](#MemStats)
        * [func ReadMemStatsSlow() (base, slow MemStats)](#ReadMemStatsSlow)
    * [type PageAlloc runtime.pageAlloc](#PageAlloc)
        * [func NewPageAlloc(chunks, scav map[ChunkIdx][]BitRange) *PageAlloc](#NewPageAlloc)
        * [func (p *PageAlloc) Alloc(npages uintptr) (uintptr, uintptr)](#PageAlloc.Alloc)
        * [func (p *PageAlloc) AllocToCache() PageCache](#PageAlloc.AllocToCache)
        * [func (p *PageAlloc) Bounds() (ChunkIdx, ChunkIdx)](#PageAlloc.Bounds)
        * [func (p *PageAlloc) Free(base, npages uintptr)](#PageAlloc.Free)
        * [func (p *PageAlloc) InUse() []AddrRange](#PageAlloc.InUse)
        * [func (p *PageAlloc) PallocData(i ChunkIdx) *PallocData](#PageAlloc.PallocData)
        * [func (p *PageAlloc) Scavenge(nbytes uintptr, mayUnlock bool) (r uintptr)](#PageAlloc.Scavenge)
    * [type PageCache runtime.pageCache](#PageCache)
        * [func NewPageCache(base uintptr, cache, scav uint64) PageCache](#NewPageCache)
        * [func (c *PageCache) Alloc(npages uintptr) (uintptr, uintptr)](#PageCache.Alloc)
        * [func (c *PageCache) Base() uintptr](#PageCache.Base)
        * [func (c *PageCache) Cache() uint64](#PageCache.Cache)
        * [func (c *PageCache) Empty() bool](#PageCache.Empty)
        * [func (c *PageCache) Flush(s *PageAlloc)](#PageCache.Flush)
        * [func (c *PageCache) Scav() uint64](#PageCache.Scav)
    * [type PallocBits runtime.pallocBits](#PallocBits)
        * [func (b *PallocBits) AllocRange(i, n uint)](#PallocBits.AllocRange)
        * [func (b *PallocBits) Find(npages uintptr, searchIdx uint) (uint, uint)](#PallocBits.Find)
        * [func (b *PallocBits) Free(i, n uint)](#PallocBits.Free)
        * [func (b *PallocBits) PopcntRange(i, n uint) uint](#PallocBits.PopcntRange)
        * [func (b *PallocBits) Summarize() PallocSum](#PallocBits.Summarize)
    * [type PallocData runtime.pallocData](#PallocData)
        * [func (d *PallocData) AllocRange(i, n uint)](#PallocData.AllocRange)
        * [func (d *PallocData) FindScavengeCandidate(searchIdx uint, min, max uintptr) (uint, uint)](#PallocData.FindScavengeCandidate)
        * [func (d *PallocData) PallocBits() *PallocBits](#PallocData.PallocBits)
        * [func (d *PallocData) Scavenged() *PallocBits](#PallocData.Scavenged)
        * [func (d *PallocData) ScavengedSetRange(i, n uint)](#PallocData.ScavengedSetRange)
    * [type PallocSum runtime.pallocSum](#PallocSum)
        * [func PackPallocSum(start, max, end uint) PallocSum](#PackPallocSum)
        * [func SummarizeSlow(b *PallocBits) PallocSum](#SummarizeSlow)
        * [func (m PallocSum) End() uint](#PallocSum.End)
        * [func (m PallocSum) Max() uint](#PallocSum.Max)
        * [func (m PallocSum) Start() uint](#PallocSum.Start)
    * [type ProfBuf runtime.profBuf](#ProfBuf)
        * [func NewProfBuf(hdrsize, bufwords, tags int) *ProfBuf](#NewProfBuf)
        * [func (p *ProfBuf) Close()](#ProfBuf.Close)
        * [func (p *ProfBuf) Read(mode profBufReadMode) ([]uint64, []unsafe.Pointer, bool)](#ProfBuf.Read)
        * [func (p *ProfBuf) Write(tag *unsafe.Pointer, now int64, hdr []uint64, stk []uintptr)](#ProfBuf.Write)
    * [type RWMutex struct](#RWMutex)
        * [func (rw *RWMutex) Lock()](#RWMutex.Lock)
        * [func (rw *RWMutex) RLock()](#RWMutex.RLock)
        * [func (rw *RWMutex) RUnlock()](#RWMutex.RUnlock)
        * [func (rw *RWMutex) Unlock()](#RWMutex.Unlock)
    * [type StackRecord struct](#StackRecord)
        * [func (r *StackRecord) Stack() []uintptr](#StackRecord.Stack)
    * [type Sudog runtime.sudog](#Sudog)
        * [func acquireSudog() *sudog](#acquireSudog)
    * [type TimeHistogram runtime.timeHistogram](#TimeHistogram)
        * [func (th *TimeHistogram) Count(bucket, subBucket uint) (uint64, bool)](#TimeHistogram.Count)
        * [func (th *TimeHistogram) Record(duration int64)](#TimeHistogram.Record)
    * [type TypeAssertionError struct](#TypeAssertionError)
        * [func (e *TypeAssertionError) Error() string](#TypeAssertionError.Error)
        * [func (*TypeAssertionError) RuntimeError()](#TypeAssertionError.RuntimeError)
    * [type addrRange struct](#addrRange)
        * [func makeAddrRange(base, limit uintptr) addrRange](#makeAddrRange)
        * [func (a addrRange) contains(addr uintptr) bool](#addrRange.contains)
        * [func (a addrRange) removeGreaterEqual(addr uintptr) addrRange](#addrRange.removeGreaterEqual)
        * [func (a addrRange) size() uintptr](#addrRange.size)
        * [func (a addrRange) subtract(b addrRange) addrRange](#addrRange.subtract)
    * [type addrRanges struct](#addrRanges)
        * [func (a *addrRanges) add(r addrRange)](#addrRanges.add)
        * [func (a *addrRanges) cloneInto(b *addrRanges)](#addrRanges.cloneInto)
        * [func (a *addrRanges) contains(addr uintptr) bool](#addrRanges.contains)
        * [func (a *addrRanges) findAddrGreaterEqual(addr uintptr) (uintptr, bool)](#addrRanges.findAddrGreaterEqual)
        * [func (a *addrRanges) findSucc(addr uintptr) int](#addrRanges.findSucc)
        * [func (a *addrRanges) init(sysStat *sysMemStat)](#addrRanges.init.mranges.go)
        * [func (a *addrRanges) removeGreaterEqual(addr uintptr)](#addrRanges.removeGreaterEqual)
        * [func (a *addrRanges) removeLast(nBytes uintptr) addrRange](#addrRanges.removeLast)
    * [type adjustinfo struct](#adjustinfo)
    * [type ancestorInfo struct](#ancestorInfo)
    * [type arenaHint struct](#arenaHint)
    * [type arenaIdx uint](#arenaIdx)
        * [func arenaIndex(p uintptr) arenaIdx](#arenaIndex)
        * [func (i arenaIdx) l1() uint](#arenaIdx.l1)
        * [func (i arenaIdx) l2() uint](#arenaIdx.l2)
    * [type argset struct](#argset)
    * [type arraytype struct](#arraytype)
    * [type bitvector struct](#bitvector)
        * [func getArgInfo(frame *stkframe, f funcInfo, needArgMap bool, ctxt *funcval) (arglen uintptr, argmap *bitvector)](#getArgInfo)
        * [func getArgInfoFast(f funcInfo, needArgMap bool) (arglen uintptr, argmap *bitvector, ok bool)](#getArgInfoFast)
        * [func getStackMap(frame *stkframe, cache *pcvalueCache, debug bool) (locals, args bitvector, objs []stackObjectRecord)](#getStackMap)
        * [func makeheapobjbv(p uintptr, size uintptr) bitvector](#makeheapobjbv)
        * [func progToPointerMask(prog *byte, size uintptr) bitvector](#progToPointerMask)
        * [func stackmapdata(stkmap *stackmap, n int32) bitvector](#stackmapdata)
        * [func (bv *bitvector) ptrbit(i uintptr) uint8](#bitvector.ptrbit)
    * [type blockRecord struct](#blockRecord)
    * [type bmap struct](#bmap)
        * [func makeBucketArray(t *maptype, b uint8, dirtyalloc unsafe.Pointer) (buckets unsafe.Pointer, nextOverflow *bmap)](#makeBucketArray)
        * [func (b *bmap) keys() unsafe.Pointer](#bmap.keys)
        * [func (b *bmap) overflow(t *maptype) *bmap](#bmap.overflow)
        * [func (b *bmap) setoverflow(t *maptype, ovf *bmap)](#bmap.setoverflow)
    * [type boundsError struct](#boundsError)
        * [func (e boundsError) Error() string](#boundsError.Error)
        * [func (e boundsError) RuntimeError()](#boundsError.RuntimeError)
    * [type boundsErrorCode uint8](#boundsErrorCode)
    * [type bucket struct](#bucket)
        * [func newBucket(typ bucketType, nstk int) *bucket](#newBucket)
        * [func stkbucket(typ bucketType, size uintptr, stk []uintptr, alloc bool) *bucket](#stkbucket)
        * [func (b *bucket) bp() *blockRecord](#bucket.bp)
        * [func (b *bucket) mp() *memRecord](#bucket.mp)
        * [func (b *bucket) stk() []uintptr](#bucket.stk)
    * [type bucketType int](#bucketType)
    * [type cgoCallers [32]uintptr](#cgoCallers)
    * [type cgoContextArg struct](#cgoContextArg)
    * [type cgoSymbolizerArg struct](#cgoSymbolizerArg)
    * [type cgoTracebackArg struct](#cgoTracebackArg)
    * [type cgothreadstart struct](#cgothreadstart)
    * [type chantype struct](#chantype)
    * [type checkmarksMap [1048576]uint8](#checkmarksMap)
    * [type childInfo struct](#childInfo)
    * [type chunkIdx uint](#chunkIdx)
        * [func chunkIndex(p uintptr) chunkIdx](#chunkIndex)
        * [func (i chunkIdx) l1() uint](#chunkIdx.l1)
        * [func (i chunkIdx) l2() uint](#chunkIdx.l2)
    * [type consistentHeapStats struct](#consistentHeapStats)
        * [func (m *consistentHeapStats) acquire() *heapStatsDelta](#consistentHeapStats.acquire)
        * [func (m *consistentHeapStats) read(out *heapStatsDelta)](#consistentHeapStats.read)
        * [func (m *consistentHeapStats) release()](#consistentHeapStats.release)
        * [func (m *consistentHeapStats) unsafeClear()](#consistentHeapStats.unsafeClear)
        * [func (m *consistentHeapStats) unsafeRead(out *heapStatsDelta)](#consistentHeapStats.unsafeRead)
    * [type cpuProfile struct](#cpuProfile)
        * [func (p *cpuProfile) add(gp *g, stk []uintptr)](#cpuProfile.add)
        * [func (p *cpuProfile) addExtra()](#cpuProfile.addExtra)
        * [func (p *cpuProfile) addNonGo(stk []uintptr)](#cpuProfile.addNonGo)
    * [type dbgVar struct](#dbgVar)
    * [type debugCallWrapArgs struct](#debugCallWrapArgs)
    * [type debugLogBuf [16384]byte](#debugLogBuf)
    * [type debugLogReader struct](#debugLogReader)
        * [func (r *debugLogReader) header() (end, tick, nano uint64, p int)](#debugLogReader.header)
        * [func (r *debugLogReader) peek() (tick uint64)](#debugLogReader.peek)
        * [func (r *debugLogReader) printVal() bool](#debugLogReader.printVal)
        * [func (r *debugLogReader) readUint16LEAt(pos uint64) uint16](#debugLogReader.readUint16LEAt)
        * [func (r *debugLogReader) readUint64LEAt(pos uint64) uint64](#debugLogReader.readUint64LEAt)
        * [func (r *debugLogReader) skip() uint64](#debugLogReader.skip)
        * [func (r *debugLogReader) uvarint() uint64](#debugLogReader.uvarint)
        * [func (r *debugLogReader) varint() int64](#debugLogReader.varint)
    * [type debugLogWriter struct](#debugLogWriter)
        * [func (l *debugLogWriter) byte(x byte)](#debugLogWriter.byte)
        * [func (l *debugLogWriter) bytes(x []byte)](#debugLogWriter.bytes)
        * [func (l *debugLogWriter) ensure(n uint64)](#debugLogWriter.ensure)
        * [func (l *debugLogWriter) uvarint(u uint64)](#debugLogWriter.uvarint)
        * [func (l *debugLogWriter) varint(x int64)](#debugLogWriter.varint)
        * [func (l *debugLogWriter) writeFrameAt(pos, size uint64) bool](#debugLogWriter.writeFrameAt)
        * [func (l *debugLogWriter) writeSync(tick, nano uint64)](#debugLogWriter.writeSync)
        * [func (l *debugLogWriter) writeUint64LE(x uint64)](#debugLogWriter.writeUint64LE)
    * [type dlogPerM struct{}](#dlogPerM)
    * [type dlogger struct](#dlogger)
        * [func dlog() *dlogger](#dlog)
        * [func getCachedDlogger() *dlogger](#getCachedDlogger)
        * [func (l *dlogger) B(x bool) *dlogger](#dlogger.B)
        * [func (l *dlogger) End()](#dlogger.End)
        * [func (l *dlogger) Hex(x uint64) *dlogger](#dlogger.Hex)
        * [func (l *dlogger) I(x int) *dlogger](#dlogger.I)
        * [func (l *dlogger) I16(x int16) *dlogger](#dlogger.I16)
        * [func (l *dlogger) P(x interface{}) *dlogger](#dlogger.P)
        * [func (l *dlogger) PC(x uintptr) *dlogger](#dlogger.PC)
        * [func (l *dlogger) S(x string) *dlogger](#dlogger.S)
        * [func (l *dlogger) U64(x uint64) *dlogger](#dlogger.U64)
        * [func (l *dlogger) b(x bool) *dlogger](#dlogger.b)
        * [func (l *dlogger) end()](#dlogger.end)
        * [func (l *dlogger) hex(x uint64) *dlogger](#dlogger.hex)
        * [func (l *dlogger) i(x int) *dlogger](#dlogger.i)
        * [func (l *dlogger) i16(x int16) *dlogger](#dlogger.i16)
        * [func (l *dlogger) i32(x int32) *dlogger](#dlogger.i32)
        * [func (l *dlogger) i64(x int64) *dlogger](#dlogger.i64)
        * [func (l *dlogger) i8(x int8) *dlogger](#dlogger.i8)
        * [func (l *dlogger) p(x interface{}) *dlogger](#dlogger.p)
        * [func (l *dlogger) pc(x uintptr) *dlogger](#dlogger.pc)
        * [func (l *dlogger) s(x string) *dlogger](#dlogger.s)
        * [func (l *dlogger) traceback(x []uintptr) *dlogger](#dlogger.traceback)
        * [func (l *dlogger) u(x uint) *dlogger](#dlogger.u)
        * [func (l *dlogger) u16(x uint16) *dlogger](#dlogger.u16)
        * [func (l *dlogger) u32(x uint32) *dlogger](#dlogger.u32)
        * [func (l *dlogger) u64(x uint64) *dlogger](#dlogger.u64)
        * [func (l *dlogger) u8(x uint8) *dlogger](#dlogger.u8)
        * [func (l *dlogger) uptr(x uintptr) *dlogger](#dlogger.uptr)
    * [type eface struct](#eface)
        * [func convT2E(t *_type, elem unsafe.Pointer) (e eface)](#convT2E)
        * [func convT2Enoptr(t *_type, elem unsafe.Pointer) (e eface)](#convT2Enoptr)
        * [func efaceOf(ep *interface{}) *eface](#efaceOf)
    * [type errorAddressString struct](#errorAddressString)
        * [func (e errorAddressString) Addr() uintptr](#errorAddressString.Addr)
        * [func (e errorAddressString) Error() string](#errorAddressString.Error)
        * [func (e errorAddressString) RuntimeError()](#errorAddressString.RuntimeError)
    * [type errorString string](#errorString)
        * [func (e errorString) Error() string](#errorString.Error)
        * [func (e errorString) RuntimeError()](#errorString.RuntimeError)
    * [type evacDst struct](#evacDst)
    * [type exceptionstate32 struct](#exceptionstate32)
    * [type exceptionstate64 struct](#exceptionstate64)
    * [type finalizer struct](#finalizer)
    * [type finblock struct](#finblock)
    * [type findfuncbucket struct](#findfuncbucket)
    * [type fixalloc struct](#fixalloc)
        * [func (f *fixalloc) alloc() unsafe.Pointer](#fixalloc.alloc)
        * [func (f *fixalloc) free(p unsafe.Pointer)](#fixalloc.free)
        * [func (f *fixalloc) init(size uintptr, first func(arg, p unsafe.Pointer), arg unsafe.Pointer, stat *sysMemStat)](#fixalloc.init.mfixalloc.go)
    * [type floatstate32 struct](#floatstate32)
    * [type floatstate64 struct](#floatstate64)
    * [type forcegcstate struct](#forcegcstate)
    * [type fpcontrol struct](#fpcontrol)
    * [type fpstatus struct](#fpstatus)
    * [type funcFlag uint8](#funcFlag)
    * [type funcID uint8](#funcID)
    * [type funcInfo struct](#funcInfo)
        * [func findfunc(pc uintptr) funcInfo](#findfunc)
        * [func (f funcInfo) valid() bool](#funcInfo.valid)
        * [func (f funcInfo) _Func() *Func](#funcInfo._Func)
    * [type funcinl struct](#funcinl)
    * [type functab struct](#functab)
    * [type functype struct](#functype)
        * [func (t *functype) dotdotdot() bool](#functype.dotdotdot)
        * [func (t *functype) in() []*_type](#functype.in)
        * [func (t *functype) out() []*_type](#functype.out)
    * [type funcval struct](#funcval)
    * [type g struct](#g)
    * [type gList struct](#gList)
        * [func netpoll(delay int64) gList](#netpoll)
        * [func (l *gList) empty() bool](#gList.empty)
        * [func (l *gList) pop() *g](#gList.pop)
        * [func (l *gList) push(gp *g)](#gList.push)
        * [func (l *gList) pushAll(q gQueue)](#gList.pushAll)
    * [type gQueue struct](#gQueue)
        * [func runqdrain(_p_ *p) (drainQ gQueue, n uint32)](#runqdrain)
        * [func (q *gQueue) empty() bool](#gQueue.empty)
        * [func (q *gQueue) pop() *g](#gQueue.pop)
        * [func (q *gQueue) popList() gList](#gQueue.popList)
        * [func (q *gQueue) push(gp *g)](#gQueue.push)
        * [func (q *gQueue) pushBack(gp *g)](#gQueue.pushBack)
        * [func (q *gQueue) pushBackAll(q2 gQueue)](#gQueue.pushBackAll)
    * [type gcBgMarkWorkerNode struct](#gcBgMarkWorkerNode)
    * [type gcBits uint8](#gcBits)
        * [func newAllocBits(nelems uintptr) *gcBits](#newAllocBits)
        * [func newMarkBits(nelems uintptr) *gcBits](#newMarkBits)
        * [func (b *gcBits) bitp(n uintptr) (bytep *uint8, mask uint8)](#gcBits.bitp)
        * [func (b *gcBits) bytep(n uintptr) *uint8](#gcBits.bytep)
    * [type gcBitsArena struct](#gcBitsArena)
        * [func newArenaMayUnlock() *gcBitsArena](#newArenaMayUnlock)
        * [func (b *gcBitsArena) tryAlloc(bytes uintptr) *gcBits](#gcBitsArena.tryAlloc)
    * [type gcBitsHeader struct](#gcBitsHeader)
    * [type gcControllerState struct](#gcControllerState)
        * [func (c *gcControllerState) commit(triggerRatio float64)](#gcControllerState.commit)
        * [func (c *gcControllerState) effectiveGrowthRatio() float64](#gcControllerState.effectiveGrowthRatio)
        * [func (c *gcControllerState) endCycle(userForced bool) float64](#gcControllerState.endCycle)
        * [func (c *gcControllerState) enlistWorker()](#gcControllerState.enlistWorker)
        * [func (c *gcControllerState) findRunnableGCWorker(_p_ *p) *g](#gcControllerState.findRunnableGCWorker)
        * [func (c *gcControllerState) init(gcPercent int32)](#gcControllerState.init.mgcpacer.go.0xc052b2af98)
        * [func (c *gcControllerState) revise()](#gcControllerState.revise)
        * [func (c *gcControllerState) setGCPercent(in int32) int32](#gcControllerState.setGCPercent)
        * [func (c *gcControllerState) startCycle()](#gcControllerState.startCycle)
    * [type gcDrainFlags int](#gcDrainFlags)
    * [type gcMarkWorkerMode int](#gcMarkWorkerMode)
    * [type gcMode int](#gcMode)
    * [type gcTrigger struct](#gcTrigger)
        * [func (t gcTrigger) test() bool](#gcTrigger.test)
    * [type gcTriggerKind int](#gcTriggerKind)
    * [type gcWork struct](#gcWork)
        * [func (w *gcWork) balance()](#gcWork.balance)
        * [func (w *gcWork) dispose()](#gcWork.dispose)
        * [func (w *gcWork) empty() bool](#gcWork.empty)
        * [func (w *gcWork) init()](#gcWork.init.mgcwork.go.0xc0530ddb50)
        * [func (w *gcWork) put(obj uintptr)](#gcWork.put)
        * [func (w *gcWork) putBatch(obj []uintptr)](#gcWork.putBatch)
        * [func (w *gcWork) putFast(obj uintptr) bool](#gcWork.putFast)
        * [func (w *gcWork) tryGet() uintptr](#gcWork.tryGet)
        * [func (w *gcWork) tryGetFast() uintptr](#gcWork.tryGetFast)
    * [type gclink struct](#gclink)
    * [type gclinkptr uintptr](#gclinkptr)
        * [func nextFreeFast(s *mspan) gclinkptr](#nextFreeFast)
        * [func stackpoolalloc(order uint8) gclinkptr](#stackpoolalloc)
        * [func (p gclinkptr) ptr() *gclink](#gclinkptr.ptr)
    * [type gobuf struct](#gobuf)
    * [type gsignalStack struct](#gsignalStack)
    * [type guintptr uintptr](#guintptr)
        * [func (gp *guintptr) cas(old, new guintptr) bool](#guintptr.cas)
        * [func (gp guintptr) ptr() *g](#guintptr.ptr)
        * [func (gp *guintptr) set(g *g)](#guintptr.set)
    * [type hchan struct](#hchan)
        * [func makechan(t *chantype, size int) *hchan](#makechan)
        * [func makechan64(t *chantype, size int64) *hchan](#makechan64)
        * [func reflect_makechan(t *chantype, size int) *hchan](#reflect_makechan)
        * [func (c *hchan) raceaddr() unsafe.Pointer](#hchan.raceaddr)
        * [func (c *hchan) sortkey() uintptr](#hchan.sortkey)
    * [type headTailIndex uint64](#headTailIndex)
        * [func makeHeadTailIndex(head, tail uint32) headTailIndex](#makeHeadTailIndex)
        * [func (h *headTailIndex) cas(old, new headTailIndex) bool](#headTailIndex.cas)
        * [func (h *headTailIndex) decHead() headTailIndex](#headTailIndex.decHead)
        * [func (h headTailIndex) head() uint32](#headTailIndex.head)
        * [func (h *headTailIndex) incHead() headTailIndex](#headTailIndex.incHead)
        * [func (h *headTailIndex) incTail() headTailIndex](#headTailIndex.incTail)
        * [func (h *headTailIndex) load() headTailIndex](#headTailIndex.load)
        * [func (h *headTailIndex) reset()](#headTailIndex.reset)
        * [func (h headTailIndex) split() (head uint32, tail uint32)](#headTailIndex.split)
        * [func (h headTailIndex) tail() uint32](#headTailIndex.tail)
    * [type heapArena struct](#heapArena)
        * [func pageIndexOf(p uintptr) (arena *heapArena, pageIdx uintptr, pageMask uint8)](#pageIndexOf)
    * [type heapBits struct](#heapBits)
        * [func heapBitsForAddr(addr uintptr) (h heapBits)](#heapBitsForAddr)
        * [func (h heapBits) bits() uint32](#heapBits.bits)
        * [func (h heapBits) forward(n uintptr) heapBits](#heapBits.forward)
        * [func (h heapBits) forwardOrBoundary(n uintptr) (heapBits, uintptr)](#heapBits.forwardOrBoundary)
        * [func (h heapBits) initSpan(s *mspan)](#heapBits.initSpan)
        * [func (h heapBits) isPointer() bool](#heapBits.isPointer)
        * [func (h heapBits) morePointers() bool](#heapBits.morePointers)
        * [func (h heapBits) next() heapBits](#heapBits.next)
        * [func (h heapBits) nextArena() heapBits](#heapBits.nextArena)
    * [type heapStatsAggregate struct](#heapStatsAggregate)
        * [func (a *heapStatsAggregate) compute()](#heapStatsAggregate.compute)
    * [type heapStatsDelta struct](#heapStatsDelta)
        * [func (a *heapStatsDelta) merge(b *heapStatsDelta)](#heapStatsDelta.merge)
    * [type heldLockInfo struct](#heldLockInfo)
    * [type hex uint64](#hex)
    * [type hiter struct](#hiter)
        * [func reflect_mapiterinit(t *maptype, h *hmap) *hiter](#reflect_mapiterinit)
    * [type hmap struct](#hmap)
        * [func makemap(t *maptype, hint int, h *hmap) *hmap](#makemap)
        * [func makemap64(t *maptype, hint int64, h *hmap) *hmap](#makemap64)
        * [func makemap_small() *hmap](#makemap_small)
        * [func reflect_makemap(t *maptype, cap int) *hmap](#reflect_makemap)
        * [func (h *hmap) createOverflow()](#hmap.createOverflow)
        * [func (h *hmap) growing() bool](#hmap.growing)
        * [func (h *hmap) incrnoverflow()](#hmap.incrnoverflow)
        * [func (h *hmap) newoverflow(t *maptype, b *bmap) *bmap](#hmap.newoverflow)
        * [func (h *hmap) noldbuckets() uintptr](#hmap.noldbuckets)
        * [func (h *hmap) oldbucketmask() uintptr](#hmap.oldbucketmask)
        * [func (h *hmap) sameSizeGrow() bool](#hmap.sameSizeGrow)
    * [type iface struct](#iface)
        * [func assertE2I2(inter *interfacetype, e eface) (r iface)](#assertE2I2)
        * [func assertI2I2(inter *interfacetype, i iface) (r iface)](#assertI2I2)
        * [func convI2I(inter *interfacetype, i iface) (r iface)](#convI2I)
        * [func convT2I(tab *itab, elem unsafe.Pointer) (i iface)](#convT2I)
        * [func convT2Inoptr(tab *itab, elem unsafe.Pointer) (i iface)](#convT2Inoptr)
    * [type imethod struct](#imethod)
    * [type initTask struct](#initTask)
    * [type inlinedCall struct](#inlinedCall)
    * [type interfacetype struct](#interfacetype)
    * [type itab struct](#itab)
        * [func assertE2I(inter *interfacetype, t *_type) *itab](#assertE2I)
        * [func assertI2I(inter *interfacetype, tab *itab) *itab](#assertI2I)
        * [func getitab(inter *interfacetype, typ *_type, canfail bool) *itab](#getitab)
        * [func (m *itab) init() string](#itab.init.iface.go)
    * [type itabTableType struct](#itabTableType)
        * [func (t *itabTableType) add(m *itab)](#itabTableType.add)
        * [func (t *itabTableType) find(inter *interfacetype, typ *_type) *itab](#itabTableType.find)
    * [type itimerval struct](#itimerval)
    * [type keventt struct](#keventt)
    * [type lfnode struct](#lfnode)
        * [func lfstackUnpack(val uint64) *lfnode](#lfstackUnpack)
    * [type lfstack uint64](#lfstack)
        * [func (head *lfstack) empty() bool](#lfstack.empty)
        * [func (head *lfstack) pop() unsafe.Pointer](#lfstack.pop)
        * [func (head *lfstack) push(node *lfnode)](#lfstack.push)
    * [type libcall struct](#libcall)
    * [type linearAlloc struct](#linearAlloc)
        * [func (l *linearAlloc) alloc(size, align uintptr, sysStat *sysMemStat) unsafe.Pointer](#linearAlloc.alloc)
        * [func (l *linearAlloc) init(base, size uintptr, mapMemory bool)](#linearAlloc.init.malloc.go)
    * [type lockRank int](#lockRank)
        * [func getLockRank(l *mutex) lockRank](#getLockRank)
        * [func (rank lockRank) String() string](#lockRank.String)
    * [type lockRankStruct struct{}](#lockRankStruct)
    * [type m struct](#m)
    * [type mOS struct](#mOS)
    * [type mSpanList struct](#mSpanList)
        * [func (list *mSpanList) init()](#mSpanList.init.mheap.go.0xc053387380)
        * [func (list *mSpanList) insert(span *mspan)](#mSpanList.insert)
        * [func (list *mSpanList) insertBack(span *mspan)](#mSpanList.insertBack)
        * [func (list *mSpanList) isEmpty() bool](#mSpanList.isEmpty)
        * [func (list *mSpanList) remove(span *mspan)](#mSpanList.remove)
        * [func (list *mSpanList) takeAll(other *mSpanList)](#mSpanList.takeAll)
    * [type mSpanState uint8](#mSpanState)
    * [type mSpanStateBox struct](#mSpanStateBox)
        * [func (b *mSpanStateBox) get() mSpanState](#mSpanStateBox.get)
        * [func (b *mSpanStateBox) set(s mSpanState)](#mSpanStateBox.set)
    * [type machTimebaseInfo struct](#machTimebaseInfo)
    * [type mapextra struct](#mapextra)
    * [type maptype struct](#maptype)
        * [func (mt *maptype) hashMightPanic() bool](#maptype.hashMightPanic)
        * [func (mt *maptype) indirectelem() bool](#maptype.indirectelem)
        * [func (mt *maptype) indirectkey() bool](#maptype.indirectkey)
        * [func (mt *maptype) needkeyupdate() bool](#maptype.needkeyupdate)
        * [func (mt *maptype) reflexivekey() bool](#maptype.reflexivekey)
    * [type markBits struct](#markBits)
        * [func markBitsForAddr(p uintptr) markBits](#markBitsForAddr)
        * [func markBitsForSpan(base uintptr) (mbits markBits)](#markBitsForSpan)
        * [func (m *markBits) advance()](#markBits.advance)
        * [func (m markBits) clearMarked()](#markBits.clearMarked)
        * [func (m markBits) isMarked() bool](#markBits.isMarked)
        * [func (m markBits) setMarked()](#markBits.setMarked)
        * [func (m markBits) setMarkedNonAtomic()](#markBits.setMarkedNonAtomic)
    * [type mcache struct](#mcache)
        * [func allocmcache() *mcache](#allocmcache)
        * [func getMCache() *mcache](#getMCache)
        * [func (c *mcache) allocLarge(size uintptr, needzero bool, noscan bool) (*mspan, bool)](#mcache.allocLarge)
        * [func (c *mcache) nextFree(spc spanClass) (v gclinkptr, s *mspan, shouldhelpgc bool)](#mcache.nextFree)
        * [func (c *mcache) prepareForSweep()](#mcache.prepareForSweep)
        * [func (c *mcache) refill(spc spanClass)](#mcache.refill)
        * [func (c *mcache) releaseAll()](#mcache.releaseAll)
    * [type mcentral struct](#mcentral)
        * [func (c *mcentral) cacheSpan() *mspan](#mcentral.cacheSpan)
        * [func (c *mcentral) fullSwept(sweepgen uint32) *spanSet](#mcentral.fullSwept)
        * [func (c *mcentral) fullUnswept(sweepgen uint32) *spanSet](#mcentral.fullUnswept)
        * [func (c *mcentral) grow() *mspan](#mcentral.grow)
        * [func (c *mcentral) init(spc spanClass)](#mcentral.init.mcentral.go)
        * [func (c *mcentral) partialSwept(sweepgen uint32) *spanSet](#mcentral.partialSwept)
        * [func (c *mcentral) partialUnswept(sweepgen uint32) *spanSet](#mcentral.partialUnswept)
        * [func (c *mcentral) uncacheSpan(s *mspan)](#mcentral.uncacheSpan)
    * [type mcontext32 struct](#mcontext32)
    * [type mcontext64 struct](#mcontext64)
    * [type memRecord struct](#memRecord)
    * [type memRecordCycle struct](#memRecordCycle)
        * [func (a *memRecordCycle) add(b *memRecordCycle)](#memRecordCycle.add)
    * [type method struct](#method)
    * [type metricData struct](#metricData)
    * [type metricFloat64Histogram struct](#metricFloat64Histogram)
    * [type metricKind int](#metricKind)
    * [type metricSample struct](#metricSample)
    * [type metricValue struct](#metricValue)
        * [func (v *metricValue) float64HistOrInit(buckets []float64) *metricFloat64Histogram](#metricValue.float64HistOrInit)
    * [type mheap struct](#mheap)
        * [func (h *mheap) alloc(npages uintptr, spanclass spanClass, needzero bool) (*mspan, bool)](#mheap.alloc)
        * [func (h *mheap) allocMSpanLocked() *mspan](#mheap.allocMSpanLocked)
        * [func (h *mheap) allocManual(npages uintptr, typ spanAllocType) *mspan](#mheap.allocManual)
        * [func (h *mheap) allocNeedsZero(base, npage uintptr) (needZero bool)](#mheap.allocNeedsZero)
        * [func (h *mheap) allocSpan(npages uintptr, typ spanAllocType, spanclass spanClass) (s *mspan)](#mheap.allocSpan)
        * [func (h *mheap) freeMSpanLocked(s *mspan)](#mheap.freeMSpanLocked)
        * [func (h *mheap) freeManual(s *mspan, typ spanAllocType)](#mheap.freeManual)
        * [func (h *mheap) freeSpan(s *mspan)](#mheap.freeSpan)
        * [func (h *mheap) freeSpanLocked(s *mspan, typ spanAllocType)](#mheap.freeSpanLocked)
        * [func (h *mheap) grow(npage uintptr) bool](#mheap.grow)
        * [func (h *mheap) init()](#mheap.init.mheap.go)
        * [func (h *mheap) nextSpanForSweep() *mspan](#mheap.nextSpanForSweep)
        * [func (h *mheap) reclaim(npage uintptr)](#mheap.reclaim)
        * [func (h *mheap) reclaimChunk(arenas []arenaIdx, pageIdx, n uintptr) uintptr](#mheap.reclaimChunk)
        * [func (h *mheap) scavengeAll()](#mheap.scavengeAll)
        * [func (h *mheap) setSpans(base, npage uintptr, s *mspan)](#mheap.setSpans)
        * [func (h *mheap) sysAlloc(n uintptr) (v unsafe.Pointer, size uintptr)](#mheap.sysAlloc)
        * [func (h *mheap) tryAllocMSpan() *mspan](#mheap.tryAllocMSpan)
    * [type mlink struct](#mlink)
    * [type moduledata struct](#moduledata)
        * [func findmoduledatap(pc uintptr) *moduledata](#findmoduledatap)
    * [type modulehash struct](#modulehash)
    * [type mspan struct](#mspan)
        * [func findObject(p, refBase, refOff uintptr) (base uintptr, s *mspan, objIndex uintptr)](#findObject)
        * [func materializeGCProg(ptrdata uintptr, prog *byte) *mspan](#materializeGCProg)
        * [func spanOf(p uintptr) *mspan](#spanOf)
        * [func spanOfHeap(p uintptr) *mspan](#spanOfHeap)
        * [func spanOfUnchecked(p uintptr) *mspan](#spanOfUnchecked)
        * [func (s *mspan) allocBitsForIndex(allocBitIndex uintptr) markBits](#mspan.allocBitsForIndex)
        * [func (s *mspan) base() uintptr](#mspan.base)
        * [func (s *mspan) countAlloc() int](#mspan.countAlloc)
        * [func (s *mspan) divideByElemSize(n uintptr) uintptr](#mspan.divideByElemSize)
        * [func (s *mspan) ensureSwept()](#mspan.ensureSwept)
        * [func (span *mspan) inList() bool](#mspan.inList)
        * [func (span *mspan) init(base uintptr, npages uintptr)](#mspan.init.mheap.go.0xc053387380)
        * [func (s *mspan) isFree(index uintptr) bool](#mspan.isFree)
        * [func (s *mspan) layout() (size, n, total uintptr)](#mspan.layout)
        * [func (s *mspan) markBitsForBase() markBits](#mspan.markBitsForBase)
        * [func (s *mspan) markBitsForIndex(objIndex uintptr) markBits](#mspan.markBitsForIndex)
        * [func (s *mspan) nextFreeIndex() uintptr](#mspan.nextFreeIndex)
        * [func (s *mspan) objIndex(p uintptr) uintptr](#mspan.objIndex)
        * [func (s *mspan) refillAllocCache(whichByte uintptr)](#mspan.refillAllocCache)
        * [func (s *mspan) reportZombies()](#mspan.reportZombies)
    * [type mstats struct](#mstats)
    * [type muintptr uintptr](#muintptr)
        * [func (mp muintptr) ptr() *m](#muintptr.ptr)
        * [func (mp *muintptr) set(m *m)](#muintptr.set)
    * [type mutex struct](#mutex)
    * [type name struct](#name)
        * [func resolveNameOff(ptrInModule unsafe.Pointer, off nameOff) name](#resolveNameOff)
        * [func (n name) data(off int) *byte](#name.data)
        * [func (n name) isBlank() bool](#name.isBlank)
        * [func (n name) isExported() bool](#name.isExported)
        * [func (n name) name() (s string)](#name.name)
        * [func (n name) pkgPath() string](#name.pkgPath)
        * [func (n name) readvarint(off int) (int, int)](#name.readvarint)
        * [func (n name) tag() (s string)](#name.tag)
    * [type nameOff int32](#nameOff)
    * [type neverCallThisFunction struct{}](#neverCallThisFunction)
    * [type notInHeap struct{}](#notInHeap)
        * [func persistentalloc1(size, align uintptr, sysStat *sysMemStat) *notInHeap](#persistentalloc1)
        * [func (p *notInHeap) add(bytes uintptr) *notInHeap](#notInHeap.add)
    * [type notInHeapSlice struct](#notInHeapSlice)
    * [type note struct](#note)
    * [type notifyList struct](#notifyList)
    * [type offAddr struct](#offAddr)
        * [func levelIndexToOffAddr(level, idx int) offAddr](#levelIndexToOffAddr)
        * [func (l offAddr) add(bytes uintptr) offAddr](#offAddr.add)
        * [func (l offAddr) addr() uintptr](#offAddr.addr)
        * [func (l1 offAddr) diff(l2 offAddr) uintptr](#offAddr.diff)
        * [func (l1 offAddr) equal(l2 offAddr) bool](#offAddr.equal)
        * [func (l1 offAddr) lessEqual(l2 offAddr) bool](#offAddr.lessEqual)
        * [func (l1 offAddr) lessThan(l2 offAddr) bool](#offAddr.lessThan)
        * [func (l offAddr) sub(bytes uintptr) offAddr](#offAddr.sub)
    * [type p struct](#p)
        * [func checkRunqsNoP(allpSnapshot []*p, idlepMaskSnapshot pMask) *p](#checkRunqsNoP)
        * [func pidleget() *p](#pidleget)
        * [func procresize(nprocs int32) *p](#procresize)
        * [func releasep() *p](#releasep)
        * [func timeSleepUntil() (int64, *p)](#timeSleepUntil)
        * [func (pp *p) destroy()](#p.destroy)
        * [func (pp *p) init(id int32)](#p.init.proc.go.0xc0585819a8)
    * [type pMask []uint32](#pMask)
        * [func (p pMask) clear(id int32)](#pMask.clear)
        * [func (p pMask) read(id uint32) bool](#pMask.read)
        * [func (p pMask) set(id int32)](#pMask.set)
    * [type pageAlloc struct](#pageAlloc)
        * [func (p *pageAlloc) alloc(npages uintptr) (addr uintptr, scav uintptr)](#pageAlloc.alloc)
        * [func (p *pageAlloc) allocRange(base, npages uintptr) uintptr](#pageAlloc.allocRange)
        * [func (p *pageAlloc) allocToCache() pageCache](#pageAlloc.allocToCache)
        * [func (p *pageAlloc) chunkOf(ci chunkIdx) *pallocData](#pageAlloc.chunkOf)
        * [func (p *pageAlloc) find(npages uintptr) (uintptr, offAddr)](#pageAlloc.find)
        * [func (p *pageAlloc) findMappedAddr(addr offAddr) offAddr](#pageAlloc.findMappedAddr)
        * [func (p *pageAlloc) free(base, npages uintptr)](#pageAlloc.free)
        * [func (p *pageAlloc) grow(base, size uintptr)](#pageAlloc.grow)
        * [func (p *pageAlloc) init(mheapLock *mutex, sysStat *sysMemStat)](#pageAlloc.init.mpagealloc.go)
        * [func (p *pageAlloc) scavenge(nbytes uintptr, mayUnlock bool) uintptr](#pageAlloc.scavenge)
        * [func (p *pageAlloc) scavengeOne(work addrRange, max uintptr, mayUnlock bool) (uintptr, addrRange)](#pageAlloc.scavengeOne)
        * [func (p *pageAlloc) scavengeRangeLocked(ci chunkIdx, base, npages uint) uintptr](#pageAlloc.scavengeRangeLocked)
        * [func (p *pageAlloc) scavengeReserve() (addrRange, uint32)](#pageAlloc.scavengeReserve)
        * [func (p *pageAlloc) scavengeStartGen()](#pageAlloc.scavengeStartGen)
        * [func (p *pageAlloc) scavengeUnreserve(r addrRange, gen uint32)](#pageAlloc.scavengeUnreserve)
        * [func (p *pageAlloc) sysGrow(base, limit uintptr)](#pageAlloc.sysGrow)
        * [func (p *pageAlloc) sysInit()](#pageAlloc.sysInit)
        * [func (p *pageAlloc) tryChunkOf(ci chunkIdx) *pallocData](#pageAlloc.tryChunkOf)
        * [func (p *pageAlloc) update(base, npages uintptr, contig, alloc bool)](#pageAlloc.update)
    * [type pageBits [8]uint64](#pageBits)
        * [func (b *pageBits) block64(i uint) uint64](#pageBits.block64)
        * [func (b *pageBits) clear(i uint)](#pageBits.clear)
        * [func (b *pageBits) clearAll()](#pageBits.clearAll)
        * [func (b *pageBits) clearRange(i, n uint)](#pageBits.clearRange)
        * [func (b *pageBits) get(i uint) uint](#pageBits.get)
        * [func (b *pageBits) popcntRange(i, n uint) (s uint)](#pageBits.popcntRange)
        * [func (b *pageBits) set(i uint)](#pageBits.set)
        * [func (b *pageBits) setAll()](#pageBits.setAll)
        * [func (b *pageBits) setRange(i, n uint)](#pageBits.setRange)
    * [type pageCache struct](#pageCache)
        * [func (c *pageCache) alloc(npages uintptr) (uintptr, uintptr)](#pageCache.alloc)
        * [func (c *pageCache) allocN(npages uintptr) (uintptr, uintptr)](#pageCache.allocN)
        * [func (c *pageCache) empty() bool](#pageCache.empty)
        * [func (c *pageCache) flush(p *pageAlloc)](#pageCache.flush)
    * [type pallocBits runtime.pageBits](#pallocBits)
        * [func (b *pallocBits) allocAll()](#pallocBits.allocAll)
        * [func (b *pallocBits) allocRange(i, n uint)](#pallocBits.allocRange)
        * [func (b *pallocBits) find(npages uintptr, searchIdx uint) (uint, uint)](#pallocBits.find)
        * [func (b *pallocBits) find1(searchIdx uint) uint](#pallocBits.find1)
        * [func (b *pallocBits) findLargeN(npages uintptr, searchIdx uint) (uint, uint)](#pallocBits.findLargeN)
        * [func (b *pallocBits) findSmallN(npages uintptr, searchIdx uint) (uint, uint)](#pallocBits.findSmallN)
        * [func (b *pallocBits) free(i, n uint)](#pallocBits.free)
        * [func (b *pallocBits) free1(i uint)](#pallocBits.free1)
        * [func (b *pallocBits) freeAll()](#pallocBits.freeAll)
        * [func (b *pallocBits) pages64(i uint) uint64](#pallocBits.pages64)
        * [func (b *pallocBits) summarize() pallocSum](#pallocBits.summarize)
    * [type pallocData struct](#pallocData)
        * [func (m *pallocData) allocAll()](#pallocData.allocAll)
        * [func (m *pallocData) allocRange(i, n uint)](#pallocData.allocRange)
        * [func (m *pallocData) findScavengeCandidate(searchIdx uint, min, max uintptr) (uint, uint)](#pallocData.findScavengeCandidate)
        * [func (m *pallocData) hasScavengeCandidate(min uintptr) bool](#pallocData.hasScavengeCandidate)
    * [type pallocSum uint64](#pallocSum)
        * [func mergeSummaries(sums []pallocSum, logMaxPagesPerSum uint) pallocSum](#mergeSummaries)
        * [func packPallocSum(start, max, end uint) pallocSum](#packPallocSum)
        * [func (p pallocSum) end() uint](#pallocSum.end)
        * [func (p pallocSum) max() uint](#pallocSum.max)
        * [func (p pallocSum) start() uint](#pallocSum.start)
        * [func (p pallocSum) unpack() (uint, uint, uint)](#pallocSum.unpack)
    * [type pcHeader struct](#pcHeader)
    * [type pcvalueCache struct](#pcvalueCache)
    * [type pcvalueCacheEnt struct](#pcvalueCacheEnt)
    * [type persistentAlloc struct](#persistentAlloc)
    * [type plainError string](#plainError)
        * [func (e plainError) Error() string](#plainError.Error)
        * [func (e plainError) RuntimeError()](#plainError.RuntimeError)
    * [type pollCache struct](#pollCache)
        * [func (c *pollCache) alloc() *pollDesc](#pollCache.alloc)
        * [func (c *pollCache) free(pd *pollDesc)](#pollCache.free)
    * [type pollDesc struct](#pollDesc)
        * [func poll_runtime_pollOpen(fd uintptr) (*pollDesc, int)](#poll_runtime_pollOpen)
        * [func (pd *pollDesc) makeArg() (i interface{})](#pollDesc.makeArg)
    * [type profAtomic uint64](#profAtomic)
        * [func (x *profAtomic) cas(old, new profIndex) bool](#profAtomic.cas)
        * [func (x *profAtomic) load() profIndex](#profAtomic.load)
        * [func (x *profAtomic) store(new profIndex)](#profAtomic.store)
    * [type profBuf struct](#profBuf)
        * [func newProfBuf(hdrsize, bufwords, tags int) *profBuf](#newProfBuf)
        * [func (b *profBuf) canWriteRecord(nstk int) bool](#profBuf.canWriteRecord)
        * [func (b *profBuf) canWriteTwoRecords(nstk1, nstk2 int) bool](#profBuf.canWriteTwoRecords)
        * [func (b *profBuf) close()](#profBuf.close)
        * [func (b *profBuf) hasOverflow() bool](#profBuf.hasOverflow)
        * [func (b *profBuf) incrementOverflow(now int64)](#profBuf.incrementOverflow)
        * [func (b *profBuf) read(mode profBufReadMode) (data []uint64, tags []unsafe.Pointer, eof bool)](#profBuf.read)
        * [func (b *profBuf) takeOverflow() (count uint32, time uint64)](#profBuf.takeOverflow)
        * [func (b *profBuf) wakeupExtra()](#profBuf.wakeupExtra)
        * [func (b *profBuf) write(tagPtr *unsafe.Pointer, now int64, hdr []uint64, stk []uintptr)](#profBuf.write)
    * [type profBufReadMode int](#profBufReadMode)
    * [type profIndex uint64](#profIndex)
        * [func (x profIndex) addCountsAndClearFlags(data, tag int) profIndex](#profIndex.addCountsAndClearFlags)
        * [func (x profIndex) dataCount() uint32](#profIndex.dataCount)
        * [func (x profIndex) tagCount() uint32](#profIndex.tagCount)
    * [type ptabEntry struct](#ptabEntry)
    * [type pthread uintptr](#pthread)
        * [func pthread_self() (t pthread)](#pthread_self)
    * [type pthreadattr struct](#pthreadattr)
    * [type pthreadcond struct](#pthreadcond)
    * [type pthreadcondattr struct](#pthreadcondattr)
    * [type pthreadmutex struct](#pthreadmutex)
    * [type pthreadmutexattr struct](#pthreadmutexattr)
    * [type ptrtype struct](#ptrtype)
    * [type puintptr uintptr](#puintptr)
        * [func (pp puintptr) ptr() *p](#puintptr.ptr)
        * [func (pp *puintptr) set(p *p)](#puintptr.set)
    * [type randomEnum struct](#randomEnum)
        * [func (enum *randomEnum) done() bool](#randomEnum.done)
        * [func (enum *randomEnum) next()](#randomEnum.next)
        * [func (enum *randomEnum) position() uint32](#randomEnum.position)
    * [type randomOrder struct](#randomOrder)
        * [func (ord *randomOrder) reset(count uint32)](#randomOrder.reset)
        * [func (ord *randomOrder) start(i uint32) randomEnum](#randomOrder.start)
    * [type reflectMethodValue struct](#reflectMethodValue)
    * [type regmmst struct](#regmmst)
    * [type regs32 struct](#regs32)
    * [type regs64 struct](#regs64)
    * [type regxmm struct](#regxmm)
    * [type runtimeSelect struct](#runtimeSelect)
    * [type rwmutex struct](#rwmutex)
        * [func (rw *rwmutex) lock()](#rwmutex.lock)
        * [func (rw *rwmutex) rlock()](#rwmutex.rlock)
        * [func (rw *rwmutex) runlock()](#rwmutex.runlock)
        * [func (rw *rwmutex) unlock()](#rwmutex.unlock)
    * [type scase struct](#scase)
    * [type schedt struct](#schedt)
    * [type selectDir int](#selectDir)
    * [type semaProfileFlags int](#semaProfileFlags)
    * [type semaRoot struct](#semaRoot)
        * [func semroot(addr *uint32) *semaRoot](#semroot)
        * [func (root *semaRoot) dequeue(addr *uint32) (found *sudog, now int64)](#semaRoot.dequeue)
        * [func (root *semaRoot) queue(addr *uint32, s *sudog, lifo bool)](#semaRoot.queue)
        * [func (root *semaRoot) rotateLeft(x *sudog)](#semaRoot.rotateLeft)
        * [func (root *semaRoot) rotateRight(y *sudog)](#semaRoot.rotateRight)
    * [type sigTabT struct](#sigTabT)
    * [type sigactiont struct](#sigactiont)
    * [type sigctxt struct](#sigctxt)
        * [func (c *sigctxt) cs() uint64](#sigctxt.cs)
        * [func (c *sigctxt) fault() uintptr](#sigctxt.fault)
        * [func (c *sigctxt) fixsigcode(sig uint32)](#sigctxt.fixsigcode)
        * [func (c *sigctxt) fs() uint64](#sigctxt.fs)
        * [func (c *sigctxt) gs() uint64](#sigctxt.gs)
        * [func (c *sigctxt) preparePanic(sig uint32, gp *g)](#sigctxt.preparePanic)
        * [func (c *sigctxt) pushCall(targetPC, resumePC uintptr)](#sigctxt.pushCall)
        * [func (c *sigctxt) r10() uint64](#sigctxt.r10)
        * [func (c *sigctxt) r11() uint64](#sigctxt.r11)
        * [func (c *sigctxt) r12() uint64](#sigctxt.r12)
        * [func (c *sigctxt) r13() uint64](#sigctxt.r13)
        * [func (c *sigctxt) r14() uint64](#sigctxt.r14)
        * [func (c *sigctxt) r15() uint64](#sigctxt.r15)
        * [func (c *sigctxt) r8() uint64](#sigctxt.r8)
        * [func (c *sigctxt) r9() uint64](#sigctxt.r9)
        * [func (c *sigctxt) rax() uint64](#sigctxt.rax)
        * [func (c *sigctxt) rbp() uint64](#sigctxt.rbp)
        * [func (c *sigctxt) rbx() uint64](#sigctxt.rbx)
        * [func (c *sigctxt) rcx() uint64](#sigctxt.rcx)
        * [func (c *sigctxt) rdi() uint64](#sigctxt.rdi)
        * [func (c *sigctxt) rdx() uint64](#sigctxt.rdx)
        * [func (c *sigctxt) regs() *regs64](#sigctxt.regs)
        * [func (c *sigctxt) rflags() uint64](#sigctxt.rflags)
        * [func (c *sigctxt) rip() uint64](#sigctxt.rip)
        * [func (c *sigctxt) rsi() uint64](#sigctxt.rsi)
        * [func (c *sigctxt) rsp() uint64](#sigctxt.rsp)
        * [func (c *sigctxt) set_rip(x uint64)](#sigctxt.set_rip)
        * [func (c *sigctxt) set_rsp(x uint64)](#sigctxt.set_rsp)
        * [func (c *sigctxt) set_sigaddr(x uint64)](#sigctxt.set_sigaddr)
        * [func (c *sigctxt) set_sigcode(x uint64)](#sigctxt.set_sigcode)
        * [func (c *sigctxt) sigaddr() uint64](#sigctxt.sigaddr)
        * [func (c *sigctxt) sigcode() uint64](#sigctxt.sigcode)
        * [func (c *sigctxt) siglr() uintptr](#sigctxt.siglr)
        * [func (c *sigctxt) sigpc() uintptr](#sigctxt.sigpc)
        * [func (c *sigctxt) sigsp() uintptr](#sigctxt.sigsp)
    * [type siginfo struct](#siginfo)
    * [type sigset uint32](#sigset)
    * [type slice struct](#slice)
        * [func growslice(et *_type, old slice, cap int) slice](#growslice)
    * [type sliceInterfacePtr []byte](#sliceInterfacePtr)
    * [type slicetype struct](#slicetype)
    * [type spanAllocType uint8](#spanAllocType)
        * [func (s spanAllocType) manual() bool](#spanAllocType.manual)
    * [type spanClass uint8](#spanClass)
        * [func makeSpanClass(sizeclass uint8, noscan bool) spanClass](#makeSpanClass)
        * [func (sc spanClass) noscan() bool](#spanClass.noscan)
        * [func (sc spanClass) sizeclass() int8](#spanClass.sizeclass)
    * [type spanSet struct](#spanSet)
        * [func (b *spanSet) pop() *mspan](#spanSet.pop)
        * [func (b *spanSet) push(s *mspan)](#spanSet.push)
        * [func (b *spanSet) reset()](#spanSet.reset)
    * [type spanSetBlock struct](#spanSetBlock)
    * [type spanSetBlockAlloc struct](#spanSetBlockAlloc)
        * [func (p *spanSetBlockAlloc) alloc() *spanSetBlock](#spanSetBlockAlloc.alloc)
        * [func (p *spanSetBlockAlloc) free(block *spanSetBlock)](#spanSetBlockAlloc.free)
    * [type special struct](#special)
        * [func removespecial(p unsafe.Pointer, kind uint8) *special](#removespecial)
    * [type specialReachable struct](#specialReachable)
    * [type specialfinalizer struct](#specialfinalizer)
    * [type specialprofile struct](#specialprofile)
    * [type specialsIter struct](#specialsIter)
        * [func newSpecialsIter(span *mspan) specialsIter](#newSpecialsIter)
        * [func (i *specialsIter) next()](#specialsIter.next)
        * [func (i *specialsIter) unlinkAndNext() *special](#specialsIter.unlinkAndNext)
        * [func (i *specialsIter) valid() bool](#specialsIter.valid)
    * [type stack struct](#stack)
        * [func stackalloc(n uint32) stack](#stackalloc)
    * [type stackObject struct](#stackObject)
        * [func binarySearchTree(x *stackObjectBuf, idx int, n int) (root *stackObject, restBuf *stackObjectBuf, restIdx int)](#binarySearchTree)
        * [func (obj *stackObject) setRecord(r *stackObjectRecord)](#stackObject.setRecord)
    * [type stackObjectBuf struct](#stackObjectBuf)
    * [type stackObjectBufHdr struct](#stackObjectBufHdr)
    * [type stackObjectRecord struct](#stackObjectRecord)
        * [func (r *stackObjectRecord) ptrdata() uintptr](#stackObjectRecord.ptrdata)
        * [func (r *stackObjectRecord) useGCProg() bool](#stackObjectRecord.useGCProg)
    * [type stackScanState struct](#stackScanState)
        * [func (s *stackScanState) addObject(addr uintptr, r *stackObjectRecord)](#stackScanState.addObject)
        * [func (s *stackScanState) buildIndex()](#stackScanState.buildIndex)
        * [func (s *stackScanState) findObject(a uintptr) *stackObject](#stackScanState.findObject)
        * [func (s *stackScanState) getPtr() (p uintptr, conservative bool)](#stackScanState.getPtr)
        * [func (s *stackScanState) putPtr(p uintptr, conservative bool)](#stackScanState.putPtr)
    * [type stackWorkBuf struct](#stackWorkBuf)
    * [type stackWorkBufHdr struct](#stackWorkBufHdr)
    * [type stackfreelist struct](#stackfreelist)
    * [type stackmap struct](#stackmap)
    * [type stackpoolItem struct](#stackpoolItem)
    * [type stackt struct](#stackt)
    * [type statAggregate struct](#statAggregate)
        * [func (a *statAggregate) ensure(deps *statDepSet)](#statAggregate.ensure)
    * [type statDep uint](#statDep)
    * [type statDepSet [1]uint64](#statDepSet)
        * [func makeStatDepSet(deps ...statDep) statDepSet](#makeStatDepSet)
        * [func (s statDepSet) difference(b statDepSet) statDepSet](#statDepSet.difference)
        * [func (s *statDepSet) empty() bool](#statDepSet.empty)
        * [func (s *statDepSet) has(d statDep) bool](#statDepSet.has)
        * [func (s statDepSet) union(b statDepSet) statDepSet](#statDepSet.union)
    * [type stkframe struct](#stkframe)
    * [type stringInterfacePtr string](#stringInterfacePtr)
    * [type stringStruct struct](#stringStruct)
        * [func stringStructOf(sp *string) *stringStruct](#stringStructOf)
    * [type stringStructDWARF struct](#stringStructDWARF)
    * [type stringer interface](#stringer)
    * [type structfield struct](#structfield)
        * [func (f *structfield) offset() uintptr](#structfield.offset)
    * [type structtype struct](#structtype)
    * [type sudog struct](#sudog)
    * [type suspendGState struct](#suspendGState)
        * [func suspendG(gp *g) suspendGState](#suspendG)
    * [type sweepClass uint32](#sweepClass)
        * [func (s *sweepClass) clear()](#sweepClass.clear)
        * [func (s *sweepClass) load() sweepClass](#sweepClass.load)
        * [func (s sweepClass) split() (spc spanClass, full bool)](#sweepClass.split)
        * [func (s *sweepClass) update(sNew sweepClass)](#sweepClass.update)
    * [type sweepLocked struct](#sweepLocked)
        * [func (sl *sweepLocked) sweep(preserve bool) bool](#sweepLocked.sweep)
    * [type sweepLocker struct](#sweepLocker)
        * [func newSweepLocker() sweepLocker](#newSweepLocker)
        * [func (l *sweepLocker) blockCompletion()](#sweepLocker.blockCompletion)
        * [func (l *sweepLocker) dispose()](#sweepLocker.dispose)
        * [func (l *sweepLocker) sweepIsDone()](#sweepLocker.sweepIsDone)
        * [func (l *sweepLocker) tryAcquire(s *mspan) (sweepLocked, bool)](#sweepLocker.tryAcquire)
    * [type sweepdata struct](#sweepdata)
    * [type sysMemStat uint64](#sysMemStat)
        * [func (s *sysMemStat) add(n int64)](#sysMemStat.add)
        * [func (s *sysMemStat) load() uint64](#sysMemStat.load)
    * [type sysStatsAggregate struct](#sysStatsAggregate)
        * [func (a *sysStatsAggregate) compute()](#sysStatsAggregate.compute)
    * [type sysmontick struct](#sysmontick)
    * [type textOff int32](#textOff)
    * [type textsect struct](#textsect)
    * [type tflag uint8](#tflag)
    * [type timeHistogram struct](#timeHistogram)
        * [func (h *timeHistogram) record(duration int64)](#timeHistogram.record)
    * [type timer struct](#timer)
    * [type timespec struct](#timespec)
        * [func (ts *timespec) setNsec(ns int64)](#timespec.setNsec)
    * [type timeval struct](#timeval)
        * [func (tv *timeval) set_usec(x int32)](#timeval.set_usec)
    * [type tmpBuf [32]byte](#tmpBuf)
    * [type traceAlloc struct](#traceAlloc)
        * [func (a *traceAlloc) alloc(n uintptr) unsafe.Pointer](#traceAlloc.alloc)
        * [func (a *traceAlloc) drop()](#traceAlloc.drop)
    * [type traceAllocBlock struct](#traceAllocBlock)
    * [type traceAllocBlockPtr uintptr](#traceAllocBlockPtr)
        * [func (p traceAllocBlockPtr) ptr() *traceAllocBlock](#traceAllocBlockPtr.ptr)
        * [func (p *traceAllocBlockPtr) set(x *traceAllocBlock)](#traceAllocBlockPtr.set)
    * [type traceBuf struct](#traceBuf)
        * [func (buf *traceBuf) byte(v byte)](#traceBuf.byte)
        * [func (buf *traceBuf) varint(v uint64)](#traceBuf.varint)
    * [type traceBufHeader struct](#traceBufHeader)
    * [type traceBufPtr uintptr](#traceBufPtr)
        * [func traceBufPtrOf(b *traceBuf) traceBufPtr](#traceBufPtrOf)
        * [func traceFlush(buf traceBufPtr, pid int32) traceBufPtr](#traceFlush)
        * [func traceFrameForPC(buf traceBufPtr, pid int32, f Frame) (traceFrame, traceBufPtr)](#traceFrameForPC)
        * [func traceFullDequeue() traceBufPtr](#traceFullDequeue)
        * [func traceString(bufp *traceBufPtr, pid int32, s string) (uint64, *traceBufPtr)](#traceString)
        * [func (tp traceBufPtr) ptr() *traceBuf](#traceBufPtr.ptr)
        * [func (tp *traceBufPtr) set(b *traceBuf)](#traceBufPtr.set)
    * [type traceFrame struct](#traceFrame)
    * [type traceStack struct](#traceStack)
        * [func (ts *traceStack) stack() []uintptr](#traceStack.stack)
    * [type traceStackPtr uintptr](#traceStackPtr)
        * [func (tp traceStackPtr) ptr() *traceStack](#traceStackPtr.ptr)
    * [type traceStackTable struct](#traceStackTable)
        * [func (tab *traceStackTable) dump()](#traceStackTable.dump)
        * [func (tab *traceStackTable) find(pcs []uintptr, hash uintptr) uint32](#traceStackTable.find)
        * [func (tab *traceStackTable) newStack(n int) *traceStack](#traceStackTable.newStack)
        * [func (tab *traceStackTable) put(pcs []uintptr) uint32](#traceStackTable.put)
    * [type tracestat struct](#tracestat)
    * [type typeCacheBucket struct](#typeCacheBucket)
    * [type typeOff int32](#typeOff)
    * [type ucontext struct](#ucontext)
    * [type uint16InterfacePtr uint16](#uint16InterfacePtr)
    * [type uint32InterfacePtr uint32](#uint32InterfacePtr)
    * [type uint64InterfacePtr uint64](#uint64InterfacePtr)
    * [type uncommontype struct](#uncommontype)
    * [type usigactiont struct](#usigactiont)
    * [type waitReason uint8](#waitReason)
        * [func (w waitReason) String() string](#waitReason.String)
    * [type waitq struct](#waitq)
        * [func (q *waitq) dequeue() *sudog](#waitq.dequeue)
        * [func (q *waitq) dequeueSudoG(sgp *sudog)](#waitq.dequeueSudoG)
        * [func (q *waitq) enqueue(sgp *sudog)](#waitq.enqueue)
    * [type wbBuf struct](#wbBuf)
        * [func (b *wbBuf) discard()](#wbBuf.discard)
        * [func (b *wbBuf) empty() bool](#wbBuf.empty)
        * [func (b *wbBuf) putFast(old, new uintptr) bool](#wbBuf.putFast)
        * [func (b *wbBuf) reset()](#wbBuf.reset)
    * [type workbuf struct](#workbuf)
        * [func getempty() *workbuf](#getempty)
        * [func handoff(b *workbuf) *workbuf](#handoff)
        * [func trygetfull() *workbuf](#trygetfull)
        * [func (b *workbuf) checkempty()](#workbuf.checkempty)
        * [func (b *workbuf) checknonempty()](#workbuf.checknonempty)
    * [type workbufhdr struct](#workbufhdr)
    * [type _defer struct](#_defer)
        * [func newdefer(siz int32) *_defer](#newdefer)
    * [type _func struct](#_func)
    * [type _panic struct](#_panic)
    * [type _type struct](#_type)
        * [func resolveTypeOff(ptrInModule unsafe.Pointer, off typeOff) *_type](#resolveTypeOff)
        * [func (t *_type) name() string](#_type.name)
        * [func (t *_type) nameOff(off nameOff) name](#_type.nameOff)
        * [func (t *_type) pkgpath() string](#_type.pkgpath)
        * [func (t *_type) string() string](#_type.string)
        * [func (t *_type) textOff(off textOff) unsafe.Pointer](#_type.textOff)
        * [func (t *_type) typeOff(off typeOff) *_type](#_type.typeOff)
        * [func (t *_type) uncommon() *uncommontype](#_type.uncommon)
    * [type _typePair struct](#_typePair)
* [Functions](#func)
    * [func BenchSetType(n int, x interface{})](#BenchSetType)
    * [func BlockOnSystemStack()](#BlockOnSystemStack)
    * [func BlockProfile(p []BlockProfileRecord) (n int, ok bool)](#BlockProfile)
    * [func Breakpoint()](#Breakpoint)
    * [func CPUProfile() []byte](#CPUProfile)
    * [func Caller(skip int) (pc uintptr, file string, line int, ok bool)](#Caller)
    * [func Callers(skip int, pc []uintptr) int](#Callers)
    * [func CheckScavengedBitsCleared(mismatches []BitsMismatch) (n int, ok bool)](#CheckScavengedBitsCleared)
    * [func CountPagesInUse() (pagesInUse, counted uintptr)](#CountPagesInUse)
    * [func DiffPallocBits(a, b *PallocBits) []BitRange](#DiffPallocBits)
    * [func DumpDebugLog() string](#DumpDebugLog)
    * [func Envs() []string](#Envs)
    * [func Fastrand() uint32](#Fastrand)
    * [func Fastrandn(n uint32) uint32](#Fastrandn)
    * [func Fcntl(fd, cmd, arg uintptr) (uintptr, uintptr)](#Fcntl)
    * [func FillAligned(x uint64, m uint) uint64](#FillAligned)
    * [func FinalizerGAsleep() bool](#FinalizerGAsleep)
    * [func FindBitRange64(c uint64, n uint) uint](#FindBitRange64)
    * [func FreeMSpan(s *MSpan)](#FreeMSpan)
    * [func FreePageAlloc(pp *PageAlloc)](#FreePageAlloc)
    * [func G0StackOverflow()](#G0StackOverflow)
    * [func GC()](#GC)
    * [func GCMask(x interface{}) (ret []byte)](#GCMask)
    * [func GCTestIsReachable(ptrs ...unsafe.Pointer) (mask uint64)](#GCTestIsReachable)
    * [func GCTestPointerClass(p unsafe.Pointer) string](#GCTestPointerClass)
    * [func GOMAXPROCS(n int) int](#GOMAXPROCS)
    * [func GOROOT() string](#GOROOT)
    * [func GetNextArenaHint() uintptr](#GetNextArenaHint)
    * [func GetPhysPageSize() uintptr](#GetPhysPageSize)
    * [func Goexit()](#Goexit)
    * [func GoroutineProfile(p []StackRecord) (n int, ok bool)](#GoroutineProfile)
    * [func Gosched()](#Gosched)
    * [func GostringW(w []uint16) (s string)](#GostringW)
    * [func KeepAlive(x interface{})](#KeepAlive)
    * [func KeepNArenaHints(n int)](#KeepNArenaHints)
    * [func LFStackPush(head *uint64, node *LFNode)](#LFStackPush)
    * [func LockOSCounts() (external, internal uint32)](#LockOSCounts)
    * [func LockOSThread()](#LockOSThread)
    * [func MSpanCountAlloc(ms *MSpan, bits []byte) int](#MSpanCountAlloc)
    * [func MapBucketsCount(m map[int]int) int](#MapBucketsCount)
    * [func MapBucketsPointerIsNil(m map[int]int) bool](#MapBucketsPointerIsNil)
    * [func MapNextArenaHint() (start, end uintptr)](#MapNextArenaHint)
    * [func MapTombstoneCheck(m map[int]int)](#MapTombstoneCheck)
    * [func MemProfile(p []MemProfileRecord, inuseZero bool) (n int, ok bool)](#MemProfile)
    * [func MemclrBytes(b []byte)](#MemclrBytes)
    * [func MutexProfile(p []BlockProfileRecord) (n int, ok bool)](#MutexProfile)
    * [func Netpoll(delta int64)](#Netpoll)
    * [func NumCPU() int](#NumCPU)
    * [func NumCgoCall() int64](#NumCgoCall)
    * [func NumGoroutine() int](#NumGoroutine)
    * [func PageBase(c ChunkIdx, pageIdx uint) uintptr](#PageBase)
    * [func PageCachePagesLeaked() (leaked uintptr)](#PageCachePagesLeaked)
    * [func PanicForTesting(b []byte, i int) byte](#PanicForTesting)
    * [func ReadMemStats(m *MemStats)](#ReadMemStats)
    * [func ReadMetricsSlow(memStats *MemStats, samplesp unsafe.Pointer, len, cap int)](#ReadMetricsSlow)
    * [func ReadTrace() []byte](#ReadTrace)
    * [func ResetDebugLog()](#ResetDebugLog)
    * [func RunGetgThreadSwitchTest()](#RunGetgThreadSwitchTest)
    * [func RunSchedLocalQueueEmptyTest(iters int)](#RunSchedLocalQueueEmptyTest)
    * [func RunSchedLocalQueueStealTest()](#RunSchedLocalQueueStealTest)
    * [func RunSchedLocalQueueTest()](#RunSchedLocalQueueTest)
    * [func RunStealOrderTest()](#RunStealOrderTest)
    * [func SemNwait(addr *uint32) uint32](#SemNwait)
    * [func SendSigusr1(mp *M)](#SendSigusr1)
    * [func SetBlockProfileRate(rate int)](#SetBlockProfileRate)
    * [func SetCPUProfileRate(hz int)](#SetCPUProfileRate)
    * [func SetCgoTraceback(version int, traceback, context, symbolizer unsafe.Pointer)](#SetCgoTraceback)
    * [func SetEnvs(e []string)](#SetEnvs)
    * [func SetFinalizer(obj interface{}, finalizer interface{})](#SetFinalizer)
    * [func SetIntArgRegs(a int) int](#SetIntArgRegs)
    * [func SetMutexProfileFraction(rate int) int](#SetMutexProfileFraction)
    * [func SetTracebackEnv(level string)](#SetTracebackEnv)
    * [func Sigisblocked(i int) bool](#Sigisblocked)
    * [func Stack(buf []byte, all bool) int](#Stack)
    * [func StartTrace() error](#StartTrace)
    * [func StopTrace()](#StopTrace)
    * [func StringifyPallocBits(b *PallocBits, r BitRange) string](#StringifyPallocBits)
    * [func ThreadCreateProfile(p []StackRecord) (n int, ok bool)](#ThreadCreateProfile)
    * [func TracebackSystemstack(stk []uintptr, i int) int](#TracebackSystemstack)
    * [func UnlockOSThread()](#UnlockOSThread)
    * [func Version() string](#Version)
    * [func WaitForSigusr1(r, w int32, ready func(mp *M)) (int64, int64)](#WaitForSigusr1)
    * [func abort()](#abort)
    * [func abs(x float64) float64](#abs)
    * [func acquireLockRank(rank lockRank)](#acquireLockRank)
    * [func acquirep(_p_ *p)](#acquirep)
    * [func activeModules() []*moduledata](#activeModules)
    * [func add(p unsafe.Pointer, x uintptr) unsafe.Pointer](#add)
    * [func add1(p *byte) *byte](#add1)
    * [func addAdjustedTimers(pp *p, moved []*timer)](#addAdjustedTimers)
    * [func addOneOpenDeferFrame(gp *g, pc uintptr, sp unsafe.Pointer)](#addOneOpenDeferFrame)
    * [func addb(p *byte, n uintptr) *byte](#addb)
    * [func addfinalizer(p unsafe.Pointer, f *funcval, nret uintptr, fint *_type, ot *ptrtype) bool](#addfinalizer)
    * [func addmoduledata()](#addmoduledata)
    * [func addrsToSummaryRange(level int, base, limit uintptr) (lo int, hi int)](#addrsToSummaryRange)
    * [func addspecial(p unsafe.Pointer, s *special) bool](#addspecial)
    * [func addtimer(t *timer)](#addtimer)
    * [func adjustSignalStack(sig uint32, mp *m, gsigStack *gsignalStack) bool](#adjustSignalStack)
    * [func adjustctxt(gp *g, adjinfo *adjustinfo)](#adjustctxt)
    * [func adjustdefers(gp *g, adjinfo *adjustinfo)](#adjustdefers)
    * [func adjustframe(frame *stkframe, arg unsafe.Pointer) bool](#adjustframe)
    * [func adjustpanics(gp *g, adjinfo *adjustinfo)](#adjustpanics)
    * [func adjustpointer(adjinfo *adjustinfo, vpp unsafe.Pointer)](#adjustpointer)
    * [func adjustpointers(scanp unsafe.Pointer, bv *bitvector, adjinfo *adjustinfo, f funcInfo)](#adjustpointers)
    * [func adjustsudogs(gp *g, adjinfo *adjustinfo)](#adjustsudogs)
    * [func adjusttimers(pp *p, now int64)](#adjusttimers)
    * [func advanceEvacuationMark(h *hmap, t *maptype, newbit uintptr)](#advanceEvacuationMark)
    * [func afterfork()](#afterfork)
    * [func alginit()](#alginit)
    * [func alignDown(n, a uintptr) uintptr](#alignDown)
    * [func alignUp(n, a uintptr) uintptr](#alignUp)
    * [func allFrames(pcs []uintptr) []Frame](#allFrames)
    * [func allgadd(gp *g)](#allgadd)
    * [func appendIntStr(b []byte, v int64, signed bool) []byte](#appendIntStr)
    * [func arenaBase(i arenaIdx) uintptr](#arenaBase)
    * [func args(c int32, v **byte)](#args)
    * [func argv_index(argv **byte, i int32) *byte](#argv_index)
    * [func asmcgocall(fn, arg unsafe.Pointer) int32](#asmcgocall)
    * [func asmcgocall_no_g(fn, arg unsafe.Pointer)](#asmcgocall_no_g)
    * [func asminit()](#asminit)
    * [func assertLockHeld(l *mutex)](#assertLockHeld)
    * [func assertRankHeld(r lockRank)](#assertRankHeld)
    * [func assertWorldStopped()](#assertWorldStopped)
    * [func assertWorldStoppedOrLockHeld(l *mutex)](#assertWorldStoppedOrLockHeld)
    * [func asyncPreempt()](#asyncPreempt)
    * [func asyncPreempt2()](#asyncPreempt2)
    * [func atoi(s string) (int, bool)](#atoi)
    * [func atoi32(s string) (int32, bool)](#atoi32)
    * [func atomicstorep(ptr unsafe.Pointer, new unsafe.Pointer)](#atomicstorep)
    * [func atomicwb(ptr *unsafe.Pointer, new unsafe.Pointer)](#atomicwb)
    * [func badPointer(s *mspan, p, refBase, refOff uintptr)](#badPointer)
    * [func badTimer()](#badTimer)
    * [func badcgocallback()](#badcgocallback)
    * [func badctxt()](#badctxt)
    * [func badmcall(fn func(*g))](#badmcall)
    * [func badmcall2(fn func(*g))](#badmcall2)
    * [func badmorestackg0()](#badmorestackg0)
    * [func badmorestackgsignal()](#badmorestackgsignal)
    * [func badreflectcall()](#badreflectcall)
    * [func badsignal(sig uintptr, c *sigctxt)](#badsignal)
    * [func badsystemstack()](#badsystemstack)
    * [func badunlockosthread()](#badunlockosthread)
    * [func beforefork()](#beforefork)
    * [func bgscavenge()](#bgscavenge)
    * [func bgsweep()](#bgsweep)
    * [func block()](#block)
    * [func blockAlignSummaryRange(level int, lo, hi int) (int, int)](#blockAlignSummaryRange)
    * [func blockOnSystemStackInternal()](#blockOnSystemStackInternal)
    * [func blockableSig(sig uint32) bool](#blockableSig)
    * [func blockevent(cycles int64, skip int)](#blockevent)
    * [func blocksampled(cycles, rate int64) bool](#blocksampled)
    * [func bool2int(x bool) int](#bool2int)
    * [func breakpoint()](#breakpoint)
    * [func bucketEvacuated(t *maptype, h *hmap, bucket uintptr) bool](#bucketEvacuated)
    * [func bucketMask(b uint8) uintptr](#bucketMask)
    * [func bucketShift(b uint8) uintptr](#bucketShift)
    * [func bulkBarrierBitmap(dst, src, size, maskOffset uintptr, bits *uint8)](#bulkBarrierBitmap)
    * [func bulkBarrierPreWrite(dst, src, size uintptr)](#bulkBarrierPreWrite)
    * [func bulkBarrierPreWriteSrcOnly(dst, src, size uintptr)](#bulkBarrierPreWriteSrcOnly)
    * [func bytes(s string) (ret []byte)](#bytes)
    * [func bytesHash(b []byte, seed uintptr) uintptr](#bytesHash)
    * [func c128equal(p, q unsafe.Pointer) bool](#c128equal)
    * [func c128hash(p unsafe.Pointer, h uintptr) uintptr](#c128hash)
    * [func c64equal(p, q unsafe.Pointer) bool](#c64equal)
    * [func c64hash(p unsafe.Pointer, h uintptr) uintptr](#c64hash)
    * [func call1024(typ, fn, stackArgs unsafe.Pointer, stackArgsSize, stackRetOffset, frameSize uint32, regArgs *abi.RegArgs)](#call1024)
    * [func call1048576(typ, fn, stackArgs unsafe.Pointer, stackArgsSize, stackRetOffset, frameSize uint32, regArgs *abi.RegArgs)](#call1048576)
    * [func call1073741824(typ, fn, stackArgs unsafe.Pointer, stackArgsSize, stackRetOffset, frameSize uint32, regArgs *abi.RegArgs)](#call1073741824)
    * [func call128(typ, fn, stackArgs unsafe.Pointer, stackArgsSize, stackRetOffset, frameSize uint32, regArgs *abi.RegArgs)](#call128)
    * [func call131072(typ, fn, stackArgs unsafe.Pointer, stackArgsSize, stackRetOffset, frameSize uint32, regArgs *abi.RegArgs)](#call131072)
    * [func call134217728(typ, fn, stackArgs unsafe.Pointer, stackArgsSize, stackRetOffset, frameSize uint32, regArgs *abi.RegArgs)](#call134217728)
    * [func call16(typ, fn, stackArgs unsafe.Pointer, stackArgsSize, stackRetOffset, frameSize uint32, regArgs *abi.RegArgs)](#call16)
    * [func call16384(typ, fn, stackArgs unsafe.Pointer, stackArgsSize, stackRetOffset, frameSize uint32, regArgs *abi.RegArgs)](#call16384)
    * [func call16777216(typ, fn, stackArgs unsafe.Pointer, stackArgsSize, stackRetOffset, frameSize uint32, regArgs *abi.RegArgs)](#call16777216)
    * [func call2048(typ, fn, stackArgs unsafe.Pointer, stackArgsSize, stackRetOffset, frameSize uint32, regArgs *abi.RegArgs)](#call2048)
    * [func call2097152(typ, fn, stackArgs unsafe.Pointer, stackArgsSize, stackRetOffset, frameSize uint32, regArgs *abi.RegArgs)](#call2097152)
    * [func call256(typ, fn, stackArgs unsafe.Pointer, stackArgsSize, stackRetOffset, frameSize uint32, regArgs *abi.RegArgs)](#call256)
    * [func call262144(typ, fn, stackArgs unsafe.Pointer, stackArgsSize, stackRetOffset, frameSize uint32, regArgs *abi.RegArgs)](#call262144)
    * [func call268435456(typ, fn, stackArgs unsafe.Pointer, stackArgsSize, stackRetOffset, frameSize uint32, regArgs *abi.RegArgs)](#call268435456)
    * [func call32(typ, fn, stackArgs unsafe.Pointer, stackArgsSize, stackRetOffset, frameSize uint32, regArgs *abi.RegArgs)](#call32)
    * [func call32768(typ, fn, stackArgs unsafe.Pointer, stackArgsSize, stackRetOffset, frameSize uint32, regArgs *abi.RegArgs)](#call32768)
    * [func call33554432(typ, fn, stackArgs unsafe.Pointer, stackArgsSize, stackRetOffset, frameSize uint32, regArgs *abi.RegArgs)](#call33554432)
    * [func call4096(typ, fn, stackArgs unsafe.Pointer, stackArgsSize, stackRetOffset, frameSize uint32, regArgs *abi.RegArgs)](#call4096)
    * [func call4194304(typ, fn, stackArgs unsafe.Pointer, stackArgsSize, stackRetOffset, frameSize uint32, regArgs *abi.RegArgs)](#call4194304)
    * [func call512(typ, fn, stackArgs unsafe.Pointer, stackArgsSize, stackRetOffset, frameSize uint32, regArgs *abi.RegArgs)](#call512)
    * [func call524288(typ, fn, stackArgs unsafe.Pointer, stackArgsSize, stackRetOffset, frameSize uint32, regArgs *abi.RegArgs)](#call524288)
    * [func call536870912(typ, fn, stackArgs unsafe.Pointer, stackArgsSize, stackRetOffset, frameSize uint32, regArgs *abi.RegArgs)](#call536870912)
    * [func call64(typ, fn, stackArgs unsafe.Pointer, stackArgsSize, stackRetOffset, frameSize uint32, regArgs *abi.RegArgs)](#call64)
    * [func call65536(typ, fn, stackArgs unsafe.Pointer, stackArgsSize, stackRetOffset, frameSize uint32, regArgs *abi.RegArgs)](#call65536)
    * [func call67108864(typ, fn, stackArgs unsafe.Pointer, stackArgsSize, stackRetOffset, frameSize uint32, regArgs *abi.RegArgs)](#call67108864)
    * [func call8192(typ, fn, stackArgs unsafe.Pointer, stackArgsSize, stackRetOffset, frameSize uint32, regArgs *abi.RegArgs)](#call8192)
    * [func call8388608(typ, fn, stackArgs unsafe.Pointer, stackArgsSize, stackRetOffset, frameSize uint32, regArgs *abi.RegArgs)](#call8388608)
    * [func callCgoSymbolizer(arg *cgoSymbolizerArg)](#callCgoSymbolizer)
    * [func callers(skip int, pcbuf []uintptr) int](#callers)
    * [func canPreemptM(mp *m) bool](#canPreemptM)
    * [func canpanic(gp *g) bool](#canpanic)
    * [func cansemacquire(addr *uint32) bool](#cansemacquire)
    * [func casGFromPreempted(gp *g, old, new uint32) bool](#casGFromPreempted)
    * [func casGToPreemptScan(gp *g, old, new uint32)](#casGToPreemptScan)
    * [func casfrom_Gscanstatus(gp *g, oldval, newval uint32)](#casfrom_Gscanstatus)
    * [func casgcopystack(gp *g) uint32](#casgcopystack)
    * [func casgstatus(gp *g, oldval, newval uint32)](#casgstatus)
    * [func castogscanstatus(gp *g, oldval, newval uint32) bool](#castogscanstatus)
    * [func cfuncname(f funcInfo) *byte](#cfuncname)
    * [func cfuncnameFromNameoff(f funcInfo, nameoff int32) *byte](#cfuncnameFromNameoff)
    * [func cgoCheckArg(t *_type, p unsafe.Pointer, indir, top bool, msg string)](#cgoCheckArg)
    * [func cgoCheckBits(src unsafe.Pointer, gcbits *byte, off, size uintptr)](#cgoCheckBits)
    * [func cgoCheckMemmove(typ *_type, dst, src unsafe.Pointer, off, size uintptr)](#cgoCheckMemmove)
    * [func cgoCheckPointer(ptr interface{}, arg interface{})](#cgoCheckPointer)
    * [func cgoCheckResult(val interface{})](#cgoCheckResult)
    * [func cgoCheckSliceCopy(typ *_type, dst, src unsafe.Pointer, n int)](#cgoCheckSliceCopy)
    * [func cgoCheckTypedBlock(typ *_type, src unsafe.Pointer, off, size uintptr)](#cgoCheckTypedBlock)
    * [func cgoCheckUnknownPointer(p unsafe.Pointer, msg string) (base, i uintptr)](#cgoCheckUnknownPointer)
    * [func cgoCheckUsingType(typ *_type, src unsafe.Pointer, off, size uintptr)](#cgoCheckUsingType)
    * [func cgoCheckWriteBarrier(dst *uintptr, src uintptr)](#cgoCheckWriteBarrier)
    * [func cgoContextPCs(ctxt uintptr, buf []uintptr)](#cgoContextPCs)
    * [func cgoInRange(p unsafe.Pointer, start, end uintptr) bool](#cgoInRange)
    * [func cgoIsGoPointer(p unsafe.Pointer) bool](#cgoIsGoPointer)
    * [func cgoSigtramp()](#cgoSigtramp)
    * [func cgoUse(interface{})](#cgoUse)
    * [func cgocall(fn, arg unsafe.Pointer) int32](#cgocall)
    * [func cgocallback(fn, frame, ctxt uintptr)](#cgocallback)
    * [func cgocallbackg(fn, frame unsafe.Pointer, ctxt uintptr)](#cgocallbackg)
    * [func cgocallbackg1(fn, frame unsafe.Pointer, ctxt uintptr)](#cgocallbackg1)
    * [func cgounimpl()](#cgounimpl)
    * [func chanbuf(c *hchan, i uint) unsafe.Pointer](#chanbuf)
    * [func chanparkcommit(gp *g, chanLock unsafe.Pointer) bool](#chanparkcommit)
    * [func chanrecv(c *hchan, ep unsafe.Pointer, block bool) (selected, received bool)](#chanrecv)
    * [func chanrecv1(c *hchan, elem unsafe.Pointer)](#chanrecv1)
    * [func chanrecv2(c *hchan, elem unsafe.Pointer) (received bool)](#chanrecv2)
    * [func chansend(c *hchan, ep unsafe.Pointer, block bool, callerpc uintptr) bool](#chansend)
    * [func chansend1(c *hchan, elem unsafe.Pointer)](#chansend1)
    * [func check()](#check)
    * [func checkASM() bool](#checkASM)
    * [func checkTimeouts()](#checkTimeouts)
    * [func checkTimers(pp *p, now int64) (rnow, pollUntil int64, ran bool)](#checkTimers)
    * [func checkTimersNoP(allpSnapshot []*p, timerpMaskSnapshot pMask, pollUntil int64) int64](#checkTimersNoP)
    * [func checkdead()](#checkdead)
    * [func checkmcount()](#checkmcount)
    * [func checkptrAlignment(p unsafe.Pointer, elem *_type, n uintptr)](#checkptrAlignment)
    * [func checkptrArithmetic(p unsafe.Pointer, originals []unsafe.Pointer)](#checkptrArithmetic)
    * [func checkptrBase(p unsafe.Pointer) uintptr](#checkptrBase)
    * [func chunkBase(ci chunkIdx) uintptr](#chunkBase)
    * [func chunkPageIndex(p uintptr) uint](#chunkPageIndex)
    * [func cleantimers(pp *p)](#cleantimers)
    * [func clearDeletedTimers(pp *p)](#clearDeletedTimers)
    * [func clearSignalHandlers()](#clearSignalHandlers)
    * [func clearpools()](#clearpools)
    * [func clobberfree(x unsafe.Pointer, size uintptr)](#clobberfree)
    * [func close_trampoline()](#close_trampoline)
    * [func closechan(c *hchan)](#closechan)
    * [func closefd(fd int32) int32](#closefd)
    * [func closeonexec(fd int32)](#closeonexec)
    * [func complex128div(n complex128, m complex128) complex128](#complex128div)
    * [func concatstring2(buf *tmpBuf, a0, a1 string) string](#concatstring2)
    * [func concatstring3(buf *tmpBuf, a0, a1, a2 string) string](#concatstring3)
    * [func concatstring4(buf *tmpBuf, a0, a1, a2, a3 string) string](#concatstring4)
    * [func concatstring5(buf *tmpBuf, a0, a1, a2, a3, a4 string) string](#concatstring5)
    * [func concatstrings(buf *tmpBuf, a []string) string](#concatstrings)
    * [func convT16(val uint16) (x unsafe.Pointer)](#convT16)
    * [func convT32(val uint32) (x unsafe.Pointer)](#convT32)
    * [func convT64(val uint64) (x unsafe.Pointer)](#convT64)
    * [func convTslice(val []byte) (x unsafe.Pointer)](#convTslice)
    * [func convTstring(val string) (x unsafe.Pointer)](#convTstring)
    * [func copysign(x, y float64) float64](#copysign)
    * [func copystack(gp *g, newsize uintptr)](#copystack)
    * [func countSub(x, y uint32) int](#countSub)
    * [func countrunes(s string) int](#countrunes)
    * [func cpuinit()](#cpuinit)
    * [func cputicks() int64](#cputicks)
    * [func crash()](#crash)
    * [func createfing()](#createfing)
    * [func crypto_x509_syscall(fn, a1, a2, a3, a4, a5, a6 uintptr) (r1 uintptr)](#crypto_x509_syscall)
    * [func cstring(s string) unsafe.Pointer](#cstring)
    * [func debugCallCheck(pc uintptr) string](#debugCallCheck)
    * [func debugCallPanicked(val interface{})](#debugCallPanicked)
    * [func debugCallV2()](#debugCallV2)
    * [func debugCallWrap(dispatch uintptr)](#debugCallWrap)
    * [func debugCallWrap1()](#debugCallWrap1)
    * [func debugCallWrap2(dispatch uintptr)](#debugCallWrap2)
    * [func debug_modinfo() string](#debug_modinfo)
    * [func decoderune(s string, k int) (r rune, pos int)](#decoderune)
    * [func deductSweepCredit(spanBytes uintptr, callerSweepPages uintptr)](#deductSweepCredit)
    * [func defaultMemProfileRate(v int) int](#defaultMemProfileRate)
    * [func deferArgs(d *_defer) unsafe.Pointer](#deferArgs)
    * [func deferCallSave(p *_panic, fn func())](#deferCallSave)
    * [func deferFunc(d *_defer) func()](#deferFunc)
    * [func deferclass(siz uintptr) uintptr](#deferclass)
    * [func deferproc(siz int32, fn *funcval)](#deferproc)
    * [func deferprocStack(d *_defer)](#deferprocStack)
    * [func deferreturn()](#deferreturn)
    * [func deltimer(t *timer) bool](#deltimer)
    * [func dematerializeGCProg(s *mspan)](#dematerializeGCProg)
    * [func dieFromSignal(sig uint32)](#dieFromSignal)
    * [func divRoundUp(n, a uintptr) uintptr](#divRoundUp)
    * [func divlu(u1, u0, v uint64) (q, r uint64)](#divlu)
    * [func doInit(t *initTask)](#doInit)
    * [func doSigPreempt(gp *g, ctxt *sigctxt)](#doSigPreempt)
    * [func doaddtimer(pp *p, t *timer)](#doaddtimer)
    * [func dodeltimer(pp *p, i int)](#dodeltimer)
    * [func dodeltimer0(pp *p)](#dodeltimer0)
    * [func dolockOSThread()](#dolockOSThread)
    * [func dopanic_m(gp *g, pc, sp uintptr) bool](#dopanic_m)
    * [func dounlockOSThread()](#dounlockOSThread)
    * [func dropg()](#dropg)
    * [func dropm()](#dropm)
    * [func duffcopy()](#duffcopy)
    * [func duffzero()](#duffzero)
    * [func dumpGCProg(p *byte)](#dumpGCProg)
    * [func dumpbool(b bool)](#dumpbool)
    * [func dumpbv(cbv *bitvector, offset uintptr)](#dumpbv)
    * [func dumpfields(bv bitvector)](#dumpfields)
    * [func dumpfinalizer(obj unsafe.Pointer, fn *funcval, fint *_type, ot *ptrtype)](#dumpfinalizer)
    * [func dumpframe(s *stkframe, arg unsafe.Pointer) bool](#dumpframe)
    * [func dumpgoroutine(gp *g)](#dumpgoroutine)
    * [func dumpgs()](#dumpgs)
    * [func dumpgstatus(gp *g)](#dumpgstatus)
    * [func dumpint(v uint64)](#dumpint)
    * [func dumpitabs()](#dumpitabs)
    * [func dumpmemprof()](#dumpmemprof)
    * [func dumpmemprof_callback(b *bucket, nstk uintptr, pstk *uintptr, size, allocs, frees uintptr)](#dumpmemprof_callback)
    * [func dumpmemrange(data unsafe.Pointer, len uintptr)](#dumpmemrange)
    * [func dumpmemstats(m *MemStats)](#dumpmemstats)
    * [func dumpms()](#dumpms)
    * [func dumpobj(obj unsafe.Pointer, size uintptr, bv bitvector)](#dumpobj)
    * [func dumpobjs()](#dumpobjs)
    * [func dumpotherroot(description string, to unsafe.Pointer)](#dumpotherroot)
    * [func dumpparams()](#dumpparams)
    * [func dumpregs(c *sigctxt)](#dumpregs)
    * [func dumproots()](#dumproots)
    * [func dumpslice(b []byte)](#dumpslice)
    * [func dumpstr(s string)](#dumpstr)
    * [func dumptype(t *_type)](#dumptype)
    * [func dwrite(data unsafe.Pointer, len uintptr)](#dwrite)
    * [func dwritebyte(b byte)](#dwritebyte)
    * [func efaceHash(i interface{}, seed uintptr) uintptr](#efaceHash)
    * [func efaceeq(t *_type, x, y unsafe.Pointer) bool](#efaceeq)
    * [func elideWrapperCalling(id funcID) bool](#elideWrapperCalling)
    * [func empty(c *hchan) bool](#empty)
    * [func encoderune(p []byte, r rune) int](#encoderune)
    * [func endCheckmarks()](#endCheckmarks)
    * [func ensureSigM()](#ensureSigM)
    * [func entersyscall()](#entersyscall)
    * [func entersyscall_gcwait()](#entersyscall_gcwait)
    * [func entersyscall_sysmon()](#entersyscall_sysmon)
    * [func entersyscallblock()](#entersyscallblock)
    * [func entersyscallblock_handoff()](#entersyscallblock_handoff)
    * [func envKeyEqual(a, b string) bool](#envKeyEqual)
    * [func environ() []string](#environ)
    * [func eqslice(x, y []uintptr) bool](#eqslice)
    * [func evacuate(t *maptype, h *hmap, oldbucket uintptr)](#evacuate)
    * [func evacuate_fast32(t *maptype, h *hmap, oldbucket uintptr)](#evacuate_fast32)
    * [func evacuate_fast64(t *maptype, h *hmap, oldbucket uintptr)](#evacuate_fast64)
    * [func evacuate_faststr(t *maptype, h *hmap, oldbucket uintptr)](#evacuate_faststr)
    * [func evacuated(b *bmap) bool](#evacuated)
    * [func execute(gp *g, inheritTime bool)](#execute)
    * [func exit(code int32)](#exit)
    * [func exitThread(wait *uint32)](#exitThread)
    * [func exit_trampoline()](#exit_trampoline)
    * [func exitsyscall()](#exitsyscall)
    * [func exitsyscall0(gp *g)](#exitsyscall0)
    * [func exitsyscallfast(oldp *p) bool](#exitsyscallfast)
    * [func exitsyscallfast_pidle() bool](#exitsyscallfast_pidle)
    * [func exitsyscallfast_reacquired()](#exitsyscallfast_reacquired)
    * [func expandCgoFrames(pc uintptr) []Frame](#expandCgoFrames)
    * [func extendRandom(r []byte, n int)](#extendRandom)
    * [func f32equal(p, q unsafe.Pointer) bool](#f32equal)
    * [func f32hash(p unsafe.Pointer, h uintptr) uintptr](#f32hash)
    * [func f32to64(f uint32) uint64](#f32to64)
    * [func f32toint32(x uint32) int32](#f32toint32)
    * [func f32toint64(x uint32) int64](#f32toint64)
    * [func f32touint64(x float32) uint64](#f32touint64)
    * [func f64equal(p, q unsafe.Pointer) bool](#f64equal)
    * [func f64hash(p unsafe.Pointer, h uintptr) uintptr](#f64hash)
    * [func f64to32(f uint64) uint32](#f64to32)
    * [func f64toint(f uint64) (val int64, ok bool)](#f64toint)
    * [func f64toint32(x uint64) int32](#f64toint32)
    * [func f64toint64(x uint64) int64](#f64toint64)
    * [func f64touint64(x float64) uint64](#f64touint64)
    * [func fadd32(x, y uint32) uint32](#fadd32)
    * [func fadd64(f, g uint64) uint64](#fadd64)
    * [func fastexprand(mean int) int32](#fastexprand)
    * [func fastlog2(x float64) float64](#fastlog2)
    * [func fastrand() uint32](#fastrand)
    * [func fastrandinit()](#fastrandinit)
    * [func fastrandn(n uint32) uint32](#fastrandn)
    * [func fatalpanic(msgs *_panic)](#fatalpanic)
    * [func fatalthrow()](#fatalthrow)
    * [func fcmp64(f, g uint64) (cmp int32, isnan bool)](#fcmp64)
    * [func fcntl(fd, cmd, arg int32) int32](#fcntl)
    * [func fcntl_trampoline()](#fcntl_trampoline)
    * [func fdiv32(x, y uint32) uint32](#fdiv32)
    * [func fdiv64(f, g uint64) uint64](#fdiv64)
    * [func feq32(x, y uint32) bool](#feq32)
    * [func feq64(x, y uint64) bool](#feq64)
    * [func fge32(x, y uint32) bool](#fge32)
    * [func fge64(x, y uint64) bool](#fge64)
    * [func fgt32(x, y uint32) bool](#fgt32)
    * [func fgt64(x, y uint64) bool](#fgt64)
    * [func fillAligned(x uint64, m uint) uint64](#fillAligned)
    * [func fillstack(stk stack, b byte)](#fillstack)
    * [func findBitRange64(c uint64, n uint) uint](#findBitRange64)
    * [func findnull(s *byte) int](#findnull)
    * [func findnullw(s *uint16) int](#findnullw)
    * [func findsghi(gp *g, stk stack) uintptr](#findsghi)
    * [func finishsweep_m()](#finishsweep_m)
    * [func finq_callback(fn *funcval, obj unsafe.Pointer, nret uintptr, fint *_type, ot *ptrtype)](#finq_callback)
    * [func fint32to32(x int32) uint32](#fint32to32)
    * [func fint32to64(x int32) uint64](#fint32to64)
    * [func fint64to32(x int64) uint32](#fint64to32)
    * [func fint64to64(x int64) uint64](#fint64to64)
    * [func fintto64(val int64) (f uint64)](#fintto64)
    * [func float64Inf() float64](#float64Inf)
    * [func float64NegInf() float64](#float64NegInf)
    * [func float64bits(f float64) uint64](#float64bits)
    * [func float64frombits(b uint64) float64](#float64frombits)
    * [func flush()](#flush)
    * [func flushallmcaches()](#flushallmcaches)
    * [func flushmcache(i int)](#flushmcache)
    * [func fmtNSAsMS(buf []byte, ns uint64) []byte](#fmtNSAsMS)
    * [func fmul32(x, y uint32) uint32](#fmul32)
    * [func fmul64(f, g uint64) uint64](#fmul64)
    * [func fneg64(f uint64) uint64](#fneg64)
    * [func forEachG(fn func(gp *g))](#forEachG)
    * [func forEachGRace(fn func(gp *g))](#forEachGRace)
    * [func forEachP(fn func(*p))](#forEachP)
    * [func forcegchelper()](#forcegchelper)
    * [func fpack32(sign, mant uint32, exp int, trunc uint32) uint32](#fpack32)
    * [func fpack64(sign, mant uint64, exp int, trunc uint64) uint64](#fpack64)
    * [func freeSomeWbufs(preemptible bool) bool](#freeSomeWbufs)
    * [func freeSpecial(s *special, p unsafe.Pointer, size uintptr)](#freeSpecial)
    * [func freeStackSpans()](#freeStackSpans)
    * [func freedefer(d *_defer)](#freedefer)
    * [func freedeferfn()](#freedeferfn)
    * [func freedeferpanic()](#freedeferpanic)
    * [func freemcache(c *mcache)](#freemcache)
    * [func freezetheworld()](#freezetheworld)
    * [func fsub64(f, g uint64) uint64](#fsub64)
    * [func fuint64to32(x uint64) float32](#fuint64to32)
    * [func fuint64to64(x uint64) float64](#fuint64to64)
    * [func full(c *hchan) bool](#full)
    * [func funcMaxSPDelta(f funcInfo) int32](#funcMaxSPDelta)
    * [func funcPC(f interface{}) uintptr](#funcPC)
    * [func funcdata(f funcInfo, i uint8) unsafe.Pointer](#funcdata)
    * [func funcfile(f funcInfo, fileno int32) string](#funcfile)
    * [func funcline(f funcInfo, targetpc uintptr) (file string, line int32)](#funcline)
    * [func funcline1(f funcInfo, targetpc uintptr, strict bool) (file string, line int32)](#funcline1)
    * [func funcname(f funcInfo) string](#funcname)
    * [func funcnameFromNameoff(f funcInfo, nameoff int32) string](#funcnameFromNameoff)
    * [func funcpkgpath(f funcInfo) string](#funcpkgpath)
    * [func funcspdelta(f funcInfo, targetpc uintptr, cache *pcvalueCache) int32](#funcspdelta)
    * [func funpack32(f uint32) (sign, mant uint32, exp int, inf, nan bool)](#funpack32)
    * [func funpack64(f uint64) (sign, mant uint64, exp int, inf, nan bool)](#funpack64)
    * [func gcAssistAlloc(gp *g)](#gcAssistAlloc)
    * [func gcAssistAlloc1(gp *g, scanWork int64)](#gcAssistAlloc1)
    * [func gcBgMarkPrepare()](#gcBgMarkPrepare)
    * [func gcBgMarkStartWorkers()](#gcBgMarkStartWorkers)
    * [func gcBgMarkWorker()](#gcBgMarkWorker)
    * [func gcDrain(gcw *gcWork, flags gcDrainFlags)](#gcDrain)
    * [func gcDrainN(gcw *gcWork, scanWork int64) int64](#gcDrainN)
    * [func gcDumpObject(label string, obj, off uintptr)](#gcDumpObject)
    * [func gcFlushBgCredit(scanWork int64)](#gcFlushBgCredit)
    * [func gcMark(startTime int64)](#gcMark)
    * [func gcMarkDone()](#gcMarkDone)
    * [func gcMarkRootCheck()](#gcMarkRootCheck)
    * [func gcMarkRootPrepare()](#gcMarkRootPrepare)
    * [func gcMarkTermination(nextTriggerRatio float64)](#gcMarkTermination)
    * [func gcMarkTinyAllocs()](#gcMarkTinyAllocs)
    * [func gcMarkWorkAvailable(p *p) bool](#gcMarkWorkAvailable)
    * [func gcPaceScavenger()](#gcPaceScavenger)
    * [func gcParkAssist() bool](#gcParkAssist)
    * [func gcResetMarkState()](#gcResetMarkState)
    * [func gcStart(trigger gcTrigger)](#gcStart)
    * [func gcSweep(mode gcMode)](#gcSweep)
    * [func gcTestIsReachable(ptrs ...unsafe.Pointer) (mask uint64)](#gcTestIsReachable)
    * [func gcTestMoveStackOnNextCall()](#gcTestMoveStackOnNextCall)
    * [func gcTestPointerClass(p unsafe.Pointer) string](#gcTestPointerClass)
    * [func gcWaitOnMark(n uint32)](#gcWaitOnMark)
    * [func gcWakeAllAssists()](#gcWakeAllAssists)
    * [func gcWriteBarrier()](#gcWriteBarrier)
    * [func gcWriteBarrierBP()](#gcWriteBarrierBP)
    * [func gcWriteBarrierBX()](#gcWriteBarrierBX)
    * [func gcWriteBarrierCX()](#gcWriteBarrierCX)
    * [func gcWriteBarrierDX()](#gcWriteBarrierDX)
    * [func gcWriteBarrierR8()](#gcWriteBarrierR8)
    * [func gcWriteBarrierR9()](#gcWriteBarrierR9)
    * [func gcWriteBarrierSI()](#gcWriteBarrierSI)
    * [func gcallers(gp *g, skip int, pcbuf []uintptr) int](#gcallers)
    * [func gcd(a, b uint32) uint32](#gcd)
    * [func gcenable()](#gcenable)
    * [func gcinit()](#gcinit)
    * [func gcmarknewobject(span *mspan, obj, size, scanSize uintptr)](#gcmarknewobject)
    * [func gcount() int32](#gcount)
    * [func gcstopm()](#gcstopm)
    * [func gentraceback(pc0, sp0, lr0 uintptr, gp *g, skip int, pcbuf *uintptr, max int, callback func(*stkframe, unsafe.Pointer) bool, v unsafe.Pointer, flags uint) int](#gentraceback)
    * [func getPageSize() uintptr](#getPageSize)
    * [func getRandomData(r []byte)](#getRandomData)
    * [func getargp() uintptr](#getargp)
    * [func getcallerpc() uintptr](#getcallerpc)
    * [func getcallersp() uintptr](#getcallersp)
    * [func getclosureptr() uintptr](#getclosureptr)
    * [func getgcmask(ep interface{}) (mask []byte)](#getgcmask)
    * [func getgcmaskcb(frame *stkframe, ctxt unsafe.Pointer) bool](#getgcmaskcb)
    * [func getm() uintptr](#getm)
    * [func getncpu() int32](#getncpu)
    * [func getsig(i uint32) uintptr](#getsig)
    * [func gfpurge(_p_ *p)](#gfpurge)
    * [func gfput(_p_ *p, gp *g)](#gfput)
    * [func globrunqput(gp *g)](#globrunqput)
    * [func globrunqputbatch(batch *gQueue, n int32)](#globrunqputbatch)
    * [func globrunqputhead(gp *g)](#globrunqputhead)
    * [func goPanicIndex(x int, y int)](#goPanicIndex)
    * [func goPanicIndexU(x uint, y int)](#goPanicIndexU)
    * [func goPanicSlice3Acap(x int, y int)](#goPanicSlice3Acap)
    * [func goPanicSlice3AcapU(x uint, y int)](#goPanicSlice3AcapU)
    * [func goPanicSlice3Alen(x int, y int)](#goPanicSlice3Alen)
    * [func goPanicSlice3AlenU(x uint, y int)](#goPanicSlice3AlenU)
    * [func goPanicSlice3B(x int, y int)](#goPanicSlice3B)
    * [func goPanicSlice3BU(x uint, y int)](#goPanicSlice3BU)
    * [func goPanicSlice3C(x int, y int)](#goPanicSlice3C)
    * [func goPanicSlice3CU(x uint, y int)](#goPanicSlice3CU)
    * [func goPanicSliceAcap(x int, y int)](#goPanicSliceAcap)
    * [func goPanicSliceAcapU(x uint, y int)](#goPanicSliceAcapU)
    * [func goPanicSliceAlen(x int, y int)](#goPanicSliceAlen)
    * [func goPanicSliceAlenU(x uint, y int)](#goPanicSliceAlenU)
    * [func goPanicSliceB(x int, y int)](#goPanicSliceB)
    * [func goPanicSliceBU(x uint, y int)](#goPanicSliceBU)
    * [func goPanicSliceConvert(x int, y int)](#goPanicSliceConvert)
    * [func goargs()](#goargs)
    * [func gobytes(p *byte, n int) (b []byte)](#gobytes)
    * [func goenvs()](#goenvs)
    * [func goenvs_unix()](#goenvs_unix)
    * [func goexit(neverCallThisFunction)](#goexit)
    * [func goexit0(gp *g)](#goexit0)
    * [func goexit1()](#goexit1)
    * [func gogetenv(key string) string](#gogetenv)
    * [func gogo(buf *gobuf)](#gogo)
    * [func gopanic(e interface{})](#gopanic)
    * [func gopark(unlockf func(*g, unsafe.Pointer) bool, lock unsafe.Pointer, reason waitReason, traceEv byte, traceskip int)](#gopark)
    * [func goparkunlock(lock *mutex, reason waitReason, traceEv byte, traceskip int)](#goparkunlock)
    * [func gopreempt_m(gp *g)](#gopreempt_m)
    * [func goready(gp *g, traceskip int)](#goready)
    * [func gorecover(argp uintptr) interface{}](#gorecover)
    * [func goroutineProfileWithLabels(p []StackRecord, labels []unsafe.Pointer) (n int, ok bool)](#goroutineProfileWithLabels)
    * [func goroutineReady(arg interface{}, seq uintptr)](#goroutineReady)
    * [func goroutineheader(gp *g)](#goroutineheader)
    * [func goschedImpl(gp *g)](#goschedImpl)
    * [func gosched_m(gp *g)](#gosched_m)
    * [func goschedguarded()](#goschedguarded)
    * [func goschedguarded_m(gp *g)](#goschedguarded_m)
    * [func gostartcall(buf *gobuf, fn, ctxt unsafe.Pointer)](#gostartcall)
    * [func gostartcallfn(gobuf *gobuf, fv *funcval)](#gostartcallfn)
    * [func gostring(p *byte) string](#gostring)
    * [func gostringn(p *byte, l int) string](#gostringn)
    * [func gostringnocopy(str *byte) string](#gostringnocopy)
    * [func gostringw(strw *uint16) string](#gostringw)
    * [func gotraceback() (level int32, all, crash bool)](#gotraceback)
    * [func goyield()](#goyield)
    * [func goyield_m(gp *g)](#goyield_m)
    * [func greyobject(obj, base, off uintptr, span *mspan, gcw *gcWork, objIndex uintptr)](#greyobject)
    * [func growWork(t *maptype, h *hmap, bucket uintptr)](#growWork)
    * [func growWork_fast32(t *maptype, h *hmap, bucket uintptr)](#growWork_fast32)
    * [func growWork_fast64(t *maptype, h *hmap, bucket uintptr)](#growWork_fast64)
    * [func growWork_faststr(t *maptype, h *hmap, bucket uintptr)](#growWork_faststr)
    * [func gwrite(b []byte)](#gwrite)
    * [func handoffp(_p_ *p)](#handoffp)
    * [func hasPrefix(s, prefix string) bool](#hasPrefix)
    * [func hashGrow(t *maptype, h *hmap)](#hashGrow)
    * [func heapBitsSetType(x, size, dataSize uintptr, typ *_type)](#heapBitsSetType)
    * [func heapBitsSetTypeGCProg(h heapBits, progSize, elemSize, dataSize, allocSize uintptr, prog *byte)](#heapBitsSetTypeGCProg)
    * [func heapRetained() uint64](#heapRetained)
    * [func hexdumpWords(p, end uintptr, mark func(uintptr) byte)](#hexdumpWords)
    * [func ifaceHash(i interface {...](#ifaceHash)
    * [func ifaceeq(tab *itab, x, y unsafe.Pointer) bool](#ifaceeq)
    * [func inHeapOrStack(b uintptr) bool](#inHeapOrStack)
    * [func inPersistentAlloc(p uintptr) bool](#inPersistentAlloc)
    * [func inRange(r0, r1, v0, v1 uintptr) bool](#inRange)
    * [func inVDSOPage(pc uintptr) bool](#inVDSOPage)
    * [func incidlelocked(v int32)](#incidlelocked)
    * [func inf2one(f float64) float64](#inf2one)
    * [func inheap(b uintptr) bool](#inheap)
    * [func init()](#init.cpuflags_amd64.go)
    * [func init()](#init.mgcpacer.go)
    * [func init()](#init.mgcstack.go)
    * [func init()](#init.mgcwork.go)
    * [func init()](#init.mstats.go)
    * [func init()](#init.panic.go)
    * [func init()](#init.preempt.go)
    * [func init()](#init.proc.go)
    * [func init()](#init.signal_unix.go)
    * [func init()](#init.stack.go)
    * [func initAlgAES()](#initAlgAES)
    * [func initMetrics()](#initMetrics)
    * [func initsig(preinit bool)](#initsig)
    * [func injectglist(glist *gList)](#injectglist)
    * [func int32Hash(i uint32, seed uintptr) uintptr](#int32Hash)
    * [func int64Hash(i uint64, seed uintptr) uintptr](#int64Hash)
    * [func interequal(p, q unsafe.Pointer) bool](#interequal)
    * [func interhash(p unsafe.Pointer, h uintptr) uintptr](#interhash)
    * [func internal_cpu_getsysctlbyname(name []byte) (int32, int32)](#internal_cpu_getsysctlbyname)
    * [func intstring(buf *[4]byte, v int64) (s string)](#intstring)
    * [func isAbortPC(pc uintptr) bool](#isAbortPC)
    * [func isAsyncSafePoint(gp *g, pc, sp, lr uintptr) (bool, uintptr)](#isAsyncSafePoint)
    * [func isDirectIface(t *_type) bool](#isDirectIface)
    * [func isEmpty(x uint8) bool](#isEmpty)
    * [func isExportedRuntime(name string) bool](#isExportedRuntime)
    * [func isFinite(f float64) bool](#isFinite)
    * [func isInf(f float64) bool](#isInf)
    * [func isNaN(f float64) (is bool)](#isNaN)
    * [func isPowerOfTwo(x uintptr) bool](#isPowerOfTwo)
    * [func isShrinkStackSafe(gp *g) bool](#isShrinkStackSafe)
    * [func isSweepDone() bool](#isSweepDone)
    * [func isSystemGoroutine(gp *g, fixed bool) bool](#isSystemGoroutine)
    * [func itabAdd(m *itab)](#itabAdd)
    * [func itabHashFunc(inter *interfacetype, typ *_type) uintptr](#itabHashFunc)
    * [func itab_callback(tab *itab)](#itab_callback)
    * [func itabsinit()](#itabsinit)
    * [func iterate_finq(callback func(*funcval, unsafe.Pointer, uintptr, *_type, *ptrtype))](#iterate_finq)
    * [func iterate_itabs(fn func(*itab))](#iterate_itabs)
    * [func iterate_memprof(fn func(*bucket, uintptr, *uintptr, uintptr, uintptr, uintptr))](#iterate_memprof)
    * [func itoa(buf []byte, val uint64) []byte](#itoa)
    * [func itoaDiv(buf []byte, val uint64, dec int) []byte](#itoaDiv)
    * [func jmpdefer(fv *funcval, argp uintptr)](#jmpdefer)
    * [func kevent(kq int32, ch *keventt, nch int32, ev *keventt, nev int32, ts *timespec) int32](#kevent)
    * [func kevent_trampoline()](#kevent_trampoline)
    * [func kqueue() int32](#kqueue)
    * [func kqueue_trampoline()](#kqueue_trampoline)
    * [func less(a, b uint32) bool](#less)
    * [func lfnodeValidate(node *lfnode)](#lfnodeValidate)
    * [func lfstackPack(node *lfnode, cnt uintptr) uint64](#lfstackPack)
    * [func libcCall(fn, arg unsafe.Pointer) int32](#libcCall)
    * [func libpreinit()](#libpreinit)
    * [func lock(l *mutex)](#lock)
    * [func lock2(l *mutex)](#lock2)
    * [func lockInit(l *mutex, rank lockRank)](#lockInit)
    * [func lockOSThread()](#lockOSThread)
    * [func lockWithRank(l *mutex, rank lockRank)](#lockWithRank)
    * [func lockWithRankMayAcquire(l *mutex, rank lockRank)](#lockWithRankMayAcquire)
    * [func lockedOSThread() bool](#lockedOSThread)
    * [func lowerASCII(c byte) byte](#lowerASCII)
    * [func mDoFixup() bool](#mDoFixup)
    * [func mDoFixupAndOSYield()](#mDoFixupAndOSYield)
    * [func mPark()](#mPark)
    * [func mProf_Flush()](#mProf_Flush)
    * [func mProf_FlushLocked()](#mProf_FlushLocked)
    * [func mProf_Free(b *bucket, size uintptr)](#mProf_Free)
    * [func mProf_Malloc(p unsafe.Pointer, size uintptr)](#mProf_Malloc)
    * [func mProf_NextCycle()](#mProf_NextCycle)
    * [func mProf_PostSweep()](#mProf_PostSweep)
    * [func mReserveID() int64](#mReserveID)
    * [func mStackIsSystemAllocated() bool](#mStackIsSystemAllocated)
    * [func madvise(addr unsafe.Pointer, n uintptr, flags int32)](#madvise)
    * [func madvise_trampoline()](#madvise_trampoline)
    * [func main()](#main)
    * [func main_main()](#main_main)
    * [func makeslice(et *_type, len, cap int) unsafe.Pointer](#makeslice)
    * [func makeslice64(et *_type, len64, cap64 int64) unsafe.Pointer](#makeslice64)
    * [func makeslicecopy(et *_type, tolen int, fromlen int, from unsafe.Pointer) unsafe.Pointer](#makeslicecopy)
    * [func mallocgc(size uintptr, typ *_type, needzero bool) unsafe.Pointer](#mallocgc)
    * [func mallocinit()](#mallocinit)
    * [func mapaccess1(t *maptype, h *hmap, key unsafe.Pointer) unsafe.Pointer](#mapaccess1)
    * [func mapaccess1_fast32(t *maptype, h *hmap, key uint32) unsafe.Pointer](#mapaccess1_fast32)
    * [func mapaccess1_fast64(t *maptype, h *hmap, key uint64) unsafe.Pointer](#mapaccess1_fast64)
    * [func mapaccess1_faststr(t *maptype, h *hmap, ky string) unsafe.Pointer](#mapaccess1_faststr)
    * [func mapaccess1_fat(t *maptype, h *hmap, key, zero unsafe.Pointer) unsafe.Pointer](#mapaccess1_fat)
    * [func mapaccess2(t *maptype, h *hmap, key unsafe.Pointer) (unsafe.Pointer, bool)](#mapaccess2)
    * [func mapaccess2_fast32(t *maptype, h *hmap, key uint32) (unsafe.Pointer, bool)](#mapaccess2_fast32)
    * [func mapaccess2_fast64(t *maptype, h *hmap, key uint64) (unsafe.Pointer, bool)](#mapaccess2_fast64)
    * [func mapaccess2_faststr(t *maptype, h *hmap, ky string) (unsafe.Pointer, bool)](#mapaccess2_faststr)
    * [func mapaccess2_fat(t *maptype, h *hmap, key, zero unsafe.Pointer) (unsafe.Pointer, bool)](#mapaccess2_fat)
    * [func mapaccessK(t *maptype, h *hmap, key unsafe.Pointer) (unsafe.Pointer, unsafe.Pointer)](#mapaccessK)
    * [func mapassign(t *maptype, h *hmap, key unsafe.Pointer) unsafe.Pointer](#mapassign)
    * [func mapassign_fast32(t *maptype, h *hmap, key uint32) unsafe.Pointer](#mapassign_fast32)
    * [func mapassign_fast32ptr(t *maptype, h *hmap, key unsafe.Pointer) unsafe.Pointer](#mapassign_fast32ptr)
    * [func mapassign_fast64(t *maptype, h *hmap, key uint64) unsafe.Pointer](#mapassign_fast64)
    * [func mapassign_fast64ptr(t *maptype, h *hmap, key unsafe.Pointer) unsafe.Pointer](#mapassign_fast64ptr)
    * [func mapassign_faststr(t *maptype, h *hmap, s string) unsafe.Pointer](#mapassign_faststr)
    * [func mapclear(t *maptype, h *hmap)](#mapclear)
    * [func mapdelete(t *maptype, h *hmap, key unsafe.Pointer)](#mapdelete)
    * [func mapdelete_fast32(t *maptype, h *hmap, key uint32)](#mapdelete_fast32)
    * [func mapdelete_fast64(t *maptype, h *hmap, key uint64)](#mapdelete_fast64)
    * [func mapdelete_faststr(t *maptype, h *hmap, ky string)](#mapdelete_faststr)
    * [func mapiterinit(t *maptype, h *hmap, it *hiter)](#mapiterinit)
    * [func mapiternext(it *hiter)](#mapiternext)
    * [func markroot(gcw *gcWork, i uint32)](#markroot)
    * [func markrootBlock(b0, n0 uintptr, ptrmask0 *uint8, gcw *gcWork, shard int)](#markrootBlock)
    * [func markrootFreeGStacks()](#markrootFreeGStacks)
    * [func markrootSpans(gcw *gcWork, shard int)](#markrootSpans)
    * [func mcall(fn func(*g))](#mcall)
    * [func mcommoninit(mp *m, id int64)](#mcommoninit)
    * [func mcount() int32](#mcount)
    * [func mdestroy(mp *m)](#mdestroy)
    * [func mdump(m *MemStats)](#mdump)
    * [func memclrHasPointers(ptr unsafe.Pointer, n uintptr)](#memclrHasPointers)
    * [func memclrNoHeapPointers(ptr unsafe.Pointer, n uintptr)](#memclrNoHeapPointers)
    * [func memclrNoHeapPointersChunked(size uintptr, x unsafe.Pointer)](#memclrNoHeapPointersChunked)
    * [func memequal(a, b unsafe.Pointer, size uintptr) bool](#memequal)
    * [func memequal0(p, q unsafe.Pointer) bool](#memequal0)
    * [func memequal128(p, q unsafe.Pointer) bool](#memequal128)
    * [func memequal16(p, q unsafe.Pointer) bool](#memequal16)
    * [func memequal32(p, q unsafe.Pointer) bool](#memequal32)
    * [func memequal64(p, q unsafe.Pointer) bool](#memequal64)
    * [func memequal8(p, q unsafe.Pointer) bool](#memequal8)
    * [func memequal_varlen(a, b unsafe.Pointer) bool](#memequal_varlen)
    * [func memhash(p unsafe.Pointer, h, s uintptr) uintptr](#memhash)
    * [func memhash0(p unsafe.Pointer, h uintptr) uintptr](#memhash0)
    * [func memhash128(p unsafe.Pointer, h uintptr) uintptr](#memhash128)
    * [func memhash16(p unsafe.Pointer, h uintptr) uintptr](#memhash16)
    * [func memhash32(p unsafe.Pointer, h uintptr) uintptr](#memhash32)
    * [func memhash32Fallback(p unsafe.Pointer, seed uintptr) uintptr](#memhash32Fallback)
    * [func memhash64(p unsafe.Pointer, h uintptr) uintptr](#memhash64)
    * [func memhash64Fallback(p unsafe.Pointer, seed uintptr) uintptr](#memhash64Fallback)
    * [func memhash8(p unsafe.Pointer, h uintptr) uintptr](#memhash8)
    * [func memhashFallback(p unsafe.Pointer, seed, s uintptr) uintptr](#memhashFallback)
    * [func memhash_varlen(p unsafe.Pointer, h uintptr) uintptr](#memhash_varlen)
    * [func memmove(to, from unsafe.Pointer, n uintptr)](#memmove)
    * [func mexit(osStack bool)](#mexit)
    * [func minit()](#minit)
    * [func minitSignalMask()](#minitSignalMask)
    * [func minitSignalStack()](#minitSignalStack)
    * [func minitSignals()](#minitSignals)
    * [func mix(a, b uintptr) uintptr](#mix)
    * [func mlock(addr unsafe.Pointer, n uintptr)](#mlock)
    * [func mlock_trampoline()](#mlock_trampoline)
    * [func mmap(addr unsafe.Pointer, n uintptr, prot, flags, fd int32, off uint32) (unsafe.Pointer, int)](#mmap)
    * [func mmap_trampoline()](#mmap_trampoline)
    * [func modTimer(t *timer, when, period int64, f func(interface{}, uintptr), arg interface{}, seq uintptr)](#modTimer)
    * [func modtimer(t *timer, when, period int64, f func(interface{}, uintptr), arg interface{}, seq uintptr) bool](#modtimer)
    * [func moduledataverify()](#moduledataverify)
    * [func moduledataverify1(datap *moduledata)](#moduledataverify1)
    * [func modulesinit()](#modulesinit)
    * [func morestack()](#morestack)
    * [func morestack_noctxt()](#morestack_noctxt)
    * [func morestackc()](#morestackc)
    * [func moveTimers(pp *p, timers []*timer)](#moveTimers)
    * [func mpreinit(mp *m)](#mpreinit)
    * [func mput(mp *m)](#mput)
    * [func msanfree(addr unsafe.Pointer, sz uintptr)](#msanfree)
    * [func msanmalloc(addr unsafe.Pointer, sz uintptr)](#msanmalloc)
    * [func msanmove(dst, src unsafe.Pointer, sz uintptr)](#msanmove)
    * [func msanread(addr unsafe.Pointer, sz uintptr)](#msanread)
    * [func msanwrite(addr unsafe.Pointer, sz uintptr)](#msanwrite)
    * [func msigrestore(sigmask sigset)](#msigrestore)
    * [func mspinning()](#mspinning)
    * [func mstart()](#mstart)
    * [func mstart0()](#mstart0)
    * [func mstart1()](#mstart1)
    * [func mstart_stub()](#mstart_stub)
    * [func mstartm0()](#mstartm0)
    * [func mullu(u, v uint64) (lo, hi uint64)](#mullu)
    * [func munmap(addr unsafe.Pointer, n uintptr)](#munmap)
    * [func munmap_trampoline()](#munmap_trampoline)
    * [func mutexevent(cycles int64, skip int)](#mutexevent)
    * [func nanotime() int64](#nanotime)
    * [func nanotime1() int64](#nanotime1)
    * [func nanotime_trampoline()](#nanotime_trampoline)
    * [func needm()](#needm)
    * [func net_fastrand() uint32](#net_fastrand)
    * [func netpollBreak()](#netpollBreak)
    * [func netpollDeadline(arg interface{}, seq uintptr)](#netpollDeadline)
    * [func netpollGenericInit()](#netpollGenericInit)
    * [func netpollIsPollDescriptor(fd uintptr) bool](#netpollIsPollDescriptor)
    * [func netpollReadDeadline(arg interface{}, seq uintptr)](#netpollReadDeadline)
    * [func netpollWriteDeadline(arg interface{}, seq uintptr)](#netpollWriteDeadline)
    * [func netpollarm(pd *pollDesc, mode int)](#netpollarm)
    * [func netpollblock(pd *pollDesc, mode int32, waitio bool) bool](#netpollblock)
    * [func netpollblockcommit(gp *g, gpp unsafe.Pointer) bool](#netpollblockcommit)
    * [func netpollcheckerr(pd *pollDesc, mode int32) int](#netpollcheckerr)
    * [func netpollclose(fd uintptr) int32](#netpollclose)
    * [func netpolldeadlineimpl(pd *pollDesc, seq uintptr, read, write bool)](#netpolldeadlineimpl)
    * [func netpollgoready(gp *g, traceskip int)](#netpollgoready)
    * [func netpollinit()](#netpollinit)
    * [func netpollinited() bool](#netpollinited)
    * [func netpollopen(fd uintptr, pd *pollDesc) int32](#netpollopen)
    * [func netpollready(toRun *gList, pd *pollDesc, mode int32)](#netpollready)
    * [func newarray(typ *_type, n int) unsafe.Pointer](#newarray)
    * [func newextram()](#newextram)
    * [func newm(fn func(), _p_ *p, id int64)](#newm)
    * [func newm1(mp *m)](#newm1)
    * [func newobject(typ *_type) unsafe.Pointer](#newobject)
    * [func newosproc(mp *m)](#newosproc)
    * [func newosproc0(stacksize uintptr, fn uintptr)](#newosproc0)
    * [func newproc(siz int32, fn *funcval)](#newproc)
    * [func newstack()](#newstack)
    * [func nextMarkBitArenaEpoch()](#nextMarkBitArenaEpoch)
    * [func nextSample() uintptr](#nextSample)
    * [func nextSampleNoFP() uintptr](#nextSampleNoFP)
    * [func nilfunc()](#nilfunc)
    * [func nilinterequal(p, q unsafe.Pointer) bool](#nilinterequal)
    * [func nilinterhash(p unsafe.Pointer, h uintptr) uintptr](#nilinterhash)
    * [func noSignalStack(sig uint32)](#noSignalStack)
    * [func nobarrierWakeTime(pp *p) int64](#nobarrierWakeTime)
    * [func noescape(p unsafe.Pointer) unsafe.Pointer](#noescape)
    * [func nonblockingPipe() (r, w int32, errno int32)](#nonblockingPipe)
    * [func noteclear(n *note)](#noteclear)
    * [func notesleep(n *note)](#notesleep)
    * [func notetsleep(n *note, ns int64) bool](#notetsleep)
    * [func notetsleep_internal(n *note, ns int64, gp *g, deadline int64) bool](#notetsleep_internal)
    * [func notetsleepg(n *note, ns int64) bool](#notetsleepg)
    * [func notewakeup(n *note)](#notewakeup)
    * [func notifyListAdd(l *notifyList) uint32](#notifyListAdd)
    * [func notifyListCheck(sz uintptr)](#notifyListCheck)
    * [func notifyListNotifyAll(l *notifyList)](#notifyListNotifyAll)
    * [func notifyListNotifyOne(l *notifyList)](#notifyListNotifyOne)
    * [func notifyListWait(l *notifyList, t uint32)](#notifyListWait)
    * [func offAddrToLevelIndex(level int, addr offAddr) int](#offAddrToLevelIndex)
    * [func oneNewExtraM()](#oneNewExtraM)
    * [func open(name *byte, mode, perm int32) (ret int32)](#open)
    * [func open_trampoline()](#open_trampoline)
    * [func osPreemptExtEnter(mp *m)](#osPreemptExtEnter)
    * [func osPreemptExtExit(mp *m)](#osPreemptExtExit)
    * [func osRelax(relax bool)](#osRelax)
    * [func osSetupTLS(mp *m)](#osSetupTLS)
    * [func osStackAlloc(s *mspan)](#osStackAlloc)
    * [func osStackFree(s *mspan)](#osStackFree)
    * [func os_beforeExit()](#os_beforeExit)
    * [func os_fastrand() uint32](#os_fastrand)
    * [func os_runtime_args() []string](#os_runtime_args)
    * [func os_sigpipe()](#os_sigpipe)
    * [func osinit()](#osinit)
    * [func osyield()](#osyield)
    * [func osyield_no_g()](#osyield_no_g)
    * [func overLoadFactor(count int, B uint8) bool](#overLoadFactor)
    * [func panicCheck1(pc uintptr, msg string)](#panicCheck1)
    * [func panicCheck2(err string)](#panicCheck2)
    * [func panicIndex(x int, y int)](#panicIndex)
    * [func panicIndexU(x uint, y int)](#panicIndexU)
    * [func panicSlice3Acap(x int, y int)](#panicSlice3Acap)
    * [func panicSlice3AcapU(x uint, y int)](#panicSlice3AcapU)
    * [func panicSlice3Alen(x int, y int)](#panicSlice3Alen)
    * [func panicSlice3AlenU(x uint, y int)](#panicSlice3AlenU)
    * [func panicSlice3B(x int, y int)](#panicSlice3B)
    * [func panicSlice3BU(x uint, y int)](#panicSlice3BU)
    * [func panicSlice3C(x int, y int)](#panicSlice3C)
    * [func panicSlice3CU(x uint, y int)](#panicSlice3CU)
    * [func panicSliceAcap(x int, y int)](#panicSliceAcap)
    * [func panicSliceAcapU(x uint, y int)](#panicSliceAcapU)
    * [func panicSliceAlen(x int, y int)](#panicSliceAlen)
    * [func panicSliceAlenU(x uint, y int)](#panicSliceAlenU)
    * [func panicSliceB(x int, y int)](#panicSliceB)
    * [func panicSliceBU(x uint, y int)](#panicSliceBU)
    * [func panicSliceConvert(x int, y int)](#panicSliceConvert)
    * [func panicdivide()](#panicdivide)
    * [func panicdottypeE(have, want, iface *_type)](#panicdottypeE)
    * [func panicdottypeI(have *itab, want, iface *_type)](#panicdottypeI)
    * [func panicfloat()](#panicfloat)
    * [func panicmakeslicecap()](#panicmakeslicecap)
    * [func panicmakeslicelen()](#panicmakeslicelen)
    * [func panicmem()](#panicmem)
    * [func panicmemAddr(addr uintptr)](#panicmemAddr)
    * [func panicnildottype(want *_type)](#panicnildottype)
    * [func panicoverflow()](#panicoverflow)
    * [func panicshift()](#panicshift)
    * [func panicunsafeslicelen()](#panicunsafeslicelen)
    * [func panicwrap()](#panicwrap)
    * [func park_m(gp *g)](#park_m)
    * [func parkunlock_c(gp *g, lock unsafe.Pointer) bool](#parkunlock_c)
    * [func parsedebugvars()](#parsedebugvars)
    * [func pcdatastart(f funcInfo, table uint32) uint32](#pcdatastart)
    * [func pcdatavalue(f funcInfo, table uint32, targetpc uintptr, cache *pcvalueCache) int32](#pcdatavalue)
    * [func pcdatavalue1(f funcInfo, table uint32, targetpc uintptr, cache *pcvalueCache, strict bool) int32](#pcdatavalue1)
    * [func pcdatavalue2(f funcInfo, table uint32, targetpc uintptr) (int32, uintptr)](#pcdatavalue2)
    * [func pcvalue(f funcInfo, off uint32, targetpc uintptr, cache *pcvalueCache, strict bool) (int32, uintptr)](#pcvalue)
    * [func pcvalueCacheKey(targetpc uintptr) uintptr](#pcvalueCacheKey)
    * [func persistentalloc(size, align uintptr, sysStat *sysMemStat) unsafe.Pointer](#persistentalloc)
    * [func pidleput(_p_ *p)](#pidleput)
    * [func pipe() (r, w int32, errno int32)](#pipe)
    * [func pipe_trampoline()](#pipe_trampoline)
    * [func plugin_lastmoduleinit() (path string, syms map[string]interface{}, errstr string)](#plugin_lastmoduleinit)
    * [func pluginftabverify(md *moduledata)](#pluginftabverify)
    * [func pollFractionalWorkerExit() bool](#pollFractionalWorkerExit)
    * [func pollWork() bool](#pollWork)
    * [func poll_runtime_Semacquire(addr *uint32)](#poll_runtime_Semacquire)
    * [func poll_runtime_Semrelease(addr *uint32)](#poll_runtime_Semrelease)
    * [func poll_runtime_isPollServerDescriptor(fd uintptr) bool](#poll_runtime_isPollServerDescriptor)
    * [func poll_runtime_pollClose(pd *pollDesc)](#poll_runtime_pollClose)
    * [func poll_runtime_pollReset(pd *pollDesc, mode int) int](#poll_runtime_pollReset)
    * [func poll_runtime_pollServerInit()](#poll_runtime_pollServerInit)
    * [func poll_runtime_pollSetDeadline(pd *pollDesc, d int64, mode int)](#poll_runtime_pollSetDeadline)
    * [func poll_runtime_pollUnblock(pd *pollDesc)](#poll_runtime_pollUnblock)
    * [func poll_runtime_pollWait(pd *pollDesc, mode int) int](#poll_runtime_pollWait)
    * [func poll_runtime_pollWaitCanceled(pd *pollDesc, mode int)](#poll_runtime_pollWaitCanceled)
    * [func preemptM(mp *m)](#preemptM)
    * [func preemptPark(gp *g)](#preemptPark)
    * [func preemptall() bool](#preemptall)
    * [func preemptone(_p_ *p) bool](#preemptone)
    * [func prepGoExitFrame(sp uintptr)](#prepGoExitFrame)
    * [func prepareFreeWorkbufs()](#prepareFreeWorkbufs)
    * [func preprintpanics(p *_panic)](#preprintpanics)
    * [func printAncestorTraceback(ancestor ancestorInfo)](#printAncestorTraceback)
    * [func printAncestorTracebackFuncInfo(f funcInfo, pc uintptr)](#printAncestorTracebackFuncInfo)
    * [func printArgs(f funcInfo, argp unsafe.Pointer)](#printArgs)
    * [func printCgoTraceback(callers *cgoCallers)](#printCgoTraceback)
    * [func printDebugLog()](#printDebugLog)
    * [func printDebugLogPC(pc uintptr, returnPC bool)](#printDebugLogPC)
    * [func printOneCgoTraceback(pc uintptr, max int, arg *cgoSymbolizerArg) int](#printOneCgoTraceback)
    * [func printScavTrace(gen uint32, released uintptr, forced bool)](#printScavTrace)
    * [func printany(i interface{})](#printany)
    * [func printanycustomtype(i interface{})](#printanycustomtype)
    * [func printbool(v bool)](#printbool)
    * [func printcomplex(c complex128)](#printcomplex)
    * [func printcreatedby(gp *g)](#printcreatedby)
    * [func printcreatedby1(f funcInfo, pc uintptr)](#printcreatedby1)
    * [func printeface(e eface)](#printeface)
    * [func printfloat(v float64)](#printfloat)
    * [func printhex(v uint64)](#printhex)
    * [func printiface(i iface)](#printiface)
    * [func printint(v int64)](#printint)
    * [func printlock()](#printlock)
    * [func printnl()](#printnl)
    * [func printpanics(p *_panic)](#printpanics)
    * [func printpointer(p unsafe.Pointer)](#printpointer)
    * [func printslice(s []byte)](#printslice)
    * [func printsp()](#printsp)
    * [func printstring(s string)](#printstring)
    * [func printuint(v uint64)](#printuint)
    * [func printuintptr(p uintptr)](#printuintptr)
    * [func printunlock()](#printunlock)
    * [func procPin() int](#procPin)
    * [func procUnpin()](#procUnpin)
    * [func procyield(cycles uint32)](#procyield)
    * [func profilealloc(mp *m, x unsafe.Pointer, size uintptr)](#profilealloc)
    * [func pthread_attr_getstacksize(attr *pthreadattr, size *uintptr) int32](#pthread_attr_getstacksize)
    * [func pthread_attr_getstacksize_trampoline()](#pthread_attr_getstacksize_trampoline)
    * [func pthread_attr_init(attr *pthreadattr) int32](#pthread_attr_init)
    * [func pthread_attr_init_trampoline()](#pthread_attr_init_trampoline)
    * [func pthread_attr_setdetachstate(attr *pthreadattr, state int) int32](#pthread_attr_setdetachstate)
    * [func pthread_attr_setdetachstate_trampoline()](#pthread_attr_setdetachstate_trampoline)
    * [func pthread_cond_init(c *pthreadcond, attr *pthreadcondattr) int32](#pthread_cond_init)
    * [func pthread_cond_init_trampoline()](#pthread_cond_init_trampoline)
    * [func pthread_cond_signal(c *pthreadcond) int32](#pthread_cond_signal)
    * [func pthread_cond_signal_trampoline()](#pthread_cond_signal_trampoline)
    * [func pthread_cond_timedwait_relative_np(c *pthreadcond, m *pthreadmutex, t *timespec) int32](#pthread_cond_timedwait_relative_np)
    * [func pthread_cond_timedwait_relative_np_trampoline()](#pthread_cond_timedwait_relative_np_trampoline)
    * [func pthread_cond_wait(c *pthreadcond, m *pthreadmutex) int32](#pthread_cond_wait)
    * [func pthread_cond_wait_trampoline()](#pthread_cond_wait_trampoline)
    * [func pthread_create(attr *pthreadattr, start uintptr, arg unsafe.Pointer) int32](#pthread_create)
    * [func pthread_create_trampoline()](#pthread_create_trampoline)
    * [func pthread_kill(t pthread, sig uint32)](#pthread_kill)
    * [func pthread_kill_trampoline()](#pthread_kill_trampoline)
    * [func pthread_mutex_init(m *pthreadmutex, attr *pthreadmutexattr) int32](#pthread_mutex_init)
    * [func pthread_mutex_init_trampoline()](#pthread_mutex_init_trampoline)
    * [func pthread_mutex_lock(m *pthreadmutex) int32](#pthread_mutex_lock)
    * [func pthread_mutex_lock_trampoline()](#pthread_mutex_lock_trampoline)
    * [func pthread_mutex_unlock(m *pthreadmutex) int32](#pthread_mutex_unlock)
    * [func pthread_mutex_unlock_trampoline()](#pthread_mutex_unlock_trampoline)
    * [func pthread_self_trampoline()](#pthread_self_trampoline)
    * [func publicationBarrier()](#publicationBarrier)
    * [func putCachedDlogger(l *dlogger) bool](#putCachedDlogger)
    * [func putempty(b *workbuf)](#putempty)
    * [func putfull(b *workbuf)](#putfull)
    * [func queuefinalizer(p unsafe.Pointer, fn *funcval, nret uintptr, fint *_type, ot *ptrtype)](#queuefinalizer)
    * [func r4(p unsafe.Pointer) uintptr](#r4)
    * [func r8(p unsafe.Pointer) uintptr](#r8)
    * [func raceReadObjectPC(t *_type, addr unsafe.Pointer, callerpc, pc uintptr)](#raceReadObjectPC)
    * [func raceWriteObjectPC(t *_type, addr unsafe.Pointer, callerpc, pc uintptr)](#raceWriteObjectPC)
    * [func raceacquire(addr unsafe.Pointer)](#raceacquire)
    * [func raceacquirectx(racectx uintptr, addr unsafe.Pointer)](#raceacquirectx)
    * [func raceacquireg(gp *g, addr unsafe.Pointer)](#raceacquireg)
    * [func racectxend(racectx uintptr)](#racectxend)
    * [func racefingo()](#racefingo)
    * [func racefini()](#racefini)
    * [func racefree(p unsafe.Pointer, sz uintptr)](#racefree)
    * [func racegoend()](#racegoend)
    * [func racegostart(pc uintptr) uintptr](#racegostart)
    * [func raceinit() (uintptr, uintptr)](#raceinit)
    * [func racemalloc(p unsafe.Pointer, sz uintptr)](#racemalloc)
    * [func racemapshadow(addr unsafe.Pointer, size uintptr)](#racemapshadow)
    * [func racenotify(c *hchan, idx uint, sg *sudog)](#racenotify)
    * [func raceproccreate() uintptr](#raceproccreate)
    * [func raceprocdestroy(ctx uintptr)](#raceprocdestroy)
    * [func racereadpc(addr unsafe.Pointer, callerpc, pc uintptr)](#racereadpc)
    * [func racereadrangepc(addr unsafe.Pointer, sz, callerpc, pc uintptr)](#racereadrangepc)
    * [func racerelease(addr unsafe.Pointer)](#racerelease)
    * [func racereleaseacquire(addr unsafe.Pointer)](#racereleaseacquire)
    * [func racereleaseacquireg(gp *g, addr unsafe.Pointer)](#racereleaseacquireg)
    * [func racereleaseg(gp *g, addr unsafe.Pointer)](#racereleaseg)
    * [func racereleasemerge(addr unsafe.Pointer)](#racereleasemerge)
    * [func racereleasemergeg(gp *g, addr unsafe.Pointer)](#racereleasemergeg)
    * [func racesync(c *hchan, sg *sudog)](#racesync)
    * [func racewritepc(addr unsafe.Pointer, callerpc, pc uintptr)](#racewritepc)
    * [func racewriterangepc(addr unsafe.Pointer, sz, callerpc, pc uintptr)](#racewriterangepc)
    * [func raise(sig uint32)](#raise)
    * [func raise_trampoline()](#raise_trampoline)
    * [func raisebadsignal(sig uint32, c *sigctxt)](#raisebadsignal)
    * [func raiseproc(sig uint32)](#raiseproc)
    * [func raiseproc_trampoline()](#raiseproc_trampoline)
    * [func rawbyteslice(size int) (b []byte)](#rawbyteslice)
    * [func rawruneslice(size int) (b []rune)](#rawruneslice)
    * [func rawstring(size int) (s string, b []byte)](#rawstring)
    * [func rawstringtmp(buf *tmpBuf, l int) (s string, b []byte)](#rawstringtmp)
    * [func read(fd int32, p unsafe.Pointer, n int32) int32](#read)
    * [func readGCStats(pauses *[]uint64)](#readGCStats)
    * [func readGCStats_m(pauses *[]uint64)](#readGCStats_m)
    * [func readGOGC() int32](#readGOGC)
    * [func readMetrics(samplesp unsafe.Pointer, len int, cap int)](#readMetrics)
    * [func readUnaligned32(p unsafe.Pointer) uint32](#readUnaligned32)
    * [func readUnaligned64(p unsafe.Pointer) uint64](#readUnaligned64)
    * [func read_trampoline()](#read_trampoline)
    * [func readgstatus(gp *g) uint32](#readgstatus)
    * [func readmemstats_m(stats *MemStats)](#readmemstats_m)
    * [func readvarint(p []byte) (read uint32, val uint32)](#readvarint)
    * [func readvarintUnsafe(fd unsafe.Pointer) (uint32, unsafe.Pointer)](#readvarintUnsafe)
    * [func ready(gp *g, traceskip int, next bool)](#ready)
    * [func readyForScavenger()](#readyForScavenger)
    * [func readyWithTime(s *sudog, traceskip int)](#readyWithTime)
    * [func record(r *MemProfileRecord, b *bucket)](#record)
    * [func recordForPanic(b []byte)](#recordForPanic)
    * [func recordspan(vh unsafe.Pointer, p unsafe.Pointer)](#recordspan)
    * [func recovery(gp *g)](#recovery)
    * [func recv(c *hchan, sg *sudog, ep unsafe.Pointer, unlockf func(), skip int)](#recv)
    * [func recvDirect(t *_type, sg *sudog, dst unsafe.Pointer)](#recvDirect)
    * [func reentersyscall(pc, sp uintptr)](#reentersyscall)
    * [func reflectOffsLock()](#reflectOffsLock)
    * [func reflectOffsUnlock()](#reflectOffsUnlock)
    * [func reflect_addReflectOff(ptr unsafe.Pointer) int32](#reflect_addReflectOff)
    * [func reflect_chancap(c *hchan) int](#reflect_chancap)
    * [func reflect_chanclose(c *hchan)](#reflect_chanclose)
    * [func reflect_chanlen(c *hchan) int](#reflect_chanlen)
    * [func reflect_chanrecv(c *hchan, nb bool, elem unsafe.Pointer) (selected bool, received bool)](#reflect_chanrecv)
    * [func reflect_chansend(c *hchan, elem unsafe.Pointer, nb bool) (selected bool)](#reflect_chansend)
    * [func reflect_gcbits(x interface{}) []byte](#reflect_gcbits)
    * [func reflect_ifaceE2I(inter *interfacetype, e eface, dst *iface)](#reflect_ifaceE2I)
    * [func reflect_mapaccess(t *maptype, h *hmap, key unsafe.Pointer) unsafe.Pointer](#reflect_mapaccess)
    * [func reflect_mapassign(t *maptype, h *hmap, key unsafe.Pointer, elem unsafe.Pointer)](#reflect_mapassign)
    * [func reflect_mapdelete(t *maptype, h *hmap, key unsafe.Pointer)](#reflect_mapdelete)
    * [func reflect_mapiterelem(it *hiter) unsafe.Pointer](#reflect_mapiterelem)
    * [func reflect_mapiterkey(it *hiter) unsafe.Pointer](#reflect_mapiterkey)
    * [func reflect_mapiternext(it *hiter)](#reflect_mapiternext)
    * [func reflect_maplen(h *hmap) int](#reflect_maplen)
    * [func reflect_memclrNoHeapPointers(ptr unsafe.Pointer, n uintptr)](#reflect_memclrNoHeapPointers)
    * [func reflect_memmove(to, from unsafe.Pointer, n uintptr)](#reflect_memmove)
    * [func reflect_resolveNameOff(ptrInModule unsafe.Pointer, off int32) unsafe.Pointer](#reflect_resolveNameOff)
    * [func reflect_resolveTextOff(rtype unsafe.Pointer, off int32) unsafe.Pointer](#reflect_resolveTextOff)
    * [func reflect_resolveTypeOff(rtype unsafe.Pointer, off int32) unsafe.Pointer](#reflect_resolveTypeOff)
    * [func reflect_rselect(cases []runtimeSelect) (int, bool)](#reflect_rselect)
    * [func reflect_typedmemclr(typ *_type, ptr unsafe.Pointer)](#reflect_typedmemclr)
    * [func reflect_typedmemclrpartial(typ *_type, ptr unsafe.Pointer, off, size uintptr)](#reflect_typedmemclrpartial)
    * [func reflect_typedmemmove(typ *_type, dst, src unsafe.Pointer)](#reflect_typedmemmove)
    * [func reflect_typedmemmovepartial(typ *_type, dst, src unsafe.Pointer, off, size uintptr)](#reflect_typedmemmovepartial)
    * [func reflect_typedslicecopy(elemType *_type, dst, src slice) int](#reflect_typedslicecopy)
    * [func reflect_typehash(t *_type, p unsafe.Pointer, h uintptr) uintptr](#reflect_typehash)
    * [func reflect_typelinks() ([]unsafe.Pointer, [][]int32)](#reflect_typelinks)
    * [func reflect_unsafe_New(typ *_type) unsafe.Pointer](#reflect_unsafe_New)
    * [func reflect_unsafe_NewArray(typ *_type, n int) unsafe.Pointer](#reflect_unsafe_NewArray)
    * [func reflectcall(stackArgsType *_type, fn, stackArgs unsafe.Pointer, stackArgsSize, stackRetOffset, frameSize uint32, regArgs *abi.RegArgs)](#reflectcall)
    * [func reflectcallSave(p *_panic, fn, arg unsafe.Pointer, argsize uint32)](#reflectcallSave)
    * [func reflectcallmove(typ *_type, dst, src unsafe.Pointer, size uintptr, regs *abi.RegArgs)](#reflectcallmove)
    * [func reflectlite_chanlen(c *hchan) int](#reflectlite_chanlen)
    * [func reflectlite_ifaceE2I(inter *interfacetype, e eface, dst *iface)](#reflectlite_ifaceE2I)
    * [func reflectlite_maplen(h *hmap) int](#reflectlite_maplen)
    * [func reflectlite_resolveNameOff(ptrInModule unsafe.Pointer, off int32) unsafe.Pointer](#reflectlite_resolveNameOff)
    * [func reflectlite_resolveTypeOff(rtype unsafe.Pointer, off int32) unsafe.Pointer](#reflectlite_resolveTypeOff)
    * [func reflectlite_typedmemmove(typ *_type, dst, src unsafe.Pointer)](#reflectlite_typedmemmove)
    * [func reflectlite_unsafe_New(typ *_type) unsafe.Pointer](#reflectlite_unsafe_New)
    * [func releaseLockRank(rank lockRank)](#releaseLockRank)
    * [func releaseSudog(s *sudog)](#releaseSudog)
    * [func releasem(mp *m)](#releasem)
    * [func removefinalizer(p unsafe.Pointer)](#removefinalizer)
    * [func resetForSleep(gp *g, ut unsafe.Pointer) bool](#resetForSleep)
    * [func resetTimer(t *timer, when int64) bool](#resetTimer)
    * [func resetspinning()](#resetspinning)
    * [func resettimer(t *timer, when int64) bool](#resettimer)
    * [func restoreGsignalStack(st *gsignalStack)](#restoreGsignalStack)
    * [func resumeG(state suspendGState)](#resumeG)
    * [func retake(now int64) uint32](#retake)
    * [func retpolineAX()](#retpolineAX)
    * [func retpolineBP()](#retpolineBP)
    * [func retpolineBX()](#retpolineBX)
    * [func retpolineCX()](#retpolineCX)
    * [func retpolineDI()](#retpolineDI)
    * [func retpolineDX()](#retpolineDX)
    * [func retpolineR10()](#retpolineR10)
    * [func retpolineR11()](#retpolineR11)
    * [func retpolineR12()](#retpolineR12)
    * [func retpolineR13()](#retpolineR13)
    * [func retpolineR14()](#retpolineR14)
    * [func retpolineR15()](#retpolineR15)
    * [func retpolineR8()](#retpolineR8)
    * [func retpolineR9()](#retpolineR9)
    * [func retpolineSI()](#retpolineSI)
    * [func return0()](#return0)
    * [func round2(x int32) int32](#round2)
    * [func roundupsize(size uintptr) uintptr](#roundupsize)
    * [func rt0_go()](#rt0_go)
    * [func runGCProg(prog, trailer, dst *byte, size int) uintptr](#runGCProg)
    * [func runOneTimer(pp *p, t *timer, now int64)](#runOneTimer)
    * [func runOpenDeferFrame(gp *g, d *_defer) bool](#runOpenDeferFrame)
    * [func runSafePointFn()](#runSafePointFn)
    * [func runfinq()](#runfinq)
    * [func runqempty(_p_ *p) bool](#runqempty)
    * [func runqgrab(_p_ *p, batch *[256]guintptr, batchHead uint32, stealRunNextG bool) uint32](#runqgrab)
    * [func runqput(_p_ *p, gp *g, next bool)](#runqput)
    * [func runqputbatch(pp *p, q *gQueue, qsize int)](#runqputbatch)
    * [func runqputslow(_p_ *p, gp *g, h, t uint32) bool](#runqputslow)
    * [func runtime_debug_WriteHeapDump(fd uintptr)](#runtime_debug_WriteHeapDump)
    * [func runtime_debug_freeOSMemory()](#runtime_debug_freeOSMemory)
    * [func runtime_expandFinalInlineFrame(stk []uintptr) []uintptr](#runtime_expandFinalInlineFrame)
    * [func runtime_getProfLabel() unsafe.Pointer](#runtime_getProfLabel)
    * [func runtime_goroutineProfileWithLabels(p []StackRecord, labels []unsafe.Pointer) (n int, ok bool)](#runtime_goroutineProfileWithLabels)
    * [func runtime_pprof_readProfile() ([]uint64, []unsafe.Pointer, bool)](#runtime_pprof_readProfile)
    * [func runtime_pprof_runtime_cyclesPerSecond() int64](#runtime_pprof_runtime_cyclesPerSecond)
    * [func runtime_setProfLabel(labels unsafe.Pointer)](#runtime_setProfLabel)
    * [func runtimer(pp *p, now int64) int64](#runtimer)
    * [func save(pc, sp uintptr)](#save)
    * [func saveAncestors(callergp *g) *[]ancestorInfo](#saveAncestors)
    * [func saveblockevent(cycles, rate int64, skip int, which bucketType)](#saveblockevent)
    * [func saveg(pc, sp uintptr, gp *g, r *StackRecord)](#saveg)
    * [func sbrk0() uintptr](#sbrk0)
    * [func scanConservative(b, n uintptr, ptrmask *uint8, gcw *gcWork, state *stackScanState)](#scanConservative)
    * [func scanblock(b0, n0 uintptr, ptrmask *uint8, gcw *gcWork, stk *stackScanState)](#scanblock)
    * [func scanframeworker(frame *stkframe, state *stackScanState, gcw *gcWork)](#scanframeworker)
    * [func scanobject(b uintptr, gcw *gcWork)](#scanobject)
    * [func scanstack(gp *g, gcw *gcWork)](#scanstack)
    * [func scavengeSleep(ns int64) int64](#scavengeSleep)
    * [func schedEnableUser(enable bool)](#schedEnableUser)
    * [func schedEnabled(gp *g) bool](#schedEnabled)
    * [func schedinit()](#schedinit)
    * [func schedtrace(detailed bool)](#schedtrace)
    * [func schedule()](#schedule)
    * [func selectgo(cas0 *scase, order0 *uint16, pc0 *uintptr, nsends, nrecvs int, block bool) (int, bool)](#selectgo)
    * [func selectnbrecv(elem unsafe.Pointer, c *hchan) (selected, received bool)](#selectnbrecv)
    * [func selectnbsend(c *hchan, elem unsafe.Pointer) (selected bool)](#selectnbsend)
    * [func selectsetpc(pc *uintptr)](#selectsetpc)
    * [func sellock(scases []scase, lockorder []uint16)](#sellock)
    * [func selparkcommit(gp *g, _ unsafe.Pointer) bool](#selparkcommit)
    * [func selunlock(scases []scase, lockorder []uint16)](#selunlock)
    * [func semacquire(addr *uint32)](#semacquire)
    * [func semacquire1(addr *uint32, lifo bool, profile semaProfileFlags, skipframes int)](#semacquire1)
    * [func semacreate(mp *m)](#semacreate)
    * [func semasleep(ns int64) int32](#semasleep)
    * [func semawakeup(mp *m)](#semawakeup)
    * [func semrelease(addr *uint32)](#semrelease)
    * [func semrelease1(addr *uint32, handoff bool, skipframes int)](#semrelease1)
    * [func send(c *hchan, sg *sudog, ep unsafe.Pointer, unlockf func(), skip int)](#send)
    * [func sendDirect(t *_type, sg *sudog, src unsafe.Pointer)](#sendDirect)
    * [func setCheckmark(obj, base, off uintptr, mbits markBits) bool](#setCheckmark)
    * [func setGCPercent(in int32) (out int32)](#setGCPercent)
    * [func setGCPhase(x uint32)](#setGCPhase)
    * [func setGNoWB(gp **g, new *g)](#setGNoWB)
    * [func setGsignalStack(st *stackt, old *gsignalStack)](#setGsignalStack)
    * [func setMNoWB(mp **m, new *m)](#setMNoWB)
    * [func setMaxStack(in int) (out int)](#setMaxStack)
    * [func setMaxThreads(in int) (out int)](#setMaxThreads)
    * [func setNonblock(fd int32)](#setNonblock)
    * [func setPanicOnFault(new bool) (old bool)](#setPanicOnFault)
    * [func setProcessCPUProfiler(hz int32)](#setProcessCPUProfiler)
    * [func setSignalstackSP(s *stackt, sp uintptr)](#setSignalstackSP)
    * [func setThreadCPUProfiler(hz int32)](#setThreadCPUProfiler)
    * [func setTraceback(level string)](#setTraceback)
    * [func setcpuprofilerate(hz int32)](#setcpuprofilerate)
    * [func setg(gg *g)](#setg)
    * [func setitimer(mode int32, new, old *itimerval)](#setitimer)
    * [func setitimer_trampoline()](#setitimer_trampoline)
    * [func setprofilebucket(p unsafe.Pointer, b *bucket)](#setprofilebucket)
    * [func setsig(i uint32, fn uintptr)](#setsig)
    * [func setsigsegv(pc uintptr)](#setsigsegv)
    * [func setsigstack(i uint32)](#setsigstack)
    * [func settls()](#settls)
    * [func shade(b uintptr)](#shade)
    * [func shouldPushSigpanic(gp *g, pc, lr uintptr) bool](#shouldPushSigpanic)
    * [func showframe(f funcInfo, gp *g, firstFrame bool, funcID, childID funcID) bool](#showframe)
    * [func showfuncinfo(f funcInfo, firstFrame bool, funcID, childID funcID) bool](#showfuncinfo)
    * [func shrinkstack(gp *g)](#shrinkstack)
    * [func siftdownTimer(t []*timer, i int)](#siftdownTimer)
    * [func siftupTimer(t []*timer, i int)](#siftupTimer)
    * [func sigInitIgnored(s uint32)](#sigInitIgnored)
    * [func sigInstallGoHandler(sig uint32) bool](#sigInstallGoHandler)
    * [func sigNotOnStack(sig uint32)](#sigNotOnStack)
    * [func sigNoteSetup(*note)](#sigNoteSetup)
    * [func sigNoteSleep(*note)](#sigNoteSleep)
    * [func sigNoteWakeup(*note)](#sigNoteWakeup)
    * [func sigRecvPrepareForFixup()](#sigRecvPrepareForFixup)
    * [func sigaction(sig uint32, new *usigactiont, old *usigactiont)](#sigaction)
    * [func sigaction_trampoline()](#sigaction_trampoline)
    * [func sigaddset(mask *sigset, i int)](#sigaddset)
    * [func sigaltstack(new *stackt, old *stackt)](#sigaltstack)
    * [func sigaltstack_trampoline()](#sigaltstack_trampoline)
    * [func sigblock(exiting bool)](#sigblock)
    * [func sigdelset(mask *sigset, i int)](#sigdelset)
    * [func sigdisable(sig uint32)](#sigdisable)
    * [func sigenable(sig uint32)](#sigenable)
    * [func sigfwd(fn uintptr, sig uint32, info *siginfo, ctx unsafe.Pointer)](#sigfwd)
    * [func sigfwdgo(sig uint32, info *siginfo, ctx unsafe.Pointer) bool](#sigfwdgo)
    * [func sighandler(sig uint32, info *siginfo, ctxt unsafe.Pointer, gp *g)](#sighandler)
    * [func sigignore(sig uint32)](#sigignore)
    * [func sigismember(mask *sigset, i int) bool](#sigismember)
    * [func signalDuringFork(sig uint32)](#signalDuringFork)
    * [func signalM(mp *m, sig int)](#signalM)
    * [func signalWaitUntilIdle()](#signalWaitUntilIdle)
    * [func signal_disable(s uint32)](#signal_disable)
    * [func signal_enable(s uint32)](#signal_enable)
    * [func signal_ignore(s uint32)](#signal_ignore)
    * [func signal_ignored(s uint32) bool](#signal_ignored)
    * [func signal_recv() uint32](#signal_recv)
    * [func signalstack(s *stack)](#signalstack)
    * [func signame(sig uint32) string](#signame)
    * [func sigpanic()](#sigpanic)
    * [func sigpanic0()](#sigpanic0)
    * [func sigpipe()](#sigpipe)
    * [func sigprocmask(how uint32, new *sigset, old *sigset)](#sigprocmask)
    * [func sigprocmask_trampoline()](#sigprocmask_trampoline)
    * [func sigprof(pc, sp, lr uintptr, gp *g, mp *m)](#sigprof)
    * [func sigprofNonGo()](#sigprofNonGo)
    * [func sigprofNonGoPC(pc uintptr)](#sigprofNonGoPC)
    * [func sigsave(p *sigset)](#sigsave)
    * [func sigsend(s uint32) bool](#sigsend)
    * [func sigtramp()](#sigtramp)
    * [func sigtrampgo(sig uint32, info *siginfo, ctx unsafe.Pointer)](#sigtrampgo)
    * [func slicebytetostring(buf *tmpBuf, ptr *byte, n int) (str string)](#slicebytetostring)
    * [func slicebytetostringtmp(ptr *byte, n int) (str string)](#slicebytetostringtmp)
    * [func slicecopy(toPtr unsafe.Pointer, toLen int, fromPtr unsafe.Pointer, fromLen int, width uintptr) int](#slicecopy)
    * [func slicerunetostring(buf *tmpBuf, a []rune) string](#slicerunetostring)
    * [func spanHasNoSpecials(s *mspan)](#spanHasNoSpecials)
    * [func spanHasSpecials(s *mspan)](#spanHasSpecials)
    * [func spillArgs()](#spillArgs)
    * [func stackOverflow(x *byte)](#stackOverflow)
    * [func stackcache_clear(c *mcache)](#stackcache_clear)
    * [func stackcacherefill(c *mcache, order uint8)](#stackcacherefill)
    * [func stackcacherelease(c *mcache, order uint8)](#stackcacherelease)
    * [func stackcheck()](#stackcheck)
    * [func stackfree(stk stack)](#stackfree)
    * [func stackinit()](#stackinit)
    * [func stacklog2(n uintptr) int](#stacklog2)
    * [func stackpoolfree(x gclinkptr, order uint8)](#stackpoolfree)
    * [func startCheckmarks()](#startCheckmarks)
    * [func startTemplateThread()](#startTemplateThread)
    * [func startTheWorld()](#startTheWorld)
    * [func startTheWorldGC()](#startTheWorldGC)
    * [func startTheWorldWithSema(emitTraceEvent bool) int64](#startTheWorldWithSema)
    * [func startTimer(t *timer)](#startTimer)
    * [func startlockedm(gp *g)](#startlockedm)
    * [func startm(_p_ *p, spinning bool)](#startm)
    * [func startpanic_m() bool](#startpanic_m)
    * [func step(p []byte, pc *uintptr, val *int32, first bool) (newp []byte, ok bool)](#step)
    * [func stopTheWorld(reason string)](#stopTheWorld)
    * [func stopTheWorldGC(reason string)](#stopTheWorldGC)
    * [func stopTheWorldWithSema()](#stopTheWorldWithSema)
    * [func stopTimer(t *timer) bool](#stopTimer)
    * [func stoplockedm()](#stoplockedm)
    * [func stopm()](#stopm)
    * [func strequal(p, q unsafe.Pointer) bool](#strequal)
    * [func strhash(p unsafe.Pointer, h uintptr) uintptr](#strhash)
    * [func strhashFallback(a unsafe.Pointer, h uintptr) uintptr](#strhashFallback)
    * [func stringDataOnStack(s string) bool](#stringDataOnStack)
    * [func stringHash(s string, seed uintptr) uintptr](#stringHash)
    * [func stringtoslicebyte(buf *tmpBuf, s string) []byte](#stringtoslicebyte)
    * [func stringtoslicerune(buf *[tmpStringBufSize]rune, s string) []rune](#stringtoslicerune)
    * [func subtract1(p *byte) *byte](#subtract1)
    * [func subtractb(p *byte, n uintptr) *byte](#subtractb)
    * [func sweepone() uintptr](#sweepone)
    * [func sync_atomic_CompareAndSwapPointer(ptr *unsafe.Pointer, old, new unsafe.Pointer) bool](#sync_atomic_CompareAndSwapPointer)
    * [func sync_atomic_CompareAndSwapUintptr(ptr *uintptr, old, new uintptr) bool](#sync_atomic_CompareAndSwapUintptr)
    * [func sync_atomic_StorePointer(ptr *unsafe.Pointer, new unsafe.Pointer)](#sync_atomic_StorePointer)
    * [func sync_atomic_StoreUintptr(ptr *uintptr, new uintptr)](#sync_atomic_StoreUintptr)
    * [func sync_atomic_SwapPointer(ptr *unsafe.Pointer, new unsafe.Pointer) unsafe.Pointer](#sync_atomic_SwapPointer)
    * [func sync_atomic_SwapUintptr(ptr *uintptr, new uintptr) uintptr](#sync_atomic_SwapUintptr)
    * [func sync_atomic_runtime_procPin() int](#sync_atomic_runtime_procPin)
    * [func sync_atomic_runtime_procUnpin()](#sync_atomic_runtime_procUnpin)
    * [func sync_fastrand() uint32](#sync_fastrand)
    * [func sync_nanotime() int64](#sync_nanotime)
    * [func sync_runtime_Semacquire(addr *uint32)](#sync_runtime_Semacquire)
    * [func sync_runtime_SemacquireMutex(addr *uint32, lifo bool, skipframes int)](#sync_runtime_SemacquireMutex)
    * [func sync_runtime_Semrelease(addr *uint32, handoff bool, skipframes int)](#sync_runtime_Semrelease)
    * [func sync_runtime_canSpin(i int) bool](#sync_runtime_canSpin)
    * [func sync_runtime_doSpin()](#sync_runtime_doSpin)
    * [func sync_runtime_procPin() int](#sync_runtime_procPin)
    * [func sync_runtime_procUnpin()](#sync_runtime_procUnpin)
    * [func sync_runtime_registerPoolCleanup(f func())](#sync_runtime_registerPoolCleanup)
    * [func sync_throw(s string)](#sync_throw)
    * [func syncadjustsudogs(gp *g, used uintptr, adjinfo *adjustinfo) uintptr](#syncadjustsudogs)
    * [func sysAlloc(n uintptr, sysStat *sysMemStat) unsafe.Pointer](#sysAlloc)
    * [func sysFault(v unsafe.Pointer, n uintptr)](#sysFault)
    * [func sysFree(v unsafe.Pointer, n uintptr, sysStat *sysMemStat)](#sysFree)
    * [func sysHugePage(v unsafe.Pointer, n uintptr)](#sysHugePage)
    * [func sysMap(v unsafe.Pointer, n uintptr, sysStat *sysMemStat)](#sysMap)
    * [func sysReserve(v unsafe.Pointer, n uintptr) unsafe.Pointer](#sysReserve)
    * [func sysReserveAligned(v unsafe.Pointer, size, align uintptr) (unsafe.Pointer, uintptr)](#sysReserveAligned)
    * [func sysUnused(v unsafe.Pointer, n uintptr)](#sysUnused)
    * [func sysUsed(v unsafe.Pointer, n uintptr)](#sysUsed)
    * [func sysargs(argc int32, argv **byte)](#sysargs)
    * [func syscall()](#syscall)
    * [func syscall6()](#syscall6)
    * [func syscall6X()](#syscall6X)
    * [func syscallNoErr()](#syscallNoErr)
    * [func syscallPtr()](#syscallPtr)
    * [func syscallX()](#syscallX)
    * [func syscall_Exit(code int)](#syscall_Exit)
    * [func syscall_Getpagesize() int](#syscall_Getpagesize)
    * [func syscall_cgocaller(fn unsafe.Pointer, args ...uintptr) uintptr](#syscall_cgocaller)
    * [func syscall_rawSyscall(fn, a1, a2, a3 uintptr) (r1, r2, err uintptr)](#syscall_rawSyscall)
    * [func syscall_rawSyscall6(fn, a1, a2, a3, a4, a5, a6 uintptr) (r1, r2, err uintptr)](#syscall_rawSyscall6)
    * [func syscall_runtime_AfterExec()](#syscall_runtime_AfterExec)
    * [func syscall_runtime_AfterFork()](#syscall_runtime_AfterFork)
    * [func syscall_runtime_AfterForkInChild()](#syscall_runtime_AfterForkInChild)
    * [func syscall_runtime_BeforeExec()](#syscall_runtime_BeforeExec)
    * [func syscall_runtime_BeforeFork()](#syscall_runtime_BeforeFork)
    * [func syscall_runtime_doAllThreadsSyscall(fn func(bool) bool)](#syscall_runtime_doAllThreadsSyscall)
    * [func syscall_runtime_envs() []string](#syscall_runtime_envs)
    * [func syscall_setenv_c(k string, v string)](#syscall_setenv_c)
    * [func syscall_syscall(fn, a1, a2, a3 uintptr) (r1, r2, err uintptr)](#syscall_syscall)
    * [func syscall_syscall6(fn, a1, a2, a3, a4, a5, a6 uintptr) (r1, r2, err uintptr)](#syscall_syscall6)
    * [func syscall_syscall6X(fn, a1, a2, a3, a4, a5, a6 uintptr) (r1, r2, err uintptr)](#syscall_syscall6X)
    * [func syscall_syscallPtr(fn, a1, a2, a3 uintptr) (r1, r2, err uintptr)](#syscall_syscallPtr)
    * [func syscall_syscallX(fn, a1, a2, a3 uintptr) (r1, r2, err uintptr)](#syscall_syscallX)
    * [func syscall_unsetenv_c(k string)](#syscall_unsetenv_c)
    * [func sysctl(mib *uint32, miblen uint32, oldp *byte, oldlenp *uintptr, newp *byte, newlen uintptr) int32](#sysctl)
    * [func sysctl_trampoline()](#sysctl_trampoline)
    * [func sysctlbyname(name *byte, oldp *byte, oldlenp *uintptr, newp *byte, newlen uintptr) int32](#sysctlbyname)
    * [func sysctlbynameInt32(name []byte) (int32, int32)](#sysctlbynameInt32)
    * [func sysctlbyname_trampoline()](#sysctlbyname_trampoline)
    * [func sysmon()](#sysmon)
    * [func systemstack(fn func())](#systemstack)
    * [func systemstack_switch()](#systemstack_switch)
    * [func templateThread()](#templateThread)
    * [func testAtomic64()](#testAtomic64)
    * [func testdefersizes()](#testdefersizes)
    * [func throw(s string)](#throw)
    * [func tickspersecond() int64](#tickspersecond)
    * [func timeHistogramMetricsBuckets() []float64](#timeHistogramMetricsBuckets)
    * [func timeSleep(ns int64)](#timeSleep)
    * [func time_now() (sec int64, nsec int32, mono int64)](#time_now)
    * [func timediv(v int64, div int32, rem *int32) int32](#timediv)
    * [func tooManyOverflowBuckets(noverflow uint16, B uint8) bool](#tooManyOverflowBuckets)
    * [func tophash(hash uintptr) uint8](#tophash)
    * [func totaldefersize(siz uintptr) uintptr](#totaldefersize)
    * [func traceAppend(buf []byte, v uint64) []byte](#traceAppend)
    * [func traceEvent(ev byte, skip int, args ...uint64)](#traceEvent)
    * [func traceEventLocked(extraBytes int, mp *m, pid int32, bufp *traceBufPtr, ev byte, skip int, args ...uint64)](#traceEventLocked)
    * [func traceFullQueue(buf traceBufPtr)](#traceFullQueue)
    * [func traceGCDone()](#traceGCDone)
    * [func traceGCMarkAssistDone()](#traceGCMarkAssistDone)
    * [func traceGCMarkAssistStart()](#traceGCMarkAssistStart)
    * [func traceGCSTWDone()](#traceGCSTWDone)
    * [func traceGCSTWStart(kind int)](#traceGCSTWStart)
    * [func traceGCStart()](#traceGCStart)
    * [func traceGCSweepDone()](#traceGCSweepDone)
    * [func traceGCSweepSpan(bytesSwept uintptr)](#traceGCSweepSpan)
    * [func traceGCSweepStart()](#traceGCSweepStart)
    * [func traceGoCreate(newg *g, pc uintptr)](#traceGoCreate)
    * [func traceGoEnd()](#traceGoEnd)
    * [func traceGoPark(traceEv byte, skip int)](#traceGoPark)
    * [func traceGoPreempt()](#traceGoPreempt)
    * [func traceGoSched()](#traceGoSched)
    * [func traceGoStart()](#traceGoStart)
    * [func traceGoSysBlock(pp *p)](#traceGoSysBlock)
    * [func traceGoSysCall()](#traceGoSysCall)
    * [func traceGoSysExit(ts int64)](#traceGoSysExit)
    * [func traceGoUnpark(gp *g, skip int)](#traceGoUnpark)
    * [func traceGomaxprocs(procs int32)](#traceGomaxprocs)
    * [func traceHeapAlloc()](#traceHeapAlloc)
    * [func traceHeapGoal()](#traceHeapGoal)
    * [func traceProcFree(pp *p)](#traceProcFree)
    * [func traceProcStart()](#traceProcStart)
    * [func traceProcStop(pp *p)](#traceProcStop)
    * [func traceReleaseBuffer(pid int32)](#traceReleaseBuffer)
    * [func traceStackID(mp *m, buf []uintptr, skip int) uint64](#traceStackID)
    * [func trace_userLog(id uint64, category, message string)](#trace_userLog)
    * [func trace_userRegion(id, mode uint64, name string)](#trace_userRegion)
    * [func trace_userTaskCreate(id, parentID uint64, taskType string)](#trace_userTaskCreate)
    * [func trace_userTaskEnd(id uint64)](#trace_userTaskEnd)
    * [func tracealloc(p unsafe.Pointer, size uintptr, typ *_type)](#tracealloc)
    * [func traceback(pc, sp, lr uintptr, gp *g)](#traceback)
    * [func traceback1(pc, sp, lr uintptr, gp *g, flags uint)](#traceback1)
    * [func tracebackCgoContext(pcbuf *uintptr, printing bool, ctxt uintptr, n, max int) int](#tracebackCgoContext)
    * [func tracebackHexdump(stk stack, frame *stkframe, bad uintptr)](#tracebackHexdump)
    * [func tracebackdefers(gp *g, callback func(*stkframe, unsafe.Pointer) bool, v unsafe.Pointer)](#tracebackdefers)
    * [func tracebackothers(me *g)](#tracebackothers)
    * [func tracebacktrap(pc, sp, lr uintptr, gp *g)](#tracebacktrap)
    * [func tracefree(p unsafe.Pointer, size uintptr)](#tracefree)
    * [func tracegc()](#tracegc)
    * [func typeBitsBulkBarrier(typ *_type, dst, src, size uintptr)](#typeBitsBulkBarrier)
    * [func typedmemclr(typ *_type, ptr unsafe.Pointer)](#typedmemclr)
    * [func typedmemmove(typ *_type, dst, src unsafe.Pointer)](#typedmemmove)
    * [func typedslicecopy(typ *_type, dstPtr unsafe.Pointer, dstLen int, srcPtr unsafe.Pointer, srcLen int) int](#typedslicecopy)
    * [func typehash(t *_type, p unsafe.Pointer, h uintptr) uintptr](#typehash)
    * [func typelinksinit()](#typelinksinit)
    * [func typesEqual(t, v *_type, seen map[_typePair]struct{}) bool](#typesEqual)
    * [func unblocksig(sig uint32)](#unblocksig)
    * [func unexportedPanicForTesting(b []byte, i int) byte](#unexportedPanicForTesting)
    * [func unimplemented(name string)](#unimplemented)
    * [func unlock(l *mutex)](#unlock)
    * [func unlock2(l *mutex)](#unlock2)
    * [func unlockOSThread()](#unlockOSThread)
    * [func unlockWithRank(l *mutex)](#unlockWithRank)
    * [func unlockextra(mp *m)](#unlockextra)
    * [func unminit()](#unminit)
    * [func unminitSignals()](#unminitSignals)
    * [func unreachableMethod()](#unreachableMethod)
    * [func unsafeslice(et *_type, len int)](#unsafeslice)
    * [func unsafeslice64(et *_type, len64 int64)](#unsafeslice64)
    * [func unspillArgs()](#unspillArgs)
    * [func unwindm(restore *bool)](#unwindm)
    * [func updateTimer0When(pp *p)](#updateTimer0When)
    * [func updateTimerModifiedEarliest(pp *p, nextwhen int64)](#updateTimerModifiedEarliest)
    * [func updateTimerPMask(pp *p)](#updateTimerPMask)
    * [func updatememstats()](#updatememstats)
    * [func usesLibcall() bool](#usesLibcall)
    * [func usleep(usec uint32)](#usleep)
    * [func usleep_no_g(usec uint32)](#usleep_no_g)
    * [func usleep_trampoline()](#usleep_trampoline)
    * [func verifyTimerHeap(pp *p)](#verifyTimerHeap)
    * [func waitForSigusr1Callback(gp *g) bool](#waitForSigusr1Callback)
    * [func wakeNetPoller(when int64)](#wakeNetPoller)
    * [func wakeScavenger()](#wakeScavenger)
    * [func wakep()](#wakep)
    * [func walltime() (int64, int32)](#walltime)
    * [func walltime_trampoline()](#walltime_trampoline)
    * [func wantAsyncPreempt(gp *g) bool](#wantAsyncPreempt)
    * [func wbBufFlush(dst *uintptr, src uintptr)](#wbBufFlush)
    * [func wbBufFlush1(_p_ *p)](#wbBufFlush1)
    * [func wirep(_p_ *p)](#wirep)
    * [func worldStarted()](#worldStarted)
    * [func worldStopped()](#worldStopped)
    * [func write(fd uintptr, p unsafe.Pointer, n int32) int32](#write)
    * [func write1(fd uintptr, p unsafe.Pointer, n int32) int32](#write1)
    * [func writeErr(b []byte)](#writeErr)
    * [func write_trampoline()](#write_trampoline)
    * [func writeheapdump_m(fd uintptr, m *MemStats)](#writeheapdump_m)
    * [func _ExternalCode()](#_ExternalCode)
    * [func _GC()](#_GC)
    * [func _LostExternalCode()](#_LostExternalCode)
    * [func _LostSIGPROFDuringAtomic64()](#_LostSIGPROFDuringAtomic64)
    * [func _System()](#_System)
    * [func _VDSO()](#_VDSO)
    * [func _cgo_panic_internal(p *byte)](#_cgo_panic_internal)


## <a id="const" href="#const">Constants</a>

```
tags: [package]
```

### <a id="Compiler" href="#Compiler">const Compiler</a>

```
searchKey: runtime.Compiler
tags: [constant string]
```

```Go
const Compiler = "gc"
```

Compiler is the name of the compiler toolchain that built the running binary. Known toolchains are: 

```
gc      Also known as cmd/compile.
gccgo   The gccgo front end, part of the GCC compiler suite.

```
### <a id="DebugLogBytes" href="#DebugLogBytes">const DebugLogBytes</a>

```
searchKey: runtime.DebugLogBytes
tags: [constant number private]
```

```Go
const DebugLogBytes = debugLogBytes
```

### <a id="DebugLogStringLimit" href="#DebugLogStringLimit">const DebugLogStringLimit</a>

```
searchKey: runtime.DebugLogStringLimit
tags: [constant number private]
```

```Go
const DebugLogStringLimit = debugLogStringLimit
```

### <a id="DlogEnabled" href="#DlogEnabled">const DlogEnabled</a>

```
searchKey: runtime.DlogEnabled
tags: [constant boolean private]
```

```Go
const DlogEnabled = dlogEnabled
```

### <a id="ENOMEM" href="#ENOMEM">const ENOMEM</a>

```
searchKey: runtime.ENOMEM
tags: [constant number private]
```

```Go
const ENOMEM = _ENOMEM
```

### <a id="GOARCH" href="#GOARCH">const GOARCH</a>

```
searchKey: runtime.GOARCH
tags: [constant string]
```

```Go
const GOARCH string = sys.GOARCH
```

GOARCH is the running program's architecture target: one of 386, amd64, arm, s390x, and so on. 

### <a id="GOOS" href="#GOOS">const GOOS</a>

```
searchKey: runtime.GOOS
tags: [constant string]
```

```Go
const GOOS string = sys.GOOS
```

GOOS is the running program's operating system target: one of darwin, freebsd, linux, and so on. To view possible combinations of GOOS and GOARCH, run "go tool dist list". 

### <a id="MAP_ANON" href="#MAP_ANON">const MAP_ANON</a>

```
searchKey: runtime.MAP_ANON
tags: [constant number private]
```

```Go
const MAP_ANON = _MAP_ANON
```

### <a id="MAP_FIXED" href="#MAP_FIXED">const MAP_FIXED</a>

```
searchKey: runtime.MAP_FIXED
tags: [constant number private]
```

```Go
const MAP_FIXED = _MAP_FIXED
```

### <a id="MAP_PRIVATE" href="#MAP_PRIVATE">const MAP_PRIVATE</a>

```
searchKey: runtime.MAP_PRIVATE
tags: [constant number private]
```

```Go
const MAP_PRIVATE = _MAP_PRIVATE
```

### <a id="PageAlloc64Bit" href="#PageAlloc64Bit">const PageAlloc64Bit</a>

```
searchKey: runtime.PageAlloc64Bit
tags: [constant number private]
```

```Go
const PageAlloc64Bit = pageAlloc64Bit
```

### <a id="PageCachePages" href="#PageCachePages">const PageCachePages</a>

```
searchKey: runtime.PageCachePages
tags: [constant number private]
```

```Go
const PageCachePages = pageCachePages
```

### <a id="PageSize" href="#PageSize">const PageSize</a>

```
searchKey: runtime.PageSize
tags: [constant number private]
```

```Go
const PageSize = pageSize
```

### <a id="PallocChunkPages" href="#PallocChunkPages">const PallocChunkPages</a>

```
searchKey: runtime.PallocChunkPages
tags: [constant number private]
```

```Go
const PallocChunkPages = pallocChunkPages
```

### <a id="PallocSumBytes" href="#PallocSumBytes">const PallocSumBytes</a>

```
searchKey: runtime.PallocSumBytes
tags: [constant number private]
```

```Go
const PallocSumBytes = pallocSumBytes
```

### <a id="PreemptMSupported" href="#PreemptMSupported">const PreemptMSupported</a>

```
searchKey: runtime.PreemptMSupported
tags: [constant boolean private]
```

```Go
const PreemptMSupported = preemptMSupported
```

### <a id="ProfBufBlocking" href="#ProfBufBlocking">const ProfBufBlocking</a>

```
searchKey: runtime.ProfBufBlocking
tags: [constant number private]
```

```Go
const ProfBufBlocking = profBufBlocking
```

### <a id="ProfBufNonBlocking" href="#ProfBufNonBlocking">const ProfBufNonBlocking</a>

```
searchKey: runtime.ProfBufNonBlocking
tags: [constant number private]
```

```Go
const ProfBufNonBlocking = profBufNonBlocking
```

### <a id="PtrSize" href="#PtrSize">const PtrSize</a>

```
searchKey: runtime.PtrSize
tags: [constant number private]
```

```Go
const PtrSize = sys.PtrSize
```

### <a id="Raceenabled" href="#Raceenabled">const Raceenabled</a>

```
searchKey: runtime.Raceenabled
tags: [constant boolean private]
```

```Go
const Raceenabled = raceenabled
```

### <a id="RuntimeHmapSize" href="#RuntimeHmapSize">const RuntimeHmapSize</a>

```
searchKey: runtime.RuntimeHmapSize
tags: [constant number private]
```

```Go
const RuntimeHmapSize = unsafe.Sizeof(hmap{})
```

### <a id="TimeHistNumSubBuckets" href="#TimeHistNumSubBuckets">const TimeHistNumSubBuckets</a>

```
searchKey: runtime.TimeHistNumSubBuckets
tags: [constant number private]
```

```Go
const TimeHistNumSubBuckets = timeHistNumSubBuckets
```

### <a id="TimeHistNumSuperBuckets" href="#TimeHistNumSuperBuckets">const TimeHistNumSuperBuckets</a>

```
searchKey: runtime.TimeHistNumSuperBuckets
tags: [constant number private]
```

```Go
const TimeHistNumSuperBuckets = timeHistNumSuperBuckets
```

### <a id="TimeHistSubBucketBits" href="#TimeHistSubBucketBits">const TimeHistSubBucketBits</a>

```
searchKey: runtime.TimeHistSubBucketBits
tags: [constant number private]
```

```Go
const TimeHistSubBucketBits = timeHistSubBucketBits
```

### <a id="active_spin" href="#active_spin">const active_spin</a>

```
searchKey: runtime.active_spin
tags: [constant number private]
```

```Go
const active_spin = 4
```

This implementation depends on OS-specific implementations of 

```
func semacreate(mp *m)
	Create a semaphore for mp, if it does not already have one.

func semasleep(ns int64) int32
	If ns < 0, acquire m's semaphore and return 0.
	If ns >= 0, try to acquire m's semaphore for at most ns nanoseconds.
	Return 0 if the semaphore was acquired, -1 if interrupted or timed out.

func semawakeup(mp *m)
	Wake up mp, which is or will soon be sleeping on its semaphore.

```
### <a id="active_spin_cnt" href="#active_spin_cnt">const active_spin_cnt</a>

```
searchKey: runtime.active_spin_cnt
tags: [constant number private]
```

```Go
const active_spin_cnt = 30
```

This implementation depends on OS-specific implementations of 

```
func semacreate(mp *m)
	Create a semaphore for mp, if it does not already have one.

func semasleep(ns int64) int32
	If ns < 0, acquire m's semaphore and return 0.
	If ns >= 0, try to acquire m's semaphore for at most ns nanoseconds.
	Return 0 if the semaphore was acquired, -1 if interrupted or timed out.

func semawakeup(mp *m)
	Wake up mp, which is or will soon be sleeping on its semaphore.

```
### <a id="addrBits" href="#addrBits">const addrBits</a>

```
searchKey: runtime.addrBits
tags: [constant number private]
```

```Go
const addrBits = 48
```

addrBits is the number of bits needed to represent a virtual address. 

See heapAddrBits for a table of address space sizes on various architectures. 48 bits is enough for all architectures except s390x. 

On AMD64, virtual addresses are 48-bit (or 57-bit) numbers sign extended to 64. We shift the address left 16 to eliminate the sign extended part and make room in the bottom for the count. 

On s390x, virtual addresses are 64-bit. There's not much we can do about this, so we just hope that the kernel doesn't get to really high addresses and panic if it does. 

### <a id="aixAddrBits" href="#aixAddrBits">const aixAddrBits</a>

```
searchKey: runtime.aixAddrBits
tags: [constant number private]
```

```Go
const aixAddrBits = 57
```

On AIX, 64-bit addresses are split into 36-bit segment number and 28-bit offset in segment.  Segment numbers in the range 0x0A0000000-0x0AFFFFFFF(LSA) are available for mmap. We assume all lfnode addresses are from memory allocated with mmap. We use one bit to distinguish between the two ranges. 

### <a id="aixCntBits" href="#aixCntBits">const aixCntBits</a>

```
searchKey: runtime.aixCntBits
tags: [constant number private]
```

```Go
const aixCntBits = 64 - aixAddrBits + 3
```

### <a id="arenaBaseOffset" href="#arenaBaseOffset">const arenaBaseOffset</a>

```
searchKey: runtime.arenaBaseOffset
tags: [constant number private]
```

```Go
const arenaBaseOffset = 0xffff800000000000*sys.GoarchAmd64 + 0x0a00000000000000*sys.GoosAix
```

arenaBaseOffset is the pointer value that corresponds to index 0 in the heap arena map. 

On amd64, the address space is 48 bits, sign extended to 64 bits. This offset lets us handle "negative" addresses (or high addresses if viewed as unsigned). 

On aix/ppc64, this offset allows to keep the heapAddrBits to 48. Otherwise, it would be 60 in order to handle mmap addresses (in range 0x0a00000000000000 - 0x0afffffffffffff). But in this case, the memory reserved in (s *pageAlloc).init for chunks is causing important slowdowns. 

On other platforms, the user address space is contiguous and starts at 0, so no offset is necessary. 

### <a id="arenaBaseOffsetUintptr" href="#arenaBaseOffsetUintptr">const arenaBaseOffsetUintptr</a>

```
searchKey: runtime.arenaBaseOffsetUintptr
tags: [constant number private]
```

```Go
const arenaBaseOffsetUintptr = uintptr(arenaBaseOffset)
```

A typed version of this constant that will make it into DWARF (for viewcore). 

### <a id="arenaBits" href="#arenaBits">const arenaBits</a>

```
searchKey: runtime.arenaBits
tags: [constant number private]
```

```Go
const arenaBits = arenaL1Bits + arenaL2Bits
```

arenaBits is the total bits in a combined arena map index. This is split between the index into the L1 arena map and the L2 arena map. 

### <a id="arenaL1Bits" href="#arenaL1Bits">const arenaL1Bits</a>

```
searchKey: runtime.arenaL1Bits
tags: [constant number private]
```

```Go
const arenaL1Bits = 6 * (_64bit * sys.GoosWindows)
```

arenaL1Bits is the number of bits of the arena number covered by the first level arena map. 

This number should be small, since the first level arena map requires PtrSize*(1<<arenaL1Bits) of space in the binary's BSS. It can be zero, in which case the first level index is effectively unused. There is a performance benefit to this, since the generated code can be more efficient, but comes at the cost of having a large L2 mapping. 

We use the L1 map on 64-bit Windows because the arena size is small, but the address space is still 48 bits, and there's a high cost to having a large L2. 

### <a id="arenaL1Shift" href="#arenaL1Shift">const arenaL1Shift</a>

```
searchKey: runtime.arenaL1Shift
tags: [constant number private]
```

```Go
const arenaL1Shift = arenaL2Bits
```

arenaL1Shift is the number of bits to shift an arena frame number by to compute an index into the first level arena map. 

### <a id="arenaL2Bits" href="#arenaL2Bits">const arenaL2Bits</a>

```
searchKey: runtime.arenaL2Bits
tags: [constant number private]
```

```Go
const arenaL2Bits = heapAddrBits - logHeapArenaBytes - arenaL1Bits
```

arenaL2Bits is the number of bits of the arena number covered by the second level arena index. 

The size of each arena map allocation is proportional to 1<<arenaL2Bits, so it's important that this not be too large. 48 bits leads to 32MB arena index allocations, which is about the practical threshold. 

### <a id="bias32" href="#bias32">const bias32</a>

```
searchKey: runtime.bias32
tags: [constant number private]
```

```Go
const bias32 = -1<<(expbits32-1) + 1
```

### <a id="bias64" href="#bias64">const bias64</a>

```
searchKey: runtime.bias64
tags: [constant number private]
```

```Go
const bias64 = -1<<(expbits64-1) + 1
```

### <a id="bitPointer" href="#bitPointer">const bitPointer</a>

```
searchKey: runtime.bitPointer
tags: [constant number private]
```

```Go
const bitPointer = 1 << 0
```

### <a id="bitPointerAll" href="#bitPointerAll">const bitPointerAll</a>

```
searchKey: runtime.bitPointerAll
tags: [constant number private]
```

```Go
const bitPointerAll = ...
```

### <a id="bitScan" href="#bitScan">const bitScan</a>

```
searchKey: runtime.bitScan
tags: [constant number private]
```

```Go
const bitScan = 1 << 4
```

### <a id="bitScanAll" href="#bitScanAll">const bitScanAll</a>

```
searchKey: runtime.bitScanAll
tags: [constant number private]
```

```Go
const bitScanAll = ...
```

all scan/pointer bits in a byte 

### <a id="blockProfile" href="#blockProfile">const blockProfile</a>

```
searchKey: runtime.blockProfile
tags: [constant number private]
```

```Go
const blockProfile
```

### <a id="boundsConvert" href="#boundsConvert">const boundsConvert</a>

```
searchKey: runtime.boundsConvert
tags: [constant number private]
```

```Go
const boundsConvert // (*[x]T)(s), 0 <= x <= len(s) failed

```

### <a id="boundsIndex" href="#boundsIndex">const boundsIndex</a>

```
searchKey: runtime.boundsIndex
tags: [constant number private]
```

```Go
const boundsIndex boundsErrorCode = iota // s[x], 0 <= x < len(s) failed

```

### <a id="boundsSlice3Acap" href="#boundsSlice3Acap">const boundsSlice3Acap</a>

```
searchKey: runtime.boundsSlice3Acap
tags: [constant number private]
```

```Go
const boundsSlice3Acap // s[?:?:x], 0 <= x <= cap(s) failed

```

### <a id="boundsSlice3Alen" href="#boundsSlice3Alen">const boundsSlice3Alen</a>

```
searchKey: runtime.boundsSlice3Alen
tags: [constant number private]
```

```Go
const boundsSlice3Alen // s[?:?:x], 0 <= x <= len(s) failed

```

### <a id="boundsSlice3B" href="#boundsSlice3B">const boundsSlice3B</a>

```
searchKey: runtime.boundsSlice3B
tags: [constant number private]
```

```Go
const boundsSlice3B // s[?:x:y], 0 <= x <= y failed (but boundsSlice3A didn't happen)

```

### <a id="boundsSlice3C" href="#boundsSlice3C">const boundsSlice3C</a>

```
searchKey: runtime.boundsSlice3C
tags: [constant number private]
```

```Go
const boundsSlice3C // s[x:y:?], 0 <= x <= y failed (but boundsSlice3A/B didn't happen)

```

### <a id="boundsSliceAcap" href="#boundsSliceAcap">const boundsSliceAcap</a>

```
searchKey: runtime.boundsSliceAcap
tags: [constant number private]
```

```Go
const boundsSliceAcap // s[?:x], 0 <= x <= cap(s) failed

```

### <a id="boundsSliceAlen" href="#boundsSliceAlen">const boundsSliceAlen</a>

```
searchKey: runtime.boundsSliceAlen
tags: [constant number private]
```

```Go
const boundsSliceAlen // s[?:x], 0 <= x <= len(s) failed

```

### <a id="boundsSliceB" href="#boundsSliceB">const boundsSliceB</a>

```
searchKey: runtime.boundsSliceB
tags: [constant number private]
```

```Go
const boundsSliceB // s[x:y], 0 <= x <= y failed (but boundsSliceA didn't happen)

```

### <a id="buckHashSize" href="#buckHashSize">const buckHashSize</a>

```
searchKey: runtime.buckHashSize
tags: [constant number private]
```

```Go
const buckHashSize = 179999
```

size of bucket hash table 

### <a id="bucketCnt" href="#bucketCnt">const bucketCnt</a>

```
searchKey: runtime.bucketCnt
tags: [constant number private]
```

```Go
const bucketCnt = 1 << bucketCntBits
```

### <a id="bucketCntBits" href="#bucketCntBits">const bucketCntBits</a>

```
searchKey: runtime.bucketCntBits
tags: [constant number private]
```

```Go
const bucketCntBits = 3
```

Maximum number of key/elem pairs a bucket can hold. 

### <a id="bufSize" href="#bufSize">const bufSize</a>

```
searchKey: runtime.bufSize
tags: [constant number private]
```

```Go
const bufSize = 4096
```

buffer of pending write data 

### <a id="c0" href="#c0">const c0</a>

```
searchKey: runtime.c0
tags: [constant number private]
```

```Go
const c0 = uintptr((8-sys.PtrSize)/4*2860486313 + (sys.PtrSize-4)/4*33054211828000289)
```

### <a id="c1" href="#c1">const c1</a>

```
searchKey: runtime.c1
tags: [constant number private]
```

```Go
const c1 = uintptr((8-sys.PtrSize)/4*3267000013 + (sys.PtrSize-4)/4*23344194077549503)
```

### <a id="cgoCheckPointerFail" href="#cgoCheckPointerFail">const cgoCheckPointerFail</a>

```
searchKey: runtime.cgoCheckPointerFail
tags: [constant string private]
```

```Go
const cgoCheckPointerFail = "cgo argument has Go pointer to Go pointer"
```

### <a id="cgoResultFail" href="#cgoResultFail">const cgoResultFail</a>

```
searchKey: runtime.cgoResultFail
tags: [constant string private]
```

```Go
const cgoResultFail = "cgo result has Go pointer"
```

### <a id="cgoWriteBarrierFail" href="#cgoWriteBarrierFail">const cgoWriteBarrierFail</a>

```
searchKey: runtime.cgoWriteBarrierFail
tags: [constant string private]
```

```Go
const cgoWriteBarrierFail = "Go pointer stored into non-Go memory"
```

### <a id="clobberdeadPtr" href="#clobberdeadPtr">const clobberdeadPtr</a>

```
searchKey: runtime.clobberdeadPtr
tags: [constant number private]
```

```Go
const clobberdeadPtr = uintptr(0xdeaddead | 0xdeaddead<<((^uintptr(0)>>63)*32))
```

clobberdeadPtr is a special value that is used by the compiler to clobber dead stack slots, when -clobberdead flag is set. 

### <a id="cntBits" href="#cntBits">const cntBits</a>

```
searchKey: runtime.cntBits
tags: [constant number private]
```

```Go
const cntBits = 64 - addrBits + 3
```

In addition to the 16 bits taken from the top, we can take 3 from the bottom, because node must be pointer-aligned, giving a total of 19 bits of count. 

### <a id="concurrentSweep" href="#concurrentSweep">const concurrentSweep</a>

```
searchKey: runtime.concurrentSweep
tags: [constant boolean private]
```

```Go
const concurrentSweep = _ConcurrentSweep
```

### <a id="dataOffset" href="#dataOffset">const dataOffset</a>

```
searchKey: runtime.dataOffset
tags: [constant number private]
```

```Go
const dataOffset = unsafe.Offsetof(struct {
	b bmap
	v int64
}{}.v)
```

data offset should be the size of the bmap struct, but needs to be aligned correctly. For amd64p32 this means 64-bit alignment even though pointers are 32 bit. 

### <a id="debugCallRuntime" href="#debugCallRuntime">const debugCallRuntime</a>

```
searchKey: runtime.debugCallRuntime
tags: [constant string private]
```

```Go
const debugCallRuntime = "call from within the Go runtime"
```

### <a id="debugCallSystemStack" href="#debugCallSystemStack">const debugCallSystemStack</a>

```
searchKey: runtime.debugCallSystemStack
tags: [constant string private]
```

```Go
const debugCallSystemStack = "executing on Go runtime stack"
```

### <a id="debugCallUnknownFunc" href="#debugCallUnknownFunc">const debugCallUnknownFunc</a>

```
searchKey: runtime.debugCallUnknownFunc
tags: [constant string private]
```

```Go
const debugCallUnknownFunc = "call from unknown function"
```

### <a id="debugCallUnsafePoint" href="#debugCallUnsafePoint">const debugCallUnsafePoint</a>

```
searchKey: runtime.debugCallUnsafePoint
tags: [constant string private]
```

```Go
const debugCallUnsafePoint = "call not at safe point"
```

### <a id="debugChan" href="#debugChan">const debugChan</a>

```
searchKey: runtime.debugChan
tags: [constant boolean private]
```

```Go
const debugChan = false
```

### <a id="debugCheckBP" href="#debugCheckBP">const debugCheckBP</a>

```
searchKey: runtime.debugCheckBP
tags: [constant boolean private]
```

```Go
const debugCheckBP = false
```

check the BP links during traceback. 

### <a id="debugLogBoolFalse" href="#debugLogBoolFalse">const debugLogBoolFalse</a>

```
searchKey: runtime.debugLogBoolFalse
tags: [constant number private]
```

```Go
const debugLogBoolFalse
```

### <a id="debugLogBoolTrue" href="#debugLogBoolTrue">const debugLogBoolTrue</a>

```
searchKey: runtime.debugLogBoolTrue
tags: [constant number private]
```

```Go
const debugLogBoolTrue
```

### <a id="debugLogBytes" href="#debugLogBytes">const debugLogBytes</a>

```
searchKey: runtime.debugLogBytes
tags: [constant number private]
```

```Go
const debugLogBytes = 16 << 10
```

debugLogBytes is the size of each per-M ring buffer. This is allocated off-heap to avoid blowing up the M and hence the GC'd heap size. 

### <a id="debugLogConstString" href="#debugLogConstString">const debugLogConstString</a>

```
searchKey: runtime.debugLogConstString
tags: [constant number private]
```

```Go
const debugLogConstString
```

### <a id="debugLogHeaderSize" href="#debugLogHeaderSize">const debugLogHeaderSize</a>

```
searchKey: runtime.debugLogHeaderSize
tags: [constant number private]
```

```Go
const debugLogHeaderSize = 2
```

debugLogHeaderSize is the number of bytes in the framing header of every dlog record. 

### <a id="debugLogHex" href="#debugLogHex">const debugLogHex</a>

```
searchKey: runtime.debugLogHex
tags: [constant number private]
```

```Go
const debugLogHex
```

### <a id="debugLogInt" href="#debugLogInt">const debugLogInt</a>

```
searchKey: runtime.debugLogInt
tags: [constant number private]
```

```Go
const debugLogInt
```

### <a id="debugLogPC" href="#debugLogPC">const debugLogPC</a>

```
searchKey: runtime.debugLogPC
tags: [constant number private]
```

```Go
const debugLogPC
```

### <a id="debugLogPtr" href="#debugLogPtr">const debugLogPtr</a>

```
searchKey: runtime.debugLogPtr
tags: [constant number private]
```

```Go
const debugLogPtr
```

### <a id="debugLogString" href="#debugLogString">const debugLogString</a>

```
searchKey: runtime.debugLogString
tags: [constant number private]
```

```Go
const debugLogString
```

### <a id="debugLogStringLimit" href="#debugLogStringLimit">const debugLogStringLimit</a>

```
searchKey: runtime.debugLogStringLimit
tags: [constant number private]
```

```Go
const debugLogStringLimit = debugLogBytes / 8
```

debugLogStringLimit is the maximum number of bytes in a string. Above this, the string will be truncated with "..(n more bytes).." 

### <a id="debugLogStringOverflow" href="#debugLogStringOverflow">const debugLogStringOverflow</a>

```
searchKey: runtime.debugLogStringOverflow
tags: [constant number private]
```

```Go
const debugLogStringOverflow
```

### <a id="debugLogSyncSize" href="#debugLogSyncSize">const debugLogSyncSize</a>

```
searchKey: runtime.debugLogSyncSize
tags: [constant number private]
```

```Go
const debugLogSyncSize = debugLogHeaderSize + 2*8
```

debugLogSyncSize is the number of bytes in a sync record. 

### <a id="debugLogTraceback" href="#debugLogTraceback">const debugLogTraceback</a>

```
searchKey: runtime.debugLogTraceback
tags: [constant number private]
```

```Go
const debugLogTraceback
```

### <a id="debugLogUint" href="#debugLogUint">const debugLogUint</a>

```
searchKey: runtime.debugLogUint
tags: [constant number private]
```

```Go
const debugLogUint
```

### <a id="debugLogUnknown" href="#debugLogUnknown">const debugLogUnknown</a>

```
searchKey: runtime.debugLogUnknown
tags: [constant number private]
```

```Go
const debugLogUnknown = 1 + iota
```

### <a id="debugMalloc" href="#debugMalloc">const debugMalloc</a>

```
searchKey: runtime.debugMalloc
tags: [constant boolean private]
```

```Go
const debugMalloc = false
```

### <a id="debugPcln" href="#debugPcln">const debugPcln</a>

```
searchKey: runtime.debugPcln
tags: [constant boolean private]
```

```Go
const debugPcln = false
```

### <a id="debugScanConservative" href="#debugScanConservative">const debugScanConservative</a>

```
searchKey: runtime.debugScanConservative
tags: [constant boolean private]
```

```Go
const debugScanConservative = false
```

debugScanConservative enables debug logging for stack frames that are scanned conservatively. 

### <a id="debugSelect" href="#debugSelect">const debugSelect</a>

```
searchKey: runtime.debugSelect
tags: [constant boolean private]
```

```Go
const debugSelect = false
```

### <a id="defaultHeapMinimum" href="#defaultHeapMinimum">const defaultHeapMinimum</a>

```
searchKey: runtime.defaultHeapMinimum
tags: [constant number private]
```

```Go
const defaultHeapMinimum = 4 << 20
```

defaultHeapMinimum is the value of heapMinimum for GOGC==100. 

### <a id="deferHeaderSize" href="#deferHeaderSize">const deferHeaderSize</a>

```
searchKey: runtime.deferHeaderSize
tags: [constant number private]
```

```Go
const deferHeaderSize = unsafe.Sizeof(_defer{})
```

### <a id="dlogEnabled" href="#dlogEnabled">const dlogEnabled</a>

```
searchKey: runtime.dlogEnabled
tags: [constant boolean private]
```

```Go
const dlogEnabled = false
```

### <a id="drainCheckThreshold" href="#drainCheckThreshold">const drainCheckThreshold</a>

```
searchKey: runtime.drainCheckThreshold
tags: [constant number private]
```

```Go
const drainCheckThreshold = 100000
```

drainCheckThreshold specifies how many units of work to do between self-preemption checks in gcDrain. Assuming a scan rate of 1 MB/ms, this is ~100 µs. Lower values have higher overhead in the scan loop (the scheduler check may perform a syscall, so its overhead is nontrivial). Higher values make the system less responsive to incoming work. 

### <a id="emptyOne" href="#emptyOne">const emptyOne</a>

```
searchKey: runtime.emptyOne
tags: [constant number private]
```

```Go
const emptyOne = 1 // this cell is empty

```

### <a id="emptyRest" href="#emptyRest">const emptyRest</a>

```
searchKey: runtime.emptyRest
tags: [constant number private]
```

```Go
const emptyRest // this cell is empty, and there are no more non-empty cells at higher indexes or overflows.
 = ...
```

Possible tophash values. We reserve a few possibilities for special marks. Each bucket (including its overflow buckets, if any) will have either all or none of its entries in the evacuated* states (except during the evacuate() method, which only happens during map writes and thus no one else can observe the map during that time). 

### <a id="evacuatedEmpty" href="#evacuatedEmpty">const evacuatedEmpty</a>

```
searchKey: runtime.evacuatedEmpty
tags: [constant number private]
```

```Go
const evacuatedEmpty = 4 // cell is empty, bucket is evacuated.

```

### <a id="evacuatedX" href="#evacuatedX">const evacuatedX</a>

```
searchKey: runtime.evacuatedX
tags: [constant number private]
```

```Go
const evacuatedX = 2 // key/elem is valid.  Entry has been evacuated to first half of larger table.

```

### <a id="evacuatedY" href="#evacuatedY">const evacuatedY</a>

```
searchKey: runtime.evacuatedY
tags: [constant number private]
```

```Go
const evacuatedY = 3 // same as above, but evacuated to second half of larger table.

```

### <a id="expbits32" href="#expbits32">const expbits32</a>

```
searchKey: runtime.expbits32
tags: [constant number private]
```

```Go
const expbits32 uint = 8
```

### <a id="expbits64" href="#expbits64">const expbits64</a>

```
searchKey: runtime.expbits64
tags: [constant number private]
```

```Go
const expbits64 uint = 11
```

### <a id="fInf" href="#fInf">const fInf</a>

```
searchKey: runtime.fInf
tags: [constant number private]
```

```Go
const fInf = 0x7FF0000000000000
```

### <a id="fNegInf" href="#fNegInf">const fNegInf</a>

```
searchKey: runtime.fNegInf
tags: [constant number private]
```

```Go
const fNegInf = 0xFFF0000000000000
```

### <a id="fastlogNumBits" href="#fastlogNumBits">const fastlogNumBits</a>

```
searchKey: runtime.fastlogNumBits
tags: [constant number private]
```

```Go
const fastlogNumBits = 5
```

### <a id="fieldKindEface" href="#fieldKindEface">const fieldKindEface</a>

```
searchKey: runtime.fieldKindEface
tags: [constant number private]
```

```Go
const fieldKindEface = 3
```

### <a id="fieldKindEol" href="#fieldKindEol">const fieldKindEol</a>

```
searchKey: runtime.fieldKindEol
tags: [constant number private]
```

```Go
const fieldKindEol = 0
```

### <a id="fieldKindIface" href="#fieldKindIface">const fieldKindIface</a>

```
searchKey: runtime.fieldKindIface
tags: [constant number private]
```

```Go
const fieldKindIface = 2
```

### <a id="fieldKindPtr" href="#fieldKindPtr">const fieldKindPtr</a>

```
searchKey: runtime.fieldKindPtr
tags: [constant number private]
```

```Go
const fieldKindPtr = 1
```

### <a id="fixedRootCount" href="#fixedRootCount">const fixedRootCount</a>

```
searchKey: runtime.fixedRootCount
tags: [constant number private]
```

```Go
const fixedRootCount
```

### <a id="fixedRootFinalizers" href="#fixedRootFinalizers">const fixedRootFinalizers</a>

```
searchKey: runtime.fixedRootFinalizers
tags: [constant number private]
```

```Go
const fixedRootFinalizers = iota
```

### <a id="fixedRootFreeGStacks" href="#fixedRootFreeGStacks">const fixedRootFreeGStacks</a>

```
searchKey: runtime.fixedRootFreeGStacks
tags: [constant number private]
```

```Go
const fixedRootFreeGStacks
```

### <a id="forcePreemptNS" href="#forcePreemptNS">const forcePreemptNS</a>

```
searchKey: runtime.forcePreemptNS
tags: [constant number private]
```

```Go
const forcePreemptNS = 10 * 1000 * 1000 // 10ms

```

forcePreemptNS is the time slice given to a G before it is preempted. 

### <a id="framepointer_enabled" href="#framepointer_enabled">const framepointer_enabled</a>

```
searchKey: runtime.framepointer_enabled
tags: [constant boolean private]
```

```Go
const framepointer_enabled = GOARCH == "amd64" || GOARCH == "arm64"
```

Must agree with internal/buildcfg.Experiment.FramePointer. 

### <a id="freeChunkSum" href="#freeChunkSum">const freeChunkSum</a>

```
searchKey: runtime.freeChunkSum
tags: [constant number private]
```

```Go
const freeChunkSum = ...
```

### <a id="freezeStopWait" href="#freezeStopWait">const freezeStopWait</a>

```
searchKey: runtime.freezeStopWait
tags: [constant number private]
```

```Go
const freezeStopWait = 0x7fffffff
```

freezeStopWait is a large value that freezetheworld sets sched.stopwait to in order to request that all Gs permanently stop. 

### <a id="funcFlag_SPWRITE" href="#funcFlag_SPWRITE">const funcFlag_SPWRITE</a>

```
searchKey: runtime.funcFlag_SPWRITE
tags: [constant number private]
```

```Go
const funcFlag_SPWRITE
```

SPWRITE indicates a function that writes an arbitrary value to SP (any write other than adding or subtracting a constant amount). The traceback routines cannot encode such changes into the pcsp tables, so the function traceback cannot safely unwind past SPWRITE functions. Stopping at an SPWRITE function is considered to be an incomplete unwinding of the stack. In certain contexts (in particular garbage collector stack scans) that is a fatal error. 

### <a id="funcFlag_TOPFRAME" href="#funcFlag_TOPFRAME">const funcFlag_TOPFRAME</a>

```
searchKey: runtime.funcFlag_TOPFRAME
tags: [constant number private]
```

```Go
const funcFlag_TOPFRAME funcFlag = 1 << iota
```

TOPFRAME indicates a function that appears at the top of its stack. The traceback routine stop at such a function and consider that a successful, complete traversal of the stack. Examples of TOPFRAME functions include goexit, which appears at the top of a user goroutine stack, and mstart, which appears at the top of a system goroutine stack. 

### <a id="funcID_abort" href="#funcID_abort">const funcID_abort</a>

```
searchKey: runtime.funcID_abort
tags: [constant number private]
```

```Go
const funcID_abort
```

### <a id="funcID_asmcgocall" href="#funcID_asmcgocall">const funcID_asmcgocall</a>

```
searchKey: runtime.funcID_asmcgocall
tags: [constant number private]
```

```Go
const funcID_asmcgocall
```

### <a id="funcID_asyncPreempt" href="#funcID_asyncPreempt">const funcID_asyncPreempt</a>

```
searchKey: runtime.funcID_asyncPreempt
tags: [constant number private]
```

```Go
const funcID_asyncPreempt
```

### <a id="funcID_cgocallback" href="#funcID_cgocallback">const funcID_cgocallback</a>

```
searchKey: runtime.funcID_cgocallback
tags: [constant number private]
```

```Go
const funcID_cgocallback
```

### <a id="funcID_debugCallV2" href="#funcID_debugCallV2">const funcID_debugCallV2</a>

```
searchKey: runtime.funcID_debugCallV2
tags: [constant number private]
```

```Go
const funcID_debugCallV2
```

### <a id="funcID_gcBgMarkWorker" href="#funcID_gcBgMarkWorker">const funcID_gcBgMarkWorker</a>

```
searchKey: runtime.funcID_gcBgMarkWorker
tags: [constant number private]
```

```Go
const funcID_gcBgMarkWorker
```

### <a id="funcID_goexit" href="#funcID_goexit">const funcID_goexit</a>

```
searchKey: runtime.funcID_goexit
tags: [constant number private]
```

```Go
const funcID_goexit
```

### <a id="funcID_gogo" href="#funcID_gogo">const funcID_gogo</a>

```
searchKey: runtime.funcID_gogo
tags: [constant number private]
```

```Go
const funcID_gogo
```

### <a id="funcID_gopanic" href="#funcID_gopanic">const funcID_gopanic</a>

```
searchKey: runtime.funcID_gopanic
tags: [constant number private]
```

```Go
const funcID_gopanic
```

### <a id="funcID_handleAsyncEvent" href="#funcID_handleAsyncEvent">const funcID_handleAsyncEvent</a>

```
searchKey: runtime.funcID_handleAsyncEvent
tags: [constant number private]
```

```Go
const funcID_handleAsyncEvent
```

### <a id="funcID_jmpdefer" href="#funcID_jmpdefer">const funcID_jmpdefer</a>

```
searchKey: runtime.funcID_jmpdefer
tags: [constant number private]
```

```Go
const funcID_jmpdefer
```

### <a id="funcID_mcall" href="#funcID_mcall">const funcID_mcall</a>

```
searchKey: runtime.funcID_mcall
tags: [constant number private]
```

```Go
const funcID_mcall
```

### <a id="funcID_morestack" href="#funcID_morestack">const funcID_morestack</a>

```
searchKey: runtime.funcID_morestack
tags: [constant number private]
```

```Go
const funcID_morestack
```

### <a id="funcID_mstart" href="#funcID_mstart">const funcID_mstart</a>

```
searchKey: runtime.funcID_mstart
tags: [constant number private]
```

```Go
const funcID_mstart
```

### <a id="funcID_normal" href="#funcID_normal">const funcID_normal</a>

```
searchKey: runtime.funcID_normal
tags: [constant number private]
```

```Go
const funcID_normal funcID = iota // not a special function

```

### <a id="funcID_panicwrap" href="#funcID_panicwrap">const funcID_panicwrap</a>

```
searchKey: runtime.funcID_panicwrap
tags: [constant number private]
```

```Go
const funcID_panicwrap
```

### <a id="funcID_rt0_go" href="#funcID_rt0_go">const funcID_rt0_go</a>

```
searchKey: runtime.funcID_rt0_go
tags: [constant number private]
```

```Go
const funcID_rt0_go
```

### <a id="funcID_runfinq" href="#funcID_runfinq">const funcID_runfinq</a>

```
searchKey: runtime.funcID_runfinq
tags: [constant number private]
```

```Go
const funcID_runfinq
```

### <a id="funcID_runtime_main" href="#funcID_runtime_main">const funcID_runtime_main</a>

```
searchKey: runtime.funcID_runtime_main
tags: [constant number private]
```

```Go
const funcID_runtime_main
```

### <a id="funcID_sigpanic" href="#funcID_sigpanic">const funcID_sigpanic</a>

```
searchKey: runtime.funcID_sigpanic
tags: [constant number private]
```

```Go
const funcID_sigpanic
```

### <a id="funcID_systemstack" href="#funcID_systemstack">const funcID_systemstack</a>

```
searchKey: runtime.funcID_systemstack
tags: [constant number private]
```

```Go
const funcID_systemstack
```

### <a id="funcID_systemstack_switch" href="#funcID_systemstack_switch">const funcID_systemstack_switch</a>

```
searchKey: runtime.funcID_systemstack_switch
tags: [constant number private]
```

```Go
const funcID_systemstack_switch
```

### <a id="funcID_wrapper" href="#funcID_wrapper">const funcID_wrapper</a>

```
searchKey: runtime.funcID_wrapper
tags: [constant number private]
```

```Go
const funcID_wrapper // any autogenerated code (hash/eq algorithms, method wrappers, etc.)

```

### <a id="gTrackingPeriod" href="#gTrackingPeriod">const gTrackingPeriod</a>

```
searchKey: runtime.gTrackingPeriod
tags: [constant number private]
```

```Go
const gTrackingPeriod = 8
```

gTrackingPeriod is the number of transitions out of _Grunning between latency tracking runs. 

### <a id="gcAssistTimeSlack" href="#gcAssistTimeSlack">const gcAssistTimeSlack</a>

```
searchKey: runtime.gcAssistTimeSlack
tags: [constant number private]
```

```Go
const gcAssistTimeSlack = 5000
```

gcAssistTimeSlack is the nanoseconds of mutator assist time that can accumulate on a P before updating gcController.assistTime. 

### <a id="gcBackgroundMode" href="#gcBackgroundMode">const gcBackgroundMode</a>

```
searchKey: runtime.gcBackgroundMode
tags: [constant number private]
```

```Go
const gcBackgroundMode gcMode = iota // concurrent GC and sweep

```

### <a id="gcBackgroundUtilization" href="#gcBackgroundUtilization">const gcBackgroundUtilization</a>

```
searchKey: runtime.gcBackgroundUtilization
tags: [constant number private]
```

```Go
const gcBackgroundUtilization = 0.25
```

gcBackgroundUtilization is the fixed CPU utilization for background marking. It must be <= gcGoalUtilization. The difference between gcGoalUtilization and gcBackgroundUtilization will be made up by mark assists. The scheduler will aim to use within 50% of this goal. 

Setting this to < gcGoalUtilization avoids saturating the trigger feedback controller when there are no assists, which allows it to better control CPU and heap growth. However, the larger the gap, the more mutator assists are expected to happen, which impact mutator latency. 

### <a id="gcBitsChunkBytes" href="#gcBitsChunkBytes">const gcBitsChunkBytes</a>

```
searchKey: runtime.gcBitsChunkBytes
tags: [constant number private]
```

```Go
const gcBitsChunkBytes = uintptr(64 << 10)
```

### <a id="gcBitsHeaderBytes" href="#gcBitsHeaderBytes">const gcBitsHeaderBytes</a>

```
searchKey: runtime.gcBitsHeaderBytes
tags: [constant number private]
```

```Go
const gcBitsHeaderBytes = unsafe.Sizeof(gcBitsHeader{})
```

### <a id="gcCreditSlack" href="#gcCreditSlack">const gcCreditSlack</a>

```
searchKey: runtime.gcCreditSlack
tags: [constant number private]
```

```Go
const gcCreditSlack = 2000
```

gcCreditSlack is the amount of scan work credit that can accumulate locally before updating gcController.scanWork and, optionally, gcController.bgScanCredit. Lower values give a more accurate assist ratio and make it more likely that assists will successfully steal background credit. Higher values reduce memory contention. 

### <a id="gcDrainFlushBgCredit" href="#gcDrainFlushBgCredit">const gcDrainFlushBgCredit</a>

```
searchKey: runtime.gcDrainFlushBgCredit
tags: [constant number private]
```

```Go
const gcDrainFlushBgCredit
```

### <a id="gcDrainFractional" href="#gcDrainFractional">const gcDrainFractional</a>

```
searchKey: runtime.gcDrainFractional
tags: [constant number private]
```

```Go
const gcDrainFractional
```

### <a id="gcDrainIdle" href="#gcDrainIdle">const gcDrainIdle</a>

```
searchKey: runtime.gcDrainIdle
tags: [constant number private]
```

```Go
const gcDrainIdle
```

### <a id="gcDrainUntilPreempt" href="#gcDrainUntilPreempt">const gcDrainUntilPreempt</a>

```
searchKey: runtime.gcDrainUntilPreempt
tags: [constant number private]
```

```Go
const gcDrainUntilPreempt gcDrainFlags = 1 << iota
```

### <a id="gcForceBlockMode" href="#gcForceBlockMode">const gcForceBlockMode</a>

```
searchKey: runtime.gcForceBlockMode
tags: [constant number private]
```

```Go
const gcForceBlockMode // stop-the-world GC now and STW sweep (forced by user)

```

### <a id="gcForceMode" href="#gcForceMode">const gcForceMode</a>

```
searchKey: runtime.gcForceMode
tags: [constant number private]
```

```Go
const gcForceMode // stop-the-world GC now, concurrent sweep

```

### <a id="gcGoalUtilization" href="#gcGoalUtilization">const gcGoalUtilization</a>

```
searchKey: runtime.gcGoalUtilization
tags: [constant number private]
```

```Go
const gcGoalUtilization = 0.30
```

gcGoalUtilization is the goal CPU utilization for marking as a fraction of GOMAXPROCS. 

### <a id="gcMarkWorkerDedicatedMode" href="#gcMarkWorkerDedicatedMode">const gcMarkWorkerDedicatedMode</a>

```
searchKey: runtime.gcMarkWorkerDedicatedMode
tags: [constant number private]
```

```Go
const gcMarkWorkerDedicatedMode
```

gcMarkWorkerDedicatedMode indicates that the P of a mark worker is dedicated to running that mark worker. The mark worker should run without preemption. 

### <a id="gcMarkWorkerFractionalMode" href="#gcMarkWorkerFractionalMode">const gcMarkWorkerFractionalMode</a>

```
searchKey: runtime.gcMarkWorkerFractionalMode
tags: [constant number private]
```

```Go
const gcMarkWorkerFractionalMode
```

gcMarkWorkerFractionalMode indicates that a P is currently running the "fractional" mark worker. The fractional worker is necessary when GOMAXPROCS*gcBackgroundUtilization is not an integer and using only dedicated workers would result in utilization too far from the target of gcBackgroundUtilization. The fractional worker should run until it is preempted and will be scheduled to pick up the fractional part of GOMAXPROCS*gcBackgroundUtilization. 

### <a id="gcMarkWorkerIdleMode" href="#gcMarkWorkerIdleMode">const gcMarkWorkerIdleMode</a>

```
searchKey: runtime.gcMarkWorkerIdleMode
tags: [constant number private]
```

```Go
const gcMarkWorkerIdleMode
```

gcMarkWorkerIdleMode indicates that a P is running the mark worker because it has nothing else to do. The idle worker should run until it is preempted and account its time against gcController.idleMarkTime. 

### <a id="gcMarkWorkerNotWorker" href="#gcMarkWorkerNotWorker">const gcMarkWorkerNotWorker</a>

```
searchKey: runtime.gcMarkWorkerNotWorker
tags: [constant number private]
```

```Go
const gcMarkWorkerNotWorker gcMarkWorkerMode = iota
```

gcMarkWorkerNotWorker indicates that the next scheduled G is not starting work and the mode should be ignored. 

### <a id="gcOverAssistWork" href="#gcOverAssistWork">const gcOverAssistWork</a>

```
searchKey: runtime.gcOverAssistWork
tags: [constant number private]
```

```Go
const gcOverAssistWork = 64 << 10
```

gcOverAssistWork determines how many extra units of scan work a GC assist does when an assist happens. This amortizes the cost of an assist by pre-paying for this many bytes of future allocations. 

### <a id="gcTriggerCycle" href="#gcTriggerCycle">const gcTriggerCycle</a>

```
searchKey: runtime.gcTriggerCycle
tags: [constant number private]
```

```Go
const gcTriggerCycle
```

gcTriggerCycle indicates that a cycle should be started if we have not yet started cycle number gcTrigger.n (relative to work.cycles). 

### <a id="gcTriggerHeap" href="#gcTriggerHeap">const gcTriggerHeap</a>

```
searchKey: runtime.gcTriggerHeap
tags: [constant number private]
```

```Go
const gcTriggerHeap gcTriggerKind = iota
```

gcTriggerHeap indicates that a cycle should be started when the heap size reaches the trigger heap size computed by the controller. 

### <a id="gcTriggerTime" href="#gcTriggerTime">const gcTriggerTime</a>

```
searchKey: runtime.gcTriggerTime
tags: [constant number private]
```

```Go
const gcTriggerTime
```

gcTriggerTime indicates that a cycle should be started when it's been more than forcegcperiod nanoseconds since the previous GC cycle. 

### <a id="hashRandomBytes" href="#hashRandomBytes">const hashRandomBytes</a>

```
searchKey: runtime.hashRandomBytes
tags: [constant number private]
```

```Go
const hashRandomBytes = sys.PtrSize / 4 * 64
```

### <a id="hashWriting" href="#hashWriting">const hashWriting</a>

```
searchKey: runtime.hashWriting
tags: [constant number private]
```

```Go
const hashWriting = 4 // a goroutine is writing to the map

```

### <a id="hchanSize" href="#hchanSize">const hchanSize</a>

```
searchKey: runtime.hchanSize
tags: [constant number private]
```

```Go
const hchanSize = unsafe.Sizeof(hchan{}) + uintptr(-int(unsafe.Sizeof(hchan{}))&(maxAlign-1))
```

### <a id="heapAddrBits" href="#heapAddrBits">const heapAddrBits</a>

```
searchKey: runtime.heapAddrBits
tags: [constant number private]
```

```Go
const heapAddrBits = ...
```

heapAddrBits is the number of bits in a heap address. On amd64, addresses are sign-extended beyond heapAddrBits. On other arches, they are zero-extended. 

On most 64-bit platforms, we limit this to 48 bits based on a combination of hardware and OS limitations. 

amd64 hardware limits addresses to 48 bits, sign-extended to 64 bits. Addresses where the top 16 bits are not either all 0 or all 1 are "non-canonical" and invalid. Because of these "negative" addresses, we offset addresses by 1<<47 (arenaBaseOffset) on amd64 before computing indexes into the heap arenas index. In 2017, amd64 hardware added support for 57 bit addresses; however, currently only Linux supports this extension and the kernel will never choose an address above 1<<47 unless mmap is called with a hint address above 1<<47 (which we never do). 

arm64 hardware (as of ARMv8) limits user addresses to 48 bits, in the range [0, 1<<48). 

ppc64, mips64, and s390x support arbitrary 64 bit addresses in hardware. On Linux, Go leans on stricter OS limits. Based on Linux's processor.h, the user address space is limited as follows on 64-bit architectures: 

Architecture  Name              Maximum Value (exclusive) --------------------------------------------------------------------- amd64         TASK_SIZE_MAX     0x007ffffffff000 (47 bit addresses) arm64         TASK_SIZE_64      0x01000000000000 (48 bit addresses) ppc64{,le}    TASK_SIZE_USER64  0x00400000000000 (46 bit addresses) mips64{,le}   TASK_SIZE64       0x00010000000000 (40 bit addresses) s390x         TASK_SIZE         1<<64 (64 bit addresses) 

These limits may increase over time, but are currently at most 48 bits except on s390x. On all architectures, Linux starts placing mmap'd regions at addresses that are significantly below 48 bits, so even if it's possible to exceed Go's 48 bit limit, it's extremely unlikely in practice. 

On 32-bit platforms, we accept the full 32-bit address space because doing so is cheap. mips32 only has access to the low 2GB of virtual memory, so we further limit it to 31 bits. 

On ios/arm64, although 64-bit pointers are presumably available, pointers are truncated to 33 bits. Furthermore, only the top 4 GiB of the address space are actually available to the application, but we allow the whole 33 bits anyway for simplicity. TODO(mknyszek): Consider limiting it to 32 bits and using arenaBaseOffset to offset into the top 4 GiB. 

WebAssembly currently has a limit of 4GB linear memory. 

### <a id="heapArenaBitmapBytes" href="#heapArenaBitmapBytes">const heapArenaBitmapBytes</a>

```
searchKey: runtime.heapArenaBitmapBytes
tags: [constant number private]
```

```Go
const heapArenaBitmapBytes = heapArenaBytes / (sys.PtrSize * 8 / 2)
```

heapArenaBitmapBytes is the size of each heap arena's bitmap. 

### <a id="heapArenaBytes" href="#heapArenaBytes">const heapArenaBytes</a>

```
searchKey: runtime.heapArenaBytes
tags: [constant number private]
```

```Go
const heapArenaBytes = 1 << logHeapArenaBytes
```

heapArenaBytes is the size of a heap arena. The heap consists of mappings of size heapArenaBytes, aligned to heapArenaBytes. The initial heap mapping is one arena. 

This is currently 64MB on 64-bit non-Windows and 4MB on 32-bit and on Windows. We use smaller arenas on Windows because all committed memory is charged to the process, even if it's not touched. Hence, for processes with small heaps, the mapped arena space needs to be commensurate. This is particularly important with the race detector, since it significantly amplifies the cost of committed memory. 

### <a id="heapBitsShift" href="#heapBitsShift">const heapBitsShift</a>

```
searchKey: runtime.heapBitsShift
tags: [constant number private]
```

```Go
const heapBitsShift = 1 // shift offset between successive bitPointer or bitScan entries

```

### <a id="heapStatsDep" href="#heapStatsDep">const heapStatsDep</a>

```
searchKey: runtime.heapStatsDep
tags: [constant number private]
```

```Go
const heapStatsDep statDep = iota // corresponds to heapStatsAggregate

```

### <a id="hicb" href="#hicb">const hicb</a>

```
searchKey: runtime.hicb
tags: [constant number private]
```

```Go
const hicb = 0xBF // 1011 1111

```

### <a id="inf32" href="#inf32">const inf32</a>

```
searchKey: runtime.inf32
tags: [constant number private]
```

```Go
const inf32 uint32 = (1<<expbits32 - 1) << mantbits32
```

### <a id="inf64" href="#inf64">const inf64</a>

```
searchKey: runtime.inf64
tags: [constant number private]
```

```Go
const inf64 uint64 = (1<<expbits64 - 1) << mantbits64
```

### <a id="itabInitSize" href="#itabInitSize">const itabInitSize</a>

```
searchKey: runtime.itabInitSize
tags: [constant number private]
```

```Go
const itabInitSize = 512
```

### <a id="iterator" href="#iterator">const iterator</a>

```
searchKey: runtime.iterator
tags: [constant number private]
```

```Go
const iterator = 1 // there may be an iterator using buckets

```

flags 

### <a id="kindArray" href="#kindArray">const kindArray</a>

```
searchKey: runtime.kindArray
tags: [constant number private]
```

```Go
const kindArray
```

### <a id="kindBool" href="#kindBool">const kindBool</a>

```
searchKey: runtime.kindBool
tags: [constant number private]
```

```Go
const kindBool = 1 + iota
```

### <a id="kindChan" href="#kindChan">const kindChan</a>

```
searchKey: runtime.kindChan
tags: [constant number private]
```

```Go
const kindChan
```

### <a id="kindComplex128" href="#kindComplex128">const kindComplex128</a>

```
searchKey: runtime.kindComplex128
tags: [constant number private]
```

```Go
const kindComplex128
```

### <a id="kindComplex64" href="#kindComplex64">const kindComplex64</a>

```
searchKey: runtime.kindComplex64
tags: [constant number private]
```

```Go
const kindComplex64
```

### <a id="kindDirectIface" href="#kindDirectIface">const kindDirectIface</a>

```
searchKey: runtime.kindDirectIface
tags: [constant number private]
```

```Go
const kindDirectIface = 1 << 5
```

### <a id="kindFloat32" href="#kindFloat32">const kindFloat32</a>

```
searchKey: runtime.kindFloat32
tags: [constant number private]
```

```Go
const kindFloat32
```

### <a id="kindFloat64" href="#kindFloat64">const kindFloat64</a>

```
searchKey: runtime.kindFloat64
tags: [constant number private]
```

```Go
const kindFloat64
```

### <a id="kindFunc" href="#kindFunc">const kindFunc</a>

```
searchKey: runtime.kindFunc
tags: [constant number private]
```

```Go
const kindFunc
```

### <a id="kindGCProg" href="#kindGCProg">const kindGCProg</a>

```
searchKey: runtime.kindGCProg
tags: [constant number private]
```

```Go
const kindGCProg = 1 << 6
```

### <a id="kindInt" href="#kindInt">const kindInt</a>

```
searchKey: runtime.kindInt
tags: [constant number private]
```

```Go
const kindInt
```

### <a id="kindInt16" href="#kindInt16">const kindInt16</a>

```
searchKey: runtime.kindInt16
tags: [constant number private]
```

```Go
const kindInt16
```

### <a id="kindInt32" href="#kindInt32">const kindInt32</a>

```
searchKey: runtime.kindInt32
tags: [constant number private]
```

```Go
const kindInt32
```

### <a id="kindInt64" href="#kindInt64">const kindInt64</a>

```
searchKey: runtime.kindInt64
tags: [constant number private]
```

```Go
const kindInt64
```

### <a id="kindInt8" href="#kindInt8">const kindInt8</a>

```
searchKey: runtime.kindInt8
tags: [constant number private]
```

```Go
const kindInt8
```

### <a id="kindInterface" href="#kindInterface">const kindInterface</a>

```
searchKey: runtime.kindInterface
tags: [constant number private]
```

```Go
const kindInterface
```

### <a id="kindMap" href="#kindMap">const kindMap</a>

```
searchKey: runtime.kindMap
tags: [constant number private]
```

```Go
const kindMap
```

### <a id="kindMask" href="#kindMask">const kindMask</a>

```
searchKey: runtime.kindMask
tags: [constant number private]
```

```Go
const kindMask = (1 << 5) - 1
```

### <a id="kindPtr" href="#kindPtr">const kindPtr</a>

```
searchKey: runtime.kindPtr
tags: [constant number private]
```

```Go
const kindPtr
```

### <a id="kindSlice" href="#kindSlice">const kindSlice</a>

```
searchKey: runtime.kindSlice
tags: [constant number private]
```

```Go
const kindSlice
```

### <a id="kindString" href="#kindString">const kindString</a>

```
searchKey: runtime.kindString
tags: [constant number private]
```

```Go
const kindString
```

### <a id="kindStruct" href="#kindStruct">const kindStruct</a>

```
searchKey: runtime.kindStruct
tags: [constant number private]
```

```Go
const kindStruct
```

### <a id="kindUint" href="#kindUint">const kindUint</a>

```
searchKey: runtime.kindUint
tags: [constant number private]
```

```Go
const kindUint
```

### <a id="kindUint16" href="#kindUint16">const kindUint16</a>

```
searchKey: runtime.kindUint16
tags: [constant number private]
```

```Go
const kindUint16
```

### <a id="kindUint32" href="#kindUint32">const kindUint32</a>

```
searchKey: runtime.kindUint32
tags: [constant number private]
```

```Go
const kindUint32
```

### <a id="kindUint64" href="#kindUint64">const kindUint64</a>

```
searchKey: runtime.kindUint64
tags: [constant number private]
```

```Go
const kindUint64
```

### <a id="kindUint8" href="#kindUint8">const kindUint8</a>

```
searchKey: runtime.kindUint8
tags: [constant number private]
```

```Go
const kindUint8
```

### <a id="kindUintptr" href="#kindUintptr">const kindUintptr</a>

```
searchKey: runtime.kindUintptr
tags: [constant number private]
```

```Go
const kindUintptr
```

### <a id="kindUnsafePointer" href="#kindUnsafePointer">const kindUnsafePointer</a>

```
searchKey: runtime.kindUnsafePointer
tags: [constant number private]
```

```Go
const kindUnsafePointer
```

### <a id="largeSizeDiv" href="#largeSizeDiv">const largeSizeDiv</a>

```
searchKey: runtime.largeSizeDiv
tags: [constant number private]
```

```Go
const largeSizeDiv = 128
```

### <a id="loadFactorDen" href="#loadFactorDen">const loadFactorDen</a>

```
searchKey: runtime.loadFactorDen
tags: [constant number private]
```

```Go
const loadFactorDen = 2
```

### <a id="loadFactorNum" href="#loadFactorNum">const loadFactorNum</a>

```
searchKey: runtime.loadFactorNum
tags: [constant number private]
```

```Go
const loadFactorNum = 13
```

Maximum average load of a bucket that triggers growth is 6.5. Represent as loadFactorNum/loadFactorDen, to allow integer math. 

### <a id="locb" href="#locb">const locb</a>

```
searchKey: runtime.locb
tags: [constant number private]
```

```Go
const locb = 0x80 // 1000 0000

```

The default lowest and highest continuation byte. 

### <a id="lockRankAllg" href="#lockRankAllg">const lockRankAllg</a>

```
searchKey: runtime.lockRankAllg
tags: [constant number private]
```

```Go
const lockRankAllg
```

Constants representing the lock rank of the architecture-independent locks in the runtime. Locks with lower rank must be taken before locks with higher rank. 

### <a id="lockRankAllp" href="#lockRankAllp">const lockRankAllp</a>

```
searchKey: runtime.lockRankAllp
tags: [constant number private]
```

```Go
const lockRankAllp
```

Constants representing the lock rank of the architecture-independent locks in the runtime. Locks with lower rank must be taken before locks with higher rank. 

### <a id="lockRankAssistQueue" href="#lockRankAssistQueue">const lockRankAssistQueue</a>

```
searchKey: runtime.lockRankAssistQueue
tags: [constant number private]
```

```Go
const lockRankAssistQueue
```

Constants representing the lock rank of the architecture-independent locks in the runtime. Locks with lower rank must be taken before locks with higher rank. 

### <a id="lockRankCpuprof" href="#lockRankCpuprof">const lockRankCpuprof</a>

```
searchKey: runtime.lockRankCpuprof
tags: [constant number private]
```

```Go
const lockRankCpuprof
```

Constants representing the lock rank of the architecture-independent locks in the runtime. Locks with lower rank must be taken before locks with higher rank. 

### <a id="lockRankDeadlock" href="#lockRankDeadlock">const lockRankDeadlock</a>

```
searchKey: runtime.lockRankDeadlock
tags: [constant number private]
```

```Go
const lockRankDeadlock
```

Constants representing the lock rank of the architecture-independent locks in the runtime. Locks with lower rank must be taken before locks with higher rank. 

### <a id="lockRankDebug" href="#lockRankDebug">const lockRankDebug</a>

```
searchKey: runtime.lockRankDebug
tags: [constant number private]
```

```Go
const lockRankDebug
```

Constants representing the lock rank of the architecture-independent locks in the runtime. Locks with lower rank must be taken before locks with higher rank. 

### <a id="lockRankDebugPtrmask" href="#lockRankDebugPtrmask">const lockRankDebugPtrmask</a>

```
searchKey: runtime.lockRankDebugPtrmask
tags: [constant number private]
```

```Go
const lockRankDebugPtrmask
```

Constants representing the lock rank of the architecture-independent locks in the runtime. Locks with lower rank must be taken before locks with higher rank. 

### <a id="lockRankDefer" href="#lockRankDefer">const lockRankDefer</a>

```
searchKey: runtime.lockRankDefer
tags: [constant number private]
```

```Go
const lockRankDefer
```

Constants representing the lock rank of the architecture-independent locks in the runtime. Locks with lower rank must be taken before locks with higher rank. 

### <a id="lockRankDummy" href="#lockRankDummy">const lockRankDummy</a>

```
searchKey: runtime.lockRankDummy
tags: [constant number private]
```

```Go
const lockRankDummy lockRank = iota
```

Constants representing the lock rank of the architecture-independent locks in the runtime. Locks with lower rank must be taken before locks with higher rank. 

### <a id="lockRankFaketimeState" href="#lockRankFaketimeState">const lockRankFaketimeState</a>

```
searchKey: runtime.lockRankFaketimeState
tags: [constant number private]
```

```Go
const lockRankFaketimeState
```

Constants representing the lock rank of the architecture-independent locks in the runtime. Locks with lower rank must be taken before locks with higher rank. 

### <a id="lockRankFin" href="#lockRankFin">const lockRankFin</a>

```
searchKey: runtime.lockRankFin
tags: [constant number private]
```

```Go
const lockRankFin
```

Constants representing the lock rank of the architecture-independent locks in the runtime. Locks with lower rank must be taken before locks with higher rank. 

### <a id="lockRankForcegc" href="#lockRankForcegc">const lockRankForcegc</a>

```
searchKey: runtime.lockRankForcegc
tags: [constant number private]
```

```Go
const lockRankForcegc
```

Constants representing the lock rank of the architecture-independent locks in the runtime. Locks with lower rank must be taken before locks with higher rank. 

### <a id="lockRankGFree" href="#lockRankGFree">const lockRankGFree</a>

```
searchKey: runtime.lockRankGFree
tags: [constant number private]
```

```Go
const lockRankGFree
```

Constants representing the lock rank of the architecture-independent locks in the runtime. Locks with lower rank must be taken before locks with higher rank. 

Other leaf locks 

### <a id="lockRankGcBitsArenas" href="#lockRankGcBitsArenas">const lockRankGcBitsArenas</a>

```
searchKey: runtime.lockRankGcBitsArenas
tags: [constant number private]
```

```Go
const lockRankGcBitsArenas
```

Constants representing the lock rank of the architecture-independent locks in the runtime. Locks with lower rank must be taken before locks with higher rank. 

### <a id="lockRankGlobalAlloc" href="#lockRankGlobalAlloc">const lockRankGlobalAlloc</a>

```
searchKey: runtime.lockRankGlobalAlloc
tags: [constant number private]
```

```Go
const lockRankGlobalAlloc
```

Constants representing the lock rank of the architecture-independent locks in the runtime. Locks with lower rank must be taken before locks with higher rank. 

Memory-related leaf locks 

### <a id="lockRankGscan" href="#lockRankGscan">const lockRankGscan</a>

```
searchKey: runtime.lockRankGscan
tags: [constant number private]
```

```Go
const lockRankGscan
```

Constants representing the lock rank of the architecture-independent locks in the runtime. Locks with lower rank must be taken before locks with higher rank. 

### <a id="lockRankHchan" href="#lockRankHchan">const lockRankHchan</a>

```
searchKey: runtime.lockRankHchan
tags: [constant number private]
```

```Go
const lockRankHchan // Multiple hchans acquired in lock order in syncadjustsudogs()

```

Constants representing the lock rank of the architecture-independent locks in the runtime. Locks with lower rank must be taken before locks with higher rank. 

### <a id="lockRankHchanLeaf" href="#lockRankHchanLeaf">const lockRankHchanLeaf</a>

```
searchKey: runtime.lockRankHchanLeaf
tags: [constant number private]
```

```Go
const lockRankHchanLeaf
```

Constants representing the lock rank of the architecture-independent locks in the runtime. Locks with lower rank must be taken before locks with higher rank. 

Generally, hchan must be acquired before gscan. But in one specific case (in syncadjustsudogs from markroot after the g has been suspended by suspendG), we allow gscan to be acquired, and then an hchan lock. To allow this case, we get this lockRankHchanLeaf rank in syncadjustsudogs(), rather than lockRankHchan. By using this special rank, we don't allow any further locks to be acquired other than more hchan locks. 

### <a id="lockRankItab" href="#lockRankItab">const lockRankItab</a>

```
searchKey: runtime.lockRankItab
tags: [constant number private]
```

```Go
const lockRankItab
```

Constants representing the lock rank of the architecture-independent locks in the runtime. Locks with lower rank must be taken before locks with higher rank. 

### <a id="lockRankLeafRank" href="#lockRankLeafRank">const lockRankLeafRank</a>

```
searchKey: runtime.lockRankLeafRank
tags: [constant number private]
```

```Go
const lockRankLeafRank lockRank = 1000
```

lockRankLeafRank is the rank of lock that does not have a declared rank, and hence is a leaf lock. 

### <a id="lockRankMheap" href="#lockRankMheap">const lockRankMheap</a>

```
searchKey: runtime.lockRankMheap
tags: [constant number private]
```

```Go
const lockRankMheap
```

Constants representing the lock rank of the architecture-independent locks in the runtime. Locks with lower rank must be taken before locks with higher rank. 

### <a id="lockRankMheapSpecial" href="#lockRankMheapSpecial">const lockRankMheapSpecial</a>

```
searchKey: runtime.lockRankMheapSpecial
tags: [constant number private]
```

```Go
const lockRankMheapSpecial
```

Constants representing the lock rank of the architecture-independent locks in the runtime. Locks with lower rank must be taken before locks with higher rank. 

### <a id="lockRankMspanSpecial" href="#lockRankMspanSpecial">const lockRankMspanSpecial</a>

```
searchKey: runtime.lockRankMspanSpecial
tags: [constant number private]
```

```Go
const lockRankMspanSpecial
```

Constants representing the lock rank of the architecture-independent locks in the runtime. Locks with lower rank must be taken before locks with higher rank. 

### <a id="lockRankNetpollInit" href="#lockRankNetpollInit">const lockRankNetpollInit</a>

```
searchKey: runtime.lockRankNetpollInit
tags: [constant number private]
```

```Go
const lockRankNetpollInit
```

Constants representing the lock rank of the architecture-independent locks in the runtime. Locks with lower rank must be taken before locks with higher rank. 

### <a id="lockRankNewmHandoff" href="#lockRankNewmHandoff">const lockRankNewmHandoff</a>

```
searchKey: runtime.lockRankNewmHandoff
tags: [constant number private]
```

```Go
const lockRankNewmHandoff
```

Constants representing the lock rank of the architecture-independent locks in the runtime. Locks with lower rank must be taken before locks with higher rank. 

Leaf locks with no dependencies, so these constants are not actually used anywhere. There are other architecture-dependent leaf locks as well. 

### <a id="lockRankNotifyList" href="#lockRankNotifyList">const lockRankNotifyList</a>

```
searchKey: runtime.lockRankNotifyList
tags: [constant number private]
```

```Go
const lockRankNotifyList
```

Constants representing the lock rank of the architecture-independent locks in the runtime. Locks with lower rank must be taken before locks with higher rank. 

### <a id="lockRankPanic" href="#lockRankPanic">const lockRankPanic</a>

```
searchKey: runtime.lockRankPanic
tags: [constant number private]
```

```Go
const lockRankPanic
```

Constants representing the lock rank of the architecture-independent locks in the runtime. Locks with lower rank must be taken before locks with higher rank. 

### <a id="lockRankPollCache" href="#lockRankPollCache">const lockRankPollCache</a>

```
searchKey: runtime.lockRankPollCache
tags: [constant number private]
```

```Go
const lockRankPollCache
```

Constants representing the lock rank of the architecture-independent locks in the runtime. Locks with lower rank must be taken before locks with higher rank. 

### <a id="lockRankPollDesc" href="#lockRankPollDesc">const lockRankPollDesc</a>

```
searchKey: runtime.lockRankPollDesc
tags: [constant number private]
```

```Go
const lockRankPollDesc
```

Constants representing the lock rank of the architecture-independent locks in the runtime. Locks with lower rank must be taken before locks with higher rank. 

### <a id="lockRankProf" href="#lockRankProf">const lockRankProf</a>

```
searchKey: runtime.lockRankProf
tags: [constant number private]
```

```Go
const lockRankProf
```

Constants representing the lock rank of the architecture-independent locks in the runtime. Locks with lower rank must be taken before locks with higher rank. 

### <a id="lockRankRaceFini" href="#lockRankRaceFini">const lockRankRaceFini</a>

```
searchKey: runtime.lockRankRaceFini
tags: [constant number private]
```

```Go
const lockRankRaceFini
```

Constants representing the lock rank of the architecture-independent locks in the runtime. Locks with lower rank must be taken before locks with higher rank. 

### <a id="lockRankReflectOffs" href="#lockRankReflectOffs">const lockRankReflectOffs</a>

```
searchKey: runtime.lockRankReflectOffs
tags: [constant number private]
```

```Go
const lockRankReflectOffs
```

Constants representing the lock rank of the architecture-independent locks in the runtime. Locks with lower rank must be taken before locks with higher rank. 

### <a id="lockRankRoot" href="#lockRankRoot">const lockRankRoot</a>

```
searchKey: runtime.lockRankRoot
tags: [constant number private]
```

```Go
const lockRankRoot
```

Constants representing the lock rank of the architecture-independent locks in the runtime. Locks with lower rank must be taken before locks with higher rank. 

### <a id="lockRankRwmutexR" href="#lockRankRwmutexR">const lockRankRwmutexR</a>

```
searchKey: runtime.lockRankRwmutexR
tags: [constant number private]
```

```Go
const lockRankRwmutexR
```

Constants representing the lock rank of the architecture-independent locks in the runtime. Locks with lower rank must be taken before locks with higher rank. 

### <a id="lockRankRwmutexW" href="#lockRankRwmutexW">const lockRankRwmutexW</a>

```
searchKey: runtime.lockRankRwmutexW
tags: [constant number private]
```

```Go
const lockRankRwmutexW
```

Constants representing the lock rank of the architecture-independent locks in the runtime. Locks with lower rank must be taken before locks with higher rank. 

### <a id="lockRankScavenge" href="#lockRankScavenge">const lockRankScavenge</a>

```
searchKey: runtime.lockRankScavenge
tags: [constant number private]
```

```Go
const lockRankScavenge
```

Constants representing the lock rank of the architecture-independent locks in the runtime. Locks with lower rank must be taken before locks with higher rank. 

### <a id="lockRankSched" href="#lockRankSched">const lockRankSched</a>

```
searchKey: runtime.lockRankSched
tags: [constant number private]
```

```Go
const lockRankSched
```

Constants representing the lock rank of the architecture-independent locks in the runtime. Locks with lower rank must be taken before locks with higher rank. 

### <a id="lockRankSpanSetSpine" href="#lockRankSpanSetSpine">const lockRankSpanSetSpine</a>

```
searchKey: runtime.lockRankSpanSetSpine
tags: [constant number private]
```

```Go
const lockRankSpanSetSpine
```

Constants representing the lock rank of the architecture-independent locks in the runtime. Locks with lower rank must be taken before locks with higher rank. 

### <a id="lockRankStackLarge" href="#lockRankStackLarge">const lockRankStackLarge</a>

```
searchKey: runtime.lockRankStackLarge
tags: [constant number private]
```

```Go
const lockRankStackLarge
```

Constants representing the lock rank of the architecture-independent locks in the runtime. Locks with lower rank must be taken before locks with higher rank. 

### <a id="lockRankStackpool" href="#lockRankStackpool">const lockRankStackpool</a>

```
searchKey: runtime.lockRankStackpool
tags: [constant number private]
```

```Go
const lockRankStackpool
```

Constants representing the lock rank of the architecture-independent locks in the runtime. Locks with lower rank must be taken before locks with higher rank. 

### <a id="lockRankSudog" href="#lockRankSudog">const lockRankSudog</a>

```
searchKey: runtime.lockRankSudog
tags: [constant number private]
```

```Go
const lockRankSudog
```

Constants representing the lock rank of the architecture-independent locks in the runtime. Locks with lower rank must be taken before locks with higher rank. 

### <a id="lockRankSweep" href="#lockRankSweep">const lockRankSweep</a>

```
searchKey: runtime.lockRankSweep
tags: [constant number private]
```

```Go
const lockRankSweep
```

Constants representing the lock rank of the architecture-independent locks in the runtime. Locks with lower rank must be taken before locks with higher rank. 

### <a id="lockRankSweepWaiters" href="#lockRankSweepWaiters">const lockRankSweepWaiters</a>

```
searchKey: runtime.lockRankSweepWaiters
tags: [constant number private]
```

```Go
const lockRankSweepWaiters
```

Constants representing the lock rank of the architecture-independent locks in the runtime. Locks with lower rank must be taken before locks with higher rank. 

### <a id="lockRankSysmon" href="#lockRankSysmon">const lockRankSysmon</a>

```
searchKey: runtime.lockRankSysmon
tags: [constant number private]
```

```Go
const lockRankSysmon
```

Constants representing the lock rank of the architecture-independent locks in the runtime. Locks with lower rank must be taken before locks with higher rank. 

Locks held above sched 

### <a id="lockRankTicks" href="#lockRankTicks">const lockRankTicks</a>

```
searchKey: runtime.lockRankTicks
tags: [constant number private]
```

```Go
const lockRankTicks
```

Constants representing the lock rank of the architecture-independent locks in the runtime. Locks with lower rank must be taken before locks with higher rank. 

### <a id="lockRankTimers" href="#lockRankTimers">const lockRankTimers</a>

```
searchKey: runtime.lockRankTimers
tags: [constant number private]
```

```Go
const lockRankTimers // Multiple timers locked simultaneously in destroy()

```

Constants representing the lock rank of the architecture-independent locks in the runtime. Locks with lower rank must be taken before locks with higher rank. 

### <a id="lockRankTrace" href="#lockRankTrace">const lockRankTrace</a>

```
searchKey: runtime.lockRankTrace
tags: [constant number private]
```

```Go
const lockRankTrace
```

Constants representing the lock rank of the architecture-independent locks in the runtime. Locks with lower rank must be taken before locks with higher rank. 

### <a id="lockRankTraceBuf" href="#lockRankTraceBuf">const lockRankTraceBuf</a>

```
searchKey: runtime.lockRankTraceBuf
tags: [constant number private]
```

```Go
const lockRankTraceBuf
```

Constants representing the lock rank of the architecture-independent locks in the runtime. Locks with lower rank must be taken before locks with higher rank. 

### <a id="lockRankTraceStackTab" href="#lockRankTraceStackTab">const lockRankTraceStackTab</a>

```
searchKey: runtime.lockRankTraceStackTab
tags: [constant number private]
```

```Go
const lockRankTraceStackTab
```

Constants representing the lock rank of the architecture-independent locks in the runtime. Locks with lower rank must be taken before locks with higher rank. 

### <a id="lockRankTraceStrings" href="#lockRankTraceStrings">const lockRankTraceStrings</a>

```
searchKey: runtime.lockRankTraceStrings
tags: [constant number private]
```

```Go
const lockRankTraceStrings
```

Constants representing the lock rank of the architecture-independent locks in the runtime. Locks with lower rank must be taken before locks with higher rank. 

### <a id="lockRankWbufSpans" href="#lockRankWbufSpans">const lockRankWbufSpans</a>

```
searchKey: runtime.lockRankWbufSpans
tags: [constant number private]
```

```Go
const lockRankWbufSpans
```

Constants representing the lock rank of the architecture-independent locks in the runtime. Locks with lower rank must be taken before locks with higher rank. 

Memory-related non-leaf locks 

### <a id="locked" href="#locked">const locked</a>

```
searchKey: runtime.locked
tags: [constant number private]
```

```Go
const locked uintptr = 1
```

This implementation depends on OS-specific implementations of 

```
func semacreate(mp *m)
	Create a semaphore for mp, if it does not already have one.

func semasleep(ns int64) int32
	If ns < 0, acquire m's semaphore and return 0.
	If ns >= 0, try to acquire m's semaphore for at most ns nanoseconds.
	Return 0 if the semaphore was acquired, -1 if interrupted or timed out.

func semawakeup(mp *m)
	Wake up mp, which is or will soon be sleeping on its semaphore.

```
### <a id="logHeapArenaBytes" href="#logHeapArenaBytes">const logHeapArenaBytes</a>

```
searchKey: runtime.logHeapArenaBytes
tags: [constant number private]
```

```Go
const logHeapArenaBytes = ...
```

logHeapArenaBytes is log_2 of heapArenaBytes. For clarity, prefer using heapArenaBytes where possible (we need the constant to compute some other constants). 

### <a id="logMaxPackedValue" href="#logMaxPackedValue">const logMaxPackedValue</a>

```
searchKey: runtime.logMaxPackedValue
tags: [constant number private]
```

```Go
const logMaxPackedValue = logPallocChunkPages + (summaryLevels-1)*summaryLevelBits
```

### <a id="logPallocChunkBytes" href="#logPallocChunkBytes">const logPallocChunkBytes</a>

```
searchKey: runtime.logPallocChunkBytes
tags: [constant number private]
```

```Go
const logPallocChunkBytes = logPallocChunkPages + pageShift
```

### <a id="logPallocChunkPages" href="#logPallocChunkPages">const logPallocChunkPages</a>

```
searchKey: runtime.logPallocChunkPages
tags: [constant number private]
```

```Go
const logPallocChunkPages = 9
```

### <a id="m1" href="#m1">const m1</a>

```
searchKey: runtime.m1
tags: [constant number private]
```

```Go
const m1 = 0xa0761d6478bd642f
```

### <a id="m2" href="#m2">const m2</a>

```
searchKey: runtime.m2
tags: [constant number private]
```

```Go
const m2 = 0xe7037ed1a0b428db
```

### <a id="m3" href="#m3">const m3</a>

```
searchKey: runtime.m3
tags: [constant number private]
```

```Go
const m3 = 0x8ebc6af09c88c6e3
```

### <a id="m4" href="#m4">const m4</a>

```
searchKey: runtime.m4
tags: [constant number private]
```

```Go
const m4 = 0x589965cc75374cc3
```

### <a id="m5" href="#m5">const m5</a>

```
searchKey: runtime.m5
tags: [constant number private]
```

```Go
const m5 = 0x1d8e4e27c47d124f
```

### <a id="mProfCycleWrap" href="#mProfCycleWrap">const mProfCycleWrap</a>

```
searchKey: runtime.mProfCycleWrap
tags: [constant number private]
```

```Go
const mProfCycleWrap = uint32(len(memRecord{}.future)) * (2 << 24)
```

### <a id="mSpanDead" href="#mSpanDead">const mSpanDead</a>

```
searchKey: runtime.mSpanDead
tags: [constant number private]
```

```Go
const mSpanDead mSpanState = iota
```

### <a id="mSpanInUse" href="#mSpanInUse">const mSpanInUse</a>

```
searchKey: runtime.mSpanInUse
tags: [constant number private]
```

```Go
const mSpanInUse // allocated for garbage collected heap

```

### <a id="mSpanManual" href="#mSpanManual">const mSpanManual</a>

```
searchKey: runtime.mSpanManual
tags: [constant number private]
```

```Go
const mSpanManual // allocated for manual management (e.g., stack allocator)

```

### <a id="mantbits32" href="#mantbits32">const mantbits32</a>

```
searchKey: runtime.mantbits32
tags: [constant number private]
```

```Go
const mantbits32 uint = 23
```

### <a id="mantbits64" href="#mantbits64">const mantbits64</a>

```
searchKey: runtime.mantbits64
tags: [constant number private]
```

```Go
const mantbits64 uint = 52
```

### <a id="mask2" href="#mask2">const mask2</a>

```
searchKey: runtime.mask2
tags: [constant number private]
```

```Go
const mask2 = 0x1F // 0001 1111

```

### <a id="mask3" href="#mask3">const mask3</a>

```
searchKey: runtime.mask3
tags: [constant number private]
```

```Go
const mask3 = 0x0F // 0000 1111

```

### <a id="mask4" href="#mask4">const mask4</a>

```
searchKey: runtime.mask4
tags: [constant number private]
```

```Go
const mask4 = 0x07 // 0000 0111

```

### <a id="maskx" href="#maskx">const maskx</a>

```
searchKey: runtime.maskx
tags: [constant number private]
```

```Go
const maskx = 0x3F // 0011 1111

```

### <a id="maxAlign" href="#maxAlign">const maxAlign</a>

```
searchKey: runtime.maxAlign
tags: [constant number private]
```

```Go
const maxAlign = 8
```

### <a id="maxAlloc" href="#maxAlloc">const maxAlloc</a>

```
searchKey: runtime.maxAlloc
tags: [constant number private]
```

```Go
const maxAlloc = (1 << heapAddrBits) - (1-_64bit)*1
```

maxAlloc is the maximum size of an allocation. On 64-bit, it's theoretically possible to allocate 1<<heapAddrBits bytes. On 32-bit, however, this is one less than 1<<32 because the number of bytes in the address space doesn't actually fit in a uintptr. 

### <a id="maxCPUProfStack" href="#maxCPUProfStack">const maxCPUProfStack</a>

```
searchKey: runtime.maxCPUProfStack
tags: [constant number private]
```

```Go
const maxCPUProfStack = 64
```

### <a id="maxElemSize" href="#maxElemSize">const maxElemSize</a>

```
searchKey: runtime.maxElemSize
tags: [constant number private]
```

```Go
const maxElemSize = 128
```

### <a id="maxInt" href="#maxInt">const maxInt</a>

```
searchKey: runtime.maxInt
tags: [constant number private]
```

```Go
const maxInt = int(maxUint >> 1)
```

### <a id="maxKeySize" href="#maxKeySize">const maxKeySize</a>

```
searchKey: runtime.maxKeySize
tags: [constant number private]
```

```Go
const maxKeySize = 128
```

Maximum key or elem size to keep inline (instead of mallocing per element). Must fit in a uint8. Fast versions cannot handle big elems - the cutoff size for fast versions in cmd/compile/internal/gc/walk.go must be at most this elem. 

### <a id="maxObjsPerSpan" href="#maxObjsPerSpan">const maxObjsPerSpan</a>

```
searchKey: runtime.maxObjsPerSpan
tags: [constant number private]
```

```Go
const maxObjsPerSpan = pageSize / 8
```

By construction, single page spans of the smallest object class have the most objects per span. 

### <a id="maxObletBytes" href="#maxObletBytes">const maxObletBytes</a>

```
searchKey: runtime.maxObletBytes
tags: [constant number private]
```

```Go
const maxObletBytes = 128 << 10
```

maxObletBytes is the maximum bytes of an object to scan at once. Larger objects will be split up into "oblets" of at most this size. Since we can scan 1–2 MB/ms, 128 KB bounds scan preemption at ~100 µs. 

This must be > _MaxSmallSize so that the object base is the span base. 

### <a id="maxPackedValue" href="#maxPackedValue">const maxPackedValue</a>

```
searchKey: runtime.maxPackedValue
tags: [constant number private]
```

```Go
const maxPackedValue = 1 << logMaxPackedValue
```

maxPackedValue is the maximum value that any of the three fields in the pallocSum may take on. 

### <a id="maxPagesPerPhysPage" href="#maxPagesPerPhysPage">const maxPagesPerPhysPage</a>

```
searchKey: runtime.maxPagesPerPhysPage
tags: [constant number private]
```

```Go
const maxPagesPerPhysPage = maxPhysPageSize / pageSize
```

maxPagesPerPhysPage is the maximum number of supported runtime pages per physical page, based on maxPhysPageSize. 

### <a id="maxPhysHugePageSize" href="#maxPhysHugePageSize">const maxPhysHugePageSize</a>

```
searchKey: runtime.maxPhysHugePageSize
tags: [constant number private]
```

```Go
const maxPhysHugePageSize = pallocChunkBytes
```

maxPhysHugePageSize sets an upper-bound on the maximum huge page size that the runtime supports. 

### <a id="maxPhysPageSize" href="#maxPhysPageSize">const maxPhysPageSize</a>

```
searchKey: runtime.maxPhysPageSize
tags: [constant number private]
```

```Go
const maxPhysPageSize = 512 << 10
```

maxPhysPageSize is the maximum page size the runtime supports. 

### <a id="maxRune" href="#maxRune">const maxRune</a>

```
searchKey: runtime.maxRune
tags: [constant number private]
```

```Go
const maxRune = '\U0010FFFF' // Maximum valid Unicode code point.

```

Numbers fundamental to the encoding. 

### <a id="maxSmallSize" href="#maxSmallSize">const maxSmallSize</a>

```
searchKey: runtime.maxSmallSize
tags: [constant number private]
```

```Go
const maxSmallSize = _MaxSmallSize
```

### <a id="maxStack" href="#maxStack">const maxStack</a>

```
searchKey: runtime.maxStack
tags: [constant number private]
```

```Go
const maxStack = 32
```

max depth of stack to record in bucket 

### <a id="maxTinySize" href="#maxTinySize">const maxTinySize</a>

```
searchKey: runtime.maxTinySize
tags: [constant number private]
```

```Go
const maxTinySize = _TinySize
```

### <a id="maxUint" href="#maxUint">const maxUint</a>

```
searchKey: runtime.maxUint
tags: [constant number private]
```

```Go
const maxUint = ^uint(0)
```

### <a id="maxWhen" href="#maxWhen">const maxWhen</a>

```
searchKey: runtime.maxWhen
tags: [constant number private]
```

```Go
const maxWhen = 1<<63 - 1
```

maxWhen is the maximum value for timer's when field. 

### <a id="maxZero" href="#maxZero">const maxZero</a>

```
searchKey: runtime.maxZero
tags: [constant number private]
```

```Go
const maxZero // must match value in reflect/value.go:maxZero cmd/compile/internal/gc/walk.go:zeroValSize
 = ...
```

### <a id="memProfile" href="#memProfile">const memProfile</a>

```
searchKey: runtime.memProfile
tags: [constant number private]
```

```Go
const memProfile bucketType = 1 + iota
```

profile types 

### <a id="metricKindBad" href="#metricKindBad">const metricKindBad</a>

```
searchKey: runtime.metricKindBad
tags: [constant number private]
```

```Go
const metricKindBad metricKind = iota
```

These values must be kept identical to their corresponding Kind* values in the runtime/metrics package. 

### <a id="metricKindFloat64" href="#metricKindFloat64">const metricKindFloat64</a>

```
searchKey: runtime.metricKindFloat64
tags: [constant number private]
```

```Go
const metricKindFloat64
```

### <a id="metricKindFloat64Histogram" href="#metricKindFloat64Histogram">const metricKindFloat64Histogram</a>

```
searchKey: runtime.metricKindFloat64Histogram
tags: [constant number private]
```

```Go
const metricKindFloat64Histogram
```

### <a id="metricKindUint64" href="#metricKindUint64">const metricKindUint64</a>

```
searchKey: runtime.metricKindUint64
tags: [constant number private]
```

```Go
const metricKindUint64
```

### <a id="minDeferAlloc" href="#minDeferAlloc">const minDeferAlloc</a>

```
searchKey: runtime.minDeferAlloc
tags: [constant number private]
```

```Go
const minDeferAlloc = (deferHeaderSize + 15) &^ 15
```

### <a id="minDeferArgs" href="#minDeferArgs">const minDeferArgs</a>

```
searchKey: runtime.minDeferArgs
tags: [constant number private]
```

```Go
const minDeferArgs = minDeferAlloc - deferHeaderSize
```

### <a id="minLegalPointer" href="#minLegalPointer">const minLegalPointer</a>

```
searchKey: runtime.minLegalPointer
tags: [constant number private]
```

```Go
const minLegalPointer uintptr = 4096
```

minLegalPointer is the smallest possible legal pointer. This is the smallest possible architectural page size, since we assume that the first page is never mapped. 

This should agree with minZeroPage in the compiler. 

### <a id="minPhysPageSize" href="#minPhysPageSize">const minPhysPageSize</a>

```
searchKey: runtime.minPhysPageSize
tags: [constant number private]
```

```Go
const minPhysPageSize = 4096
```

minPhysPageSize is a lower-bound on the physical page size. The true physical page size may be larger than this. In contrast, sys.PhysPageSize is an upper-bound on the physical page size. 

### <a id="minTopHash" href="#minTopHash">const minTopHash</a>

```
searchKey: runtime.minTopHash
tags: [constant number private]
```

```Go
const minTopHash = 5 // minimum tophash for a normal filled cell.

```

### <a id="minfunc" href="#minfunc">const minfunc</a>

```
searchKey: runtime.minfunc
tags: [constant number private]
```

```Go
const minfunc = 16 // minimum function size

```

### <a id="msanenabled" href="#msanenabled">const msanenabled</a>

```
searchKey: runtime.msanenabled
tags: [constant boolean private]
```

```Go
const msanenabled = false
```

### <a id="mutexProfile" href="#mutexProfile">const mutexProfile</a>

```
searchKey: runtime.mutexProfile
tags: [constant number private]
```

```Go
const mutexProfile
```

### <a id="nan32" href="#nan32">const nan32</a>

```
searchKey: runtime.nan32
tags: [constant number private]
```

```Go
const nan32 uint32 = (1<<expbits32-1)<<mantbits32 + 1<<(mantbits32-1) // quiet NaN, 0 payload

```

### <a id="nan64" href="#nan64">const nan64</a>

```
searchKey: runtime.nan64
tags: [constant number private]
```

```Go
const nan64 uint64 = (1<<expbits64-1)<<mantbits64 + 1<<(mantbits64-1) // quiet NaN, 0 payload

```

### <a id="neg32" href="#neg32">const neg32</a>

```
searchKey: runtime.neg32
tags: [constant number private]
```

```Go
const neg32 uint32 = 1 << (expbits32 + mantbits32)
```

### <a id="neg64" href="#neg64">const neg64</a>

```
searchKey: runtime.neg64
tags: [constant number private]
```

```Go
const neg64 uint64 = 1 << (expbits64 + mantbits64)
```

### <a id="noCheck" href="#noCheck">const noCheck</a>

```
searchKey: runtime.noCheck
tags: [constant number private]
```

```Go
const noCheck = 1<<(8*sys.PtrSize) - 1
```

sentinel bucket ID for iterator checks 

### <a id="numSpanClasses" href="#numSpanClasses">const numSpanClasses</a>

```
searchKey: runtime.numSpanClasses
tags: [constant number private]
```

```Go
const numSpanClasses = _NumSizeClasses << 1
```

### <a id="numStatsDeps" href="#numStatsDeps">const numStatsDeps</a>

```
searchKey: runtime.numStatsDeps
tags: [constant number private]
```

```Go
const numStatsDeps
```

### <a id="numSweepClasses" href="#numSweepClasses">const numSweepClasses</a>

```
searchKey: runtime.numSweepClasses
tags: [constant number private]
```

```Go
const numSweepClasses = numSpanClasses * 2
```

### <a id="offsetARMHasIDIVA" href="#offsetARMHasIDIVA">const offsetARMHasIDIVA</a>

```
searchKey: runtime.offsetARMHasIDIVA
tags: [constant number private]
```

```Go
const offsetARMHasIDIVA = unsafe.Offsetof(cpu.ARM.HasIDIVA)
```

Offsets into internal/cpu records for use in assembly. 

### <a id="offsetMIPS64XHasMSA" href="#offsetMIPS64XHasMSA">const offsetMIPS64XHasMSA</a>

```
searchKey: runtime.offsetMIPS64XHasMSA
tags: [constant number private]
```

```Go
const offsetMIPS64XHasMSA = unsafe.Offsetof(cpu.MIPS64X.HasMSA)
```

Offsets into internal/cpu records for use in assembly. 

### <a id="offsetX86HasAVX" href="#offsetX86HasAVX">const offsetX86HasAVX</a>

```
searchKey: runtime.offsetX86HasAVX
tags: [constant number private]
```

```Go
const offsetX86HasAVX = unsafe.Offsetof(cpu.X86.HasAVX)
```

Offsets into internal/cpu records for use in assembly. 

### <a id="offsetX86HasAVX2" href="#offsetX86HasAVX2">const offsetX86HasAVX2</a>

```
searchKey: runtime.offsetX86HasAVX2
tags: [constant number private]
```

```Go
const offsetX86HasAVX2 = unsafe.Offsetof(cpu.X86.HasAVX2)
```

Offsets into internal/cpu records for use in assembly. 

### <a id="offsetX86HasERMS" href="#offsetX86HasERMS">const offsetX86HasERMS</a>

```
searchKey: runtime.offsetX86HasERMS
tags: [constant number private]
```

```Go
const offsetX86HasERMS = unsafe.Offsetof(cpu.X86.HasERMS)
```

Offsets into internal/cpu records for use in assembly. 

### <a id="offsetX86HasSSE2" href="#offsetX86HasSSE2">const offsetX86HasSSE2</a>

```
searchKey: runtime.offsetX86HasSSE2
tags: [constant number private]
```

```Go
const offsetX86HasSSE2 = unsafe.Offsetof(cpu.X86.HasSSE2)
```

Offsets into internal/cpu records for use in assembly. 

### <a id="oldIterator" href="#oldIterator">const oldIterator</a>

```
searchKey: runtime.oldIterator
tags: [constant number private]
```

```Go
const oldIterator = 2 // there may be an iterator using oldbuckets

```

### <a id="osRelaxMinNS" href="#osRelaxMinNS">const osRelaxMinNS</a>

```
searchKey: runtime.osRelaxMinNS
tags: [constant number private]
```

```Go
const osRelaxMinNS = 0
```

osRelaxMinNS is the number of nanoseconds of idleness to tolerate without performing an osRelax. Since osRelax may reduce the precision of timers, this should be enough larger than the relaxed timer precision to keep the timer error acceptable. 

### <a id="pageAlloc32Bit" href="#pageAlloc32Bit">const pageAlloc32Bit</a>

```
searchKey: runtime.pageAlloc32Bit
tags: [constant number private]
```

```Go
const pageAlloc32Bit = 0
```

Constants for testing. 

### <a id="pageAlloc64Bit" href="#pageAlloc64Bit">const pageAlloc64Bit</a>

```
searchKey: runtime.pageAlloc64Bit
tags: [constant number private]
```

```Go
const pageAlloc64Bit = 1
```

### <a id="pageCachePages" href="#pageCachePages">const pageCachePages</a>

```
searchKey: runtime.pageCachePages
tags: [constant number private]
```

```Go
const pageCachePages = 8 * unsafe.Sizeof(pageCache{}.cache)
```

### <a id="pageMask" href="#pageMask">const pageMask</a>

```
searchKey: runtime.pageMask
tags: [constant number private]
```

```Go
const pageMask = _PageMask
```

### <a id="pageShift" href="#pageShift">const pageShift</a>

```
searchKey: runtime.pageShift
tags: [constant number private]
```

```Go
const pageShift = _PageShift
```

### <a id="pageSize" href="#pageSize">const pageSize</a>

```
searchKey: runtime.pageSize
tags: [constant number private]
```

```Go
const pageSize = _PageSize
```

### <a id="pagesPerArena" href="#pagesPerArena">const pagesPerArena</a>

```
searchKey: runtime.pagesPerArena
tags: [constant number private]
```

```Go
const pagesPerArena = heapArenaBytes / pageSize
```

### <a id="pagesPerReclaimerChunk" href="#pagesPerReclaimerChunk">const pagesPerReclaimerChunk</a>

```
searchKey: runtime.pagesPerReclaimerChunk
tags: [constant number private]
```

```Go
const pagesPerReclaimerChunk = 512
```

pagesPerReclaimerChunk indicates how many pages to scan from the pageInUse bitmap at a time. Used by the page reclaimer. 

Higher values reduce contention on scanning indexes (such as h.reclaimIndex), but increase the minimum latency of the operation. 

The time required to scan this many pages can vary a lot depending on how many spans are actually freed. Experimentally, it can scan for pages at ~300 GB/ms on a 2.6GHz Core i7, but can only free spans at ~32 MB/ms. Using 512 pages bounds this at roughly 100µs. 

Must be a multiple of the pageInUse bitmap element size and must also evenly divide pagesPerArena. 

### <a id="pagesPerSpanRoot" href="#pagesPerSpanRoot">const pagesPerSpanRoot</a>

```
searchKey: runtime.pagesPerSpanRoot
tags: [constant number private]
```

```Go
const pagesPerSpanRoot = 512
```

pagesPerSpanRoot indicates how many pages to scan from a span root at a time. Used by special root marking. 

Higher values improve throughput by increasing locality, but increase the minimum latency of a marking operation. 

Must be a multiple of the pageInUse bitmap element size and must also evenly divide pagesPerArena. 

### <a id="pallocChunkBytes" href="#pallocChunkBytes">const pallocChunkBytes</a>

```
searchKey: runtime.pallocChunkBytes
tags: [constant number private]
```

```Go
const pallocChunkBytes = pallocChunkPages * pageSize
```

### <a id="pallocChunkPages" href="#pallocChunkPages">const pallocChunkPages</a>

```
searchKey: runtime.pallocChunkPages
tags: [constant number private]
```

```Go
const pallocChunkPages = 1 << logPallocChunkPages
```

The size of a bitmap chunk, i.e. the amount of bits (that is, pages) to consider in the bitmap at once. 

### <a id="pallocChunksL1Bits" href="#pallocChunksL1Bits">const pallocChunksL1Bits</a>

```
searchKey: runtime.pallocChunksL1Bits
tags: [constant number private]
```

```Go
const pallocChunksL1Bits = 13
```

Number of bits needed to represent all indices into the L1 of the chunks map. 

See (*pageAlloc).chunks for more details. Update the documentation there should this number change. 

### <a id="pallocChunksL1Shift" href="#pallocChunksL1Shift">const pallocChunksL1Shift</a>

```
searchKey: runtime.pallocChunksL1Shift
tags: [constant number private]
```

```Go
const pallocChunksL1Shift = pallocChunksL2Bits
```

### <a id="pallocChunksL2Bits" href="#pallocChunksL2Bits">const pallocChunksL2Bits</a>

```
searchKey: runtime.pallocChunksL2Bits
tags: [constant number private]
```

```Go
const pallocChunksL2Bits = heapAddrBits - logPallocChunkBytes - pallocChunksL1Bits
```

pallocChunksL2Bits is the number of bits of the chunk index number covered by the second level of the chunks map. 

See (*pageAlloc).chunks for more details. Update the documentation there should this change. 

### <a id="pallocSumBytes" href="#pallocSumBytes">const pallocSumBytes</a>

```
searchKey: runtime.pallocSumBytes
tags: [constant number private]
```

```Go
const pallocSumBytes = unsafe.Sizeof(pallocSum(0))
```

### <a id="passive_spin" href="#passive_spin">const passive_spin</a>

```
searchKey: runtime.passive_spin
tags: [constant number private]
```

```Go
const passive_spin = 1
```

This implementation depends on OS-specific implementations of 

```
func semacreate(mp *m)
	Create a semaphore for mp, if it does not already have one.

func semasleep(ns int64) int32
	If ns < 0, acquire m's semaphore and return 0.
	If ns >= 0, try to acquire m's semaphore for at most ns nanoseconds.
	Return 0 if the semaphore was acquired, -1 if interrupted or timed out.

func semawakeup(mp *m)
	Wake up mp, which is or will soon be sleeping on its semaphore.

```
### <a id="pcbucketsize" href="#pcbucketsize">const pcbucketsize</a>

```
searchKey: runtime.pcbucketsize
tags: [constant number private]
```

```Go
const pcbucketsize = 256 * minfunc // size of bucket in the pc->func lookup table

```

### <a id="pdReady" href="#pdReady">const pdReady</a>

```
searchKey: runtime.pdReady
tags: [constant number private]
```

```Go
const pdReady uintptr = 1
```

pollDesc contains 2 binary semaphores, rg and wg, to park reader and writer goroutines respectively. The semaphore can be in the following states: pdReady - io readiness notification is pending; 

```
a goroutine consumes the notification by changing the state to nil.

```
pdWait - a goroutine prepares to park on the semaphore, but not yet parked; 

```
the goroutine commits to park by changing the state to G pointer,
or, alternatively, concurrent io notification changes the state to pdReady,
or, alternatively, concurrent timeout/close changes the state to nil.

```
G pointer - the goroutine is blocked on the semaphore; 

```
io notification or timeout/close changes the state to pdReady or nil respectively
and unparks the goroutine.

```
nil - none of the above. 

### <a id="pdWait" href="#pdWait">const pdWait</a>

```
searchKey: runtime.pdWait
tags: [constant number private]
```

```Go
const pdWait uintptr = 2
```

pollDesc contains 2 binary semaphores, rg and wg, to park reader and writer goroutines respectively. The semaphore can be in the following states: pdReady - io readiness notification is pending; 

```
a goroutine consumes the notification by changing the state to nil.

```
pdWait - a goroutine prepares to park on the semaphore, but not yet parked; 

```
the goroutine commits to park by changing the state to G pointer,
or, alternatively, concurrent io notification changes the state to pdReady,
or, alternatively, concurrent timeout/close changes the state to nil.

```
G pointer - the goroutine is blocked on the semaphore; 

```
io notification or timeout/close changes the state to pdReady or nil respectively
and unparks the goroutine.

```
nil - none of the above. 

### <a id="persistentChunkSize" href="#persistentChunkSize">const persistentChunkSize</a>

```
searchKey: runtime.persistentChunkSize
tags: [constant number private]
```

```Go
const persistentChunkSize = 256 << 10
```

persistentChunkSize is the number of bytes we allocate when we grow a persistentAlloc. 

### <a id="physPageAlignedStacks" href="#physPageAlignedStacks">const physPageAlignedStacks</a>

```
searchKey: runtime.physPageAlignedStacks
tags: [constant boolean private]
```

```Go
const physPageAlignedStacks = GOOS == "openbsd"
```

physPageAlignedStacks indicates whether stack allocations must be physical page aligned. This is a requirement for MAP_STACK on OpenBSD. 

### <a id="pollBlockSize" href="#pollBlockSize">const pollBlockSize</a>

```
searchKey: runtime.pollBlockSize
tags: [constant number private]
```

```Go
const pollBlockSize = 4 * 1024
```

### <a id="pollErrClosing" href="#pollErrClosing">const pollErrClosing</a>

```
searchKey: runtime.pollErrClosing
tags: [constant number private]
```

```Go
const pollErrClosing = 1 // descriptor is closed

```

Error codes returned by runtime_pollReset and runtime_pollWait. These must match the values in internal/poll/fd_poll_runtime.go. 

### <a id="pollErrNotPollable" href="#pollErrNotPollable">const pollErrNotPollable</a>

```
searchKey: runtime.pollErrNotPollable
tags: [constant number private]
```

```Go
const pollErrNotPollable = 3 // general error polling descriptor

```

Error codes returned by runtime_pollReset and runtime_pollWait. These must match the values in internal/poll/fd_poll_runtime.go. 

### <a id="pollErrTimeout" href="#pollErrTimeout">const pollErrTimeout</a>

```
searchKey: runtime.pollErrTimeout
tags: [constant number private]
```

```Go
const pollErrTimeout = 2 // I/O timeout

```

Error codes returned by runtime_pollReset and runtime_pollWait. These must match the values in internal/poll/fd_poll_runtime.go. 

### <a id="pollNoError" href="#pollNoError">const pollNoError</a>

```
searchKey: runtime.pollNoError
tags: [constant number private]
```

```Go
const pollNoError = 0 // no error

```

Error codes returned by runtime_pollReset and runtime_pollWait. These must match the values in internal/poll/fd_poll_runtime.go. 

### <a id="preemptMSupported" href="#preemptMSupported">const preemptMSupported</a>

```
searchKey: runtime.preemptMSupported
tags: [constant boolean private]
```

```Go
const preemptMSupported = true
```

### <a id="profBufBlocking" href="#profBufBlocking">const profBufBlocking</a>

```
searchKey: runtime.profBufBlocking
tags: [constant number private]
```

```Go
const profBufBlocking profBufReadMode = iota
```

### <a id="profBufNonBlocking" href="#profBufNonBlocking">const profBufNonBlocking</a>

```
searchKey: runtime.profBufNonBlocking
tags: [constant number private]
```

```Go
const profBufNonBlocking
```

### <a id="profReaderSleeping" href="#profReaderSleeping">const profReaderSleeping</a>

```
searchKey: runtime.profReaderSleeping
tags: [constant number private]
```

```Go
const profReaderSleeping profIndex = 1 << 32 // reader is sleeping and must be woken up

```

### <a id="profWriteExtra" href="#profWriteExtra">const profWriteExtra</a>

```
searchKey: runtime.profWriteExtra
tags: [constant number private]
```

```Go
const profWriteExtra profIndex = 1 << 33 // overflow or eof waiting

```

### <a id="raceenabled" href="#raceenabled">const raceenabled</a>

```
searchKey: runtime.raceenabled
tags: [constant boolean private]
```

```Go
const raceenabled = false
```

### <a id="randomizeScheduler" href="#randomizeScheduler">const randomizeScheduler</a>

```
searchKey: runtime.randomizeScheduler
tags: [constant boolean private]
```

```Go
const randomizeScheduler = raceenabled
```

To shake out latent assumptions about scheduling order, we introduce some randomness into scheduling decisions when running with the race detector. The need for this was made obvious by changing the (deterministic) scheduling order in Go 1.5 and breaking many poorly-written tests. With the randomness here, as long as the tests pass consistently with -race, they shouldn't have latent scheduling assumptions. 

### <a id="retainExtraPercent" href="#retainExtraPercent">const retainExtraPercent</a>

```
searchKey: runtime.retainExtraPercent
tags: [constant number private]
```

```Go
const retainExtraPercent = 10
```

retainExtraPercent represents the amount of memory over the heap goal that the scavenger should keep as a buffer space for the allocator. 

The purpose of maintaining this overhead is to have a greater pool of unscavenged memory available for allocation (since using scavenged memory incurs an additional cost), to account for heap fragmentation and the ever-changing layout of the heap. 

### <a id="rootBlockBytes" href="#rootBlockBytes">const rootBlockBytes</a>

```
searchKey: runtime.rootBlockBytes
tags: [constant number private]
```

```Go
const rootBlockBytes = 256 << 10
```

rootBlockBytes is the number of bytes to scan per data or BSS root. 

### <a id="rune1Max" href="#rune1Max">const rune1Max</a>

```
searchKey: runtime.rune1Max
tags: [constant number private]
```

```Go
const rune1Max = 1<<7 - 1
```

### <a id="rune2Max" href="#rune2Max">const rune2Max</a>

```
searchKey: runtime.rune2Max
tags: [constant number private]
```

```Go
const rune2Max = 1<<11 - 1
```

### <a id="rune3Max" href="#rune3Max">const rune3Max</a>

```
searchKey: runtime.rune3Max
tags: [constant number private]
```

```Go
const rune3Max = 1<<16 - 1
```

### <a id="runeError" href="#runeError">const runeError</a>

```
searchKey: runtime.runeError
tags: [constant number private]
```

```Go
const runeError = '\uFFFD' // the "error" Rune or "Unicode replacement character"

```

Numbers fundamental to the encoding. 

### <a id="runeSelf" href="#runeSelf">const runeSelf</a>

```
searchKey: runtime.runeSelf
tags: [constant number private]
```

```Go
const runeSelf = 0x80 // characters below runeSelf are represented as themselves in a single byte.

```

Numbers fundamental to the encoding. 

### <a id="rwmutexMaxReaders" href="#rwmutexMaxReaders">const rwmutexMaxReaders</a>

```
searchKey: runtime.rwmutexMaxReaders
tags: [constant number private]
```

```Go
const rwmutexMaxReaders = 1 << 30
```

### <a id="sameSizeGrow" href="#sameSizeGrow">const sameSizeGrow</a>

```
searchKey: runtime.sameSizeGrow
tags: [constant number private]
```

```Go
const sameSizeGrow = 8 // the current map growth is to a new map of the same size

```

### <a id="scavengeCostRatio" href="#scavengeCostRatio">const scavengeCostRatio</a>

```
searchKey: runtime.scavengeCostRatio
tags: [constant number private]
```

```Go
const scavengeCostRatio = 0.7 * (sys.GoosDarwin + sys.GoosIos)
```

scavengeCostRatio is the approximate ratio between the costs of using previously scavenged memory and scavenging memory. 

For most systems the cost of scavenging greatly outweighs the costs associated with using scavenged memory, making this constant 0. On other systems (especially ones where "sysUsed" is not just a no-op) this cost is non-trivial. 

This ratio is used as part of multiplicative factor to help the scavenger account for the additional costs of using scavenged memory in its pacing. 

### <a id="scavengePercent" href="#scavengePercent">const scavengePercent</a>

```
searchKey: runtime.scavengePercent
tags: [constant number private]
```

```Go
const scavengePercent = 1 // 1%

```

The background scavenger is paced according to these parameters. 

scavengePercent represents the portion of mutator time we're willing to spend on scavenging in percent. 

### <a id="scavengeReservationShards" href="#scavengeReservationShards">const scavengeReservationShards</a>

```
searchKey: runtime.scavengeReservationShards
tags: [constant number private]
```

```Go
const scavengeReservationShards = 64
```

scavengeReservationShards determines the amount of memory the scavenger should reserve for scavenging at a time. Specifically, the amount of memory reserved is (heap size in bytes) / scavengeReservationShards. 

### <a id="selectDefault" href="#selectDefault">const selectDefault</a>

```
searchKey: runtime.selectDefault
tags: [constant number private]
```

```Go
const selectDefault // default

```

### <a id="selectRecv" href="#selectRecv">const selectRecv</a>

```
searchKey: runtime.selectRecv
tags: [constant number private]
```

```Go
const selectRecv // case <-Chan:

```

### <a id="selectSend" href="#selectSend">const selectSend</a>

```
searchKey: runtime.selectSend
tags: [constant number private]
```

```Go
const selectSend // case Chan <- Send

```

### <a id="semTabSize" href="#semTabSize">const semTabSize</a>

```
searchKey: runtime.semTabSize
tags: [constant number private]
```

```Go
const semTabSize = 251
```

Prime to not correlate with any user patterns. 

### <a id="semaBlockProfile" href="#semaBlockProfile">const semaBlockProfile</a>

```
searchKey: runtime.semaBlockProfile
tags: [constant number private]
```

```Go
const semaBlockProfile semaProfileFlags = 1 << iota
```

### <a id="semaMutexProfile" href="#semaMutexProfile">const semaMutexProfile</a>

```
searchKey: runtime.semaMutexProfile
tags: [constant number private]
```

```Go
const semaMutexProfile
```

### <a id="sigFixup" href="#sigFixup">const sigFixup</a>

```
searchKey: runtime.sigFixup
tags: [constant number private]
```

```Go
const sigFixup
```

### <a id="sigIdle" href="#sigIdle">const sigIdle</a>

```
searchKey: runtime.sigIdle
tags: [constant number private]
```

```Go
const sigIdle = iota
```

### <a id="sigPreempt" href="#sigPreempt">const sigPreempt</a>

```
searchKey: runtime.sigPreempt
tags: [constant number private]
```

```Go
const sigPreempt = _SIGURG
```

sigPreempt is the signal used for non-cooperative preemption. 

There's no good way to choose this signal, but there are some heuristics: 

1. It should be a signal that's passed-through by debuggers by default. On Linux, this is SIGALRM, SIGURG, SIGCHLD, SIGIO, SIGVTALRM, SIGPROF, and SIGWINCH, plus some glibc-internal signals. 

2. It shouldn't be used internally by libc in mixed Go/C binaries because libc may assume it's the only thing that can handle these signals. For example SIGCANCEL or SIGSETXID. 

3. It should be a signal that can happen spuriously without consequences. For example, SIGALRM is a bad choice because the signal handler can't tell if it was caused by the real process alarm or not (arguably this means the signal is broken, but I digress). SIGUSR1 and SIGUSR2 are also bad because those are often used in meaningful ways by applications. 

4. We need to deal with platforms without real-time signals (like macOS), so those are out. 

We use SIGURG because it meets all of these criteria, is extremely unlikely to be used by an application for its "real" meaning (both because out-of-band data is basically unused and because SIGURG doesn't report which socket has the condition, making it pretty useless), and even if it is, the application has to be ready for spurious SIGURG. SIGIO wouldn't be a bad choice either, but is more likely to be used for real. 

### <a id="sigReceiving" href="#sigReceiving">const sigReceiving</a>

```
searchKey: runtime.sigReceiving
tags: [constant number private]
```

```Go
const sigReceiving
```

### <a id="sigSending" href="#sigSending">const sigSending</a>

```
searchKey: runtime.sigSending
tags: [constant number private]
```

```Go
const sigSending
```

### <a id="smallSizeDiv" href="#smallSizeDiv">const smallSizeDiv</a>

```
searchKey: runtime.smallSizeDiv
tags: [constant number private]
```

```Go
const smallSizeDiv = 8
```

### <a id="smallSizeMax" href="#smallSizeMax">const smallSizeMax</a>

```
searchKey: runtime.smallSizeMax
tags: [constant number private]
```

```Go
const smallSizeMax = 1024
```

### <a id="spanAllocHeap" href="#spanAllocHeap">const spanAllocHeap</a>

```
searchKey: runtime.spanAllocHeap
tags: [constant number private]
```

```Go
const spanAllocHeap spanAllocType = iota // heap span

```

### <a id="spanAllocPtrScalarBits" href="#spanAllocPtrScalarBits">const spanAllocPtrScalarBits</a>

```
searchKey: runtime.spanAllocPtrScalarBits
tags: [constant number private]
```

```Go
const spanAllocPtrScalarBits // unrolled GC prog bitmap span

```

### <a id="spanAllocStack" href="#spanAllocStack">const spanAllocStack</a>

```
searchKey: runtime.spanAllocStack
tags: [constant number private]
```

```Go
const spanAllocStack // stack span

```

### <a id="spanAllocWorkBuf" href="#spanAllocWorkBuf">const spanAllocWorkBuf</a>

```
searchKey: runtime.spanAllocWorkBuf
tags: [constant number private]
```

```Go
const spanAllocWorkBuf // work buf span

```

### <a id="spanSetBlockEntries" href="#spanSetBlockEntries">const spanSetBlockEntries</a>

```
searchKey: runtime.spanSetBlockEntries
tags: [constant number private]
```

```Go
const spanSetBlockEntries = 512 // 4KB on 64-bit

```

### <a id="spanSetInitSpineCap" href="#spanSetInitSpineCap">const spanSetInitSpineCap</a>

```
searchKey: runtime.spanSetInitSpineCap
tags: [constant number private]
```

```Go
const spanSetInitSpineCap = 256 // Enough for 1GB heap on 64-bit

```

### <a id="stackDebug" href="#stackDebug">const stackDebug</a>

```
searchKey: runtime.stackDebug
tags: [constant number private]
```

```Go
const stackDebug = 0
```

stackDebug == 0: no logging 

```
== 1: logging of per-stack operations
== 2: logging of per-frame operations
== 3: logging of per-word updates
== 4: logging of per-word reads

```
### <a id="stackFaultOnFree" href="#stackFaultOnFree">const stackFaultOnFree</a>

```
searchKey: runtime.stackFaultOnFree
tags: [constant number private]
```

```Go
const stackFaultOnFree = 0 // old stacks are mapped noaccess to detect use after free

```

### <a id="stackForceMove" href="#stackForceMove">const stackForceMove</a>

```
searchKey: runtime.stackForceMove
tags: [constant number private]
```

```Go
const stackForceMove = uintptrMask & -275
```

Force a stack movement. Used for debugging. 0xfffffeed in hex. 

### <a id="stackFork" href="#stackFork">const stackFork</a>

```
searchKey: runtime.stackFork
tags: [constant number private]
```

```Go
const stackFork = uintptrMask & -1234
```

Thread is forking. Causes a split stack check failure. 0xfffffb2e in hex. 

### <a id="stackFromSystem" href="#stackFromSystem">const stackFromSystem</a>

```
searchKey: runtime.stackFromSystem
tags: [constant number private]
```

```Go
const stackFromSystem = 0 // allocate stacks from system memory instead of the heap

```

### <a id="stackNoCache" href="#stackNoCache">const stackNoCache</a>

```
searchKey: runtime.stackNoCache
tags: [constant number private]
```

```Go
const stackNoCache = 0 // disable per-P small stack caches

```

### <a id="stackPoisonCopy" href="#stackPoisonCopy">const stackPoisonCopy</a>

```
searchKey: runtime.stackPoisonCopy
tags: [constant number private]
```

```Go
const stackPoisonCopy // fill stack that should not be accessed with garbage, to detect bad dereferences during copy
 = ...
```

### <a id="stackPreempt" href="#stackPreempt">const stackPreempt</a>

```
searchKey: runtime.stackPreempt
tags: [constant number private]
```

```Go
const stackPreempt = uintptrMask & -1314
```

Goroutine preemption request. 0xfffffade in hex. 

### <a id="stackTraceDebug" href="#stackTraceDebug">const stackTraceDebug</a>

```
searchKey: runtime.stackTraceDebug
tags: [constant boolean private]
```

```Go
const stackTraceDebug = false
```

### <a id="summaryL0Bits" href="#summaryL0Bits">const summaryL0Bits</a>

```
searchKey: runtime.summaryL0Bits
tags: [constant number private]
```

```Go
const summaryL0Bits = heapAddrBits - logPallocChunkBytes - (summaryLevels-1)*summaryLevelBits
```

### <a id="summaryLevelBits" href="#summaryLevelBits">const summaryLevelBits</a>

```
searchKey: runtime.summaryLevelBits
tags: [constant number private]
```

```Go
const summaryLevelBits = 3
```

The number of radix bits for each level. 

The value of 3 is chosen such that the block of summaries we need to scan at each level fits in 64 bytes (2^3 summaries * 8 bytes per summary), which is close to the L1 cache line width on many systems. Also, a value of 3 fits 4 tree levels perfectly into the 21-bit pallocBits summary field at the root level. 

The following equation explains how each of the constants relate: summaryL0Bits + (summaryLevels-1)*summaryLevelBits + logPallocChunkBytes = heapAddrBits 

summaryLevels is an architecture-dependent value defined in mpagealloc_*.go. 

### <a id="summaryLevels" href="#summaryLevels">const summaryLevels</a>

```
searchKey: runtime.summaryLevels
tags: [constant number private]
```

```Go
const summaryLevels = 5
```

The number of levels in the radix tree. 

### <a id="surrogateMax" href="#surrogateMax">const surrogateMax</a>

```
searchKey: runtime.surrogateMax
tags: [constant number private]
```

```Go
const surrogateMax = 0xDFFF
```

Code points in the surrogate range are not valid for UTF-8. 

### <a id="surrogateMin" href="#surrogateMin">const surrogateMin</a>

```
searchKey: runtime.surrogateMin
tags: [constant number private]
```

```Go
const surrogateMin = 0xD800
```

Code points in the surrogate range are not valid for UTF-8. 

### <a id="sweepClassDone" href="#sweepClassDone">const sweepClassDone</a>

```
searchKey: runtime.sweepClassDone
tags: [constant number private]
```

```Go
const sweepClassDone sweepClass = sweepClass(^uint32(0))
```

### <a id="sweepMinHeapDistance" href="#sweepMinHeapDistance">const sweepMinHeapDistance</a>

```
searchKey: runtime.sweepMinHeapDistance
tags: [constant number private]
```

```Go
const sweepMinHeapDistance = 1024 * 1024
```

sweepMinHeapDistance is a lower bound on the heap distance (in bytes) reserved for concurrent sweeping between GC cycles. 

### <a id="sysStatsDep" href="#sysStatsDep">const sysStatsDep</a>

```
searchKey: runtime.sysStatsDep
tags: [constant number private]
```

```Go
const sysStatsDep // corresponds to sysStatsAggregate

```

### <a id="t1" href="#t1">const t1</a>

```
searchKey: runtime.t1
tags: [constant number private]
```

```Go
const t1 = 0x00 // 0000 0000

```

### <a id="t2" href="#t2">const t2</a>

```
searchKey: runtime.t2
tags: [constant number private]
```

```Go
const t2 = 0xC0 // 1100 0000

```

### <a id="t3" href="#t3">const t3</a>

```
searchKey: runtime.t3
tags: [constant number private]
```

```Go
const t3 = 0xE0 // 1110 0000

```

### <a id="t4" href="#t4">const t4</a>

```
searchKey: runtime.t4
tags: [constant number private]
```

```Go
const t4 = 0xF0 // 1111 0000

```

### <a id="t5" href="#t5">const t5</a>

```
searchKey: runtime.t5
tags: [constant number private]
```

```Go
const t5 = 0xF8 // 1111 1000

```

### <a id="tagAllocSample" href="#tagAllocSample">const tagAllocSample</a>

```
searchKey: runtime.tagAllocSample
tags: [constant number private]
```

```Go
const tagAllocSample = 17
```

### <a id="tagBSS" href="#tagBSS">const tagBSS</a>

```
searchKey: runtime.tagBSS
tags: [constant number private]
```

```Go
const tagBSS = 13
```

### <a id="tagData" href="#tagData">const tagData</a>

```
searchKey: runtime.tagData
tags: [constant number private]
```

```Go
const tagData = 12
```

### <a id="tagDefer" href="#tagDefer">const tagDefer</a>

```
searchKey: runtime.tagDefer
tags: [constant number private]
```

```Go
const tagDefer = 14
```

### <a id="tagEOF" href="#tagEOF">const tagEOF</a>

```
searchKey: runtime.tagEOF
tags: [constant number private]
```

```Go
const tagEOF = 0
```

### <a id="tagFinalizer" href="#tagFinalizer">const tagFinalizer</a>

```
searchKey: runtime.tagFinalizer
tags: [constant number private]
```

```Go
const tagFinalizer = 7
```

### <a id="tagGoroutine" href="#tagGoroutine">const tagGoroutine</a>

```
searchKey: runtime.tagGoroutine
tags: [constant number private]
```

```Go
const tagGoroutine = 4
```

### <a id="tagItab" href="#tagItab">const tagItab</a>

```
searchKey: runtime.tagItab
tags: [constant number private]
```

```Go
const tagItab = 8
```

### <a id="tagMemProf" href="#tagMemProf">const tagMemProf</a>

```
searchKey: runtime.tagMemProf
tags: [constant number private]
```

```Go
const tagMemProf = 16
```

### <a id="tagMemStats" href="#tagMemStats">const tagMemStats</a>

```
searchKey: runtime.tagMemStats
tags: [constant number private]
```

```Go
const tagMemStats = 10
```

### <a id="tagOSThread" href="#tagOSThread">const tagOSThread</a>

```
searchKey: runtime.tagOSThread
tags: [constant number private]
```

```Go
const tagOSThread = 9
```

### <a id="tagObject" href="#tagObject">const tagObject</a>

```
searchKey: runtime.tagObject
tags: [constant number private]
```

```Go
const tagObject = 1
```

### <a id="tagOtherRoot" href="#tagOtherRoot">const tagOtherRoot</a>

```
searchKey: runtime.tagOtherRoot
tags: [constant number private]
```

```Go
const tagOtherRoot = 2
```

### <a id="tagPanic" href="#tagPanic">const tagPanic</a>

```
searchKey: runtime.tagPanic
tags: [constant number private]
```

```Go
const tagPanic = 15
```

### <a id="tagParams" href="#tagParams">const tagParams</a>

```
searchKey: runtime.tagParams
tags: [constant number private]
```

```Go
const tagParams = 6
```

### <a id="tagQueuedFinalizer" href="#tagQueuedFinalizer">const tagQueuedFinalizer</a>

```
searchKey: runtime.tagQueuedFinalizer
tags: [constant number private]
```

```Go
const tagQueuedFinalizer = 11
```

### <a id="tagStackFrame" href="#tagStackFrame">const tagStackFrame</a>

```
searchKey: runtime.tagStackFrame
tags: [constant number private]
```

```Go
const tagStackFrame = 5
```

### <a id="tagType" href="#tagType">const tagType</a>

```
searchKey: runtime.tagType
tags: [constant number private]
```

```Go
const tagType = 3
```

### <a id="testSmallBuf" href="#testSmallBuf">const testSmallBuf</a>

```
searchKey: runtime.testSmallBuf
tags: [constant boolean private]
```

```Go
const testSmallBuf = false
```

testSmallBuf forces a small write barrier buffer to stress write barrier flushing. 

### <a id="tflagExtraStar" href="#tflagExtraStar">const tflagExtraStar</a>

```
searchKey: runtime.tflagExtraStar
tags: [constant number private]
```

```Go
const tflagExtraStar tflag = 1 << 1
```

### <a id="tflagNamed" href="#tflagNamed">const tflagNamed</a>

```
searchKey: runtime.tflagNamed
tags: [constant number private]
```

```Go
const tflagNamed tflag = 1 << 2
```

### <a id="tflagRegularMemory" href="#tflagRegularMemory">const tflagRegularMemory</a>

```
searchKey: runtime.tflagRegularMemory
tags: [constant number private]
```

```Go
const tflagRegularMemory tflag // equal and hash can treat values of this type as a single region of t.size bytes
 = ...
```

### <a id="tflagUncommon" href="#tflagUncommon">const tflagUncommon</a>

```
searchKey: runtime.tflagUncommon
tags: [constant number private]
```

```Go
const tflagUncommon tflag = 1 << 0
```

### <a id="timeHistNumSubBuckets" href="#timeHistNumSubBuckets">const timeHistNumSubBuckets</a>

```
searchKey: runtime.timeHistNumSubBuckets
tags: [constant number private]
```

```Go
const timeHistNumSubBuckets = 1 << timeHistSubBucketBits
```

### <a id="timeHistNumSuperBuckets" href="#timeHistNumSuperBuckets">const timeHistNumSuperBuckets</a>

```
searchKey: runtime.timeHistNumSuperBuckets
tags: [constant number private]
```

```Go
const timeHistNumSuperBuckets = 45
```

### <a id="timeHistSubBucketBits" href="#timeHistSubBucketBits">const timeHistSubBucketBits</a>

```
searchKey: runtime.timeHistSubBucketBits
tags: [constant number private]
```

```Go
const timeHistSubBucketBits = 4
```

For the time histogram type, we use an HDR histogram. Values are placed in super-buckets based solely on the most significant set bit. Thus, super-buckets are power-of-2 sized. Values are then placed into sub-buckets based on the value of the next timeHistSubBucketBits most significant bits. Thus, sub-buckets are linear within a super-bucket. 

Therefore, the number of sub-buckets (timeHistNumSubBuckets) defines the error. This error may be computed as 1/timeHistNumSubBuckets*100%. For example, for 16 sub-buckets per super-bucket the error is approximately 6%. 

The number of super-buckets (timeHistNumSuperBuckets), on the other hand, defines the range. To reserve room for sub-buckets, bit timeHistSubBucketBits is the first bit considered for super-buckets, so super-bucket indices are adjusted accordingly. 

As an example, consider 45 super-buckets with 16 sub-buckets. 

```
00110
^----
│  ^
│  └---- Lowest 4 bits -> sub-bucket 6
└------- Bit 4 unset -> super-bucket 0

10110
^----
│  ^
│  └---- Next 4 bits -> sub-bucket 6
└------- Bit 4 set -> super-bucket 1
100010
^----^
│  ^ └-- Lower bits ignored
│  └---- Next 4 bits -> sub-bucket 1
└------- Bit 5 set -> super-bucket 2

```
Following this pattern, bucket 45 will have the bit 48 set. We don't have any buckets for higher values, so the highest sub-bucket will contain values of 2^48-1 nanoseconds or approx. 3 days. This range is more than enough to handle durations produced by the runtime. 

### <a id="timeHistTotalBuckets" href="#timeHistTotalBuckets">const timeHistTotalBuckets</a>

```
searchKey: runtime.timeHistTotalBuckets
tags: [constant number private]
```

```Go
const timeHistTotalBuckets = timeHistNumSuperBuckets*timeHistNumSubBuckets + 1
```

### <a id="timerDeleted" href="#timerDeleted">const timerDeleted</a>

```
searchKey: runtime.timerDeleted
tags: [constant number private]
```

```Go
const timerDeleted
```

Values for the timer status field. 

The timer is deleted and should be removed. It should not be run, but it is still in some P's heap. 

### <a id="timerModifiedEarlier" href="#timerModifiedEarlier">const timerModifiedEarlier</a>

```
searchKey: runtime.timerModifiedEarlier
tags: [constant number private]
```

```Go
const timerModifiedEarlier
```

Values for the timer status field. 

The timer has been modified to an earlier time. The new when value is in the nextwhen field. The timer is in some P's heap, possibly in the wrong place. 

### <a id="timerModifiedLater" href="#timerModifiedLater">const timerModifiedLater</a>

```
searchKey: runtime.timerModifiedLater
tags: [constant number private]
```

```Go
const timerModifiedLater
```

Values for the timer status field. 

The timer has been modified to the same or a later time. The new when value is in the nextwhen field. The timer is in some P's heap, possibly in the wrong place. 

### <a id="timerModifying" href="#timerModifying">const timerModifying</a>

```
searchKey: runtime.timerModifying
tags: [constant number private]
```

```Go
const timerModifying
```

Values for the timer status field. 

The timer is being modified. The timer will only have this status briefly. 

### <a id="timerMoving" href="#timerMoving">const timerMoving</a>

```
searchKey: runtime.timerMoving
tags: [constant number private]
```

```Go
const timerMoving
```

Values for the timer status field. 

The timer has been modified and is being moved. The timer will only have this status briefly. 

### <a id="timerNoStatus" href="#timerNoStatus">const timerNoStatus</a>

```
searchKey: runtime.timerNoStatus
tags: [constant number private]
```

```Go
const timerNoStatus = iota
```

Values for the timer status field. 

Timer has no status set yet. 

### <a id="timerRemoved" href="#timerRemoved">const timerRemoved</a>

```
searchKey: runtime.timerRemoved
tags: [constant number private]
```

```Go
const timerRemoved
```

Values for the timer status field. 

The timer has been stopped. It is not in any P's heap. 

### <a id="timerRemoving" href="#timerRemoving">const timerRemoving</a>

```
searchKey: runtime.timerRemoving
tags: [constant number private]
```

```Go
const timerRemoving
```

Values for the timer status field. 

The timer is being removed. The timer will only have this status briefly. 

### <a id="timerRunning" href="#timerRunning">const timerRunning</a>

```
searchKey: runtime.timerRunning
tags: [constant number private]
```

```Go
const timerRunning
```

Values for the timer status field. 

Running the timer function. A timer will only have this status briefly. 

### <a id="timerWaiting" href="#timerWaiting">const timerWaiting</a>

```
searchKey: runtime.timerWaiting
tags: [constant number private]
```

```Go
const timerWaiting
```

Values for the timer status field. 

Waiting for timer to fire. The timer is in some P's heap. 

### <a id="tinySizeClass" href="#tinySizeClass">const tinySizeClass</a>

```
searchKey: runtime.tinySizeClass
tags: [constant number private]
```

```Go
const tinySizeClass = _TinySizeClass
```

### <a id="tinySpanClass" href="#tinySpanClass">const tinySpanClass</a>

```
searchKey: runtime.tinySpanClass
tags: [constant number private]
```

```Go
const tinySpanClass = spanClass(tinySizeClass<<1 | 1)
```

### <a id="tlsSize" href="#tlsSize">const tlsSize</a>

```
searchKey: runtime.tlsSize
tags: [constant number private]
```

```Go
const tlsSize = tlsSlots * sys.PtrSize
```

### <a id="tlsSlots" href="#tlsSlots">const tlsSlots</a>

```
searchKey: runtime.tlsSlots
tags: [constant number private]
```

```Go
const tlsSlots = 6
```

tlsSlots is the number of pointer-sized slots reserved for TLS on some platforms, like Windows. 

### <a id="tmpStringBufSize" href="#tmpStringBufSize">const tmpStringBufSize</a>

```
searchKey: runtime.tmpStringBufSize
tags: [constant number private]
```

```Go
const tmpStringBufSize = 32
```

The constant is known to the compiler. There is no fundamental theory behind this number. 

### <a id="traceArgCountShift" href="#traceArgCountShift">const traceArgCountShift</a>

```
searchKey: runtime.traceArgCountShift
tags: [constant number private]
```

```Go
const traceArgCountShift = 6
```

Shift of the number of arguments in the first event byte. 

### <a id="traceBytesPerNumber" href="#traceBytesPerNumber">const traceBytesPerNumber</a>

```
searchKey: runtime.traceBytesPerNumber
tags: [constant number private]
```

```Go
const traceBytesPerNumber = 10
```

Maximum number of bytes to encode uint64 in base-128. 

### <a id="traceEvBatch" href="#traceEvBatch">const traceEvBatch</a>

```
searchKey: runtime.traceEvBatch
tags: [constant number private]
```

```Go
const traceEvBatch = 1 // start of per-P batch of events [pid, timestamp]

```

Event types in the trace, args are given in square brackets. 

### <a id="traceEvCount" href="#traceEvCount">const traceEvCount</a>

```
searchKey: runtime.traceEvCount
tags: [constant number private]
```

```Go
const traceEvCount = 49
```

Event types in the trace, args are given in square brackets. 

### <a id="traceEvFrequency" href="#traceEvFrequency">const traceEvFrequency</a>

```
searchKey: runtime.traceEvFrequency
tags: [constant number private]
```

```Go
const traceEvFrequency = 2 // contains tracer timer frequency [frequency (ticks per second)]

```

Event types in the trace, args are given in square brackets. 

### <a id="traceEvFutileWakeup" href="#traceEvFutileWakeup">const traceEvFutileWakeup</a>

```
searchKey: runtime.traceEvFutileWakeup
tags: [constant number private]
```

```Go
const traceEvFutileWakeup // denotes that the previous wakeup of this goroutine was futile [timestamp]
 = ...
```

Event types in the trace, args are given in square brackets. 

### <a id="traceEvGCDone" href="#traceEvGCDone">const traceEvGCDone</a>

```
searchKey: runtime.traceEvGCDone
tags: [constant number private]
```

```Go
const traceEvGCDone = 8 // GC done [timestamp]

```

Event types in the trace, args are given in square brackets. 

### <a id="traceEvGCMarkAssistDone" href="#traceEvGCMarkAssistDone">const traceEvGCMarkAssistDone</a>

```
searchKey: runtime.traceEvGCMarkAssistDone
tags: [constant number private]
```

```Go
const traceEvGCMarkAssistDone = 44 // GC mark assist done [timestamp]

```

Event types in the trace, args are given in square brackets. 

### <a id="traceEvGCMarkAssistStart" href="#traceEvGCMarkAssistStart">const traceEvGCMarkAssistStart</a>

```
searchKey: runtime.traceEvGCMarkAssistStart
tags: [constant number private]
```

```Go
const traceEvGCMarkAssistStart = 43 // GC mark assist start [timestamp, stack]

```

Event types in the trace, args are given in square brackets. 

### <a id="traceEvGCSTWDone" href="#traceEvGCSTWDone">const traceEvGCSTWDone</a>

```
searchKey: runtime.traceEvGCSTWDone
tags: [constant number private]
```

```Go
const traceEvGCSTWDone = 10 // GC STW done [timestamp]

```

Event types in the trace, args are given in square brackets. 

### <a id="traceEvGCSTWStart" href="#traceEvGCSTWStart">const traceEvGCSTWStart</a>

```
searchKey: runtime.traceEvGCSTWStart
tags: [constant number private]
```

```Go
const traceEvGCSTWStart = 9 // GC STW start [timestamp, kind]

```

Event types in the trace, args are given in square brackets. 

### <a id="traceEvGCStart" href="#traceEvGCStart">const traceEvGCStart</a>

```
searchKey: runtime.traceEvGCStart
tags: [constant number private]
```

```Go
const traceEvGCStart = 7 // GC start [timestamp, seq, stack id]

```

Event types in the trace, args are given in square brackets. 

### <a id="traceEvGCSweepDone" href="#traceEvGCSweepDone">const traceEvGCSweepDone</a>

```
searchKey: runtime.traceEvGCSweepDone
tags: [constant number private]
```

```Go
const traceEvGCSweepDone = 12 // GC sweep done [timestamp, swept, reclaimed]

```

Event types in the trace, args are given in square brackets. 

### <a id="traceEvGCSweepStart" href="#traceEvGCSweepStart">const traceEvGCSweepStart</a>

```
searchKey: runtime.traceEvGCSweepStart
tags: [constant number private]
```

```Go
const traceEvGCSweepStart = 11 // GC sweep start [timestamp, stack id]

```

Event types in the trace, args are given in square brackets. 

### <a id="traceEvGoBlock" href="#traceEvGoBlock">const traceEvGoBlock</a>

```
searchKey: runtime.traceEvGoBlock
tags: [constant number private]
```

```Go
const traceEvGoBlock = 20 // goroutine blocks [timestamp, stack]

```

Event types in the trace, args are given in square brackets. 

### <a id="traceEvGoBlockCond" href="#traceEvGoBlockCond">const traceEvGoBlockCond</a>

```
searchKey: runtime.traceEvGoBlockCond
tags: [constant number private]
```

```Go
const traceEvGoBlockCond = 26 // goroutine blocks on Cond [timestamp, stack]

```

Event types in the trace, args are given in square brackets. 

### <a id="traceEvGoBlockGC" href="#traceEvGoBlockGC">const traceEvGoBlockGC</a>

```
searchKey: runtime.traceEvGoBlockGC
tags: [constant number private]
```

```Go
const traceEvGoBlockGC = 42 // goroutine blocks on GC assist [timestamp, stack]

```

Event types in the trace, args are given in square brackets. 

### <a id="traceEvGoBlockNet" href="#traceEvGoBlockNet">const traceEvGoBlockNet</a>

```
searchKey: runtime.traceEvGoBlockNet
tags: [constant number private]
```

```Go
const traceEvGoBlockNet = 27 // goroutine blocks on network [timestamp, stack]

```

Event types in the trace, args are given in square brackets. 

### <a id="traceEvGoBlockRecv" href="#traceEvGoBlockRecv">const traceEvGoBlockRecv</a>

```
searchKey: runtime.traceEvGoBlockRecv
tags: [constant number private]
```

```Go
const traceEvGoBlockRecv = 23 // goroutine blocks on chan recv [timestamp, stack]

```

Event types in the trace, args are given in square brackets. 

### <a id="traceEvGoBlockSelect" href="#traceEvGoBlockSelect">const traceEvGoBlockSelect</a>

```
searchKey: runtime.traceEvGoBlockSelect
tags: [constant number private]
```

```Go
const traceEvGoBlockSelect = 24 // goroutine blocks on select [timestamp, stack]

```

Event types in the trace, args are given in square brackets. 

### <a id="traceEvGoBlockSend" href="#traceEvGoBlockSend">const traceEvGoBlockSend</a>

```
searchKey: runtime.traceEvGoBlockSend
tags: [constant number private]
```

```Go
const traceEvGoBlockSend = 22 // goroutine blocks on chan send [timestamp, stack]

```

Event types in the trace, args are given in square brackets. 

### <a id="traceEvGoBlockSync" href="#traceEvGoBlockSync">const traceEvGoBlockSync</a>

```
searchKey: runtime.traceEvGoBlockSync
tags: [constant number private]
```

```Go
const traceEvGoBlockSync = 25 // goroutine blocks on Mutex/RWMutex [timestamp, stack]

```

Event types in the trace, args are given in square brackets. 

### <a id="traceEvGoCreate" href="#traceEvGoCreate">const traceEvGoCreate</a>

```
searchKey: runtime.traceEvGoCreate
tags: [constant number private]
```

```Go
const traceEvGoCreate // goroutine creation [timestamp, new goroutine id, new stack id, stack id]
 = ...
```

Event types in the trace, args are given in square brackets. 

### <a id="traceEvGoEnd" href="#traceEvGoEnd">const traceEvGoEnd</a>

```
searchKey: runtime.traceEvGoEnd
tags: [constant number private]
```

```Go
const traceEvGoEnd = 15 // goroutine ends [timestamp]

```

Event types in the trace, args are given in square brackets. 

### <a id="traceEvGoInSyscall" href="#traceEvGoInSyscall">const traceEvGoInSyscall</a>

```
searchKey: runtime.traceEvGoInSyscall
tags: [constant number private]
```

```Go
const traceEvGoInSyscall // denotes that goroutine is in syscall when tracing starts [timestamp, goroutine id]
 = ...
```

Event types in the trace, args are given in square brackets. 

### <a id="traceEvGoPreempt" href="#traceEvGoPreempt">const traceEvGoPreempt</a>

```
searchKey: runtime.traceEvGoPreempt
tags: [constant number private]
```

```Go
const traceEvGoPreempt = 18 // goroutine is preempted [timestamp, stack]

```

Event types in the trace, args are given in square brackets. 

### <a id="traceEvGoSched" href="#traceEvGoSched">const traceEvGoSched</a>

```
searchKey: runtime.traceEvGoSched
tags: [constant number private]
```

```Go
const traceEvGoSched = 17 // goroutine calls Gosched [timestamp, stack]

```

Event types in the trace, args are given in square brackets. 

### <a id="traceEvGoSleep" href="#traceEvGoSleep">const traceEvGoSleep</a>

```
searchKey: runtime.traceEvGoSleep
tags: [constant number private]
```

```Go
const traceEvGoSleep = 19 // goroutine calls Sleep [timestamp, stack]

```

Event types in the trace, args are given in square brackets. 

### <a id="traceEvGoStart" href="#traceEvGoStart">const traceEvGoStart</a>

```
searchKey: runtime.traceEvGoStart
tags: [constant number private]
```

```Go
const traceEvGoStart = 14 // goroutine starts running [timestamp, goroutine id, seq]

```

Event types in the trace, args are given in square brackets. 

### <a id="traceEvGoStartLabel" href="#traceEvGoStartLabel">const traceEvGoStartLabel</a>

```
searchKey: runtime.traceEvGoStartLabel
tags: [constant number private]
```

```Go
const traceEvGoStartLabel // goroutine starts running with label [timestamp, goroutine id, seq, label string id]
 = ...
```

Event types in the trace, args are given in square brackets. 

### <a id="traceEvGoStartLocal" href="#traceEvGoStartLocal">const traceEvGoStartLocal</a>

```
searchKey: runtime.traceEvGoStartLocal
tags: [constant number private]
```

```Go
const traceEvGoStartLocal // goroutine starts running on the same P as the last event [timestamp, goroutine id]
 = ...
```

Event types in the trace, args are given in square brackets. 

### <a id="traceEvGoStop" href="#traceEvGoStop">const traceEvGoStop</a>

```
searchKey: runtime.traceEvGoStop
tags: [constant number private]
```

```Go
const traceEvGoStop = 16 // goroutine stops (like in select{}) [timestamp, stack]

```

Event types in the trace, args are given in square brackets. 

### <a id="traceEvGoSysBlock" href="#traceEvGoSysBlock">const traceEvGoSysBlock</a>

```
searchKey: runtime.traceEvGoSysBlock
tags: [constant number private]
```

```Go
const traceEvGoSysBlock = 30 // syscall blocks [timestamp]

```

Event types in the trace, args are given in square brackets. 

### <a id="traceEvGoSysCall" href="#traceEvGoSysCall">const traceEvGoSysCall</a>

```
searchKey: runtime.traceEvGoSysCall
tags: [constant number private]
```

```Go
const traceEvGoSysCall = 28 // syscall enter [timestamp, stack]

```

Event types in the trace, args are given in square brackets. 

### <a id="traceEvGoSysExit" href="#traceEvGoSysExit">const traceEvGoSysExit</a>

```
searchKey: runtime.traceEvGoSysExit
tags: [constant number private]
```

```Go
const traceEvGoSysExit = 29 // syscall exit [timestamp, goroutine id, seq, real timestamp]

```

Event types in the trace, args are given in square brackets. 

### <a id="traceEvGoSysExitLocal" href="#traceEvGoSysExitLocal">const traceEvGoSysExitLocal</a>

```
searchKey: runtime.traceEvGoSysExitLocal
tags: [constant number private]
```

```Go
const traceEvGoSysExitLocal // syscall exit on the same P as the last event [timestamp, goroutine id, real timestamp]
 = ...
```

Event types in the trace, args are given in square brackets. 

### <a id="traceEvGoUnblock" href="#traceEvGoUnblock">const traceEvGoUnblock</a>

```
searchKey: runtime.traceEvGoUnblock
tags: [constant number private]
```

```Go
const traceEvGoUnblock = 21 // goroutine is unblocked [timestamp, goroutine id, seq, stack]

```

Event types in the trace, args are given in square brackets. 

### <a id="traceEvGoUnblockLocal" href="#traceEvGoUnblockLocal">const traceEvGoUnblockLocal</a>

```
searchKey: runtime.traceEvGoUnblockLocal
tags: [constant number private]
```

```Go
const traceEvGoUnblockLocal // goroutine is unblocked on the same P as the last event [timestamp, goroutine id, stack]
 = ...
```

Event types in the trace, args are given in square brackets. 

### <a id="traceEvGoWaiting" href="#traceEvGoWaiting">const traceEvGoWaiting</a>

```
searchKey: runtime.traceEvGoWaiting
tags: [constant number private]
```

```Go
const traceEvGoWaiting // denotes that goroutine is blocked when tracing starts [timestamp, goroutine id]
 = ...
```

Event types in the trace, args are given in square brackets. 

### <a id="traceEvGomaxprocs" href="#traceEvGomaxprocs">const traceEvGomaxprocs</a>

```
searchKey: runtime.traceEvGomaxprocs
tags: [constant number private]
```

```Go
const traceEvGomaxprocs = 4 // current value of GOMAXPROCS [timestamp, GOMAXPROCS, stack id]

```

Event types in the trace, args are given in square brackets. 

### <a id="traceEvHeapAlloc" href="#traceEvHeapAlloc">const traceEvHeapAlloc</a>

```
searchKey: runtime.traceEvHeapAlloc
tags: [constant number private]
```

```Go
const traceEvHeapAlloc = 33 // gcController.heapLive change [timestamp, heap_alloc]

```

Event types in the trace, args are given in square brackets. 

### <a id="traceEvHeapGoal" href="#traceEvHeapGoal">const traceEvHeapGoal</a>

```
searchKey: runtime.traceEvHeapGoal
tags: [constant number private]
```

```Go
const traceEvHeapGoal // gcController.heapGoal (formerly next_gc) change [timestamp, heap goal in bytes]
 = ...
```

Event types in the trace, args are given in square brackets. 

### <a id="traceEvNone" href="#traceEvNone">const traceEvNone</a>

```
searchKey: runtime.traceEvNone
tags: [constant number private]
```

```Go
const traceEvNone = 0 // unused

```

Event types in the trace, args are given in square brackets. 

### <a id="traceEvProcStart" href="#traceEvProcStart">const traceEvProcStart</a>

```
searchKey: runtime.traceEvProcStart
tags: [constant number private]
```

```Go
const traceEvProcStart = 5 // start of P [timestamp, thread id]

```

Event types in the trace, args are given in square brackets. 

### <a id="traceEvProcStop" href="#traceEvProcStop">const traceEvProcStop</a>

```
searchKey: runtime.traceEvProcStop
tags: [constant number private]
```

```Go
const traceEvProcStop = 6 // stop of P [timestamp]

```

Event types in the trace, args are given in square brackets. 

### <a id="traceEvStack" href="#traceEvStack">const traceEvStack</a>

```
searchKey: runtime.traceEvStack
tags: [constant number private]
```

```Go
const traceEvStack // stack [stack id, number of PCs, array of {PC, func string ID, file string ID, line}]
 = ...
```

Event types in the trace, args are given in square brackets. 

### <a id="traceEvString" href="#traceEvString">const traceEvString</a>

```
searchKey: runtime.traceEvString
tags: [constant number private]
```

```Go
const traceEvString = 37 // string dictionary entry [ID, length, string]

```

Event types in the trace, args are given in square brackets. 

### <a id="traceEvTimerGoroutine" href="#traceEvTimerGoroutine">const traceEvTimerGoroutine</a>

```
searchKey: runtime.traceEvTimerGoroutine
tags: [constant number private]
```

```Go
const traceEvTimerGoroutine // not currently used; previously denoted timer goroutine [timer goroutine id]
 = ...
```

Event types in the trace, args are given in square brackets. 

### <a id="traceEvUserLog" href="#traceEvUserLog">const traceEvUserLog</a>

```
searchKey: runtime.traceEvUserLog
tags: [constant number private]
```

```Go
const traceEvUserLog // trace.Log [timestamp, internal task id, key string id, stack, value string]
 = ...
```

Event types in the trace, args are given in square brackets. 

### <a id="traceEvUserRegion" href="#traceEvUserRegion">const traceEvUserRegion</a>

```
searchKey: runtime.traceEvUserRegion
tags: [constant number private]
```

```Go
const traceEvUserRegion // trace.WithRegion [timestamp, internal task id, mode(0:start, 1:end), stack, name string]
 = ...
```

Event types in the trace, args are given in square brackets. 

### <a id="traceEvUserTaskCreate" href="#traceEvUserTaskCreate">const traceEvUserTaskCreate</a>

```
searchKey: runtime.traceEvUserTaskCreate
tags: [constant number private]
```

```Go
const traceEvUserTaskCreate // trace.NewContext [timestamp, internal task id, internal parent task id, stack, name string]
 = ...
```

Event types in the trace, args are given in square brackets. 

### <a id="traceEvUserTaskEnd" href="#traceEvUserTaskEnd">const traceEvUserTaskEnd</a>

```
searchKey: runtime.traceEvUserTaskEnd
tags: [constant number private]
```

```Go
const traceEvUserTaskEnd = 46 // end of a task [timestamp, internal task id, stack]

```

Event types in the trace, args are given in square brackets. 

### <a id="traceFutileWakeup" href="#traceFutileWakeup">const traceFutileWakeup</a>

```
searchKey: runtime.traceFutileWakeup
tags: [constant number private]
```

```Go
const traceFutileWakeup byte = 128
```

Flag passed to traceGoPark to denote that the previous wakeup of this goroutine was futile. For example, a goroutine was unblocked on a mutex, but another goroutine got ahead and acquired the mutex before the first goroutine is scheduled, so the first goroutine has to block again. Such wakeups happen on buffered channels and sync.Mutex, but are generally not interesting for end user. 

### <a id="traceGlobProc" href="#traceGlobProc">const traceGlobProc</a>

```
searchKey: runtime.traceGlobProc
tags: [constant number private]
```

```Go
const traceGlobProc = -1
```

Identifier of a fake P that is used when we trace without a real P. 

### <a id="traceStackSize" href="#traceStackSize">const traceStackSize</a>

```
searchKey: runtime.traceStackSize
tags: [constant number private]
```

```Go
const traceStackSize = 128
```

Maximum number of PCs in a single stack trace. Since events contain only stack id rather than whole stack trace, we can allow quite large values here. 

### <a id="traceTickDiv" href="#traceTickDiv">const traceTickDiv</a>

```
searchKey: runtime.traceTickDiv
tags: [constant number private]
```

```Go
const traceTickDiv = 16 + 48*(sys.Goarch386|sys.GoarchAmd64)
```

Timestamps in trace are cputicks/traceTickDiv. This makes absolute values of timestamp diffs smaller, and so they are encoded in less number of bytes. 64 on x86 is somewhat arbitrary (one tick is ~20ns on a 3GHz machine). The suggested increment frequency for PowerPC's time base register is 512 MHz according to Power ISA v2.07 section 6.2, so we use 16 on ppc64 and ppc64le. Tracing won't work reliably for architectures where cputicks is emulated by nanotime, so the value doesn't matter for those architectures. 

### <a id="tracebackAll" href="#tracebackAll">const tracebackAll</a>

```
searchKey: runtime.tracebackAll
tags: [constant number private]
```

```Go
const tracebackAll
```

Keep a cached value to make gotraceback fast, since we call it on every call to gentraceback. The cached value is a uint32 in which the low bits are the "crash" and "all" settings and the remaining bits are the traceback value (0 off, 1 on, 2 include system). 

### <a id="tracebackCrash" href="#tracebackCrash">const tracebackCrash</a>

```
searchKey: runtime.tracebackCrash
tags: [constant number private]
```

```Go
const tracebackCrash = 1 << iota
```

Keep a cached value to make gotraceback fast, since we call it on every call to gentraceback. The cached value is a uint32 in which the low bits are the "crash" and "all" settings and the remaining bits are the traceback value (0 off, 1 on, 2 include system). 

### <a id="tracebackShift" href="#tracebackShift">const tracebackShift</a>

```
searchKey: runtime.tracebackShift
tags: [constant number private]
```

```Go
const tracebackShift = iota
```

Keep a cached value to make gotraceback fast, since we call it on every call to gentraceback. The cached value is a uint32 in which the low bits are the "crash" and "all" settings and the remaining bits are the traceback value (0 off, 1 on, 2 include system). 

### <a id="tx" href="#tx">const tx</a>

```
searchKey: runtime.tx
tags: [constant number private]
```

```Go
const tx = 0x80 // 1000 0000

```

### <a id="typeCacheAssoc" href="#typeCacheAssoc">const typeCacheAssoc</a>

```
searchKey: runtime.typeCacheAssoc
tags: [constant number private]
```

```Go
const typeCacheAssoc = 4
```

Cache of types that have been serialized already. We use a type's hash field to pick a bucket. Inside a bucket, we keep a list of types that have been serialized so far, most recently used first. Note: when a bucket overflows we may end up serializing a type more than once. That's ok. 

### <a id="typeCacheBuckets" href="#typeCacheBuckets">const typeCacheBuckets</a>

```
searchKey: runtime.typeCacheBuckets
tags: [constant number private]
```

```Go
const typeCacheBuckets = 256
```

Cache of types that have been serialized already. We use a type's hash field to pick a bucket. Inside a bucket, we keep a list of types that have been serialized so far, most recently used first. Note: when a bucket overflows we may end up serializing a type more than once. That's ok. 

### <a id="uintptrMask" href="#uintptrMask">const uintptrMask</a>

```
searchKey: runtime.uintptrMask
tags: [constant number private]
```

```Go
const uintptrMask = 1<<(8*sys.PtrSize) - 1
```

### <a id="usesLR" href="#usesLR">const usesLR</a>

```
searchKey: runtime.usesLR
tags: [constant boolean private]
```

```Go
const usesLR = sys.MinFrameSize > 0
```

### <a id="verifyTimers" href="#verifyTimers">const verifyTimers</a>

```
searchKey: runtime.verifyTimers
tags: [constant boolean private]
```

```Go
const verifyTimers = false
```

verifyTimers can be set to true to add debugging checks that the timer heaps are valid. 

### <a id="waitReasonChanReceive" href="#waitReasonChanReceive">const waitReasonChanReceive</a>

```
searchKey: runtime.waitReasonChanReceive
tags: [constant number private]
```

```Go
const waitReasonChanReceive // "chan receive"

```

### <a id="waitReasonChanReceiveNilChan" href="#waitReasonChanReceiveNilChan">const waitReasonChanReceiveNilChan</a>

```
searchKey: runtime.waitReasonChanReceiveNilChan
tags: [constant number private]
```

```Go
const waitReasonChanReceiveNilChan // "chan receive (nil chan)"

```

### <a id="waitReasonChanSend" href="#waitReasonChanSend">const waitReasonChanSend</a>

```
searchKey: runtime.waitReasonChanSend
tags: [constant number private]
```

```Go
const waitReasonChanSend // "chan send"

```

### <a id="waitReasonChanSendNilChan" href="#waitReasonChanSendNilChan">const waitReasonChanSendNilChan</a>

```
searchKey: runtime.waitReasonChanSendNilChan
tags: [constant number private]
```

```Go
const waitReasonChanSendNilChan // "chan send (nil chan)"

```

### <a id="waitReasonDebugCall" href="#waitReasonDebugCall">const waitReasonDebugCall</a>

```
searchKey: runtime.waitReasonDebugCall
tags: [constant number private]
```

```Go
const waitReasonDebugCall // "debug call"

```

### <a id="waitReasonDumpingHeap" href="#waitReasonDumpingHeap">const waitReasonDumpingHeap</a>

```
searchKey: runtime.waitReasonDumpingHeap
tags: [constant number private]
```

```Go
const waitReasonDumpingHeap // "dumping heap"

```

### <a id="waitReasonFinalizerWait" href="#waitReasonFinalizerWait">const waitReasonFinalizerWait</a>

```
searchKey: runtime.waitReasonFinalizerWait
tags: [constant number private]
```

```Go
const waitReasonFinalizerWait // "finalizer wait"

```

### <a id="waitReasonForceGCIdle" href="#waitReasonForceGCIdle">const waitReasonForceGCIdle</a>

```
searchKey: runtime.waitReasonForceGCIdle
tags: [constant number private]
```

```Go
const waitReasonForceGCIdle // "force gc (idle)"

```

### <a id="waitReasonGCAssistMarking" href="#waitReasonGCAssistMarking">const waitReasonGCAssistMarking</a>

```
searchKey: runtime.waitReasonGCAssistMarking
tags: [constant number private]
```

```Go
const waitReasonGCAssistMarking // "GC assist marking"

```

### <a id="waitReasonGCAssistWait" href="#waitReasonGCAssistWait">const waitReasonGCAssistWait</a>

```
searchKey: runtime.waitReasonGCAssistWait
tags: [constant number private]
```

```Go
const waitReasonGCAssistWait // "GC assist wait"

```

### <a id="waitReasonGCScavengeWait" href="#waitReasonGCScavengeWait">const waitReasonGCScavengeWait</a>

```
searchKey: runtime.waitReasonGCScavengeWait
tags: [constant number private]
```

```Go
const waitReasonGCScavengeWait // "GC scavenge wait"

```

### <a id="waitReasonGCSweepWait" href="#waitReasonGCSweepWait">const waitReasonGCSweepWait</a>

```
searchKey: runtime.waitReasonGCSweepWait
tags: [constant number private]
```

```Go
const waitReasonGCSweepWait // "GC sweep wait"

```

### <a id="waitReasonGCWorkerIdle" href="#waitReasonGCWorkerIdle">const waitReasonGCWorkerIdle</a>

```
searchKey: runtime.waitReasonGCWorkerIdle
tags: [constant number private]
```

```Go
const waitReasonGCWorkerIdle // "GC worker (idle)"

```

### <a id="waitReasonGarbageCollection" href="#waitReasonGarbageCollection">const waitReasonGarbageCollection</a>

```
searchKey: runtime.waitReasonGarbageCollection
tags: [constant number private]
```

```Go
const waitReasonGarbageCollection // "garbage collection"

```

### <a id="waitReasonGarbageCollectionScan" href="#waitReasonGarbageCollectionScan">const waitReasonGarbageCollectionScan</a>

```
searchKey: runtime.waitReasonGarbageCollectionScan
tags: [constant number private]
```

```Go
const waitReasonGarbageCollectionScan // "garbage collection scan"

```

### <a id="waitReasonIOWait" href="#waitReasonIOWait">const waitReasonIOWait</a>

```
searchKey: runtime.waitReasonIOWait
tags: [constant number private]
```

```Go
const waitReasonIOWait // "IO wait"

```

### <a id="waitReasonPanicWait" href="#waitReasonPanicWait">const waitReasonPanicWait</a>

```
searchKey: runtime.waitReasonPanicWait
tags: [constant number private]
```

```Go
const waitReasonPanicWait // "panicwait"

```

### <a id="waitReasonPreempted" href="#waitReasonPreempted">const waitReasonPreempted</a>

```
searchKey: runtime.waitReasonPreempted
tags: [constant number private]
```

```Go
const waitReasonPreempted // "preempted"

```

### <a id="waitReasonSelect" href="#waitReasonSelect">const waitReasonSelect</a>

```
searchKey: runtime.waitReasonSelect
tags: [constant number private]
```

```Go
const waitReasonSelect // "select"

```

### <a id="waitReasonSelectNoCases" href="#waitReasonSelectNoCases">const waitReasonSelectNoCases</a>

```
searchKey: runtime.waitReasonSelectNoCases
tags: [constant number private]
```

```Go
const waitReasonSelectNoCases // "select (no cases)"

```

### <a id="waitReasonSemacquire" href="#waitReasonSemacquire">const waitReasonSemacquire</a>

```
searchKey: runtime.waitReasonSemacquire
tags: [constant number private]
```

```Go
const waitReasonSemacquire // "semacquire"

```

### <a id="waitReasonSleep" href="#waitReasonSleep">const waitReasonSleep</a>

```
searchKey: runtime.waitReasonSleep
tags: [constant number private]
```

```Go
const waitReasonSleep // "sleep"

```

### <a id="waitReasonSyncCondWait" href="#waitReasonSyncCondWait">const waitReasonSyncCondWait</a>

```
searchKey: runtime.waitReasonSyncCondWait
tags: [constant number private]
```

```Go
const waitReasonSyncCondWait // "sync.Cond.Wait"

```

### <a id="waitReasonTimerGoroutineIdle" href="#waitReasonTimerGoroutineIdle">const waitReasonTimerGoroutineIdle</a>

```
searchKey: runtime.waitReasonTimerGoroutineIdle
tags: [constant number private]
```

```Go
const waitReasonTimerGoroutineIdle // "timer goroutine (idle)"

```

### <a id="waitReasonTraceReaderBlocked" href="#waitReasonTraceReaderBlocked">const waitReasonTraceReaderBlocked</a>

```
searchKey: runtime.waitReasonTraceReaderBlocked
tags: [constant number private]
```

```Go
const waitReasonTraceReaderBlocked // "trace reader (blocked)"

```

### <a id="waitReasonWaitForGCCycle" href="#waitReasonWaitForGCCycle">const waitReasonWaitForGCCycle</a>

```
searchKey: runtime.waitReasonWaitForGCCycle
tags: [constant number private]
```

```Go
const waitReasonWaitForGCCycle // "wait for GC cycle"

```

### <a id="waitReasonZero" href="#waitReasonZero">const waitReasonZero</a>

```
searchKey: runtime.waitReasonZero
tags: [constant number private]
```

```Go
const waitReasonZero waitReason = iota // ""

```

### <a id="wbBufEntries" href="#wbBufEntries">const wbBufEntries</a>

```
searchKey: runtime.wbBufEntries
tags: [constant number private]
```

```Go
const wbBufEntries = 256
```

wbBufEntries is the number of write barriers between flushes of the write barrier buffer. 

This trades latency for throughput amortization. Higher values amortize flushing overhead more, but increase the latency of flushing. Higher values also increase the cache footprint of the buffer. 

TODO: What is the latency cost of this? Tune this value. 

### <a id="wbBufEntryPointers" href="#wbBufEntryPointers">const wbBufEntryPointers</a>

```
searchKey: runtime.wbBufEntryPointers
tags: [constant number private]
```

```Go
const wbBufEntryPointers = 2
```

wbBufEntryPointers is the number of pointers added to the buffer by each write barrier. 

### <a id="wordsPerBitmapByte" href="#wordsPerBitmapByte">const wordsPerBitmapByte</a>

```
searchKey: runtime.wordsPerBitmapByte
tags: [constant number private]
```

```Go
const wordsPerBitmapByte = 8 / 2 // heap words described by one bitmap byte

```

### <a id="workbufAlloc" href="#workbufAlloc">const workbufAlloc</a>

```
searchKey: runtime.workbufAlloc
tags: [constant number private]
```

```Go
const workbufAlloc = 32 << 10
```

workbufAlloc is the number of bytes to allocate at a time for new workbufs. This must be a multiple of pageSize and should be a multiple of _WorkbufSize. 

Larger values reduce workbuf allocation overhead. Smaller values reduce heap fragmentation. 

### <a id="_64bit" href="#_64bit">const _64bit</a>

```
searchKey: runtime._64bit
tags: [constant number private]
```

```Go
const _64bit = 1 << (^uintptr(0) >> 63) / 2
```

_64bit = 1 on 64-bit systems, 0 on 32-bit systems 

### <a id="_ArgsSizeUnknown" href="#_ArgsSizeUnknown">const _ArgsSizeUnknown</a>

```
searchKey: runtime._ArgsSizeUnknown
tags: [constant number private]
```

```Go
const _ArgsSizeUnknown = -0x80000000
```

PCDATA and FUNCDATA table indexes. 

See funcdata.h and ../cmd/internal/objabi/funcdata.go. 

### <a id="_BUS_ADRALN" href="#_BUS_ADRALN">const _BUS_ADRALN</a>

```
searchKey: runtime._BUS_ADRALN
tags: [constant number private]
```

```Go
const _BUS_ADRALN = 0x1
```

### <a id="_BUS_ADRERR" href="#_BUS_ADRERR">const _BUS_ADRERR</a>

```
searchKey: runtime._BUS_ADRERR
tags: [constant number private]
```

```Go
const _BUS_ADRERR = 0x2
```

### <a id="_BUS_OBJERR" href="#_BUS_OBJERR">const _BUS_OBJERR</a>

```
searchKey: runtime._BUS_OBJERR
tags: [constant number private]
```

```Go
const _BUS_OBJERR = 0x3
```

### <a id="_CTL_HW" href="#_CTL_HW">const _CTL_HW</a>

```
searchKey: runtime._CTL_HW
tags: [constant number private]
```

```Go
const _CTL_HW = 6
```

### <a id="_ConcurrentSweep" href="#_ConcurrentSweep">const _ConcurrentSweep</a>

```
searchKey: runtime._ConcurrentSweep
tags: [constant boolean private]
```

```Go
const _ConcurrentSweep = true
```

### <a id="_DebugGC" href="#_DebugGC">const _DebugGC</a>

```
searchKey: runtime._DebugGC
tags: [constant number private]
```

```Go
const _DebugGC = 0
```

### <a id="_EAGAIN" href="#_EAGAIN">const _EAGAIN</a>

```
searchKey: runtime._EAGAIN
tags: [constant number private]
```

```Go
const _EAGAIN = 0x23
```

### <a id="_EFAULT" href="#_EFAULT">const _EFAULT</a>

```
searchKey: runtime._EFAULT
tags: [constant number private]
```

```Go
const _EFAULT = 0xe
```

### <a id="_EINTR" href="#_EINTR">const _EINTR</a>

```
searchKey: runtime._EINTR
tags: [constant number private]
```

```Go
const _EINTR = 0x4
```

### <a id="_ENOMEM" href="#_ENOMEM">const _ENOMEM</a>

```
searchKey: runtime._ENOMEM
tags: [constant number private]
```

```Go
const _ENOMEM = 12
```

### <a id="_ETIMEDOUT" href="#_ETIMEDOUT">const _ETIMEDOUT</a>

```
searchKey: runtime._ETIMEDOUT
tags: [constant number private]
```

```Go
const _ETIMEDOUT = 0x3c
```

### <a id="_EVFILT_READ" href="#_EVFILT_READ">const _EVFILT_READ</a>

```
searchKey: runtime._EVFILT_READ
tags: [constant number private]
```

```Go
const _EVFILT_READ = -0x1
```

### <a id="_EVFILT_WRITE" href="#_EVFILT_WRITE">const _EVFILT_WRITE</a>

```
searchKey: runtime._EVFILT_WRITE
tags: [constant number private]
```

```Go
const _EVFILT_WRITE = -0x2
```

### <a id="_EV_ADD" href="#_EV_ADD">const _EV_ADD</a>

```
searchKey: runtime._EV_ADD
tags: [constant number private]
```

```Go
const _EV_ADD = 0x1
```

### <a id="_EV_CLEAR" href="#_EV_CLEAR">const _EV_CLEAR</a>

```
searchKey: runtime._EV_CLEAR
tags: [constant number private]
```

```Go
const _EV_CLEAR = 0x20
```

### <a id="_EV_DELETE" href="#_EV_DELETE">const _EV_DELETE</a>

```
searchKey: runtime._EV_DELETE
tags: [constant number private]
```

```Go
const _EV_DELETE = 0x2
```

### <a id="_EV_EOF" href="#_EV_EOF">const _EV_EOF</a>

```
searchKey: runtime._EV_EOF
tags: [constant number private]
```

```Go
const _EV_EOF = 0x8000
```

### <a id="_EV_ERROR" href="#_EV_ERROR">const _EV_ERROR</a>

```
searchKey: runtime._EV_ERROR
tags: [constant number private]
```

```Go
const _EV_ERROR = 0x4000
```

### <a id="_EV_RECEIPT" href="#_EV_RECEIPT">const _EV_RECEIPT</a>

```
searchKey: runtime._EV_RECEIPT
tags: [constant number private]
```

```Go
const _EV_RECEIPT = 0x40
```

### <a id="_FD_CLOEXEC" href="#_FD_CLOEXEC">const _FD_CLOEXEC</a>

```
searchKey: runtime._FD_CLOEXEC
tags: [constant number private]
```

```Go
const _FD_CLOEXEC = 0x1
```

### <a id="_FPE_FLTDIV" href="#_FPE_FLTDIV">const _FPE_FLTDIV</a>

```
searchKey: runtime._FPE_FLTDIV
tags: [constant number private]
```

```Go
const _FPE_FLTDIV = 0x1
```

### <a id="_FPE_FLTINV" href="#_FPE_FLTINV">const _FPE_FLTINV</a>

```
searchKey: runtime._FPE_FLTINV
tags: [constant number private]
```

```Go
const _FPE_FLTINV = 0x5
```

### <a id="_FPE_FLTOVF" href="#_FPE_FLTOVF">const _FPE_FLTOVF</a>

```
searchKey: runtime._FPE_FLTOVF
tags: [constant number private]
```

```Go
const _FPE_FLTOVF = 0x2
```

### <a id="_FPE_FLTRES" href="#_FPE_FLTRES">const _FPE_FLTRES</a>

```
searchKey: runtime._FPE_FLTRES
tags: [constant number private]
```

```Go
const _FPE_FLTRES = 0x4
```

### <a id="_FPE_FLTSUB" href="#_FPE_FLTSUB">const _FPE_FLTSUB</a>

```
searchKey: runtime._FPE_FLTSUB
tags: [constant number private]
```

```Go
const _FPE_FLTSUB = 0x6
```

### <a id="_FPE_FLTUND" href="#_FPE_FLTUND">const _FPE_FLTUND</a>

```
searchKey: runtime._FPE_FLTUND
tags: [constant number private]
```

```Go
const _FPE_FLTUND = 0x3
```

### <a id="_FPE_INTDIV" href="#_FPE_INTDIV">const _FPE_INTDIV</a>

```
searchKey: runtime._FPE_INTDIV
tags: [constant number private]
```

```Go
const _FPE_INTDIV = 0x7
```

### <a id="_FPE_INTOVF" href="#_FPE_INTOVF">const _FPE_INTOVF</a>

```
searchKey: runtime._FPE_INTOVF
tags: [constant number private]
```

```Go
const _FPE_INTOVF = 0x8
```

### <a id="_FUNCDATA_ArgInfo" href="#_FUNCDATA_ArgInfo">const _FUNCDATA_ArgInfo</a>

```
searchKey: runtime._FUNCDATA_ArgInfo
tags: [constant number private]
```

```Go
const _FUNCDATA_ArgInfo = 5
```

PCDATA and FUNCDATA table indexes. 

See funcdata.h and ../cmd/internal/objabi/funcdata.go. 

### <a id="_FUNCDATA_ArgsPointerMaps" href="#_FUNCDATA_ArgsPointerMaps">const _FUNCDATA_ArgsPointerMaps</a>

```
searchKey: runtime._FUNCDATA_ArgsPointerMaps
tags: [constant number private]
```

```Go
const _FUNCDATA_ArgsPointerMaps = 0
```

PCDATA and FUNCDATA table indexes. 

See funcdata.h and ../cmd/internal/objabi/funcdata.go. 

### <a id="_FUNCDATA_InlTree" href="#_FUNCDATA_InlTree">const _FUNCDATA_InlTree</a>

```
searchKey: runtime._FUNCDATA_InlTree
tags: [constant number private]
```

```Go
const _FUNCDATA_InlTree = 3
```

PCDATA and FUNCDATA table indexes. 

See funcdata.h and ../cmd/internal/objabi/funcdata.go. 

### <a id="_FUNCDATA_LocalsPointerMaps" href="#_FUNCDATA_LocalsPointerMaps">const _FUNCDATA_LocalsPointerMaps</a>

```
searchKey: runtime._FUNCDATA_LocalsPointerMaps
tags: [constant number private]
```

```Go
const _FUNCDATA_LocalsPointerMaps = 1
```

PCDATA and FUNCDATA table indexes. 

See funcdata.h and ../cmd/internal/objabi/funcdata.go. 

### <a id="_FUNCDATA_OpenCodedDeferInfo" href="#_FUNCDATA_OpenCodedDeferInfo">const _FUNCDATA_OpenCodedDeferInfo</a>

```
searchKey: runtime._FUNCDATA_OpenCodedDeferInfo
tags: [constant number private]
```

```Go
const _FUNCDATA_OpenCodedDeferInfo = 4
```

PCDATA and FUNCDATA table indexes. 

See funcdata.h and ../cmd/internal/objabi/funcdata.go. 

### <a id="_FUNCDATA_StackObjects" href="#_FUNCDATA_StackObjects">const _FUNCDATA_StackObjects</a>

```
searchKey: runtime._FUNCDATA_StackObjects
tags: [constant number private]
```

```Go
const _FUNCDATA_StackObjects = 2
```

PCDATA and FUNCDATA table indexes. 

See funcdata.h and ../cmd/internal/objabi/funcdata.go. 

### <a id="_F_GETFL" href="#_F_GETFL">const _F_GETFL</a>

```
searchKey: runtime._F_GETFL
tags: [constant number private]
```

```Go
const _F_GETFL = 0x3
```

### <a id="_F_SETFD" href="#_F_SETFD">const _F_SETFD</a>

```
searchKey: runtime._F_SETFD
tags: [constant number private]
```

```Go
const _F_SETFD = 0x2
```

### <a id="_F_SETFL" href="#_F_SETFL">const _F_SETFL</a>

```
searchKey: runtime._F_SETFL
tags: [constant number private]
```

```Go
const _F_SETFL = 0x4
```

### <a id="_FinBlockSize" href="#_FinBlockSize">const _FinBlockSize</a>

```
searchKey: runtime._FinBlockSize
tags: [constant number private]
```

```Go
const _FinBlockSize = 4 * 1024
```

### <a id="_FixAllocChunk" href="#_FixAllocChunk">const _FixAllocChunk</a>

```
searchKey: runtime._FixAllocChunk
tags: [constant number private]
```

```Go
const _FixAllocChunk = 16 << 10 // Chunk size for FixAlloc

```

### <a id="_FixedStack" href="#_FixedStack">const _FixedStack</a>

```
searchKey: runtime._FixedStack
tags: [constant number private]
```

```Go
const _FixedStack = _FixedStack6 + 1
```

### <a id="_FixedStack0" href="#_FixedStack0">const _FixedStack0</a>

```
searchKey: runtime._FixedStack0
tags: [constant number private]
```

```Go
const _FixedStack0 = _StackMin + _StackSystem
```

The minimum stack size to allocate. The hackery here rounds FixedStack0 up to a power of 2. 

### <a id="_FixedStack1" href="#_FixedStack1">const _FixedStack1</a>

```
searchKey: runtime._FixedStack1
tags: [constant number private]
```

```Go
const _FixedStack1 = _FixedStack0 - 1
```

### <a id="_FixedStack2" href="#_FixedStack2">const _FixedStack2</a>

```
searchKey: runtime._FixedStack2
tags: [constant number private]
```

```Go
const _FixedStack2 = _FixedStack1 | (_FixedStack1 >> 1)
```

### <a id="_FixedStack3" href="#_FixedStack3">const _FixedStack3</a>

```
searchKey: runtime._FixedStack3
tags: [constant number private]
```

```Go
const _FixedStack3 = _FixedStack2 | (_FixedStack2 >> 2)
```

### <a id="_FixedStack4" href="#_FixedStack4">const _FixedStack4</a>

```
searchKey: runtime._FixedStack4
tags: [constant number private]
```

```Go
const _FixedStack4 = _FixedStack3 | (_FixedStack3 >> 4)
```

### <a id="_FixedStack5" href="#_FixedStack5">const _FixedStack5</a>

```
searchKey: runtime._FixedStack5
tags: [constant number private]
```

```Go
const _FixedStack5 = _FixedStack4 | (_FixedStack4 >> 8)
```

### <a id="_FixedStack6" href="#_FixedStack6">const _FixedStack6</a>

```
searchKey: runtime._FixedStack6
tags: [constant number private]
```

```Go
const _FixedStack6 = _FixedStack5 | (_FixedStack5 >> 16)
```

### <a id="_GCmark" href="#_GCmark">const _GCmark</a>

```
searchKey: runtime._GCmark
tags: [constant number private]
```

```Go
const _GCmark // GC marking roots and workbufs: allocate black, write barrier ENABLED

```

### <a id="_GCmarktermination" href="#_GCmarktermination">const _GCmarktermination</a>

```
searchKey: runtime._GCmarktermination
tags: [constant number private]
```

```Go
const _GCmarktermination // GC mark termination: allocate black, P's help GC, write barrier ENABLED

```

### <a id="_GCoff" href="#_GCoff">const _GCoff</a>

```
searchKey: runtime._GCoff
tags: [constant number private]
```

```Go
const _GCoff = iota // GC not running; sweeping in background, write barrier disabled

```

### <a id="_Gcopystack" href="#_Gcopystack">const _Gcopystack</a>

```
searchKey: runtime._Gcopystack
tags: [constant number private]
```

```Go
const _Gcopystack // 8

```

defined constants 

_Gcopystack means this goroutine's stack is being moved. It is not executing user code and is not on a run queue. The stack is owned by the goroutine that put it in _Gcopystack. 

### <a id="_Gdead" href="#_Gdead">const _Gdead</a>

```
searchKey: runtime._Gdead
tags: [constant number private]
```

```Go
const _Gdead // 6

```

defined constants 

_Gdead means this goroutine is currently unused. It may be just exited, on a free list, or just being initialized. It is not executing user code. It may or may not have a stack allocated. The G and its stack (if any) are owned by the M that is exiting the G or that obtained the G from the free list. 

### <a id="_Genqueue_unused" href="#_Genqueue_unused">const _Genqueue_unused</a>

```
searchKey: runtime._Genqueue_unused
tags: [constant number private]
```

```Go
const _Genqueue_unused // 7

```

defined constants 

_Genqueue_unused is currently unused. 

### <a id="_Gidle" href="#_Gidle">const _Gidle</a>

```
searchKey: runtime._Gidle
tags: [constant number private]
```

```Go
const _Gidle = iota // 0

```

defined constants 

_Gidle means this goroutine was just allocated and has not yet been initialized. 

### <a id="_Gmoribund_unused" href="#_Gmoribund_unused">const _Gmoribund_unused</a>

```
searchKey: runtime._Gmoribund_unused
tags: [constant number private]
```

```Go
const _Gmoribund_unused // 5

```

defined constants 

_Gmoribund_unused is currently unused, but hardcoded in gdb scripts. 

### <a id="_GoidCacheBatch" href="#_GoidCacheBatch">const _GoidCacheBatch</a>

```
searchKey: runtime._GoidCacheBatch
tags: [constant number private]
```

```Go
const _GoidCacheBatch = 16
```

Number of goroutine ids to grab from sched.goidgen to local per-P cache at once. 16 seems to provide enough amortization, but other than that it's mostly arbitrary number. 

### <a id="_Gpreempted" href="#_Gpreempted">const _Gpreempted</a>

```
searchKey: runtime._Gpreempted
tags: [constant number private]
```

```Go
const _Gpreempted // 9

```

defined constants 

_Gpreempted means this goroutine stopped itself for a suspendG preemption. It is like _Gwaiting, but nothing is yet responsible for ready()ing it. Some suspendG must CAS the status to _Gwaiting to take responsibility for ready()ing this G. 

### <a id="_Grunnable" href="#_Grunnable">const _Grunnable</a>

```
searchKey: runtime._Grunnable
tags: [constant number private]
```

```Go
const _Grunnable // 1

```

defined constants 

_Grunnable means this goroutine is on a run queue. It is not currently executing user code. The stack is not owned. 

### <a id="_Grunning" href="#_Grunning">const _Grunning</a>

```
searchKey: runtime._Grunning
tags: [constant number private]
```

```Go
const _Grunning // 2

```

defined constants 

_Grunning means this goroutine may execute user code. The stack is owned by this goroutine. It is not on a run queue. It is assigned an M and a P (g.m and g.m.p are valid). 

### <a id="_Gscan" href="#_Gscan">const _Gscan</a>

```
searchKey: runtime._Gscan
tags: [constant number private]
```

```Go
const _Gscan = 0x1000
```

defined constants 

_Gscan combined with one of the above states other than _Grunning indicates that GC is scanning the stack. The goroutine is not executing user code and the stack is owned by the goroutine that set the _Gscan bit. 

_Gscanrunning is different: it is used to briefly block state transitions while GC signals the G to scan its own stack. This is otherwise like _Grunning. 

atomicstatus&~Gscan gives the state the goroutine will return to when the scan completes. 

### <a id="_Gscanpreempted" href="#_Gscanpreempted">const _Gscanpreempted</a>

```
searchKey: runtime._Gscanpreempted
tags: [constant number private]
```

```Go
const _Gscanpreempted = _Gscan + _Gpreempted // 0x1009

```

defined constants 

### <a id="_Gscanrunnable" href="#_Gscanrunnable">const _Gscanrunnable</a>

```
searchKey: runtime._Gscanrunnable
tags: [constant number private]
```

```Go
const _Gscanrunnable = _Gscan + _Grunnable // 0x1001

```

defined constants 

### <a id="_Gscanrunning" href="#_Gscanrunning">const _Gscanrunning</a>

```
searchKey: runtime._Gscanrunning
tags: [constant number private]
```

```Go
const _Gscanrunning = _Gscan + _Grunning // 0x1002

```

defined constants 

### <a id="_Gscansyscall" href="#_Gscansyscall">const _Gscansyscall</a>

```
searchKey: runtime._Gscansyscall
tags: [constant number private]
```

```Go
const _Gscansyscall = _Gscan + _Gsyscall // 0x1003

```

defined constants 

### <a id="_Gscanwaiting" href="#_Gscanwaiting">const _Gscanwaiting</a>

```
searchKey: runtime._Gscanwaiting
tags: [constant number private]
```

```Go
const _Gscanwaiting = _Gscan + _Gwaiting // 0x1004

```

defined constants 

### <a id="_Gsyscall" href="#_Gsyscall">const _Gsyscall</a>

```
searchKey: runtime._Gsyscall
tags: [constant number private]
```

```Go
const _Gsyscall // 3

```

defined constants 

_Gsyscall means this goroutine is executing a system call. It is not executing user code. The stack is owned by this goroutine. It is not on a run queue. It is assigned an M. 

### <a id="_Gwaiting" href="#_Gwaiting">const _Gwaiting</a>

```
searchKey: runtime._Gwaiting
tags: [constant number private]
```

```Go
const _Gwaiting // 4

```

defined constants 

_Gwaiting means this goroutine is blocked in the runtime. It is not executing user code. It is not on a run queue, but should be recorded somewhere (e.g., a channel wait queue) so it can be ready()d when necessary. The stack is not owned *except* that a channel operation may read or write parts of the stack under the appropriate channel lock. Otherwise, it is not safe to access the stack after a goroutine enters _Gwaiting (e.g., it may get moved). 

### <a id="_HW_NCPU" href="#_HW_NCPU">const _HW_NCPU</a>

```
searchKey: runtime._HW_NCPU
tags: [constant number private]
```

```Go
const _HW_NCPU = 3
```

### <a id="_HW_PAGESIZE" href="#_HW_PAGESIZE">const _HW_PAGESIZE</a>

```
searchKey: runtime._HW_PAGESIZE
tags: [constant number private]
```

```Go
const _HW_PAGESIZE = 7
```

### <a id="_ITIMER_PROF" href="#_ITIMER_PROF">const _ITIMER_PROF</a>

```
searchKey: runtime._ITIMER_PROF
tags: [constant number private]
```

```Go
const _ITIMER_PROF = 0x2
```

### <a id="_ITIMER_REAL" href="#_ITIMER_REAL">const _ITIMER_REAL</a>

```
searchKey: runtime._ITIMER_REAL
tags: [constant number private]
```

```Go
const _ITIMER_REAL = 0x0
```

### <a id="_ITIMER_VIRTUAL" href="#_ITIMER_VIRTUAL">const _ITIMER_VIRTUAL</a>

```
searchKey: runtime._ITIMER_VIRTUAL
tags: [constant number private]
```

```Go
const _ITIMER_VIRTUAL = 0x1
```

### <a id="_KindSpecialFinalizer" href="#_KindSpecialFinalizer">const _KindSpecialFinalizer</a>

```
searchKey: runtime._KindSpecialFinalizer
tags: [constant number private]
```

```Go
const _KindSpecialFinalizer = 1
```

### <a id="_KindSpecialProfile" href="#_KindSpecialProfile">const _KindSpecialProfile</a>

```
searchKey: runtime._KindSpecialProfile
tags: [constant number private]
```

```Go
const _KindSpecialProfile = 2
```

### <a id="_KindSpecialReachable" href="#_KindSpecialReachable">const _KindSpecialReachable</a>

```
searchKey: runtime._KindSpecialReachable
tags: [constant number private]
```

```Go
const _KindSpecialReachable = 3
```

_KindSpecialReachable is a special used for tracking reachability during testing. 

### <a id="_MADV_DONTNEED" href="#_MADV_DONTNEED">const _MADV_DONTNEED</a>

```
searchKey: runtime._MADV_DONTNEED
tags: [constant number private]
```

```Go
const _MADV_DONTNEED = 0x4
```

### <a id="_MADV_FREE" href="#_MADV_FREE">const _MADV_FREE</a>

```
searchKey: runtime._MADV_FREE
tags: [constant number private]
```

```Go
const _MADV_FREE = 0x5
```

### <a id="_MADV_FREE_REUSABLE" href="#_MADV_FREE_REUSABLE">const _MADV_FREE_REUSABLE</a>

```
searchKey: runtime._MADV_FREE_REUSABLE
tags: [constant number private]
```

```Go
const _MADV_FREE_REUSABLE = 0x7
```

### <a id="_MADV_FREE_REUSE" href="#_MADV_FREE_REUSE">const _MADV_FREE_REUSE</a>

```
searchKey: runtime._MADV_FREE_REUSE
tags: [constant number private]
```

```Go
const _MADV_FREE_REUSE = 0x8
```

### <a id="_MAP_ANON" href="#_MAP_ANON">const _MAP_ANON</a>

```
searchKey: runtime._MAP_ANON
tags: [constant number private]
```

```Go
const _MAP_ANON = 0x1000
```

### <a id="_MAP_FIXED" href="#_MAP_FIXED">const _MAP_FIXED</a>

```
searchKey: runtime._MAP_FIXED
tags: [constant number private]
```

```Go
const _MAP_FIXED = 0x10
```

### <a id="_MAP_PRIVATE" href="#_MAP_PRIVATE">const _MAP_PRIVATE</a>

```
searchKey: runtime._MAP_PRIVATE
tags: [constant number private]
```

```Go
const _MAP_PRIVATE = 0x2
```

### <a id="_MaxGcproc" href="#_MaxGcproc">const _MaxGcproc</a>

```
searchKey: runtime._MaxGcproc
tags: [constant number private]
```

```Go
const _MaxGcproc = 32
```

Max number of threads to run garbage collection. 2, 3, and 4 are all plausible maximums depending on the hardware details of the machine. The garbage collector scales well to 32 cpus. 

### <a id="_MaxSmallSize" href="#_MaxSmallSize">const _MaxSmallSize</a>

```
searchKey: runtime._MaxSmallSize
tags: [constant number private]
```

```Go
const _MaxSmallSize = 32768
```

### <a id="_NSIG" href="#_NSIG">const _NSIG</a>

```
searchKey: runtime._NSIG
tags: [constant number private]
```

```Go
const _NSIG = 32
```

### <a id="_NumSizeClasses" href="#_NumSizeClasses">const _NumSizeClasses</a>

```
searchKey: runtime._NumSizeClasses
tags: [constant number private]
```

```Go
const _NumSizeClasses = 68
```

### <a id="_NumStackOrders" href="#_NumStackOrders">const _NumStackOrders</a>

```
searchKey: runtime._NumStackOrders
tags: [constant number private]
```

```Go
const _NumStackOrders = 4 - sys.PtrSize/4*sys.GoosWindows - 1*sys.GoosPlan9
```

Number of orders that get caching. Order 0 is FixedStack and each successive order is twice as large. We want to cache 2KB, 4KB, 8KB, and 16KB stacks. Larger stacks will be allocated directly. Since FixedStack is different on different systems, we must vary NumStackOrders to keep the same maximum cached size. 

```
OS               | FixedStack | NumStackOrders
-----------------+------------+---------------
linux/darwin/bsd | 2KB        | 4
windows/32       | 4KB        | 3
windows/64       | 8KB        | 2
plan9            | 4KB        | 3

```
### <a id="_O_NONBLOCK" href="#_O_NONBLOCK">const _O_NONBLOCK</a>

```
searchKey: runtime._O_NONBLOCK
tags: [constant number private]
```

```Go
const _O_NONBLOCK = 4
```

### <a id="_PCDATA_InlTreeIndex" href="#_PCDATA_InlTreeIndex">const _PCDATA_InlTreeIndex</a>

```
searchKey: runtime._PCDATA_InlTreeIndex
tags: [constant number private]
```

```Go
const _PCDATA_InlTreeIndex = 2
```

PCDATA and FUNCDATA table indexes. 

See funcdata.h and ../cmd/internal/objabi/funcdata.go. 

### <a id="_PCDATA_Restart1" href="#_PCDATA_Restart1">const _PCDATA_Restart1</a>

```
searchKey: runtime._PCDATA_Restart1
tags: [constant number private]
```

```Go
const _PCDATA_Restart1 = -3
```

_PCDATA_Restart1(2) apply on a sequence of instructions, within which if an async preemption happens, we should back off the PC to the start of the sequence when resume. We need two so we can distinguish the start/end of the sequence in case that two sequences are next to each other. 

### <a id="_PCDATA_Restart2" href="#_PCDATA_Restart2">const _PCDATA_Restart2</a>

```
searchKey: runtime._PCDATA_Restart2
tags: [constant number private]
```

```Go
const _PCDATA_Restart2 = -4
```

### <a id="_PCDATA_RestartAtEntry" href="#_PCDATA_RestartAtEntry">const _PCDATA_RestartAtEntry</a>

```
searchKey: runtime._PCDATA_RestartAtEntry
tags: [constant number private]
```

```Go
const _PCDATA_RestartAtEntry = -5
```

Like _PCDATA_RestartAtEntry, but back to function entry if async preempted. 

### <a id="_PCDATA_StackMapIndex" href="#_PCDATA_StackMapIndex">const _PCDATA_StackMapIndex</a>

```
searchKey: runtime._PCDATA_StackMapIndex
tags: [constant number private]
```

```Go
const _PCDATA_StackMapIndex = 1
```

PCDATA and FUNCDATA table indexes. 

See funcdata.h and ../cmd/internal/objabi/funcdata.go. 

### <a id="_PCDATA_UnsafePoint" href="#_PCDATA_UnsafePoint">const _PCDATA_UnsafePoint</a>

```
searchKey: runtime._PCDATA_UnsafePoint
tags: [constant number private]
```

```Go
const _PCDATA_UnsafePoint = 0
```

PCDATA and FUNCDATA table indexes. 

See funcdata.h and ../cmd/internal/objabi/funcdata.go. 

### <a id="_PCDATA_UnsafePointSafe" href="#_PCDATA_UnsafePointSafe">const _PCDATA_UnsafePointSafe</a>

```
searchKey: runtime._PCDATA_UnsafePointSafe
tags: [constant number private]
```

```Go
const _PCDATA_UnsafePointSafe = -1 // Safe for async preemption

```

PCDATA_UnsafePoint values. 

### <a id="_PCDATA_UnsafePointUnsafe" href="#_PCDATA_UnsafePointUnsafe">const _PCDATA_UnsafePointUnsafe</a>

```
searchKey: runtime._PCDATA_UnsafePointUnsafe
tags: [constant number private]
```

```Go
const _PCDATA_UnsafePointUnsafe = -2 // Unsafe for async preemption

```

### <a id="_PROT_EXEC" href="#_PROT_EXEC">const _PROT_EXEC</a>

```
searchKey: runtime._PROT_EXEC
tags: [constant number private]
```

```Go
const _PROT_EXEC = 0x4
```

### <a id="_PROT_NONE" href="#_PROT_NONE">const _PROT_NONE</a>

```
searchKey: runtime._PROT_NONE
tags: [constant number private]
```

```Go
const _PROT_NONE = 0x0
```

### <a id="_PROT_READ" href="#_PROT_READ">const _PROT_READ</a>

```
searchKey: runtime._PROT_READ
tags: [constant number private]
```

```Go
const _PROT_READ = 0x1
```

### <a id="_PROT_WRITE" href="#_PROT_WRITE">const _PROT_WRITE</a>

```
searchKey: runtime._PROT_WRITE
tags: [constant number private]
```

```Go
const _PROT_WRITE = 0x2
```

### <a id="_PTHREAD_CREATE_DETACHED" href="#_PTHREAD_CREATE_DETACHED">const _PTHREAD_CREATE_DETACHED</a>

```
searchKey: runtime._PTHREAD_CREATE_DETACHED
tags: [constant number private]
```

```Go
const _PTHREAD_CREATE_DETACHED = 0x2
```

### <a id="_PageMask" href="#_PageMask">const _PageMask</a>

```
searchKey: runtime._PageMask
tags: [constant number private]
```

```Go
const _PageMask = _PageSize - 1
```

### <a id="_PageShift" href="#_PageShift">const _PageShift</a>

```
searchKey: runtime._PageShift
tags: [constant number private]
```

```Go
const _PageShift = 13
```

### <a id="_PageSize" href="#_PageSize">const _PageSize</a>

```
searchKey: runtime._PageSize
tags: [constant number private]
```

```Go
const _PageSize = 1 << _PageShift
```

### <a id="_Pdead" href="#_Pdead">const _Pdead</a>

```
searchKey: runtime._Pdead
tags: [constant number private]
```

```Go
const _Pdead
```

_Pdead means a P is no longer used (GOMAXPROCS shrank). We reuse Ps if GOMAXPROCS increases. A dead P is mostly stripped of its resources, though a few things remain (e.g., trace buffers). 

### <a id="_Pgcstop" href="#_Pgcstop">const _Pgcstop</a>

```
searchKey: runtime._Pgcstop
tags: [constant number private]
```

```Go
const _Pgcstop
```

_Pgcstop means a P is halted for STW and owned by the M that stopped the world. The M that stopped the world continues to use its P, even in _Pgcstop. Transitioning from _Prunning to _Pgcstop causes an M to release its P and park. 

The P retains its run queue and startTheWorld will restart the scheduler on Ps with non-empty run queues. 

### <a id="_Pidle" href="#_Pidle">const _Pidle</a>

```
searchKey: runtime._Pidle
tags: [constant number private]
```

```Go
const _Pidle = iota
```

_Pidle means a P is not being used to run user code or the scheduler. Typically, it's on the idle P list and available to the scheduler, but it may just be transitioning between other states. 

The P is owned by the idle list or by whatever is transitioning its state. Its run queue is empty. 

### <a id="_Prunning" href="#_Prunning">const _Prunning</a>

```
searchKey: runtime._Prunning
tags: [constant number private]
```

```Go
const _Prunning
```

_Prunning means a P is owned by an M and is being used to run user code or the scheduler. Only the M that owns this P is allowed to change the P's status from _Prunning. The M may transition the P to _Pidle (if it has no more work to do), _Psyscall (when entering a syscall), or _Pgcstop (to halt for the GC). The M may also hand ownership of the P off directly to another M (e.g., to schedule a locked G). 

### <a id="_Psyscall" href="#_Psyscall">const _Psyscall</a>

```
searchKey: runtime._Psyscall
tags: [constant number private]
```

```Go
const _Psyscall
```

_Psyscall means a P is not running user code. It has affinity to an M in a syscall but is not owned by it and may be stolen by another M. This is similar to _Pidle but uses lightweight transitions and maintains M affinity. 

Leaving _Psyscall must be done with a CAS, either to steal or retake the P. Note that there's an ABA hazard: even if an M successfully CASes its original P back to _Prunning after a syscall, it must understand the P may have been used by another M in the interim. 

### <a id="_SA_64REGSET" href="#_SA_64REGSET">const _SA_64REGSET</a>

```
searchKey: runtime._SA_64REGSET
tags: [constant number private]
```

```Go
const _SA_64REGSET = 0x200
```

### <a id="_SA_ONSTACK" href="#_SA_ONSTACK">const _SA_ONSTACK</a>

```
searchKey: runtime._SA_ONSTACK
tags: [constant number private]
```

```Go
const _SA_ONSTACK = 0x1
```

### <a id="_SA_RESTART" href="#_SA_RESTART">const _SA_RESTART</a>

```
searchKey: runtime._SA_RESTART
tags: [constant number private]
```

```Go
const _SA_RESTART = 0x2
```

### <a id="_SA_SIGINFO" href="#_SA_SIGINFO">const _SA_SIGINFO</a>

```
searchKey: runtime._SA_SIGINFO
tags: [constant number private]
```

```Go
const _SA_SIGINFO = 0x40
```

### <a id="_SA_USERTRAMP" href="#_SA_USERTRAMP">const _SA_USERTRAMP</a>

```
searchKey: runtime._SA_USERTRAMP
tags: [constant number private]
```

```Go
const _SA_USERTRAMP = 0x100
```

### <a id="_SEGV_ACCERR" href="#_SEGV_ACCERR">const _SEGV_ACCERR</a>

```
searchKey: runtime._SEGV_ACCERR
tags: [constant number private]
```

```Go
const _SEGV_ACCERR = 0x2
```

### <a id="_SEGV_MAPERR" href="#_SEGV_MAPERR">const _SEGV_MAPERR</a>

```
searchKey: runtime._SEGV_MAPERR
tags: [constant number private]
```

```Go
const _SEGV_MAPERR = 0x1
```

### <a id="_SIGABRT" href="#_SIGABRT">const _SIGABRT</a>

```
searchKey: runtime._SIGABRT
tags: [constant number private]
```

```Go
const _SIGABRT = 0x6
```

### <a id="_SIGALRM" href="#_SIGALRM">const _SIGALRM</a>

```
searchKey: runtime._SIGALRM
tags: [constant number private]
```

```Go
const _SIGALRM = 0xe
```

### <a id="_SIGBUS" href="#_SIGBUS">const _SIGBUS</a>

```
searchKey: runtime._SIGBUS
tags: [constant number private]
```

```Go
const _SIGBUS = 0xa
```

### <a id="_SIGCHLD" href="#_SIGCHLD">const _SIGCHLD</a>

```
searchKey: runtime._SIGCHLD
tags: [constant number private]
```

```Go
const _SIGCHLD = 0x14
```

### <a id="_SIGCONT" href="#_SIGCONT">const _SIGCONT</a>

```
searchKey: runtime._SIGCONT
tags: [constant number private]
```

```Go
const _SIGCONT = 0x13
```

### <a id="_SIGEMT" href="#_SIGEMT">const _SIGEMT</a>

```
searchKey: runtime._SIGEMT
tags: [constant number private]
```

```Go
const _SIGEMT = 0x7
```

### <a id="_SIGFPE" href="#_SIGFPE">const _SIGFPE</a>

```
searchKey: runtime._SIGFPE
tags: [constant number private]
```

```Go
const _SIGFPE = 0x8
```

### <a id="_SIGHUP" href="#_SIGHUP">const _SIGHUP</a>

```
searchKey: runtime._SIGHUP
tags: [constant number private]
```

```Go
const _SIGHUP = 0x1
```

### <a id="_SIGILL" href="#_SIGILL">const _SIGILL</a>

```
searchKey: runtime._SIGILL
tags: [constant number private]
```

```Go
const _SIGILL = 0x4
```

### <a id="_SIGINFO" href="#_SIGINFO">const _SIGINFO</a>

```
searchKey: runtime._SIGINFO
tags: [constant number private]
```

```Go
const _SIGINFO = 0x1d
```

### <a id="_SIGINT" href="#_SIGINT">const _SIGINT</a>

```
searchKey: runtime._SIGINT
tags: [constant number private]
```

```Go
const _SIGINT = 0x2
```

### <a id="_SIGIO" href="#_SIGIO">const _SIGIO</a>

```
searchKey: runtime._SIGIO
tags: [constant number private]
```

```Go
const _SIGIO = 0x17
```

### <a id="_SIGKILL" href="#_SIGKILL">const _SIGKILL</a>

```
searchKey: runtime._SIGKILL
tags: [constant number private]
```

```Go
const _SIGKILL = 0x9
```

### <a id="_SIGPIPE" href="#_SIGPIPE">const _SIGPIPE</a>

```
searchKey: runtime._SIGPIPE
tags: [constant number private]
```

```Go
const _SIGPIPE = 0xd
```

### <a id="_SIGPROF" href="#_SIGPROF">const _SIGPROF</a>

```
searchKey: runtime._SIGPROF
tags: [constant number private]
```

```Go
const _SIGPROF = 0x1b
```

### <a id="_SIGQUIT" href="#_SIGQUIT">const _SIGQUIT</a>

```
searchKey: runtime._SIGQUIT
tags: [constant number private]
```

```Go
const _SIGQUIT = 0x3
```

### <a id="_SIGSEGV" href="#_SIGSEGV">const _SIGSEGV</a>

```
searchKey: runtime._SIGSEGV
tags: [constant number private]
```

```Go
const _SIGSEGV = 0xb
```

### <a id="_SIGSTOP" href="#_SIGSTOP">const _SIGSTOP</a>

```
searchKey: runtime._SIGSTOP
tags: [constant number private]
```

```Go
const _SIGSTOP = 0x11
```

### <a id="_SIGSYS" href="#_SIGSYS">const _SIGSYS</a>

```
searchKey: runtime._SIGSYS
tags: [constant number private]
```

```Go
const _SIGSYS = 0xc
```

### <a id="_SIGTERM" href="#_SIGTERM">const _SIGTERM</a>

```
searchKey: runtime._SIGTERM
tags: [constant number private]
```

```Go
const _SIGTERM = 0xf
```

### <a id="_SIGTRAP" href="#_SIGTRAP">const _SIGTRAP</a>

```
searchKey: runtime._SIGTRAP
tags: [constant number private]
```

```Go
const _SIGTRAP = 0x5
```

### <a id="_SIGTSTP" href="#_SIGTSTP">const _SIGTSTP</a>

```
searchKey: runtime._SIGTSTP
tags: [constant number private]
```

```Go
const _SIGTSTP = 0x12
```

### <a id="_SIGTTIN" href="#_SIGTTIN">const _SIGTTIN</a>

```
searchKey: runtime._SIGTTIN
tags: [constant number private]
```

```Go
const _SIGTTIN = 0x15
```

### <a id="_SIGTTOU" href="#_SIGTTOU">const _SIGTTOU</a>

```
searchKey: runtime._SIGTTOU
tags: [constant number private]
```

```Go
const _SIGTTOU = 0x16
```

### <a id="_SIGURG" href="#_SIGURG">const _SIGURG</a>

```
searchKey: runtime._SIGURG
tags: [constant number private]
```

```Go
const _SIGURG = 0x10
```

### <a id="_SIGUSR1" href="#_SIGUSR1">const _SIGUSR1</a>

```
searchKey: runtime._SIGUSR1
tags: [constant number private]
```

```Go
const _SIGUSR1 = 0x1e
```

### <a id="_SIGUSR2" href="#_SIGUSR2">const _SIGUSR2</a>

```
searchKey: runtime._SIGUSR2
tags: [constant number private]
```

```Go
const _SIGUSR2 = 0x1f
```

### <a id="_SIGVTALRM" href="#_SIGVTALRM">const _SIGVTALRM</a>

```
searchKey: runtime._SIGVTALRM
tags: [constant number private]
```

```Go
const _SIGVTALRM = 0x1a
```

### <a id="_SIGWINCH" href="#_SIGWINCH">const _SIGWINCH</a>

```
searchKey: runtime._SIGWINCH
tags: [constant number private]
```

```Go
const _SIGWINCH = 0x1c
```

### <a id="_SIGXCPU" href="#_SIGXCPU">const _SIGXCPU</a>

```
searchKey: runtime._SIGXCPU
tags: [constant number private]
```

```Go
const _SIGXCPU = 0x18
```

### <a id="_SIGXFSZ" href="#_SIGXFSZ">const _SIGXFSZ</a>

```
searchKey: runtime._SIGXFSZ
tags: [constant number private]
```

```Go
const _SIGXFSZ = 0x19
```

### <a id="_SIG_BLOCK" href="#_SIG_BLOCK">const _SIG_BLOCK</a>

```
searchKey: runtime._SIG_BLOCK
tags: [constant number private]
```

```Go
const _SIG_BLOCK = 1
```

### <a id="_SIG_DFL" href="#_SIG_DFL">const _SIG_DFL</a>

```
searchKey: runtime._SIG_DFL
tags: [constant number private]
```

```Go
const _SIG_DFL uintptr = 0
```

### <a id="_SIG_IGN" href="#_SIG_IGN">const _SIG_IGN</a>

```
searchKey: runtime._SIG_IGN
tags: [constant number private]
```

```Go
const _SIG_IGN uintptr = 1
```

### <a id="_SIG_SETMASK" href="#_SIG_SETMASK">const _SIG_SETMASK</a>

```
searchKey: runtime._SIG_SETMASK
tags: [constant number private]
```

```Go
const _SIG_SETMASK = 3
```

### <a id="_SIG_UNBLOCK" href="#_SIG_UNBLOCK">const _SIG_UNBLOCK</a>

```
searchKey: runtime._SIG_UNBLOCK
tags: [constant number private]
```

```Go
const _SIG_UNBLOCK = 2
```

### <a id="_SI_USER" href="#_SI_USER">const _SI_USER</a>

```
searchKey: runtime._SI_USER
tags: [constant number private]
```

```Go
const _SI_USER = 0 /* empirically true, but not what headers say */

```

### <a id="_SS_DISABLE" href="#_SS_DISABLE">const _SS_DISABLE</a>

```
searchKey: runtime._SS_DISABLE
tags: [constant number private]
```

```Go
const _SS_DISABLE = 4
```

### <a id="_SigDefault" href="#_SigDefault">const _SigDefault</a>

```
searchKey: runtime._SigDefault
tags: [constant number private]
```

```Go
const _SigDefault // if the signal isn't explicitly requested, don't monitor it

```

Values for the flags field of a sigTabT. 

### <a id="_SigGoExit" href="#_SigGoExit">const _SigGoExit</a>

```
searchKey: runtime._SigGoExit
tags: [constant number private]
```

```Go
const _SigGoExit // cause all runtime procs to exit (only used on Plan 9).

```

Values for the flags field of a sigTabT. 

### <a id="_SigIgn" href="#_SigIgn">const _SigIgn</a>

```
searchKey: runtime._SigIgn
tags: [constant number private]
```

```Go
const _SigIgn // _SIG_DFL action is to ignore the signal

```

Values for the flags field of a sigTabT. 

### <a id="_SigKill" href="#_SigKill">const _SigKill</a>

```
searchKey: runtime._SigKill
tags: [constant number private]
```

```Go
const _SigKill // if signal.Notify doesn't take it, exit quietly

```

Values for the flags field of a sigTabT. 

### <a id="_SigNotify" href="#_SigNotify">const _SigNotify</a>

```
searchKey: runtime._SigNotify
tags: [constant number private]
```

```Go
const _SigNotify = 1 << iota // let signal.Notify have signal, even if from kernel

```

Values for the flags field of a sigTabT. 

### <a id="_SigPanic" href="#_SigPanic">const _SigPanic</a>

```
searchKey: runtime._SigPanic
tags: [constant number private]
```

```Go
const _SigPanic // if the signal is from the kernel, panic

```

Values for the flags field of a sigTabT. 

### <a id="_SigSetStack" href="#_SigSetStack">const _SigSetStack</a>

```
searchKey: runtime._SigSetStack
tags: [constant number private]
```

```Go
const _SigSetStack // add SA_ONSTACK to libc handler

```

Values for the flags field of a sigTabT. 

### <a id="_SigThrow" href="#_SigThrow">const _SigThrow</a>

```
searchKey: runtime._SigThrow
tags: [constant number private]
```

```Go
const _SigThrow // if signal.Notify doesn't take it, exit loudly

```

Values for the flags field of a sigTabT. 

### <a id="_SigUnblock" href="#_SigUnblock">const _SigUnblock</a>

```
searchKey: runtime._SigUnblock
tags: [constant number private]
```

```Go
const _SigUnblock // always unblock; see blockableSig

```

Values for the flags field of a sigTabT. 

### <a id="_StackBig" href="#_StackBig">const _StackBig</a>

```
searchKey: runtime._StackBig
tags: [constant number private]
```

```Go
const _StackBig = 4096
```

Functions that need frames bigger than this use an extra instruction to do the stack split check, to avoid overflow in case SP - framesize wraps below zero. This value can be no bigger than the size of the unmapped space at zero. 

### <a id="_StackCacheSize" href="#_StackCacheSize">const _StackCacheSize</a>

```
searchKey: runtime._StackCacheSize
tags: [constant number private]
```

```Go
const _StackCacheSize = 32 * 1024
```

Per-P, per order stack segment cache size. 

### <a id="_StackGuard" href="#_StackGuard">const _StackGuard</a>

```
searchKey: runtime._StackGuard
tags: [constant number private]
```

```Go
const _StackGuard = 928*sys.StackGuardMultiplier + _StackSystem
```

The stack guard is a pointer this many bytes above the bottom of the stack. 

The guard leaves enough room for one _StackSmall frame plus a _StackLimit chain of NOSPLIT calls plus _StackSystem bytes for the OS. 

### <a id="_StackLimit" href="#_StackLimit">const _StackLimit</a>

```
searchKey: runtime._StackLimit
tags: [constant number private]
```

```Go
const _StackLimit = _StackGuard - _StackSystem - _StackSmall
```

The maximum number of bytes that a chain of NOSPLIT functions can use. 

### <a id="_StackMin" href="#_StackMin">const _StackMin</a>

```
searchKey: runtime._StackMin
tags: [constant number private]
```

```Go
const _StackMin = 2048
```

The minimum size of stack used by Go code 

### <a id="_StackSmall" href="#_StackSmall">const _StackSmall</a>

```
searchKey: runtime._StackSmall
tags: [constant number private]
```

```Go
const _StackSmall = 128
```

After a stack split check the SP is allowed to be this many bytes below the stack guard. This saves an instruction in the checking sequence for tiny frames. 

### <a id="_StackSystem" href="#_StackSystem">const _StackSystem</a>

```
searchKey: runtime._StackSystem
tags: [constant number private]
```

```Go
const _StackSystem = ...
```

StackSystem is a number of additional bytes to add to each stack below the usual guard area for OS-specific purposes like signal handling. Used on Windows, Plan 9, and iOS because they do not use a separate stack. 

### <a id="_TinySize" href="#_TinySize">const _TinySize</a>

```
searchKey: runtime._TinySize
tags: [constant number private]
```

```Go
const _TinySize = 16
```

Tiny allocator parameters, see "Tiny allocator" comment in malloc.go. 

### <a id="_TinySizeClass" href="#_TinySizeClass">const _TinySizeClass</a>

```
searchKey: runtime._TinySizeClass
tags: [constant number private]
```

```Go
const _TinySizeClass = int8(2)
```

### <a id="_TraceJumpStack" href="#_TraceJumpStack">const _TraceJumpStack</a>

```
searchKey: runtime._TraceJumpStack
tags: [constant number private]
```

```Go
const _TraceJumpStack // if traceback is on a systemstack, resume trace at g that called into it

```

### <a id="_TraceRuntimeFrames" href="#_TraceRuntimeFrames">const _TraceRuntimeFrames</a>

```
searchKey: runtime._TraceRuntimeFrames
tags: [constant number private]
```

```Go
const _TraceRuntimeFrames = 1 << iota // include frames for internal runtime functions.

```

### <a id="_TraceTrap" href="#_TraceTrap">const _TraceTrap</a>

```
searchKey: runtime._TraceTrap
tags: [constant number private]
```

```Go
const _TraceTrap // the initial PC, SP are from a trap, not a return PC from a call

```

### <a id="_TracebackMaxFrames" href="#_TracebackMaxFrames">const _TracebackMaxFrames</a>

```
searchKey: runtime._TracebackMaxFrames
tags: [constant number private]
```

```Go
const _TracebackMaxFrames = 100
```

The maximum number of frames we print for a traceback 

### <a id="_WorkbufSize" href="#_WorkbufSize">const _WorkbufSize</a>

```
searchKey: runtime._WorkbufSize
tags: [constant number private]
```

```Go
const _WorkbufSize = 2048 // in bytes; larger values result in less contention

```

## <a id="var" href="#var">Variables</a>

```
tags: [package]
```

### <a id="Atoi" href="#Atoi">var Atoi</a>

```
searchKey: runtime.Atoi
tags: [variable function private]
```

```Go
var Atoi = atoi
```

### <a id="Atoi32" href="#Atoi32">var Atoi32</a>

```
searchKey: runtime.Atoi32
tags: [variable function private]
```

```Go
var Atoi32 = atoi32
```

### <a id="BaseChunkIdx" href="#BaseChunkIdx">var BaseChunkIdx</a>

```
searchKey: runtime.BaseChunkIdx
tags: [variable number private]
```

```Go
var BaseChunkIdx = ...
```

BaseChunkIdx is a convenient chunkIdx value which works on both 64 bit and 32 bit platforms, allowing the tests to share code between the two. 

This should not be higher than 0x100*pallocChunkBytes to support mips and mipsle, which only have 31-bit address spaces. 

### <a id="BigEndian" href="#BigEndian">var BigEndian</a>

```
searchKey: runtime.BigEndian
tags: [variable boolean private]
```

```Go
var BigEndian = sys.BigEndian
```

### <a id="BytesHash" href="#BytesHash">var BytesHash</a>

```
searchKey: runtime.BytesHash
tags: [variable function private]
```

```Go
var BytesHash = bytesHash
```

### <a id="Close" href="#Close">var Close</a>

```
searchKey: runtime.Close
tags: [variable function private]
```

```Go
var Close = closefd
```

### <a id="Closeonexec" href="#Closeonexec">var Closeonexec</a>

```
searchKey: runtime.Closeonexec
tags: [variable function private]
```

```Go
var Closeonexec = closeonexec
```

### <a id="Dlog" href="#Dlog">var Dlog</a>

```
searchKey: runtime.Dlog
tags: [variable function private]
```

```Go
var Dlog = dlog
```

### <a id="EfaceHash" href="#EfaceHash">var EfaceHash</a>

```
searchKey: runtime.EfaceHash
tags: [variable function private]
```

```Go
var EfaceHash = efaceHash
```

### <a id="Entersyscall" href="#Entersyscall">var Entersyscall</a>

```
searchKey: runtime.Entersyscall
tags: [variable function private]
```

```Go
var Entersyscall = entersyscall
```

### <a id="Exitsyscall" href="#Exitsyscall">var Exitsyscall</a>

```
searchKey: runtime.Exitsyscall
tags: [variable function private]
```

```Go
var Exitsyscall = exitsyscall
```

### <a id="F32to64" href="#F32to64">var F32to64</a>

```
searchKey: runtime.F32to64
tags: [variable function private]
```

```Go
var F32to64 = f32to64
```

### <a id="F64to32" href="#F64to32">var F64to32</a>

```
searchKey: runtime.F64to32
tags: [variable function private]
```

```Go
var F64to32 = f64to32
```

### <a id="F64toint" href="#F64toint">var F64toint</a>

```
searchKey: runtime.F64toint
tags: [variable function private]
```

```Go
var F64toint = f64toint
```

### <a id="Fadd64" href="#Fadd64">var Fadd64</a>

```
searchKey: runtime.Fadd64
tags: [variable function private]
```

```Go
var Fadd64 = fadd64
```

### <a id="Fastlog2" href="#Fastlog2">var Fastlog2</a>

```
searchKey: runtime.Fastlog2
tags: [variable function private]
```

```Go
var Fastlog2 = fastlog2
```

### <a id="Fcmp64" href="#Fcmp64">var Fcmp64</a>

```
searchKey: runtime.Fcmp64
tags: [variable function private]
```

```Go
var Fcmp64 = fcmp64
```

### <a id="Fdiv64" href="#Fdiv64">var Fdiv64</a>

```
searchKey: runtime.Fdiv64
tags: [variable function private]
```

```Go
var Fdiv64 = fdiv64
```

### <a id="Fintto64" href="#Fintto64">var Fintto64</a>

```
searchKey: runtime.Fintto64
tags: [variable function private]
```

```Go
var Fintto64 = fintto64
```

### <a id="Fmul64" href="#Fmul64">var Fmul64</a>

```
searchKey: runtime.Fmul64
tags: [variable function private]
```

```Go
var Fmul64 = fmul64
```

### <a id="ForceGCPeriod" href="#ForceGCPeriod">var ForceGCPeriod</a>

```
searchKey: runtime.ForceGCPeriod
tags: [variable number private]
```

```Go
var ForceGCPeriod = &forcegcperiod
```

### <a id="Fsub64" href="#Fsub64">var Fsub64</a>

```
searchKey: runtime.Fsub64
tags: [variable function private]
```

```Go
var Fsub64 = fsub64
```

### <a id="FuncPC" href="#FuncPC">var FuncPC</a>

```
searchKey: runtime.FuncPC
tags: [variable function private]
```

```Go
var FuncPC = funcPC
```

### <a id="GCTestMoveStackOnNextCall" href="#GCTestMoveStackOnNextCall">var GCTestMoveStackOnNextCall</a>

```
searchKey: runtime.GCTestMoveStackOnNextCall
tags: [variable function private]
```

```Go
var GCTestMoveStackOnNextCall = gcTestMoveStackOnNextCall
```

For GCTestMoveStackOnNextCall, it's important not to introduce an extra layer of call, since then there's a return before the "real" next call. 

### <a id="HashLoad" href="#HashLoad">var HashLoad</a>

```
searchKey: runtime.HashLoad
tags: [variable number private]
```

```Go
var HashLoad = &hashLoad
```

### <a id="IfaceHash" href="#IfaceHash">var IfaceHash</a>

```
searchKey: runtime.IfaceHash
tags: [variable function private]
```

```Go
var IfaceHash = ifaceHash
```

### <a id="Int32Hash" href="#Int32Hash">var Int32Hash</a>

```
searchKey: runtime.Int32Hash
tags: [variable function private]
```

```Go
var Int32Hash = int32Hash
```

### <a id="Int64Hash" href="#Int64Hash">var Int64Hash</a>

```
searchKey: runtime.Int64Hash
tags: [variable function private]
```

```Go
var Int64Hash = int64Hash
```

### <a id="LockPartialOrder" href="#LockPartialOrder">var LockPartialOrder</a>

```
searchKey: runtime.LockPartialOrder
tags: [variable array array number private]
```

```Go
var LockPartialOrder = lockPartialOrder
```

### <a id="LockedOSThread" href="#LockedOSThread">var LockedOSThread</a>

```
searchKey: runtime.LockedOSThread
tags: [variable function private]
```

```Go
var LockedOSThread = lockedOSThread
```

### <a id="MemHash" href="#MemHash">var MemHash</a>

```
searchKey: runtime.MemHash
tags: [variable function private]
```

```Go
var MemHash = memhash
```

### <a id="MemHash32" href="#MemHash32">var MemHash32</a>

```
searchKey: runtime.MemHash32
tags: [variable function private]
```

```Go
var MemHash32 = memhash32
```

### <a id="MemHash64" href="#MemHash64">var MemHash64</a>

```
searchKey: runtime.MemHash64
tags: [variable function private]
```

```Go
var MemHash64 = memhash64
```

### <a id="MemProfileRate" href="#MemProfileRate">var MemProfileRate</a>

```
searchKey: runtime.MemProfileRate
tags: [variable number]
```

```Go
var MemProfileRate int = defaultMemProfileRate(512 * 1024)
```

MemProfileRate controls the fraction of memory allocations that are recorded and reported in the memory profile. The profiler aims to sample an average of one allocation per MemProfileRate bytes allocated. 

To include every allocated block in the profile, set MemProfileRate to 1. To turn off profiling entirely, set MemProfileRate to 0. 

The tools that process the memory profiles assume that the profile rate is constant across the lifetime of the program and equal to the current value. Programs that change the memory profiling rate should do so just once, as early as possible in the execution of the program (for example, at the beginning of main). 

### <a id="MemclrNoHeapPointers" href="#MemclrNoHeapPointers">var MemclrNoHeapPointers</a>

```
searchKey: runtime.MemclrNoHeapPointers
tags: [variable function private]
```

```Go
var MemclrNoHeapPointers = memclrNoHeapPointers
```

### <a id="Memmove" href="#Memmove">var Memmove</a>

```
searchKey: runtime.Memmove
tags: [variable function private]
```

```Go
var Memmove = memmove
```

### <a id="Mmap" href="#Mmap">var Mmap</a>

```
searchKey: runtime.Mmap
tags: [variable function private]
```

```Go
var Mmap = mmap
```

### <a id="Munmap" href="#Munmap">var Munmap</a>

```
searchKey: runtime.Munmap
tags: [variable function private]
```

```Go
var Munmap = munmap
```

### <a id="Nanotime" href="#Nanotime">var Nanotime</a>

```
searchKey: runtime.Nanotime
tags: [variable function private]
```

```Go
var Nanotime = nanotime
```

### <a id="NetpollBreak" href="#NetpollBreak">var NetpollBreak</a>

```
searchKey: runtime.NetpollBreak
tags: [variable function private]
```

```Go
var NetpollBreak = netpollBreak
```

### <a id="NetpollGenericInit" href="#NetpollGenericInit">var NetpollGenericInit</a>

```
searchKey: runtime.NetpollGenericInit
tags: [variable function private]
```

```Go
var NetpollGenericInit = netpollGenericInit
```

### <a id="NonblockingPipe" href="#NonblockingPipe">var NonblockingPipe</a>

```
searchKey: runtime.NonblockingPipe
tags: [variable function private]
```

```Go
var NonblockingPipe = nonblockingPipe
```

### <a id="Open" href="#Open">var Open</a>

```
searchKey: runtime.Open
tags: [variable function private]
```

```Go
var Open = open
```

### <a id="PhysHugePageSize" href="#PhysHugePageSize">var PhysHugePageSize</a>

```
searchKey: runtime.PhysHugePageSize
tags: [variable number private]
```

```Go
var PhysHugePageSize = physHugePageSize
```

### <a id="PhysPageSize" href="#PhysPageSize">var PhysPageSize</a>

```
searchKey: runtime.PhysPageSize
tags: [variable number private]
```

```Go
var PhysPageSize = physPageSize
```

### <a id="Pipe" href="#Pipe">var Pipe</a>

```
searchKey: runtime.Pipe
tags: [variable function private]
```

```Go
var Pipe = pipe
```

### <a id="Read" href="#Read">var Read</a>

```
searchKey: runtime.Read
tags: [variable function private]
```

```Go
var Read = read
```

### <a id="ReadUnaligned32" href="#ReadUnaligned32">var ReadUnaligned32</a>

```
searchKey: runtime.ReadUnaligned32
tags: [variable function private]
```

```Go
var ReadUnaligned32 = readUnaligned32
```

### <a id="ReadUnaligned64" href="#ReadUnaligned64">var ReadUnaligned64</a>

```
searchKey: runtime.ReadUnaligned64
tags: [variable function private]
```

```Go
var ReadUnaligned64 = readUnaligned64
```

### <a id="RunSchedLocalQueueEmptyState" href="#RunSchedLocalQueueEmptyState">var RunSchedLocalQueueEmptyState</a>

```
searchKey: runtime.RunSchedLocalQueueEmptyState
tags: [variable struct private]
```

```Go
var RunSchedLocalQueueEmptyState struct {
	done  chan bool
	ready *uint32
	p     *p
}
```

Temporary to enable register ABI bringup. TODO(register args): convert back to local variables in RunSchedLocalQueueEmptyTest that get passed to the "go" stmts there. 

### <a id="Semacquire" href="#Semacquire">var Semacquire</a>

```
searchKey: runtime.Semacquire
tags: [variable function private]
```

```Go
var Semacquire = semacquire
```

### <a id="Semrelease1" href="#Semrelease1">var Semrelease1</a>

```
searchKey: runtime.Semrelease1
tags: [variable function private]
```

```Go
var Semrelease1 = semrelease1
```

### <a id="SetNonblock" href="#SetNonblock">var SetNonblock</a>

```
searchKey: runtime.SetNonblock
tags: [variable function private]
```

```Go
var SetNonblock = setNonblock
```

### <a id="StringHash" href="#StringHash">var StringHash</a>

```
searchKey: runtime.StringHash
tags: [variable function private]
```

```Go
var StringHash = stringHash
```

### <a id="UseAeshash" href="#UseAeshash">var UseAeshash</a>

```
searchKey: runtime.UseAeshash
tags: [variable boolean private]
```

```Go
var UseAeshash = &useAeshash
```

### <a id="Usleep" href="#Usleep">var Usleep</a>

```
searchKey: runtime.Usleep
tags: [variable function private]
```

```Go
var Usleep = usleep
```

### <a id="Write" href="#Write">var Write</a>

```
searchKey: runtime.Write
tags: [variable function private]
```

```Go
var Write = write
```

### <a id="Xadduintptr" href="#Xadduintptr">var Xadduintptr</a>

```
searchKey: runtime.Xadduintptr
tags: [variable function private]
```

```Go
var Xadduintptr = atomic.Xadduintptr
```

### <a id="abiRegArgsEface" href="#abiRegArgsEface">var abiRegArgsEface</a>

```
searchKey: runtime.abiRegArgsEface
tags: [variable interface private]
```

```Go
var abiRegArgsEface interface{} = abi.RegArgs{}
```

### <a id="abiRegArgsType" href="#abiRegArgsType">var abiRegArgsType</a>

```
searchKey: runtime.abiRegArgsType
tags: [variable struct private]
```

```Go
var abiRegArgsType *_type = efaceOf(&abiRegArgsEface)._type
```

### <a id="aeskeysched" href="#aeskeysched">var aeskeysched</a>

```
searchKey: runtime.aeskeysched
tags: [variable array number private]
```

```Go
var aeskeysched [hashRandomBytes]byte
```

used in asm_{386,amd64,arm64}.s to seed the hash function 

### <a id="agg" href="#agg">var agg</a>

```
searchKey: runtime.agg
tags: [variable struct private]
```

```Go
var agg statAggregate
```

agg is used by readMetrics, and is protected by metricsSema. 

Managed as a global variable because its pointer will be an argument to a dynamically-defined function, and we'd like to avoid it escaping to the heap. 

### <a id="allDloggers" href="#allDloggers">var allDloggers</a>

```
searchKey: runtime.allDloggers
tags: [variable struct private]
```

```Go
var allDloggers *dlogger
```

allDloggers is a list of all dloggers, linked through dlogger.allLink. This is accessed atomically. This is prepend only, so it doesn't need to protect against ABA races. 

### <a id="allfin" href="#allfin">var allfin</a>

```
searchKey: runtime.allfin
tags: [variable struct private]
```

```Go
var allfin *finblock // list of all blocks

```

### <a id="allglen" href="#allglen">var allglen</a>

```
searchKey: runtime.allglen
tags: [variable number private]
```

```Go
var allglen uintptr
```

allglen and allgptr are atomic variables that contain len(allgs) and &allgs[0] respectively. Proper ordering depends on totally-ordered loads and stores. Writes are protected by allglock. 

allgptr is updated before allglen. Readers should read allglen before allgptr to ensure that allglen is always <= len(allgptr). New Gs appended during the race can be missed. For a consistent view of all Gs, allglock must be held. 

allgptr copies should always be stored as a concrete type or unsafe.Pointer, not uintptr, to ensure that GC can still reach it even if it points to a stale array. 

### <a id="allglock" href="#allglock">var allglock</a>

```
searchKey: runtime.allglock
tags: [variable struct private]
```

```Go
var allglock mutex
```

allgs contains all Gs ever created (including dead Gs), and thus never shrinks. 

Access via the slice is protected by allglock or stop-the-world. Readers that cannot take the lock may (carefully!) use the atomic variables below. 

### <a id="allgptr" href="#allgptr">var allgptr</a>

```
searchKey: runtime.allgptr
tags: [variable struct private]
```

```Go
var allgptr **g
```

### <a id="allgs" href="#allgs">var allgs</a>

```
searchKey: runtime.allgs
tags: [variable array struct private]
```

```Go
var allgs []*g
```

### <a id="allm" href="#allm">var allm</a>

```
searchKey: runtime.allm
tags: [variable struct private]
```

```Go
var allm *m
```

### <a id="allp" href="#allp">var allp</a>

```
searchKey: runtime.allp
tags: [variable array struct private]
```

```Go
var allp []*p
```

len(allp) == gomaxprocs; may change at safe points, otherwise immutable. 

### <a id="allpLock" href="#allpLock">var allpLock</a>

```
searchKey: runtime.allpLock
tags: [variable struct private]
```

```Go
var allpLock mutex
```

allpLock protects P-less reads and size changes of allp, idlepMask, and timerpMask, and all writes to allp. 

### <a id="argc" href="#argc">var argc</a>

```
searchKey: runtime.argc
tags: [variable number private]
```

```Go
var argc int32
```

### <a id="argslice" href="#argslice">var argslice</a>

```
searchKey: runtime.argslice
tags: [variable array string private]
```

```Go
var argslice []string
```

### <a id="argv" href="#argv">var argv</a>

```
searchKey: runtime.argv
tags: [variable number private]
```

```Go
var argv **byte
```

### <a id="arm64HasATOMICS" href="#arm64HasATOMICS">var arm64HasATOMICS</a>

```
searchKey: runtime.arm64HasATOMICS
tags: [variable boolean private]
```

```Go
var arm64HasATOMICS bool
```

### <a id="armHasVFPv4" href="#armHasVFPv4">var armHasVFPv4</a>

```
searchKey: runtime.armHasVFPv4
tags: [variable boolean private]
```

```Go
var armHasVFPv4 bool
```

### <a id="asyncPreemptStack" href="#asyncPreemptStack">var asyncPreemptStack</a>

```
searchKey: runtime.asyncPreemptStack
tags: [variable number private]
```

```Go
var asyncPreemptStack = ^uintptr(0)
```

asyncPreemptStack is the bytes of stack space required to inject an asyncPreempt call. 

### <a id="badginsignalMsg" href="#badginsignalMsg">var badginsignalMsg</a>

```
searchKey: runtime.badginsignalMsg
tags: [variable string private]
```

```Go
var badginsignalMsg = "fatal: bad g in signal handler\n"
```

### <a id="badmorestackg0Msg" href="#badmorestackg0Msg">var badmorestackg0Msg</a>

```
searchKey: runtime.badmorestackg0Msg
tags: [variable string private]
```

```Go
var badmorestackg0Msg = "fatal: morestack on g0\n"
```

### <a id="badmorestackgsignalMsg" href="#badmorestackgsignalMsg">var badmorestackgsignalMsg</a>

```
searchKey: runtime.badmorestackgsignalMsg
tags: [variable string private]
```

```Go
var badmorestackgsignalMsg = "fatal: morestack on gsignal\n"
```

### <a id="badsystemstackMsg" href="#badsystemstackMsg">var badsystemstackMsg</a>

```
searchKey: runtime.badsystemstackMsg
tags: [variable string private]
```

```Go
var badsystemstackMsg = "fatal: systemstack called from unexpected goroutine"
```

### <a id="bbuckets" href="#bbuckets">var bbuckets</a>

```
searchKey: runtime.bbuckets
tags: [variable struct private]
```

```Go
var bbuckets *bucket // blocking profile buckets

```

### <a id="blockprofilerate" href="#blockprofilerate">var blockprofilerate</a>

```
searchKey: runtime.blockprofilerate
tags: [variable number private]
```

```Go
var blockprofilerate uint64 // in CPU ticks

```

### <a id="boundsErrorFmts" href="#boundsErrorFmts">var boundsErrorFmts</a>

```
searchKey: runtime.boundsErrorFmts
tags: [variable array string private]
```

```Go
var boundsErrorFmts = ...
```

boundsErrorFmts provide error text for various out-of-bounds panics. Note: if you change these strings, you should adjust the size of the buffer in boundsError.Error below as well. 

### <a id="boundsNegErrorFmts" href="#boundsNegErrorFmts">var boundsNegErrorFmts</a>

```
searchKey: runtime.boundsNegErrorFmts
tags: [variable array string private]
```

```Go
var boundsNegErrorFmts = ...
```

boundsNegErrorFmts are overriding formats if x is negative. In this case there's no need to report y. 

### <a id="bucketmem" href="#bucketmem">var bucketmem</a>

```
searchKey: runtime.bucketmem
tags: [variable number private]
```

```Go
var bucketmem uintptr
```

### <a id="buckhash" href="#buckhash">var buckhash</a>

```
searchKey: runtime.buckhash
tags: [variable array struct private]
```

```Go
var buckhash *[179999]*bucket
```

### <a id="buf" href="#buf">var buf</a>

```
searchKey: runtime.buf
tags: [variable array number private]
```

```Go
var buf [bufSize]byte
```

### <a id="buildVersion" href="#buildVersion">var buildVersion</a>

```
searchKey: runtime.buildVersion
tags: [variable string private]
```

```Go
var buildVersion string
```

buildVersion is the Go tree's version string at build time. 

If any GOEXPERIMENTs are set to non-default values, it will include "X:<GOEXPERIMENT>". 

This is set by the linker. 

This is accessed by "go version <binary>". 

### <a id="cgoAlwaysFalse" href="#cgoAlwaysFalse">var cgoAlwaysFalse</a>

```
searchKey: runtime.cgoAlwaysFalse
tags: [variable boolean private]
```

```Go
var cgoAlwaysFalse bool
```

cgoAlwaysFalse is a boolean value that is always false. The cgo-generated code says if cgoAlwaysFalse { cgoUse(p) }. The compiler cannot see that cgoAlwaysFalse is always false, so it emits the test and keeps the call, giving the desired escape analysis result. The test is cheaper than the call. 

### <a id="cgoContext" href="#cgoContext">var cgoContext</a>

```
searchKey: runtime.cgoContext
tags: [variable number private]
```

```Go
var cgoContext unsafe.Pointer
```

### <a id="cgoHasExtraM" href="#cgoHasExtraM">var cgoHasExtraM</a>

```
searchKey: runtime.cgoHasExtraM
tags: [variable boolean private]
```

```Go
var cgoHasExtraM bool
```

cgoHasExtraM is set on startup when an extra M is created for cgo. The extra M must be created before any C/C++ code calls cgocallback. 

### <a id="cgoSymbolizer" href="#cgoSymbolizer">var cgoSymbolizer</a>

```
searchKey: runtime.cgoSymbolizer
tags: [variable number private]
```

```Go
var cgoSymbolizer unsafe.Pointer
```

### <a id="cgoThreadStart" href="#cgoThreadStart">var cgoThreadStart</a>

```
searchKey: runtime.cgoThreadStart
tags: [variable number private]
```

```Go
var cgoThreadStart unsafe.Pointer
```

When running with cgo, we call _cgo_thread_start to start threads for us so that we can play nicely with foreign code. 

### <a id="cgoTraceback" href="#cgoTraceback">var cgoTraceback</a>

```
searchKey: runtime.cgoTraceback
tags: [variable number private]
```

```Go
var cgoTraceback unsafe.Pointer
```

### <a id="cgo_yield" href="#cgo_yield">var cgo_yield</a>

```
searchKey: runtime.cgo_yield
tags: [variable number private]
```

```Go
var cgo_yield = &_cgo_yield
```

### <a id="chanrecvpc" href="#chanrecvpc">var chanrecvpc</a>

```
searchKey: runtime.chanrecvpc
tags: [variable number private]
```

```Go
var chanrecvpc = funcPC(chanrecv)
```

### <a id="chansendpc" href="#chansendpc">var chansendpc</a>

```
searchKey: runtime.chansendpc
tags: [variable number private]
```

```Go
var chansendpc = funcPC(chansend)
```

### <a id="class_to_allocnpages" href="#class_to_allocnpages">var class_to_allocnpages</a>

```
searchKey: runtime.class_to_allocnpages
tags: [variable array number private]
```

```Go
var class_to_allocnpages = ...
```

### <a id="class_to_divmagic" href="#class_to_divmagic">var class_to_divmagic</a>

```
searchKey: runtime.class_to_divmagic
tags: [variable array number private]
```

```Go
var class_to_divmagic = ...
```

### <a id="class_to_size" href="#class_to_size">var class_to_size</a>

```
searchKey: runtime.class_to_size
tags: [variable array number private]
```

```Go
var class_to_size = ...
```

### <a id="cpuprof" href="#cpuprof">var cpuprof</a>

```
searchKey: runtime.cpuprof
tags: [variable struct private]
```

```Go
var cpuprof cpuProfile
```

### <a id="crashing" href="#crashing">var crashing</a>

```
searchKey: runtime.crashing
tags: [variable number private]
```

```Go
var crashing int32
```

crashing is the number of m's we have waited for when implementing GOTRACEBACK=crash when a signal is received. 

### <a id="dbgvars" href="#dbgvars">var dbgvars</a>

```
searchKey: runtime.dbgvars
tags: [variable array struct private]
```

```Go
var dbgvars = ...
```

### <a id="deadlock" href="#deadlock">var deadlock</a>

```
searchKey: runtime.deadlock
tags: [variable struct private]
```

```Go
var deadlock mutex
```

### <a id="debug" href="#debug">var debug</a>

```
searchKey: runtime.debug
tags: [variable struct private]
```

```Go
var debug struct {
	cgocheck           int32
	clobberfree        int32
	efence             int32
	gccheckmark        int32
	gcpacertrace       int32
	gcshrinkstackoff   int32
	gcstoptheworld     int32
	gctrace            int32
	invalidptr         int32
	madvdontneed       int32 // for Linux; issue 28466
	scavtrace          int32
	scheddetail        int32
	schedtrace         int32
	tracebackancestors int32
	asyncpreemptoff    int32

	// debug.malloc is used as a combined debug check
	// in the malloc function and should be set
	// if any of the below debug options is != 0.
	malloc         bool
	allocfreetrace int32
	inittrace      int32
	sbrk           int32
} = ...
```

Holds variables parsed from GODEBUG env var, except for "memprofilerate" since there is an existing int var for that value, which may already have an initial value. 

### <a id="debugPtrmask" href="#debugPtrmask">var debugPtrmask</a>

```
searchKey: runtime.debugPtrmask
tags: [variable struct private]
```

```Go
var debugPtrmask struct {
	lock mutex
	data *byte
}
```

### <a id="debuglock" href="#debuglock">var debuglock</a>

```
searchKey: runtime.debuglock
tags: [variable struct private]
```

```Go
var debuglock mutex
```

### <a id="defaultGOROOT" href="#defaultGOROOT">var defaultGOROOT</a>

```
searchKey: runtime.defaultGOROOT
tags: [variable string private]
```

```Go
var defaultGOROOT string // set by cmd/link

```

### <a id="deferType" href="#deferType">var deferType</a>

```
searchKey: runtime.deferType
tags: [variable struct private]
```

```Go
var deferType *_type // type of _defer struct

```

### <a id="didothers" href="#didothers">var didothers</a>

```
searchKey: runtime.didothers
tags: [variable boolean private]
```

```Go
var didothers bool
```

### <a id="disableMemoryProfiling" href="#disableMemoryProfiling">var disableMemoryProfiling</a>

```
searchKey: runtime.disableMemoryProfiling
tags: [variable boolean private]
```

```Go
var disableMemoryProfiling bool
```

disableMemoryProfiling is set by the linker if runtime.MemProfile is not used and the link type guarantees nobody else could use it elsewhere. 

### <a id="disableSigChan" href="#disableSigChan">var disableSigChan</a>

```
searchKey: runtime.disableSigChan
tags: [variable private]
```

```Go
var disableSigChan chan uint32
```

channels for synchronizing signal mask updates with the signal mask thread 

### <a id="divideError" href="#divideError">var divideError</a>

```
searchKey: runtime.divideError
tags: [variable interface private]
```

```Go
var divideError = error(errorString("integer divide by zero"))
```

### <a id="dumpfd" href="#dumpfd">var dumpfd</a>

```
searchKey: runtime.dumpfd
tags: [variable number private]
```

```Go
var dumpfd uintptr // fd to write the dump to.

```

### <a id="dumphdr" href="#dumphdr">var dumphdr</a>

```
searchKey: runtime.dumphdr
tags: [variable array number private]
```

```Go
var dumphdr = []byte("go1.7 heap dump\n")
```

### <a id="earlycgocallback" href="#earlycgocallback">var earlycgocallback</a>

```
searchKey: runtime.earlycgocallback
tags: [variable array number private]
```

```Go
var earlycgocallback = []byte("fatal error: cgo callback before cgo call\n")
```

### <a id="emptymspan" href="#emptymspan">var emptymspan</a>

```
searchKey: runtime.emptymspan
tags: [variable struct private]
```

```Go
var emptymspan mspan
```

dummy mspan that contains no free objects. 

### <a id="enableSigChan" href="#enableSigChan">var enableSigChan</a>

```
searchKey: runtime.enableSigChan
tags: [variable private]
```

```Go
var enableSigChan chan uint32
```

channels for synchronizing signal mask updates with the signal mask thread 

### <a id="envs" href="#envs">var envs</a>

```
searchKey: runtime.envs
tags: [variable array string private]
```

```Go
var envs []string
```

### <a id="execLock" href="#execLock">var execLock</a>

```
searchKey: runtime.execLock
tags: [variable struct private]
```

```Go
var execLock rwmutex
```

execLock serializes exec and clone to avoid bugs or unspecified behaviour around exec'ing while creating/destroying threads.  See issue #19546. 

### <a id="executablePath" href="#executablePath">var executablePath</a>

```
searchKey: runtime.executablePath
tags: [variable string private]
```

```Go
var executablePath string
```

### <a id="extraMCount" href="#extraMCount">var extraMCount</a>

```
searchKey: runtime.extraMCount
tags: [variable number private]
```

```Go
var extraMCount uint32 // Protected by lockextra

```

### <a id="extraMWaiters" href="#extraMWaiters">var extraMWaiters</a>

```
searchKey: runtime.extraMWaiters
tags: [variable number private]
```

```Go
var extraMWaiters uint32
```

### <a id="extram" href="#extram">var extram</a>

```
searchKey: runtime.extram
tags: [variable number private]
```

```Go
var extram uintptr
```

### <a id="failallocatestack" href="#failallocatestack">var failallocatestack</a>

```
searchKey: runtime.failallocatestack
tags: [variable array number private]
```

```Go
var failallocatestack = []byte("runtime: failed to allocate stack for the new OS thread\n")
```

### <a id="failthreadcreate" href="#failthreadcreate">var failthreadcreate</a>

```
searchKey: runtime.failthreadcreate
tags: [variable array number private]
```

```Go
var failthreadcreate = []byte("runtime: failed to create new OS thread\n")
```

### <a id="faketime" href="#faketime">var faketime</a>

```
searchKey: runtime.faketime
tags: [variable number private]
```

```Go
var faketime int64
```

faketime is the simulated time in nanoseconds since 1970 for the playground. 

Zero means not to use faketime. 

### <a id="fastlog2Table" href="#fastlog2Table">var fastlog2Table</a>

```
searchKey: runtime.fastlog2Table
tags: [variable array number private]
```

```Go
var fastlog2Table = ...
```

### <a id="fastrandseed" href="#fastrandseed">var fastrandseed</a>

```
searchKey: runtime.fastrandseed
tags: [variable number private]
```

```Go
var fastrandseed uintptr
```

### <a id="finalizer1" href="#finalizer1">var finalizer1</a>

```
searchKey: runtime.finalizer1
tags: [variable array number private]
```

```Go
var finalizer1 = ...
```

### <a id="finc" href="#finc">var finc</a>

```
searchKey: runtime.finc
tags: [variable struct private]
```

```Go
var finc *finblock // cache of free blocks

```

### <a id="fing" href="#fing">var fing</a>

```
searchKey: runtime.fing
tags: [variable struct private]
```

```Go
var fing *g // goroutine that runs finalizers

```

### <a id="fingCreate" href="#fingCreate">var fingCreate</a>

```
searchKey: runtime.fingCreate
tags: [variable number private]
```

```Go
var fingCreate uint32
```

### <a id="fingRunning" href="#fingRunning">var fingRunning</a>

```
searchKey: runtime.fingRunning
tags: [variable boolean private]
```

```Go
var fingRunning bool
```

### <a id="fingwait" href="#fingwait">var fingwait</a>

```
searchKey: runtime.fingwait
tags: [variable boolean private]
```

```Go
var fingwait bool
```

### <a id="fingwake" href="#fingwake">var fingwake</a>

```
searchKey: runtime.fingwake
tags: [variable boolean private]
```

```Go
var fingwake bool
```

### <a id="finlock" href="#finlock">var finlock</a>

```
searchKey: runtime.finlock
tags: [variable struct private]
```

```Go
var finlock mutex // protects the following variables

```

### <a id="finptrmask" href="#finptrmask">var finptrmask</a>

```
searchKey: runtime.finptrmask
tags: [variable array number private]
```

```Go
var finptrmask [_FinBlockSize / sys.PtrSize / 8]byte
```

### <a id="finq" href="#finq">var finq</a>

```
searchKey: runtime.finq
tags: [variable struct private]
```

```Go
var finq *finblock // list of finalizers that are to be executed

```

### <a id="firstmoduledata" href="#firstmoduledata">var firstmoduledata</a>

```
searchKey: runtime.firstmoduledata
tags: [variable struct private]
```

```Go
var firstmoduledata moduledata // linker symbol

```

### <a id="floatError" href="#floatError">var floatError</a>

```
searchKey: runtime.floatError
tags: [variable interface private]
```

```Go
var floatError = error(errorString("floating point error"))
```

### <a id="forcegc" href="#forcegc">var forcegc</a>

```
searchKey: runtime.forcegc
tags: [variable struct private]
```

```Go
var forcegc forcegcstate
```

### <a id="forcegcperiod" href="#forcegcperiod">var forcegcperiod</a>

```
searchKey: runtime.forcegcperiod
tags: [variable number private]
```

```Go
var forcegcperiod int64 = 2 * 60 * 1e9
```

forcegcperiod is the maximum time in nanoseconds between garbage collections. If we go this long without a garbage collection, one is forced to run. 

This is a variable for testing purposes. It normally doesn't change. 

### <a id="freemark" href="#freemark">var freemark</a>

```
searchKey: runtime.freemark
tags: [variable array boolean private]
```

```Go
var freemark [_PageSize / 8]bool
```

Bit vector of free marks. Needs to be as big as the largest number of objects per span. 

### <a id="freezing" href="#freezing">var freezing</a>

```
searchKey: runtime.freezing
tags: [variable number private]
```

```Go
var freezing uint32
```

freezing is set to non-zero if the runtime is trying to freeze the world. 

### <a id="fwdSig" href="#fwdSig">var fwdSig</a>

```
searchKey: runtime.fwdSig
tags: [variable array number private]
```

```Go
var fwdSig [_NSIG]uintptr
```

Stores the signal handlers registered before Go installed its own. These signal handlers will be invoked in cases where Go doesn't want to handle a particular signal (e.g., signal occurred on a non-Go thread). See sigfwdgo for more information on when the signals are forwarded. 

This is read by the signal handler; accesses should use atomic.Loaduintptr and atomic.Storeuintptr. 

### <a id="g0" href="#g0">var g0</a>

```
searchKey: runtime.g0
tags: [variable struct private]
```

```Go
var g0 g
```

### <a id="gStatusStrings" href="#gStatusStrings">var gStatusStrings</a>

```
searchKey: runtime.gStatusStrings
tags: [variable array string private]
```

```Go
var gStatusStrings = ...
```

### <a id="gcBgMarkWorkerCount" href="#gcBgMarkWorkerCount">var gcBgMarkWorkerCount</a>

```
searchKey: runtime.gcBgMarkWorkerCount
tags: [variable number private]
```

```Go
var gcBgMarkWorkerCount int32
```

Total number of gcBgMarkWorker goroutines. Protected by worldsema. 

### <a id="gcBgMarkWorkerPool" href="#gcBgMarkWorkerPool">var gcBgMarkWorkerPool</a>

```
searchKey: runtime.gcBgMarkWorkerPool
tags: [variable number private]
```

```Go
var gcBgMarkWorkerPool lfstack
```

Pool of GC parked background workers. Entries are type *gcBgMarkWorkerNode. 

### <a id="gcBitsArenas" href="#gcBitsArenas">var gcBitsArenas</a>

```
searchKey: runtime.gcBitsArenas
tags: [variable struct private]
```

```Go
var gcBitsArenas struct {
	lock     mutex
	free     *gcBitsArena
	next     *gcBitsArena // Read atomically. Write atomically under lock.
	current  *gcBitsArena
	previous *gcBitsArena
} = ...
```

### <a id="gcBlackenEnabled" href="#gcBlackenEnabled">var gcBlackenEnabled</a>

```
searchKey: runtime.gcBlackenEnabled
tags: [variable number private]
```

```Go
var gcBlackenEnabled uint32
```

gcBlackenEnabled is 1 if mutator assists and background mark workers are allowed to blacken objects. This must only be set when gcphase == _GCmark. 

### <a id="gcController" href="#gcController">var gcController</a>

```
searchKey: runtime.gcController
tags: [variable struct private]
```

```Go
var gcController gcControllerState
```

gcController implements the GC pacing controller that determines when to trigger concurrent garbage collection and how much marking work to do in mutator assists and background marking. 

It uses a feedback control algorithm to adjust the gcController.trigger trigger based on the heap growth and GC CPU utilization each cycle. This algorithm optimizes for heap growth to match GOGC and for CPU utilization between assist and background marking to be 25% of GOMAXPROCS. The high-level design of this algorithm is documented at [https://golang.org/s/go15gcpacing](https://golang.org/s/go15gcpacing). 

All fields of gcController are used only during a single mark cycle. 

### <a id="gcMarkDoneFlushed" href="#gcMarkDoneFlushed">var gcMarkDoneFlushed</a>

```
searchKey: runtime.gcMarkDoneFlushed
tags: [variable number private]
```

```Go
var gcMarkDoneFlushed uint32
```

gcMarkDoneFlushed counts the number of P's with flushed work. 

Ideally this would be a captured local in gcMarkDone, but forEachP escapes its callback closure, so it can't capture anything. 

This is protected by markDoneSema. 

### <a id="gcMarkWorkerModeStrings" href="#gcMarkWorkerModeStrings">var gcMarkWorkerModeStrings</a>

```
searchKey: runtime.gcMarkWorkerModeStrings
tags: [variable array string private]
```

```Go
var gcMarkWorkerModeStrings = ...
```

gcMarkWorkerModeStrings are the strings labels of gcMarkWorkerModes to use in execution traces. 

### <a id="gcenable_setup" href="#gcenable_setup">var gcenable_setup</a>

```
searchKey: runtime.gcenable_setup
tags: [variable private]
```

```Go
var gcenable_setup chan int
```

Temporary in order to enable register ABI work. TODO(register args): convert back to local chan in gcenabled, passed to "go" stmts. 

### <a id="gcphase" href="#gcphase">var gcphase</a>

```
searchKey: runtime.gcphase
tags: [variable number private]
```

```Go
var gcphase uint32
```

Garbage collector phase. Indicates to write barrier and synchronization task to perform. 

### <a id="gcsema" href="#gcsema">var gcsema</a>

```
searchKey: runtime.gcsema
tags: [variable number private]
```

```Go
var gcsema uint32 = 1
```

Holding gcsema grants the M the right to block a GC, and blocks until the current GC is done. In particular, it prevents gomaxprocs from changing concurrently. 

TODO(mknyszek): Once gomaxprocs and the execution tracer can handle being changed/enabled during a GC, remove this. 

### <a id="globalAlloc" href="#globalAlloc">var globalAlloc</a>

```
searchKey: runtime.globalAlloc
tags: [variable struct private]
```

```Go
var globalAlloc struct {
	mutex
	persistentAlloc
}
```

### <a id="goarm" href="#goarm">var goarm</a>

```
searchKey: runtime.goarm
tags: [variable number private]
```

```Go
var goarm uint8 // set by cmd/link on arm systems

```

### <a id="gomaxprocs" href="#gomaxprocs">var gomaxprocs</a>

```
searchKey: runtime.gomaxprocs
tags: [variable number private]
```

```Go
var gomaxprocs int32
```

### <a id="handlingSig" href="#handlingSig">var handlingSig</a>

```
searchKey: runtime.handlingSig
tags: [variable array number private]
```

```Go
var handlingSig [_NSIG]uint32
```

handlingSig is indexed by signal number and is non-zero if we are currently handling the signal. Or, to put it another way, whether the signal handler is currently set to the Go signal handler or not. This is uint32 rather than bool so that we can use atomic instructions. 

### <a id="hashLoad" href="#hashLoad">var hashLoad</a>

```
searchKey: runtime.hashLoad
tags: [variable number private]
```

```Go
var hashLoad = float32(loadFactorNum) / float32(loadFactorDen)
```

exported value for testing 

### <a id="hashkey" href="#hashkey">var hashkey</a>

```
searchKey: runtime.hashkey
tags: [variable array number private]
```

```Go
var hashkey [4]uintptr
```

used in hash{32,64}.go to seed the hash function 

### <a id="idlepMask" href="#idlepMask">var idlepMask</a>

```
searchKey: runtime.idlepMask
tags: [variable array number private]
```

```Go
var idlepMask pMask
```

Bitmask of Ps in _Pidle list, one bit per P. Reads and writes must be atomic. Length may change at safe points. 

Each P must update only its own bit. In order to maintain consistency, a P going idle must the idle mask simultaneously with updates to the idle P list under the sched.lock, otherwise a racing pidleget may clear the mask before pidleput sets the mask, corrupting the bitmap. 

N.B., procresize takes ownership of all Ps in stopTheWorldWithSema. 

### <a id="inForkedChild" href="#inForkedChild">var inForkedChild</a>

```
searchKey: runtime.inForkedChild
tags: [variable boolean private]
```

```Go
var inForkedChild bool
```

inForkedChild is true while manipulating signals in the child process. This is used to avoid calling libc functions in case we are using vfork. 

### <a id="inf" href="#inf">var inf</a>

```
searchKey: runtime.inf
tags: [variable number private]
```

```Go
var inf = float64frombits(0x7FF0000000000000)
```

### <a id="initSigmask" href="#initSigmask">var initSigmask</a>

```
searchKey: runtime.initSigmask
tags: [variable number private]
```

```Go
var initSigmask sigset
```

Value to use for signal mask for newly created M's. 

### <a id="inittrace" href="#inittrace">var inittrace</a>

```
searchKey: runtime.inittrace
tags: [variable struct private]
```

```Go
var inittrace tracestat
```

inittrace stores statistics for init functions which are updated by malloc and newproc when active is true. 

### <a id="intArgRegs" href="#intArgRegs">var intArgRegs</a>

```
searchKey: runtime.intArgRegs
tags: [variable private]
```

```Go
var intArgRegs = abi.IntArgRegs * goexperiment.RegabiArgsInt
```

intArgRegs is used by the various register assignment algorithm implementations in the runtime. These include:. - Finalizers (mfinal.go) - Windows callbacks (syscall_windows.go) 

Both are stripped-down versions of the algorithm since they only have to deal with a subset of cases (finalizers only take a pointer or interface argument, Go Windows callbacks don't support floating point). 

It should be modified with care and are generally only modified when testing this package. 

It should never be set higher than its internal/abi constant counterparts, because the system relies on a structure that is at least large enough to hold the registers the system supports. 

Currently it's set to zero because using the actual constant will break every part of the toolchain that uses finalizers or Windows callbacks to call functions The value that is currently commented out there should be the actual value once we're ready to use the register ABI everywhere. 

Protected by finlock. 

### <a id="isIntel" href="#isIntel">var isIntel</a>

```
searchKey: runtime.isIntel
tags: [variable boolean private]
```

```Go
var isIntel bool
```

### <a id="isarchive" href="#isarchive">var isarchive</a>

```
searchKey: runtime.isarchive
tags: [variable boolean private]
```

```Go
var isarchive bool // -buildmode=c-archive

```

Set by the linker so the runtime can determine the buildmode. 

### <a id="iscgo" href="#iscgo">var iscgo</a>

```
searchKey: runtime.iscgo
tags: [variable boolean private]
```

```Go
var iscgo bool
```

iscgo is set to true by the runtime/cgo package 

### <a id="islibrary" href="#islibrary">var islibrary</a>

```
searchKey: runtime.islibrary
tags: [variable boolean private]
```

```Go
var islibrary bool // -buildmode=c-shared

```

Set by the linker so the runtime can determine the buildmode. 

### <a id="itabLock" href="#itabLock">var itabLock</a>

```
searchKey: runtime.itabLock
tags: [variable struct private]
```

```Go
var itabLock mutex // lock for accessing itab table

```

### <a id="itabTable" href="#itabTable">var itabTable</a>

```
searchKey: runtime.itabTable
tags: [variable struct private]
```

```Go
var itabTable = &itabTableInit // pointer to current table

```

### <a id="itabTableInit" href="#itabTableInit">var itabTableInit</a>

```
searchKey: runtime.itabTableInit
tags: [variable struct private]
```

```Go
var itabTableInit = itabTableType{size: itabInitSize} // starter table

```

### <a id="kq" href="#kq">var kq</a>

```
searchKey: runtime.kq
tags: [variable number private]
```

```Go
var kq int32 = -1
```

### <a id="labelSync" href="#labelSync">var labelSync</a>

```
searchKey: runtime.labelSync
tags: [variable number private]
```

```Go
var labelSync uintptr
```

### <a id="lastmoduledatap" href="#lastmoduledatap">var lastmoduledatap</a>

```
searchKey: runtime.lastmoduledatap
tags: [variable struct private]
```

```Go
var lastmoduledatap *moduledata // linker symbol

```

### <a id="levelBits" href="#levelBits">var levelBits</a>

```
searchKey: runtime.levelBits
tags: [variable array number private]
```

```Go
var levelBits = ...
```

levelBits is the number of bits in the radix for a given level in the super summary structure. 

The sum of all the entries of levelBits should equal heapAddrBits. 

### <a id="levelLogPages" href="#levelLogPages">var levelLogPages</a>

```
searchKey: runtime.levelLogPages
tags: [variable array number private]
```

```Go
var levelLogPages = ...
```

levelLogPages is log2 the maximum number of runtime pages in the address space a summary in the given level represents. 

The leaf level always represents exactly log2 of 1 chunk's worth of pages. 

### <a id="levelShift" href="#levelShift">var levelShift</a>

```
searchKey: runtime.levelShift
tags: [variable array number private]
```

```Go
var levelShift = ...
```

levelShift is the number of bits to shift to acquire the radix for a given level in the super summary structure. 

With levelShift, one can compute the index of the summary at level l related to a pointer p by doing: 

```
p >> levelShift[l]

```
### <a id="lfenceBeforeRdtsc" href="#lfenceBeforeRdtsc">var lfenceBeforeRdtsc</a>

```
searchKey: runtime.lfenceBeforeRdtsc
tags: [variable boolean private]
```

```Go
var lfenceBeforeRdtsc bool
```

### <a id="lockNames" href="#lockNames">var lockNames</a>

```
searchKey: runtime.lockNames
tags: [variable array string private]
```

```Go
var lockNames = ...
```

lockNames gives the names associated with each of the above ranks 

### <a id="lockPartialOrder" href="#lockPartialOrder">var lockPartialOrder</a>

```
searchKey: runtime.lockPartialOrder
tags: [variable array array number private]
```

```Go
var lockPartialOrder [][]lockRank = ...
```

lockPartialOrder is a partial order among the various lock types, listing the immediate ordering that has actually been observed in the runtime. Each entry (which corresponds to a particular lock rank) specifies the list of locks that can already be held immediately "above" it. 

So, for example, the lockRankSched entry shows that all the locks preceding it in rank can actually be held. The allp lock shows that only the sysmon or sched lock can be held immediately above it when it is acquired. 

### <a id="m0" href="#m0">var m0</a>

```
searchKey: runtime.m0
tags: [variable struct private]
```

```Go
var m0 m
```

### <a id="mFixupRace" href="#mFixupRace">var mFixupRace</a>

```
searchKey: runtime.mFixupRace
tags: [variable struct private]
```

```Go
var mFixupRace struct {
	lock mutex
	ctx  uintptr
}
```

mFixupRace is used to temporarily borrow the race context from the coordinating m during a syscall_runtime_doAllThreadsSyscall and loan it out to each of the m's of the runtime so they can execute a mFixup.fn in that context. 

### <a id="mProf" href="#mProf">var mProf</a>

```
searchKey: runtime.mProf
tags: [variable struct private]
```

```Go
var mProf struct {

	// cycle is the global heap profile cycle. This wraps
	// at mProfCycleWrap.
	cycle uint32
	// flushed indicates that future[cycle] in all buckets
	// has been flushed to the active profile.
	flushed bool
} = ...
```

### <a id="mSpanStateNames" href="#mSpanStateNames">var mSpanStateNames</a>

```
searchKey: runtime.mSpanStateNames
tags: [variable array string private]
```

```Go
var mSpanStateNames = []string{
	"mSpanDead",
	"mSpanInUse",
	"mSpanManual",
	"mSpanFree",
}
```

mSpanStateNames are the names of the span states, indexed by mSpanState. 

### <a id="mainStarted" href="#mainStarted">var mainStarted</a>

```
searchKey: runtime.mainStarted
tags: [variable boolean private]
```

```Go
var mainStarted bool
```

mainStarted indicates that the main M has started. 

### <a id="main_init_done" href="#main_init_done">var main_init_done</a>

```
searchKey: runtime.main_init_done
tags: [variable private]
```

```Go
var main_init_done chan bool
```

main_init_done is a signal used by cgocallbackg that initialization has been completed. It is made before _cgo_notify_runtime_init_done, so all cgo calls can rely on it existing. When main_init is complete, it is closed, meaning cgocallbackg can reliably receive from it. 

### <a id="main_inittask" href="#main_inittask">var main_inittask</a>

```
searchKey: runtime.main_inittask
tags: [variable struct private]
```

```Go
var main_inittask initTask
```

### <a id="maskUpdatedChan" href="#maskUpdatedChan">var maskUpdatedChan</a>

```
searchKey: runtime.maskUpdatedChan
tags: [variable private]
```

```Go
var maskUpdatedChan chan struct{}
```

channels for synchronizing signal mask updates with the signal mask thread 

### <a id="maxOffAddr" href="#maxOffAddr">var maxOffAddr</a>

```
searchKey: runtime.maxOffAddr
tags: [variable struct private]
```

```Go
var maxOffAddr = offAddr{(((1 << heapAddrBits) - 1) + arenaBaseOffset) & uintptrMask}
```

maxOffAddr is the maximum address in the offset address space. It corresponds to the highest virtual address representable by the page alloc chunk and heap arena maps. 

### <a id="maxSearchAddr" href="#maxSearchAddr">var maxSearchAddr</a>

```
searchKey: runtime.maxSearchAddr
tags: [variable struct private]
```

```Go
var maxSearchAddr = maxOffAddr
```

Maximum searchAddr value, which indicates that the heap has no free space. 

We alias maxOffAddr just to make it clear that this is the maximum address for the page allocator's search space. See maxOffAddr for details. 

### <a id="maxstackceiling" href="#maxstackceiling">var maxstackceiling</a>

```
searchKey: runtime.maxstackceiling
tags: [variable number private]
```

```Go
var maxstackceiling = maxstacksize
```

### <a id="maxstacksize" href="#maxstacksize">var maxstacksize</a>

```
searchKey: runtime.maxstacksize
tags: [variable number private]
```

```Go
var maxstacksize uintptr = 1 << 20 // enough until runtime.main sets it for real

```

### <a id="mbuckets" href="#mbuckets">var mbuckets</a>

```
searchKey: runtime.mbuckets
tags: [variable struct private]
```

```Go
var mbuckets *bucket // memory profile buckets

```

### <a id="mcache0" href="#mcache0">var mcache0</a>

```
searchKey: runtime.mcache0
tags: [variable struct private]
```

```Go
var mcache0 *mcache
```

### <a id="memoryError" href="#memoryError">var memoryError</a>

```
searchKey: runtime.memoryError
tags: [variable interface private]
```

```Go
var memoryError = error(errorString("invalid memory address or nil pointer dereference"))
```

### <a id="memstats" href="#memstats">var memstats</a>

```
searchKey: runtime.memstats
tags: [variable struct private]
```

```Go
var memstats mstats
```

### <a id="methodValueCallFrameObjs" href="#methodValueCallFrameObjs">var methodValueCallFrameObjs</a>

```
searchKey: runtime.methodValueCallFrameObjs
tags: [variable array struct private]
```

```Go
var methodValueCallFrameObjs = ...
```

### <a id="metrics" href="#metrics">var metrics</a>

```
searchKey: runtime.metrics
tags: [variable object private]
```

```Go
var metrics map[string]metricData
```

### <a id="metricsInit" href="#metricsInit">var metricsInit</a>

```
searchKey: runtime.metricsInit
tags: [variable boolean private]
```

```Go
var metricsInit bool
```

### <a id="metricsSema" href="#metricsSema">var metricsSema</a>

```
searchKey: runtime.metricsSema
tags: [variable number private]
```

```Go
var metricsSema uint32 = 1
```

metrics is a map of runtime/metrics keys to data used by the runtime to sample each metric's value. 

### <a id="mheap_" href="#mheap_">var mheap_</a>

```
searchKey: runtime.mheap_
tags: [variable struct private]
```

```Go
var mheap_ mheap
```

### <a id="minOffAddr" href="#minOffAddr">var minOffAddr</a>

```
searchKey: runtime.minOffAddr
tags: [variable struct private]
```

```Go
var minOffAddr = offAddr{arenaBaseOffset}
```

minOffAddr is the minimum address in the offset space, and it corresponds to the virtual address arenaBaseOffset. 

### <a id="minhexdigits" href="#minhexdigits">var minhexdigits</a>

```
searchKey: runtime.minhexdigits
tags: [variable number private]
```

```Go
var minhexdigits = 0 // protected by printlock

```

### <a id="modinfo" href="#modinfo">var modinfo</a>

```
searchKey: runtime.modinfo
tags: [variable string private]
```

```Go
var modinfo string
```

set using cmd/go/internal/modload.ModInfoProg 

### <a id="modulesSlice" href="#modulesSlice">var modulesSlice</a>

```
searchKey: runtime.modulesSlice
tags: [variable array struct private]
```

```Go
var modulesSlice *[]*moduledata // see activeModules

```

### <a id="mutexprofilerate" href="#mutexprofilerate">var mutexprofilerate</a>

```
searchKey: runtime.mutexprofilerate
tags: [variable number private]
```

```Go
var mutexprofilerate uint64 // fraction sampled

```

### <a id="nbuf" href="#nbuf">var nbuf</a>

```
searchKey: runtime.nbuf
tags: [variable number private]
```

```Go
var nbuf uintptr
```

### <a id="ncpu" href="#ncpu">var ncpu</a>

```
searchKey: runtime.ncpu
tags: [variable number private]
```

```Go
var ncpu int32
```

### <a id="netpollBreakRd" href="#netpollBreakRd">var netpollBreakRd</a>

```
searchKey: runtime.netpollBreakRd
tags: [variable number private]
```

```Go
var netpollBreakRd, netpollBreakWr uintptr // for netpollBreak

```

### <a id="netpollBreakWr" href="#netpollBreakWr">var netpollBreakWr</a>

```
searchKey: runtime.netpollBreakWr
tags: [variable number private]
```

```Go
var netpollBreakRd, netpollBreakWr uintptr // for netpollBreak

```

### <a id="netpollInitLock" href="#netpollInitLock">var netpollInitLock</a>

```
searchKey: runtime.netpollInitLock
tags: [variable struct private]
```

```Go
var netpollInitLock mutex
```

### <a id="netpollInited" href="#netpollInited">var netpollInited</a>

```
searchKey: runtime.netpollInited
tags: [variable number private]
```

```Go
var netpollInited uint32
```

### <a id="netpollWaiters" href="#netpollWaiters">var netpollWaiters</a>

```
searchKey: runtime.netpollWaiters
tags: [variable number private]
```

```Go
var netpollWaiters uint32
```

### <a id="netpollWakeSig" href="#netpollWakeSig">var netpollWakeSig</a>

```
searchKey: runtime.netpollWakeSig
tags: [variable number private]
```

```Go
var netpollWakeSig uint32 // used to avoid duplicate calls of netpollBreak

```

### <a id="newmHandoff" href="#newmHandoff">var newmHandoff</a>

```
searchKey: runtime.newmHandoff
tags: [variable struct private]
```

```Go
var newmHandoff struct {
	lock mutex

	// newm points to a list of M structures that need new OS
	// threads. The list is linked through m.schedlink.
	newm muintptr

	// waiting indicates that wake needs to be notified when an m
	// is put on the list.
	waiting bool
	wake    note

	// haveTemplateThread indicates that the templateThread has
	// been started. This is not protected by lock. Use cas to set
	// to 1.
	haveTemplateThread uint32
} = ...
```

newmHandoff contains a list of m structures that need new OS threads. This is used by newm in situations where newm itself can't safely start an OS thread. 

### <a id="newprocs" href="#newprocs">var newprocs</a>

```
searchKey: runtime.newprocs
tags: [variable number private]
```

```Go
var newprocs int32
```

### <a id="no_pointers_stackmap" href="#no_pointers_stackmap">var no_pointers_stackmap</a>

```
searchKey: runtime.no_pointers_stackmap
tags: [variable number private]
```

```Go
var no_pointers_stackmap uint64 // defined in assembly, for NO_LOCAL_POINTERS macro

```

### <a id="oneptrmask" href="#oneptrmask">var oneptrmask</a>

```
searchKey: runtime.oneptrmask
tags: [variable array number private]
```

```Go
var oneptrmask = [...]uint8{1}
```

ptrmask for an allocation containing a single pointer. 

### <a id="overflowError" href="#overflowError">var overflowError</a>

```
searchKey: runtime.overflowError
tags: [variable interface private]
```

```Go
var overflowError = error(errorString("integer overflow"))
```

### <a id="overflowTag" href="#overflowTag">var overflowTag</a>

```
searchKey: runtime.overflowTag
tags: [variable array number private]
```

```Go
var overflowTag [1]unsafe.Pointer // always nil

```

### <a id="panicking" href="#panicking">var panicking</a>

```
searchKey: runtime.panicking
tags: [variable number private]
```

```Go
var panicking uint32
```

panicking is non-zero when crashing the program for an unrecovered panic. panicking is incremented and decremented atomically. 

### <a id="paniclk" href="#paniclk">var paniclk</a>

```
searchKey: runtime.paniclk
tags: [variable struct private]
```

```Go
var paniclk mutex
```

paniclk is held while printing the panic information and stack trace, so that two concurrent panics don't overlap their output. 

### <a id="pdEface" href="#pdEface">var pdEface</a>

```
searchKey: runtime.pdEface
tags: [variable interface private]
```

```Go
var pdEface interface{} = (*pollDesc)(nil)
```

### <a id="pdType" href="#pdType">var pdType</a>

```
searchKey: runtime.pdType
tags: [variable struct private]
```

```Go
var pdType *_type = efaceOf(&pdEface)._type
```

### <a id="pendingPreemptSignals" href="#pendingPreemptSignals">var pendingPreemptSignals</a>

```
searchKey: runtime.pendingPreemptSignals
tags: [variable number private]
```

```Go
var pendingPreemptSignals uint32
```

pendingPreemptSignals is the number of preemption signals that have been sent but not received. This is only used on Darwin. For #41702. 

### <a id="persistentChunks" href="#persistentChunks">var persistentChunks</a>

```
searchKey: runtime.persistentChunks
tags: [variable struct private]
```

```Go
var persistentChunks *notInHeap
```

persistentChunks is a list of all the persistent chunks we have allocated. The list is maintained through the first word in the persistent chunk. This is updated atomically. 

### <a id="physHugePageShift" href="#physHugePageShift">var physHugePageShift</a>

```
searchKey: runtime.physHugePageShift
tags: [variable number private]
```

```Go
var physHugePageShift uint
```

physHugePageSize is the size in bytes of the OS's default physical huge page size whose allocation is opaque to the application. It is assumed and verified to be a power of two. 

If set, this must be set by the OS init code (typically in osinit) before mallocinit. However, setting it at all is optional, and leaving the default value is always safe (though potentially less efficient). 

Since physHugePageSize is always assumed to be a power of two, physHugePageShift is defined as physHugePageSize == 1 << physHugePageShift. The purpose of physHugePageShift is to avoid doing divisions in performance critical functions. 

### <a id="physHugePageSize" href="#physHugePageSize">var physHugePageSize</a>

```
searchKey: runtime.physHugePageSize
tags: [variable number private]
```

```Go
var physHugePageSize uintptr
```

physHugePageSize is the size in bytes of the OS's default physical huge page size whose allocation is opaque to the application. It is assumed and verified to be a power of two. 

If set, this must be set by the OS init code (typically in osinit) before mallocinit. However, setting it at all is optional, and leaving the default value is always safe (though potentially less efficient). 

Since physHugePageSize is always assumed to be a power of two, physHugePageShift is defined as physHugePageSize == 1 << physHugePageShift. The purpose of physHugePageShift is to avoid doing divisions in performance critical functions. 

### <a id="physPageSize" href="#physPageSize">var physPageSize</a>

```
searchKey: runtime.physPageSize
tags: [variable number private]
```

```Go
var physPageSize uintptr
```

physPageSize is the size in bytes of the OS's physical pages. Mapping and unmapping operations must be done at multiples of physPageSize. 

This must be set by the OS init code (typically in osinit) before mallocinit. 

### <a id="pinnedTypemaps" href="#pinnedTypemaps">var pinnedTypemaps</a>

```
searchKey: runtime.pinnedTypemaps
tags: [variable array object private]
```

```Go
var pinnedTypemaps []map[typeOff]*_type
```

pinnedTypemaps are the map[typeOff]*_type from the moduledata objects. 

These typemap objects are allocated at run time on the heap, but the only direct reference to them is in the moduledata, created by the linker and marked SNOPTRDATA so it is ignored by the GC. 

To make sure the map isn't collected, we keep a second reference here. 

### <a id="pollcache" href="#pollcache">var pollcache</a>

```
searchKey: runtime.pollcache
tags: [variable struct private]
```

```Go
var pollcache pollCache
```

### <a id="poolcleanup" href="#poolcleanup">var poolcleanup</a>

```
searchKey: runtime.poolcleanup
tags: [variable function private]
```

```Go
var poolcleanup func()
```

### <a id="printBacklog" href="#printBacklog">var printBacklog</a>

```
searchKey: runtime.printBacklog
tags: [variable array number private]
```

```Go
var printBacklog [512]byte
```

printBacklog is a circular buffer of messages written with the builtin print* functions, for use in postmortem analysis of core dumps. 

### <a id="printBacklogIndex" href="#printBacklogIndex">var printBacklogIndex</a>

```
searchKey: runtime.printBacklogIndex
tags: [variable number private]
```

```Go
var printBacklogIndex int
```

### <a id="processorVersionInfo" href="#processorVersionInfo">var processorVersionInfo</a>

```
searchKey: runtime.processorVersionInfo
tags: [variable number private]
```

```Go
var processorVersionInfo uint32
```

Information about what cpu features are available. Packages outside the runtime should not use these as they are not an external api. Set on startup in asm_{386,amd64}.s 

### <a id="prof" href="#prof">var prof</a>

```
searchKey: runtime.prof
tags: [variable struct private]
```

```Go
var prof struct {
	signalLock uint32
	hz         int32
}
```

### <a id="proflock" href="#proflock">var proflock</a>

```
searchKey: runtime.proflock
tags: [variable struct private]
```

```Go
var proflock mutex
```

NOTE(rsc): Everything here could use cas if contention became an issue. 

### <a id="ptrnames" href="#ptrnames">var ptrnames</a>

```
searchKey: runtime.ptrnames
tags: [variable array string private]
```

```Go
var ptrnames = []string{
	0: "scalar",
	1: "ptr",
}
```

### <a id="racecgosync" href="#racecgosync">var racecgosync</a>

```
searchKey: runtime.racecgosync
tags: [variable number private]
```

```Go
var racecgosync uint64 // represents possible synchronization in C code

```

### <a id="raceprocctx0" href="#raceprocctx0">var raceprocctx0</a>

```
searchKey: runtime.raceprocctx0
tags: [variable number private]
```

```Go
var raceprocctx0 uintptr
```

### <a id="reflectOffs" href="#reflectOffs">var reflectOffs</a>

```
searchKey: runtime.reflectOffs
tags: [variable struct private]
```

```Go
var reflectOffs struct {
	lock mutex
	next int32
	m    map[int32]unsafe.Pointer
	minv map[unsafe.Pointer]int32
} = ...
```

reflectOffs holds type offsets defined at run time by the reflect package. 

When a type is defined at run time, its *rtype data lives on the heap. There are a wide range of possible addresses the heap may use, that may not be representable as a 32-bit offset. Moreover the GC may one day start moving heap memory, in which case there is no stable offset that can be defined. 

To provide stable offsets, we add pin *rtype objects in a global map and treat the offset as an identifier. We use negative offsets that do not overlap with any compile-time module offsets. 

Entries are created by reflect.addReflectOff. 

### <a id="runningPanicDefers" href="#runningPanicDefers">var runningPanicDefers</a>

```
searchKey: runtime.runningPanicDefers
tags: [variable number private]
```

```Go
var runningPanicDefers uint32
```

runningPanicDefers is non-zero while running deferred functions for panic. runningPanicDefers is incremented and decremented atomically. This is used to try hard to get a panic stack trace out when exiting. 

### <a id="runtimeInitTime" href="#runtimeInitTime">var runtimeInitTime</a>

```
searchKey: runtime.runtimeInitTime
tags: [variable number private]
```

```Go
var runtimeInitTime int64
```

runtimeInitTime is the nanotime() at which the runtime started. 

### <a id="runtime_inittask" href="#runtime_inittask">var runtime_inittask</a>

```
searchKey: runtime.runtime_inittask
tags: [variable struct private]
```

```Go
var runtime_inittask initTask
```

### <a id="scavenge" href="#scavenge">var scavenge</a>

```
searchKey: runtime.scavenge
tags: [variable struct private]
```

```Go
var scavenge struct {
	lock       mutex
	g          *g
	parked     bool
	timer      *timer
	sysmonWake uint32 // Set atomically.
} = ...
```

Sleep/wait state of the background scavenger. 

### <a id="sched" href="#sched">var sched</a>

```
searchKey: runtime.sched
tags: [variable struct private]
```

```Go
var sched schedt
```

### <a id="semtable" href="#semtable">var semtable</a>

```
searchKey: runtime.semtable
tags: [variable array struct private]
```

```Go
var semtable [semTabSize]struct {
	root semaRoot
	pad  [cpu.CacheLinePadSize - unsafe.Sizeof(semaRoot{})]byte
} = ...
```

### <a id="shiftError" href="#shiftError">var shiftError</a>

```
searchKey: runtime.shiftError
tags: [variable interface private]
```

```Go
var shiftError = error(errorString("negative shift amount"))
```

### <a id="sig" href="#sig">var sig</a>

```
searchKey: runtime.sig
tags: [variable struct private]
```

```Go
var sig struct {
	note       note
	mask       [(_NSIG + 31) / 32]uint32
	wanted     [(_NSIG + 31) / 32]uint32
	ignored    [(_NSIG + 31) / 32]uint32
	recv       [(_NSIG + 31) / 32]uint32
	state      uint32
	delivering uint32
	inuse      bool
} = ...
```

sig handles communication between the signal handler and os/signal. Other than the inuse and recv fields, the fields are accessed atomically. 

The wanted and ignored fields are only written by one goroutine at a time; access is controlled by the handlers Mutex in os/signal. The fields are only read by that one goroutine and by the signal handler. We access them atomically to minimize the race between setting them in the goroutine calling os/signal and the signal handler, which may be running in a different thread. That race is unavoidable, as there is no connection between handling a signal and receiving one, but atomic instructions should minimize it. 

### <a id="sigNoteRead" href="#sigNoteRead">var sigNoteRead</a>

```
searchKey: runtime.sigNoteRead
tags: [variable number private]
```

```Go
var sigNoteRead, sigNoteWrite int32
```

The read and write file descriptors used by the sigNote functions. 

### <a id="sigNoteWrite" href="#sigNoteWrite">var sigNoteWrite</a>

```
searchKey: runtime.sigNoteWrite
tags: [variable number private]
```

```Go
var sigNoteRead, sigNoteWrite int32
```

The read and write file descriptors used by the sigNote functions. 

### <a id="signalsOK" href="#signalsOK">var signalsOK</a>

```
searchKey: runtime.signalsOK
tags: [variable boolean private]
```

```Go
var signalsOK bool
```

### <a id="sigprofCallers" href="#sigprofCallers">var sigprofCallers</a>

```
searchKey: runtime.sigprofCallers
tags: [variable array number private]
```

```Go
var sigprofCallers cgoCallers
```

If the signal handler receives a SIGPROF signal on a non-Go thread, it tries to collect a traceback into sigprofCallers. sigprofCallersUse is set to non-zero while sigprofCallers holds a traceback. 

### <a id="sigprofCallersUse" href="#sigprofCallersUse">var sigprofCallersUse</a>

```
searchKey: runtime.sigprofCallersUse
tags: [variable number private]
```

```Go
var sigprofCallersUse uint32
```

### <a id="sigsetAllExiting" href="#sigsetAllExiting">var sigsetAllExiting</a>

```
searchKey: runtime.sigsetAllExiting
tags: [variable number private]
```

```Go
var sigsetAllExiting = sigset_all
```

sigsetAllExiting is used by sigblock(true) when a thread is exiting. sigset_all is defined in OS specific code, and per GOOS behavior may override this default for sigsetAllExiting: see osinit(). 

### <a id="sigset_all" href="#sigset_all">var sigset_all</a>

```
searchKey: runtime.sigset_all
tags: [variable number private]
```

```Go
var sigset_all = ^sigset(0)
```

### <a id="sigtable" href="#sigtable">var sigtable</a>

```
searchKey: runtime.sigtable
tags: [variable array struct private]
```

```Go
var sigtable = ...
```

### <a id="sizeClassBuckets" href="#sizeClassBuckets">var sizeClassBuckets</a>

```
searchKey: runtime.sizeClassBuckets
tags: [variable array number private]
```

```Go
var sizeClassBuckets []float64
```

### <a id="size_to_class128" href="#size_to_class128">var size_to_class128</a>

```
searchKey: runtime.size_to_class128
tags: [variable array number private]
```

```Go
var size_to_class128 = ...
```

### <a id="size_to_class8" href="#size_to_class8">var size_to_class8</a>

```
searchKey: runtime.size_to_class8
tags: [variable array number private]
```

```Go
var size_to_class8 = ...
```

### <a id="sliceEface" href="#sliceEface">var sliceEface</a>

```
searchKey: runtime.sliceEface
tags: [variable interface private]
```

```Go
var sliceEface interface{} = sliceInterfacePtr(nil)
```

### <a id="sliceType" href="#sliceType">var sliceType</a>

```
searchKey: runtime.sliceType
tags: [variable struct private]
```

```Go
var sliceType *_type = efaceOf(&sliceEface)._type
```

### <a id="spanSetBlockPool" href="#spanSetBlockPool">var spanSetBlockPool</a>

```
searchKey: runtime.spanSetBlockPool
tags: [variable struct private]
```

```Go
var spanSetBlockPool spanSetBlockAlloc
```

spanSetBlockPool is a global pool of spanSetBlocks. 

### <a id="stackLarge" href="#stackLarge">var stackLarge</a>

```
searchKey: runtime.stackLarge
tags: [variable struct private]
```

```Go
var stackLarge struct {
	lock mutex
	free [heapAddrBits - pageShift]mSpanList // free lists by log_2(s.npages)
} = ...
```

Global pool of large stack spans. 

### <a id="stackpool" href="#stackpool">var stackpool</a>

```
searchKey: runtime.stackpool
tags: [variable array struct private]
```

```Go
var stackpool [_NumStackOrders]struct {
	item stackpoolItem
	_    [cpu.CacheLinePadSize - unsafe.Sizeof(stackpoolItem{})%cpu.CacheLinePadSize]byte
} = ...
```

Global pool of spans that have free stacks. Stacks are assigned an order according to size. 

```
order = log_2(size/FixedStack)

```
There is a free list for each order. 

### <a id="starttime" href="#starttime">var starttime</a>

```
searchKey: runtime.starttime
tags: [variable number private]
```

```Go
var starttime int64
```

### <a id="staticuint64s" href="#staticuint64s">var staticuint64s</a>

```
searchKey: runtime.staticuint64s
tags: [variable array number private]
```

```Go
var staticuint64s = ...
```

staticuint64s is used to avoid allocating in convTx for small integer values. 

### <a id="stealOrder" href="#stealOrder">var stealOrder</a>

```
searchKey: runtime.stealOrder
tags: [variable struct private]
```

```Go
var stealOrder randomOrder
```

### <a id="stringEface" href="#stringEface">var stringEface</a>

```
searchKey: runtime.stringEface
tags: [variable interface private]
```

```Go
var stringEface interface{} = stringInterfacePtr("")
```

### <a id="stringType" href="#stringType">var stringType</a>

```
searchKey: runtime.stringType
tags: [variable struct private]
```

```Go
var stringType *_type = efaceOf(&stringEface)._type
```

### <a id="sweep" href="#sweep">var sweep</a>

```
searchKey: runtime.sweep
tags: [variable struct private]
```

```Go
var sweep sweepdata
```

### <a id="testSigtrap" href="#testSigtrap">var testSigtrap</a>

```
searchKey: runtime.testSigtrap
tags: [variable function private]
```

```Go
var testSigtrap func(info *siginfo, ctxt *sigctxt, gp *g) bool
```

testSigtrap and testSigusr1 are used by the runtime tests. If non-nil, it is called on SIGTRAP/SIGUSR1. If it returns true, the normal behavior on this signal is suppressed. 

### <a id="testSigusr1" href="#testSigusr1">var testSigusr1</a>

```
searchKey: runtime.testSigusr1
tags: [variable function private]
```

```Go
var testSigusr1 func(gp *g) bool
```

### <a id="test_x64" href="#test_x64">var test_x64</a>

```
searchKey: runtime.test_x64
tags: [variable number private]
```

```Go
var test_z64, test_x64 uint64
```

TODO: These should be locals in testAtomic64, but we don't 8-byte align stack variables on 386. 

### <a id="test_z64" href="#test_z64">var test_z64</a>

```
searchKey: runtime.test_z64
tags: [variable number private]
```

```Go
var test_z64, test_x64 uint64
```

TODO: These should be locals in testAtomic64, but we don't 8-byte align stack variables on 386. 

### <a id="ticks" href="#ticks">var ticks</a>

```
searchKey: runtime.ticks
tags: [variable struct private]
```

```Go
var ticks struct {
	lock mutex
	pad  uint32 // ensure 8-byte alignment of val on 386
	val  uint64
}
```

### <a id="timeHistBuckets" href="#timeHistBuckets">var timeHistBuckets</a>

```
searchKey: runtime.timeHistBuckets
tags: [variable array number private]
```

```Go
var timeHistBuckets []float64
```

### <a id="timerpMask" href="#timerpMask">var timerpMask</a>

```
searchKey: runtime.timerpMask
tags: [variable array number private]
```

```Go
var timerpMask pMask
```

Bitmask of Ps that may have a timer, one bit per P. Reads and writes must be atomic. Length may change at safe points. 

### <a id="tmpbuf" href="#tmpbuf">var tmpbuf</a>

```
searchKey: runtime.tmpbuf
tags: [variable array number private]
```

```Go
var tmpbuf []byte
```

### <a id="trace" href="#trace">var trace</a>

```
searchKey: runtime.trace
tags: [variable struct private]
```

```Go
var trace struct {
	lock          mutex       // protects the following members
	lockOwner     *g          // to avoid deadlocks during recursive lock locks
	enabled       bool        // when set runtime traces events
	shutdown      bool        // set when we are waiting for trace reader to finish after setting enabled to false
	headerWritten bool        // whether ReadTrace has emitted trace header
	footerWritten bool        // whether ReadTrace has emitted trace footer
	shutdownSema  uint32      // used to wait for ReadTrace completion
	seqStart      uint64      // sequence number when tracing was started
	ticksStart    int64       // cputicks when tracing was started
	ticksEnd      int64       // cputicks when tracing was stopped
	timeStart     int64       // nanotime when tracing was started
	timeEnd       int64       // nanotime when tracing was stopped
	seqGC         uint64      // GC start/done sequencer
	reading       traceBufPtr // buffer currently handed off to user
	empty         traceBufPtr // stack of empty buffers
	fullHead      traceBufPtr // queue of full buffers
	fullTail      traceBufPtr
	reader        guintptr        // goroutine that called ReadTrace, or nil
	stackTab      traceStackTable // maps stack traces to unique ids

	// Dictionary for traceEvString.
	//
	// TODO: central lock to access the map is not ideal.
	//   option: pre-assign ids to all user annotation region names and tags
	//   option: per-P cache
	//   option: sync.Map like data structure
	stringsLock mutex
	strings     map[string]uint64
	stringSeq   uint64

	// markWorkerLabels maps gcMarkWorkerMode to string ID.
	markWorkerLabels [len(gcMarkWorkerModeStrings)]uint64

	bufLock mutex       // protects buf
	buf     traceBufPtr // global trace buffer, used when running without a p
} = ...
```

trace is global tracing context. 

### <a id="traceback_cache" href="#traceback_cache">var traceback_cache</a>

```
searchKey: runtime.traceback_cache
tags: [variable number private]
```

```Go
var traceback_cache uint32 = 2 << tracebackShift
```

### <a id="traceback_env" href="#traceback_env">var traceback_env</a>

```
searchKey: runtime.traceback_env
tags: [variable number private]
```

```Go
var traceback_env uint32
```

### <a id="tracelock" href="#tracelock">var tracelock</a>

```
searchKey: runtime.tracelock
tags: [variable struct private]
```

```Go
var tracelock mutex
```

### <a id="typecache" href="#typecache">var typecache</a>

```
searchKey: runtime.typecache
tags: [variable array struct private]
```

```Go
var typecache [typeCacheBuckets]typeCacheBucket
```

### <a id="uint16Eface" href="#uint16Eface">var uint16Eface</a>

```
searchKey: runtime.uint16Eface
tags: [variable interface private]
```

```Go
var uint16Eface interface{} = uint16InterfacePtr(0)
```

### <a id="uint16Type" href="#uint16Type">var uint16Type</a>

```
searchKey: runtime.uint16Type
tags: [variable struct private]
```

```Go
var uint16Type *_type = efaceOf(&uint16Eface)._type
```

### <a id="uint32Eface" href="#uint32Eface">var uint32Eface</a>

```
searchKey: runtime.uint32Eface
tags: [variable interface private]
```

```Go
var uint32Eface interface{} = uint32InterfacePtr(0)
```

### <a id="uint32Type" href="#uint32Type">var uint32Type</a>

```
searchKey: runtime.uint32Type
tags: [variable struct private]
```

```Go
var uint32Type *_type = efaceOf(&uint32Eface)._type
```

### <a id="uint64Eface" href="#uint64Eface">var uint64Eface</a>

```
searchKey: runtime.uint64Eface
tags: [variable interface private]
```

```Go
var uint64Eface interface{} = uint64InterfacePtr(0)
```

### <a id="uint64Type" href="#uint64Type">var uint64Type</a>

```
searchKey: runtime.uint64Type
tags: [variable struct private]
```

```Go
var uint64Type *_type = efaceOf(&uint64Eface)._type
```

### <a id="urandom_dev" href="#urandom_dev">var urandom_dev</a>

```
searchKey: runtime.urandom_dev
tags: [variable array number private]
```

```Go
var urandom_dev = []byte("/dev/urandom\x00")
```

### <a id="useAVXmemmove" href="#useAVXmemmove">var useAVXmemmove</a>

```
searchKey: runtime.useAVXmemmove
tags: [variable boolean private]
```

```Go
var useAVXmemmove bool
```

### <a id="useAeshash" href="#useAeshash">var useAeshash</a>

```
searchKey: runtime.useAeshash
tags: [variable boolean private]
```

```Go
var useAeshash bool
```

runtime variable to check if the processor we're running on actually supports the instructions used by the AES-based hash implementation. 

### <a id="useCheckmark" href="#useCheckmark">var useCheckmark</a>

```
searchKey: runtime.useCheckmark
tags: [variable boolean private]
```

```Go
var useCheckmark = false
```

If useCheckmark is true, marking of an object uses the checkmark bits instead of the standard mark bits. 

### <a id="waitForSigusr1" href="#waitForSigusr1">var waitForSigusr1</a>

```
searchKey: runtime.waitForSigusr1
tags: [variable struct private]
```

```Go
var waitForSigusr1 struct {
	rdpipe int32
	wrpipe int32
	mID    int64
}
```

### <a id="waitReasonStrings" href="#waitReasonStrings">var waitReasonStrings</a>

```
searchKey: runtime.waitReasonStrings
tags: [variable array string private]
```

```Go
var waitReasonStrings = ...
```

### <a id="work" href="#work">var work</a>

```
searchKey: runtime.work
tags: [variable struct private]
```

```Go
var work struct {
	full  lfstack          // lock-free list of full blocks workbuf
	empty lfstack          // lock-free list of empty blocks workbuf
	pad0  cpu.CacheLinePad // prevents false-sharing between full/empty and nproc/nwait

	wbufSpans struct {
		lock mutex
		// free is a list of spans dedicated to workbufs, but
		// that don't currently contain any workbufs.
		free mSpanList
		// busy is a list of all spans containing workbufs on
		// one of the workbuf lists.
		busy mSpanList
	}

	// Restore 64-bit alignment on 32-bit.
	_ uint32

	// bytesMarked is the number of bytes marked this cycle. This
	// includes bytes blackened in scanned objects, noscan objects
	// that go straight to black, and permagrey objects scanned by
	// markroot during the concurrent scan phase. This is updated
	// atomically during the cycle. Updates may be batched
	// arbitrarily, since the value is only read at the end of the
	// cycle.
	//
	// Because of benign races during marking, this number may not
	// be the exact number of marked bytes, but it should be very
	// close.
	//
	// Put this field here because it needs 64-bit atomic access
	// (and thus 8-byte alignment even on 32-bit architectures).
	bytesMarked uint64

	markrootNext uint32 // next markroot job
	markrootJobs uint32 // number of markroot jobs

	nproc  uint32
	tstart int64
	nwait  uint32

	// Number of roots of various root types. Set by gcMarkRootPrepare.
	nDataRoots, nBSSRoots, nSpanRoots, nStackRoots int

	// Base indexes of each root type. Set by gcMarkRootPrepare.
	baseData, baseBSS, baseSpans, baseStacks, baseEnd uint32

	// Each type of GC state transition is protected by a lock.
	// Since multiple threads can simultaneously detect the state
	// transition condition, any thread that detects a transition
	// condition must acquire the appropriate transition lock,
	// re-check the transition condition and return if it no
	// longer holds or perform the transition if it does.
	// Likewise, any transition must invalidate the transition
	// condition before releasing the lock. This ensures that each
	// transition is performed by exactly one thread and threads
	// that need the transition to happen block until it has
	// happened.
	//
	// startSema protects the transition from "off" to mark or
	// mark termination.
	startSema uint32
	// markDoneSema protects transitions from mark to mark termination.
	markDoneSema uint32

	bgMarkReady note   // signal background mark worker has started
	bgMarkDone  uint32 // cas to 1 when at a background mark completion point

	// mode is the concurrency mode of the current GC cycle.
	mode gcMode

	// userForced indicates the current GC cycle was forced by an
	// explicit user call.
	userForced bool

	// totaltime is the CPU nanoseconds spent in GC since the
	// program started if debug.gctrace > 0.
	totaltime int64

	// initialHeapLive is the value of gcController.heapLive at the
	// beginning of this GC cycle.
	initialHeapLive uint64

	// assistQueue is a queue of assists that are blocked because
	// there was neither enough credit to steal or enough work to
	// do.
	assistQueue struct {
		lock mutex
		q    gQueue
	}

	// sweepWaiters is a list of blocked goroutines to wake when
	// we transition from mark termination to sweep.
	sweepWaiters struct {
		lock mutex
		list gList
	}

	// cycles is the number of completed GC cycles, where a GC
	// cycle is sweep termination, mark, mark termination, and
	// sweep. This differs from memstats.numgc, which is
	// incremented at mark termination.
	cycles uint32

	// Timing/utilization stats for this cycle.
	stwprocs, maxprocs                 int32
	tSweepTerm, tMark, tMarkTerm, tEnd int64 // nanotime() of phase start

	pauseNS    int64 // total STW time this cycle
	pauseStart int64 // nanotime() of last STW

	// debug.gctrace heap sizes for this cycle.
	heap0, heap1, heap2, heapGoal uint64
} = ...
```

### <a id="worldsema" href="#worldsema">var worldsema</a>

```
searchKey: runtime.worldsema
tags: [variable number private]
```

```Go
var worldsema uint32 = 1
```

Holding worldsema grants an M the right to try to stop the world. 

### <a id="writeBarrier" href="#writeBarrier">var writeBarrier</a>

```
searchKey: runtime.writeBarrier
tags: [variable struct private]
```

```Go
var writeBarrier struct {
	enabled bool    // compiler emits a check of this before calling write barrier
	pad     [3]byte // compiler uses 32-bit load for "enabled" field
	needed  bool    // whether we need a write barrier for current GC phase
	cgo     bool    // whether we need a write barrier for a cgo check
	alignme uint64  // guarantee alignment so that compiler can use a 32 or 64-bit load
} = ...
```

The compiler knows about this variable. If you change it, you must change builtin/runtime.go, too. If you change the first four bytes, you must also change the write barrier insertion code. 

### <a id="x86HasFMA" href="#x86HasFMA">var x86HasFMA</a>

```
searchKey: runtime.x86HasFMA
tags: [variable boolean private]
```

```Go
var x86HasFMA bool
```

### <a id="x86HasPOPCNT" href="#x86HasPOPCNT">var x86HasPOPCNT</a>

```
searchKey: runtime.x86HasPOPCNT
tags: [variable boolean private]
```

```Go
var x86HasPOPCNT bool
```

Set in runtime.cpuinit. TODO: deprecate these; use internal/cpu directly. 

### <a id="x86HasSSE41" href="#x86HasSSE41">var x86HasSSE41</a>

```
searchKey: runtime.x86HasSSE41
tags: [variable boolean private]
```

```Go
var x86HasSSE41 bool
```

### <a id="xbuckets" href="#xbuckets">var xbuckets</a>

```
searchKey: runtime.xbuckets
tags: [variable struct private]
```

```Go
var xbuckets *bucket // mutex profile buckets

```

### <a id="zeroVal" href="#zeroVal">var zeroVal</a>

```
searchKey: runtime.zeroVal
tags: [variable array number private]
```

```Go
var zeroVal [maxZero]byte
```

### <a id="zerobase" href="#zerobase">var zerobase</a>

```
searchKey: runtime.zerobase
tags: [variable number private]
```

```Go
var zerobase uintptr
```

base address for all 0-byte allocations 

### <a id="_cgo_callers" href="#_cgo_callers">var _cgo_callers</a>

```
searchKey: runtime._cgo_callers
tags: [variable number private]
```

```Go
var _cgo_callers unsafe.Pointer
```

### <a id="_cgo_init" href="#_cgo_init">var _cgo_init</a>

```
searchKey: runtime._cgo_init
tags: [variable number private]
```

```Go
var _cgo_init unsafe.Pointer
```

### <a id="_cgo_notify_runtime_init_done" href="#_cgo_notify_runtime_init_done">var _cgo_notify_runtime_init_done</a>

```
searchKey: runtime._cgo_notify_runtime_init_done
tags: [variable number private]
```

```Go
var _cgo_notify_runtime_init_done unsafe.Pointer
```

### <a id="_cgo_set_context_function" href="#_cgo_set_context_function">var _cgo_set_context_function</a>

```
searchKey: runtime._cgo_set_context_function
tags: [variable number private]
```

```Go
var _cgo_set_context_function unsafe.Pointer
```

### <a id="_cgo_setenv" href="#_cgo_setenv">var _cgo_setenv</a>

```
searchKey: runtime._cgo_setenv
tags: [variable number private]
```

```Go
var _cgo_setenv unsafe.Pointer // pointer to C function

```

### <a id="_cgo_sys_thread_create" href="#_cgo_sys_thread_create">var _cgo_sys_thread_create</a>

```
searchKey: runtime._cgo_sys_thread_create
tags: [variable number private]
```

```Go
var _cgo_sys_thread_create unsafe.Pointer
```

### <a id="_cgo_thread_start" href="#_cgo_thread_start">var _cgo_thread_start</a>

```
searchKey: runtime._cgo_thread_start
tags: [variable number private]
```

```Go
var _cgo_thread_start unsafe.Pointer
```

### <a id="_cgo_unsetenv" href="#_cgo_unsetenv">var _cgo_unsetenv</a>

```
searchKey: runtime._cgo_unsetenv
tags: [variable number private]
```

```Go
var _cgo_unsetenv unsafe.Pointer // pointer to C function

```

### <a id="_cgo_yield" href="#_cgo_yield">var _cgo_yield</a>

```
searchKey: runtime._cgo_yield
tags: [variable number private]
```

```Go
var _cgo_yield unsafe.Pointer
```

## <a id="type" href="#type">Types</a>

```
tags: [package]
```

### <a id="AddrRange" href="#AddrRange">type AddrRange struct</a>

```
searchKey: runtime.AddrRange
tags: [struct private]
```

```Go
type AddrRange struct {
	addrRange
}
```

AddrRange is a wrapper around addrRange for testing. 

#### <a id="MakeAddrRange" href="#MakeAddrRange">func MakeAddrRange(base, limit uintptr) AddrRange</a>

```
searchKey: runtime.MakeAddrRange
tags: [method private]
```

```Go
func MakeAddrRange(base, limit uintptr) AddrRange
```

MakeAddrRange creates a new address range. 

#### <a id="AddrRange.Base" href="#AddrRange.Base">func (a AddrRange) Base() uintptr</a>

```
searchKey: runtime.AddrRange.Base
tags: [function private]
```

```Go
func (a AddrRange) Base() uintptr
```

Base returns the virtual base address of the address range. 

#### <a id="AddrRange.Equals" href="#AddrRange.Equals">func (a AddrRange) Equals(b AddrRange) bool</a>

```
searchKey: runtime.AddrRange.Equals
tags: [method private]
```

```Go
func (a AddrRange) Equals(b AddrRange) bool
```

Equals returns true if the two address ranges are exactly equal. 

#### <a id="AddrRange.Limit" href="#AddrRange.Limit">func (a AddrRange) Limit() uintptr</a>

```
searchKey: runtime.AddrRange.Limit
tags: [function private]
```

```Go
func (a AddrRange) Limit() uintptr
```

Base returns the virtual address of the limit of the address range. 

#### <a id="AddrRange.Size" href="#AddrRange.Size">func (a AddrRange) Size() uintptr</a>

```
searchKey: runtime.AddrRange.Size
tags: [function private]
```

```Go
func (a AddrRange) Size() uintptr
```

Size returns the size in bytes of the address range. 

### <a id="AddrRanges" href="#AddrRanges">type AddrRanges struct</a>

```
searchKey: runtime.AddrRanges
tags: [struct private]
```

```Go
type AddrRanges struct {
	addrRanges
	mutable bool
}
```

AddrRanges is a wrapper around addrRanges for testing. 

#### <a id="MakeAddrRanges" href="#MakeAddrRanges">func MakeAddrRanges(a ...AddrRange) AddrRanges</a>

```
searchKey: runtime.MakeAddrRanges
tags: [method private]
```

```Go
func MakeAddrRanges(a ...AddrRange) AddrRanges
```

MakeAddrRanges creates a new addrRanges populated with the ranges in a. 

The returned AddrRanges is immutable, so methods like Add will fail. 

#### <a id="NewAddrRanges" href="#NewAddrRanges">func NewAddrRanges() AddrRanges</a>

```
searchKey: runtime.NewAddrRanges
tags: [function private]
```

```Go
func NewAddrRanges() AddrRanges
```

NewAddrRanges creates a new empty addrRanges. 

Note that this initializes addrRanges just like in the runtime, so its memory is persistentalloc'd. Call this function sparingly since the memory it allocates is leaked. 

This AddrRanges is mutable, so we can test methods like Add. 

#### <a id="AddrRanges.Add" href="#AddrRanges.Add">func (a *AddrRanges) Add(r AddrRange)</a>

```
searchKey: runtime.AddrRanges.Add
tags: [method private]
```

```Go
func (a *AddrRanges) Add(r AddrRange)
```

Add adds a new AddrRange to the AddrRanges. 

The AddrRange must be mutable (i.e. created by NewAddrRanges), otherwise this method will throw. 

#### <a id="AddrRanges.FindSucc" href="#AddrRanges.FindSucc">func (a *AddrRanges) FindSucc(base uintptr) int</a>

```
searchKey: runtime.AddrRanges.FindSucc
tags: [method private]
```

```Go
func (a *AddrRanges) FindSucc(base uintptr) int
```

FindSucc returns the successor to base. See addrRanges.findSucc for more details. 

#### <a id="AddrRanges.Ranges" href="#AddrRanges.Ranges">func (a *AddrRanges) Ranges() []AddrRange</a>

```
searchKey: runtime.AddrRanges.Ranges
tags: [function private]
```

```Go
func (a *AddrRanges) Ranges() []AddrRange
```

Ranges returns a copy of the ranges described by the addrRanges. 

#### <a id="AddrRanges.TotalBytes" href="#AddrRanges.TotalBytes">func (a *AddrRanges) TotalBytes() uintptr</a>

```
searchKey: runtime.AddrRanges.TotalBytes
tags: [function private]
```

```Go
func (a *AddrRanges) TotalBytes() uintptr
```

TotalBytes returns the totalBytes field of the addrRanges. 

### <a id="BitRange" href="#BitRange">type BitRange struct</a>

```
searchKey: runtime.BitRange
tags: [struct private]
```

```Go
type BitRange struct {
	I, N uint // bit index and length in bits
}
```

BitRange represents a range over a bitmap. 

### <a id="BitsMismatch" href="#BitsMismatch">type BitsMismatch struct</a>

```
searchKey: runtime.BitsMismatch
tags: [struct private]
```

```Go
type BitsMismatch struct {
	Base      uintptr
	Got, Want uint64
}
```

### <a id="BlockProfileRecord" href="#BlockProfileRecord">type BlockProfileRecord struct</a>

```
searchKey: runtime.BlockProfileRecord
tags: [struct]
```

```Go
type BlockProfileRecord struct {
	Count  int64
	Cycles int64
	StackRecord
}
```

BlockProfileRecord describes blocking events originated at a particular call sequence (stack trace). 

### <a id="ChunkIdx" href="#ChunkIdx">type ChunkIdx runtime.chunkIdx</a>

```
searchKey: runtime.ChunkIdx
tags: [number private]
```

```Go
type ChunkIdx chunkIdx
```

Expose chunk index type. 

### <a id="Error" href="#Error">type Error interface</a>

```
searchKey: runtime.Error
tags: [interface]
```

```Go
type Error interface {
	error

	// RuntimeError is a no-op function but
	// serves to distinguish types that are run time
	// errors from ordinary errors: a type is a
	// run time error if it has a RuntimeError method.
	RuntimeError()
}
```

The Error interface identifies a run time error. 

### <a id="Frame" href="#Frame">type Frame struct</a>

```
searchKey: runtime.Frame
tags: [struct]
```

```Go
type Frame struct {
	// PC is the program counter for the location in this frame.
	// For a frame that calls another frame, this will be the
	// program counter of a call instruction. Because of inlining,
	// multiple frames may have the same PC value, but different
	// symbolic information.
	PC uintptr

	// Func is the Func value of this call frame. This may be nil
	// for non-Go code or fully inlined functions.
	Func *Func

	// Function is the package path-qualified function name of
	// this call frame. If non-empty, this string uniquely
	// identifies a single function in the program.
	// This may be the empty string if not known.
	// If Func is not nil then Function == Func.Name().
	Function string

	// File and Line are the file name and line number of the
	// location in this frame. For non-leaf frames, this will be
	// the location of a call. These may be the empty string and
	// zero, respectively, if not known.
	File string
	Line int

	// Entry point program counter for the function; may be zero
	// if not known. If Func is not nil then Entry ==
	// Func.Entry().
	Entry uintptr

	// The runtime's internal view of the function. This field
	// is set (funcInfo.valid() returns true) only for Go functions,
	// not for C functions.
	funcInfo funcInfo
}
```

Frame is the information returned by Frames for each call frame. 

### <a id="Frames" href="#Frames">type Frames struct</a>

```
searchKey: runtime.Frames
tags: [struct]
```

```Go
type Frames struct {
	// callers is a slice of PCs that have not yet been expanded to frames.
	callers []uintptr

	// frames is a slice of Frames that have yet to be returned.
	frames     []Frame
	frameStore [2]Frame
}
```

Frames may be used to get function/file/line information for a slice of PC values returned by Callers. 

#### <a id="CallersFrames" href="#CallersFrames">func CallersFrames(callers []uintptr) *Frames</a>

```
searchKey: runtime.CallersFrames
tags: [method]
```

```Go
func CallersFrames(callers []uintptr) *Frames
```

CallersFrames takes a slice of PC values returned by Callers and prepares to return function/file/line information. Do not change the slice until you are done with the Frames. 

#### <a id="Frames.Next" href="#Frames.Next">func (ci *Frames) Next() (frame Frame, more bool)</a>

```
searchKey: runtime.Frames.Next
tags: [function]
```

```Go
func (ci *Frames) Next() (frame Frame, more bool)
```

Next returns a Frame representing the next call frame in the slice of PC values. If it has already returned all call frames, Next returns a zero Frame. 

The more result indicates whether the next call to Next will return a valid Frame. It does not necessarily indicate whether this call returned one. 

See the Frames example for idiomatic usage. 

### <a id="Func" href="#Func">type Func struct</a>

```
searchKey: runtime.Func
tags: [struct]
```

```Go
type Func struct {
	opaque struct{} // unexported field to disallow conversions
}
```

A Func represents a Go function in the running binary. 

#### <a id="FuncForPC" href="#FuncForPC">func FuncForPC(pc uintptr) *Func</a>

```
searchKey: runtime.FuncForPC
tags: [method]
```

```Go
func FuncForPC(pc uintptr) *Func
```

FuncForPC returns a *Func describing the function that contains the given program counter address, or else nil. 

If pc represents multiple functions because of inlining, it returns the *Func describing the innermost function, but with an entry of the outermost function. 

#### <a id="Func.Entry" href="#Func.Entry">func (f *Func) Entry() uintptr</a>

```
searchKey: runtime.Func.Entry
tags: [function]
```

```Go
func (f *Func) Entry() uintptr
```

Entry returns the entry address of the function. 

#### <a id="Func.FileLine" href="#Func.FileLine">func (f *Func) FileLine(pc uintptr) (file string, line int)</a>

```
searchKey: runtime.Func.FileLine
tags: [method]
```

```Go
func (f *Func) FileLine(pc uintptr) (file string, line int)
```

FileLine returns the file name and line number of the source code corresponding to the program counter pc. The result will not be accurate if pc is not a program counter within f. 

#### <a id="Func.Name" href="#Func.Name">func (f *Func) Name() string</a>

```
searchKey: runtime.Func.Name
tags: [function]
```

```Go
func (f *Func) Name() string
```

Name returns the name of the function. 

#### <a id="Func.funcInfo" href="#Func.funcInfo">func (f *Func) funcInfo() funcInfo</a>

```
searchKey: runtime.Func.funcInfo
tags: [function private]
```

```Go
func (f *Func) funcInfo() funcInfo
```

#### <a id="Func.raw" href="#Func.raw">func (f *Func) raw() *_func</a>

```
searchKey: runtime.Func.raw
tags: [function private]
```

```Go
func (f *Func) raw() *_func
```

### <a id="G" href="#G">type G runtime.g</a>

```
searchKey: runtime.G
tags: [struct private]
```

```Go
type G = g
```

#### <a id="Getg" href="#Getg">func Getg() *G</a>

```
searchKey: runtime.Getg
tags: [function private]
```

```Go
func Getg() *G
```

#### <a id="atomicAllG" href="#atomicAllG">func atomicAllG() (**g, uintptr)</a>

```
searchKey: runtime.atomicAllG
tags: [function private]
```

```Go
func atomicAllG() (**g, uintptr)
```

atomicAllG returns &allgs[0] and len(allgs) for use with atomicAllGIndex. 

#### <a id="atomicAllGIndex" href="#atomicAllGIndex">func atomicAllGIndex(ptr **g, i uintptr) *g</a>

```
searchKey: runtime.atomicAllGIndex
tags: [method private]
```

```Go
func atomicAllGIndex(ptr **g, i uintptr) *g
```

atomicAllGIndex returns ptr[i] with the allgptr returned from atomicAllG. 

#### <a id="beforeIdle" href="#beforeIdle">func beforeIdle(int64, int64) (*g, bool)</a>

```
searchKey: runtime.beforeIdle
tags: [method private]
```

```Go
func beforeIdle(int64, int64) (*g, bool)
```

#### <a id="checkIdleGCNoP" href="#checkIdleGCNoP">func checkIdleGCNoP() (*p, *g)</a>

```
searchKey: runtime.checkIdleGCNoP
tags: [function private]
```

```Go
func checkIdleGCNoP() (*p, *g)
```

Check for idle-priority GC, without a P on entry. 

If some GC work, a P, and a worker G are all available, the P and G will be returned. The returned P has not been wired yet. 

#### <a id="findrunnable" href="#findrunnable">func findrunnable() (gp *g, inheritTime bool)</a>

```
searchKey: runtime.findrunnable
tags: [function private]
```

```Go
func findrunnable() (gp *g, inheritTime bool)
```

Finds a runnable goroutine to execute. Tries to steal from other P's, get g from local or global queue, poll network. 

#### <a id="getg" href="#getg">func getg() *g</a>

```
searchKey: runtime.getg
tags: [function private]
```

```Go
func getg() *g
```

getg returns the pointer to the current g. The compiler rewrites calls to this function into instructions that fetch the g directly (from TLS or from the dedicated register). 

#### <a id="gfget" href="#gfget">func gfget(_p_ *p) *g</a>

```
searchKey: runtime.gfget
tags: [method private]
```

```Go
func gfget(_p_ *p) *g
```

Get from gfree list. If local list is empty, grab a batch from global list. 

#### <a id="globrunqget" href="#globrunqget">func globrunqget(_p_ *p, max int32) *g</a>

```
searchKey: runtime.globrunqget
tags: [method private]
```

```Go
func globrunqget(_p_ *p, max int32) *g
```

Try get a batch of G's from the global runnable queue. sched.lock must be held. 

#### <a id="malg" href="#malg">func malg(stacksize int32) *g</a>

```
searchKey: runtime.malg
tags: [method private]
```

```Go
func malg(stacksize int32) *g
```

Allocate a new g, with a stack big enough for stacksize bytes. 

#### <a id="netpollunblock" href="#netpollunblock">func netpollunblock(pd *pollDesc, mode int32, ioready bool) *g</a>

```
searchKey: runtime.netpollunblock
tags: [method private]
```

```Go
func netpollunblock(pd *pollDesc, mode int32, ioready bool) *g
```

#### <a id="newproc1" href="#newproc1">func newproc1(fn *funcval, argp unsafe.Pointer, narg int32, callergp *g, callerpc uintptr) *g</a>

```
searchKey: runtime.newproc1
tags: [method private]
```

```Go
func newproc1(fn *funcval, argp unsafe.Pointer, narg int32, callergp *g, callerpc uintptr) *g
```

Create a new g in state _Grunnable, starting at fn, with narg bytes of arguments starting at argp. callerpc is the address of the go statement that created this. The caller is responsible for adding the new g to the scheduler. 

This must run on the system stack because it's the continuation of newproc, which cannot split the stack. 

#### <a id="runqget" href="#runqget">func runqget(_p_ *p) (gp *g, inheritTime bool)</a>

```
searchKey: runtime.runqget
tags: [method private]
```

```Go
func runqget(_p_ *p) (gp *g, inheritTime bool)
```

Get g from local runnable queue. If inheritTime is true, gp should inherit the remaining time in the current time slice. Otherwise, it should start a new time slice. Executed only by the owner P. 

#### <a id="runqsteal" href="#runqsteal">func runqsteal(_p_, p2 *p, stealRunNextG bool) *g</a>

```
searchKey: runtime.runqsteal
tags: [method private]
```

```Go
func runqsteal(_p_, p2 *p, stealRunNextG bool) *g
```

Steal half of elements from local runnable queue of p2 and put onto local runnable queue of p. Returns one of the stolen elements (or nil if failed). 

#### <a id="sigFetchG" href="#sigFetchG">func sigFetchG(c *sigctxt) *g</a>

```
searchKey: runtime.sigFetchG
tags: [method private]
```

```Go
func sigFetchG(c *sigctxt) *g
```

sigFetchG fetches the value of G safely when running in a signal handler. On some architectures, the g value may be clobbered when running in a VDSO. See issue #32912. 

#### <a id="stealWork" href="#stealWork">func stealWork(now int64) (gp *g, inheritTime bool, rnow, pollUntil int64, newWork bool)</a>

```
searchKey: runtime.stealWork
tags: [method private]
```

```Go
func stealWork(now int64) (gp *g, inheritTime bool, rnow, pollUntil int64, newWork bool)
```

stealWork attempts to steal a runnable goroutine or timer from any P. 

If newWork is true, new work may have been readied. 

If now is not 0 it is the current time. stealWork returns the passed time or the current time if now was passed as 0. 

#### <a id="traceReader" href="#traceReader">func traceReader() *g</a>

```
searchKey: runtime.traceReader
tags: [function private]
```

```Go
func traceReader() *g
```

traceReader returns the trace reader that should be woken up, if any. 

#### <a id="wakefing" href="#wakefing">func wakefing() *g</a>

```
searchKey: runtime.wakefing
tags: [function private]
```

```Go
func wakefing() *g
```

### <a id="LFNode" href="#LFNode">type LFNode struct</a>

```
searchKey: runtime.LFNode
tags: [struct private]
```

```Go
type LFNode struct {
	Next    uint64
	Pushcnt uintptr
}
```

#### <a id="LFStackPop" href="#LFStackPop">func LFStackPop(head *uint64) *LFNode</a>

```
searchKey: runtime.LFStackPop
tags: [method private]
```

```Go
func LFStackPop(head *uint64) *LFNode
```

### <a id="LockRank" href="#LockRank">type LockRank runtime.lockRank</a>

```
searchKey: runtime.LockRank
tags: [number private]
```

```Go
type LockRank lockRank
```

#### <a id="LockRank.String" href="#LockRank.String">func (l LockRank) String() string</a>

```
searchKey: runtime.LockRank.String
tags: [function private]
```

```Go
func (l LockRank) String() string
```

### <a id="M" href="#M">type M runtime.m</a>

```
searchKey: runtime.M
tags: [struct private]
```

```Go
type M = m
```

#### <a id="acquirem" href="#acquirem">func acquirem() *m</a>

```
searchKey: runtime.acquirem
tags: [function private]
```

```Go
func acquirem() *m
```

#### <a id="allocm" href="#allocm">func allocm(_p_ *p, fn func(), id int64) *m</a>

```
searchKey: runtime.allocm
tags: [method private]
```

```Go
func allocm(_p_ *p, fn func(), id int64) *m
```

Allocate a new m unassociated with any thread. Can use p for allocation context if needed. fn is recorded as the new m's m.mstartfn. id is optional pre-allocated m ID. Omit by passing -1. 

This function is allowed to have write barriers even if the caller isn't because it borrows _p_. 

#### <a id="lockextra" href="#lockextra">func lockextra(nilokay bool) *m</a>

```
searchKey: runtime.lockextra
tags: [method private]
```

```Go
func lockextra(nilokay bool) *m
```

lockextra locks the extra list and returns the list head. The caller must unlock the list by storing a new list head to extram. If nilokay is true, then lockextra will return a nil list head if that's what it finds. If nilokay is false, lockextra will keep waiting until the list head is no longer nil. 

#### <a id="mget" href="#mget">func mget() *m</a>

```
searchKey: runtime.mget
tags: [function private]
```

```Go
func mget() *m
```

Try to get an m from midle list. sched.lock must be held. May run during STW, so write barriers are not allowed. 

#### <a id="traceAcquireBuffer" href="#traceAcquireBuffer">func traceAcquireBuffer() (mp *m, pid int32, bufp *traceBufPtr)</a>

```
searchKey: runtime.traceAcquireBuffer
tags: [function private]
```

```Go
func traceAcquireBuffer() (mp *m, pid int32, bufp *traceBufPtr)
```

traceAcquireBuffer returns trace buffer to use and, if necessary, locks it. 

### <a id="MSpan" href="#MSpan">type MSpan runtime.mspan</a>

```
searchKey: runtime.MSpan
tags: [struct private]
```

```Go
type MSpan mspan
```

mspan wrapper for testing. 

#### <a id="AllocMSpan" href="#AllocMSpan">func AllocMSpan() *MSpan</a>

```
searchKey: runtime.AllocMSpan
tags: [function private]
```

```Go
func AllocMSpan() *MSpan
```

Allocate an mspan for testing. 

### <a id="MemProfileRecord" href="#MemProfileRecord">type MemProfileRecord struct</a>

```
searchKey: runtime.MemProfileRecord
tags: [struct]
```

```Go
type MemProfileRecord struct {
	AllocBytes, FreeBytes     int64       // number of bytes allocated, freed
	AllocObjects, FreeObjects int64       // number of objects allocated, freed
	Stack0                    [32]uintptr // stack trace for this record; ends at first 0 entry
}
```

A MemProfileRecord describes the live objects allocated by a particular call sequence (stack trace). 

#### <a id="MemProfileRecord.InUseBytes" href="#MemProfileRecord.InUseBytes">func (r *MemProfileRecord) InUseBytes() int64</a>

```
searchKey: runtime.MemProfileRecord.InUseBytes
tags: [function]
```

```Go
func (r *MemProfileRecord) InUseBytes() int64
```

InUseBytes returns the number of bytes in use (AllocBytes - FreeBytes). 

#### <a id="MemProfileRecord.InUseObjects" href="#MemProfileRecord.InUseObjects">func (r *MemProfileRecord) InUseObjects() int64</a>

```
searchKey: runtime.MemProfileRecord.InUseObjects
tags: [function]
```

```Go
func (r *MemProfileRecord) InUseObjects() int64
```

InUseObjects returns the number of objects in use (AllocObjects - FreeObjects). 

#### <a id="MemProfileRecord.Stack" href="#MemProfileRecord.Stack">func (r *MemProfileRecord) Stack() []uintptr</a>

```
searchKey: runtime.MemProfileRecord.Stack
tags: [function]
```

```Go
func (r *MemProfileRecord) Stack() []uintptr
```

Stack returns the stack trace associated with the record, a prefix of r.Stack0. 

### <a id="MemStats" href="#MemStats">type MemStats struct</a>

```
searchKey: runtime.MemStats
tags: [struct]
```

```Go
type MemStats struct {

	// Alloc is bytes of allocated heap objects.
	//
	// This is the same as HeapAlloc (see below).
	Alloc uint64

	// TotalAlloc is cumulative bytes allocated for heap objects.
	//
	// TotalAlloc increases as heap objects are allocated, but
	// unlike Alloc and HeapAlloc, it does not decrease when
	// objects are freed.
	TotalAlloc uint64

	// Sys is the total bytes of memory obtained from the OS.
	//
	// Sys is the sum of the XSys fields below. Sys measures the
	// virtual address space reserved by the Go runtime for the
	// heap, stacks, and other internal data structures. It's
	// likely that not all of the virtual address space is backed
	// by physical memory at any given moment, though in general
	// it all was at some point.
	Sys uint64

	// Lookups is the number of pointer lookups performed by the
	// runtime.
	//
	// This is primarily useful for debugging runtime internals.
	Lookups uint64

	// Mallocs is the cumulative count of heap objects allocated.
	// The number of live objects is Mallocs - Frees.
	Mallocs uint64

	// Frees is the cumulative count of heap objects freed.
	Frees uint64

	// HeapAlloc is bytes of allocated heap objects.
	//
	// "Allocated" heap objects include all reachable objects, as
	// well as unreachable objects that the garbage collector has
	// not yet freed. Specifically, HeapAlloc increases as heap
	// objects are allocated and decreases as the heap is swept
	// and unreachable objects are freed. Sweeping occurs
	// incrementally between GC cycles, so these two processes
	// occur simultaneously, and as a result HeapAlloc tends to
	// change smoothly (in contrast with the sawtooth that is
	// typical of stop-the-world garbage collectors).
	HeapAlloc uint64

	// HeapSys is bytes of heap memory obtained from the OS.
	//
	// HeapSys measures the amount of virtual address space
	// reserved for the heap. This includes virtual address space
	// that has been reserved but not yet used, which consumes no
	// physical memory, but tends to be small, as well as virtual
	// address space for which the physical memory has been
	// returned to the OS after it became unused (see HeapReleased
	// for a measure of the latter).
	//
	// HeapSys estimates the largest size the heap has had.
	HeapSys uint64

	// HeapIdle is bytes in idle (unused) spans.
	//
	// Idle spans have no objects in them. These spans could be
	// (and may already have been) returned to the OS, or they can
	// be reused for heap allocations, or they can be reused as
	// stack memory.
	//
	// HeapIdle minus HeapReleased estimates the amount of memory
	// that could be returned to the OS, but is being retained by
	// the runtime so it can grow the heap without requesting more
	// memory from the OS. If this difference is significantly
	// larger than the heap size, it indicates there was a recent
	// transient spike in live heap size.
	HeapIdle uint64

	// HeapInuse is bytes in in-use spans.
	//
	// In-use spans have at least one object in them. These spans
	// can only be used for other objects of roughly the same
	// size.
	//
	// HeapInuse minus HeapAlloc estimates the amount of memory
	// that has been dedicated to particular size classes, but is
	// not currently being used. This is an upper bound on
	// fragmentation, but in general this memory can be reused
	// efficiently.
	HeapInuse uint64

	// HeapReleased is bytes of physical memory returned to the OS.
	//
	// This counts heap memory from idle spans that was returned
	// to the OS and has not yet been reacquired for the heap.
	HeapReleased uint64

	// HeapObjects is the number of allocated heap objects.
	//
	// Like HeapAlloc, this increases as objects are allocated and
	// decreases as the heap is swept and unreachable objects are
	// freed.
	HeapObjects uint64

	// StackInuse is bytes in stack spans.
	//
	// In-use stack spans have at least one stack in them. These
	// spans can only be used for other stacks of the same size.
	//
	// There is no StackIdle because unused stack spans are
	// returned to the heap (and hence counted toward HeapIdle).
	StackInuse uint64

	// StackSys is bytes of stack memory obtained from the OS.
	//
	// StackSys is StackInuse, plus any memory obtained directly
	// from the OS for OS thread stacks (which should be minimal).
	StackSys uint64

	// MSpanInuse is bytes of allocated mspan structures.
	MSpanInuse uint64

	// MSpanSys is bytes of memory obtained from the OS for mspan
	// structures.
	MSpanSys uint64

	// MCacheInuse is bytes of allocated mcache structures.
	MCacheInuse uint64

	// MCacheSys is bytes of memory obtained from the OS for
	// mcache structures.
	MCacheSys uint64

	// BuckHashSys is bytes of memory in profiling bucket hash tables.
	BuckHashSys uint64

	// GCSys is bytes of memory in garbage collection metadata.
	GCSys uint64

	// OtherSys is bytes of memory in miscellaneous off-heap
	// runtime allocations.
	OtherSys uint64

	// NextGC is the target heap size of the next GC cycle.
	//
	// The garbage collector's goal is to keep HeapAlloc ≤ NextGC.
	// At the end of each GC cycle, the target for the next cycle
	// is computed based on the amount of reachable data and the
	// value of GOGC.
	NextGC uint64

	// LastGC is the time the last garbage collection finished, as
	// nanoseconds since 1970 (the UNIX epoch).
	LastGC uint64

	// PauseTotalNs is the cumulative nanoseconds in GC
	// stop-the-world pauses since the program started.
	//
	// During a stop-the-world pause, all goroutines are paused
	// and only the garbage collector can run.
	PauseTotalNs uint64

	// PauseNs is a circular buffer of recent GC stop-the-world
	// pause times in nanoseconds.
	//
	// The most recent pause is at PauseNs[(NumGC+255)%256]. In
	// general, PauseNs[N%256] records the time paused in the most
	// recent N%256th GC cycle. There may be multiple pauses per
	// GC cycle; this is the sum of all pauses during a cycle.
	PauseNs [256]uint64

	// PauseEnd is a circular buffer of recent GC pause end times,
	// as nanoseconds since 1970 (the UNIX epoch).
	//
	// This buffer is filled the same way as PauseNs. There may be
	// multiple pauses per GC cycle; this records the end of the
	// last pause in a cycle.
	PauseEnd [256]uint64

	// NumGC is the number of completed GC cycles.
	NumGC uint32

	// NumForcedGC is the number of GC cycles that were forced by
	// the application calling the GC function.
	NumForcedGC uint32

	// GCCPUFraction is the fraction of this program's available
	// CPU time used by the GC since the program started.
	//
	// GCCPUFraction is expressed as a number between 0 and 1,
	// where 0 means GC has consumed none of this program's CPU. A
	// program's available CPU time is defined as the integral of
	// GOMAXPROCS since the program started. That is, if
	// GOMAXPROCS is 2 and a program has been running for 10
	// seconds, its "available CPU" is 20 seconds. GCCPUFraction
	// does not include CPU time used for write barrier activity.
	//
	// This is the same as the fraction of CPU reported by
	// GODEBUG=gctrace=1.
	GCCPUFraction float64

	// EnableGC indicates that GC is enabled. It is always true,
	// even if GOGC=off.
	EnableGC bool

	// DebugGC is currently unused.
	DebugGC bool

	// BySize reports per-size class allocation statistics.
	//
	// BySize[N] gives statistics for allocations of size S where
	// BySize[N-1].Size < S ≤ BySize[N].Size.
	//
	// This does not report allocations larger than BySize[60].Size.
	BySize [61]struct {
		// Size is the maximum byte size of an object in this
		// size class.
		Size uint32

		// Mallocs is the cumulative count of heap objects
		// allocated in this size class. The cumulative bytes
		// of allocation is Size*Mallocs. The number of live
		// objects in this size class is Mallocs - Frees.
		Mallocs uint64

		// Frees is the cumulative count of heap objects freed
		// in this size class.
		Frees uint64
	}
}
```

A MemStats records statistics about the memory allocator. 

#### <a id="ReadMemStatsSlow" href="#ReadMemStatsSlow">func ReadMemStatsSlow() (base, slow MemStats)</a>

```
searchKey: runtime.ReadMemStatsSlow
tags: [function private]
```

```Go
func ReadMemStatsSlow() (base, slow MemStats)
```

ReadMemStatsSlow returns both the runtime-computed MemStats and MemStats accumulated by scanning the heap. 

### <a id="PageAlloc" href="#PageAlloc">type PageAlloc runtime.pageAlloc</a>

```
searchKey: runtime.PageAlloc
tags: [struct private]
```

```Go
type PageAlloc pageAlloc
```

Expose pageAlloc for testing. Note that because pageAlloc is not in the heap, so is PageAlloc. 

#### <a id="NewPageAlloc" href="#NewPageAlloc">func NewPageAlloc(chunks, scav map[ChunkIdx][]BitRange) *PageAlloc</a>

```
searchKey: runtime.NewPageAlloc
tags: [method private]
```

```Go
func NewPageAlloc(chunks, scav map[ChunkIdx][]BitRange) *PageAlloc
```

NewPageAlloc creates a new page allocator for testing and initializes it with the scav and chunks maps. Each key in these maps represents a chunk index and each value is a series of bit ranges to set within each bitmap's chunk. 

The initialization of the pageAlloc preserves the invariant that if a scavenged bit is set the alloc bit is necessarily unset, so some of the bits described by scav may be cleared in the final bitmap if ranges in chunks overlap with them. 

scav is optional, and if nil, the scavenged bitmap will be cleared (as opposed to all 1s, which it usually is). Furthermore, every chunk index in scav must appear in chunks; ones that do not are ignored. 

#### <a id="PageAlloc.Alloc" href="#PageAlloc.Alloc">func (p *PageAlloc) Alloc(npages uintptr) (uintptr, uintptr)</a>

```
searchKey: runtime.PageAlloc.Alloc
tags: [method private]
```

```Go
func (p *PageAlloc) Alloc(npages uintptr) (uintptr, uintptr)
```

#### <a id="PageAlloc.AllocToCache" href="#PageAlloc.AllocToCache">func (p *PageAlloc) AllocToCache() PageCache</a>

```
searchKey: runtime.PageAlloc.AllocToCache
tags: [function private]
```

```Go
func (p *PageAlloc) AllocToCache() PageCache
```

#### <a id="PageAlloc.Bounds" href="#PageAlloc.Bounds">func (p *PageAlloc) Bounds() (ChunkIdx, ChunkIdx)</a>

```
searchKey: runtime.PageAlloc.Bounds
tags: [function private]
```

```Go
func (p *PageAlloc) Bounds() (ChunkIdx, ChunkIdx)
```

#### <a id="PageAlloc.Free" href="#PageAlloc.Free">func (p *PageAlloc) Free(base, npages uintptr)</a>

```
searchKey: runtime.PageAlloc.Free
tags: [method private]
```

```Go
func (p *PageAlloc) Free(base, npages uintptr)
```

#### <a id="PageAlloc.InUse" href="#PageAlloc.InUse">func (p *PageAlloc) InUse() []AddrRange</a>

```
searchKey: runtime.PageAlloc.InUse
tags: [function private]
```

```Go
func (p *PageAlloc) InUse() []AddrRange
```

#### <a id="PageAlloc.PallocData" href="#PageAlloc.PallocData">func (p *PageAlloc) PallocData(i ChunkIdx) *PallocData</a>

```
searchKey: runtime.PageAlloc.PallocData
tags: [method private]
```

```Go
func (p *PageAlloc) PallocData(i ChunkIdx) *PallocData
```

Returns nil if the PallocData's L2 is missing. 

#### <a id="PageAlloc.Scavenge" href="#PageAlloc.Scavenge">func (p *PageAlloc) Scavenge(nbytes uintptr, mayUnlock bool) (r uintptr)</a>

```
searchKey: runtime.PageAlloc.Scavenge
tags: [method private]
```

```Go
func (p *PageAlloc) Scavenge(nbytes uintptr, mayUnlock bool) (r uintptr)
```

### <a id="PageCache" href="#PageCache">type PageCache runtime.pageCache</a>

```
searchKey: runtime.PageCache
tags: [struct private]
```

```Go
type PageCache pageCache
```

Expose pageCache for testing. 

#### <a id="NewPageCache" href="#NewPageCache">func NewPageCache(base uintptr, cache, scav uint64) PageCache</a>

```
searchKey: runtime.NewPageCache
tags: [method private]
```

```Go
func NewPageCache(base uintptr, cache, scav uint64) PageCache
```

#### <a id="PageCache.Alloc" href="#PageCache.Alloc">func (c *PageCache) Alloc(npages uintptr) (uintptr, uintptr)</a>

```
searchKey: runtime.PageCache.Alloc
tags: [method private]
```

```Go
func (c *PageCache) Alloc(npages uintptr) (uintptr, uintptr)
```

#### <a id="PageCache.Base" href="#PageCache.Base">func (c *PageCache) Base() uintptr</a>

```
searchKey: runtime.PageCache.Base
tags: [function private]
```

```Go
func (c *PageCache) Base() uintptr
```

#### <a id="PageCache.Cache" href="#PageCache.Cache">func (c *PageCache) Cache() uint64</a>

```
searchKey: runtime.PageCache.Cache
tags: [function private]
```

```Go
func (c *PageCache) Cache() uint64
```

#### <a id="PageCache.Empty" href="#PageCache.Empty">func (c *PageCache) Empty() bool</a>

```
searchKey: runtime.PageCache.Empty
tags: [function private]
```

```Go
func (c *PageCache) Empty() bool
```

#### <a id="PageCache.Flush" href="#PageCache.Flush">func (c *PageCache) Flush(s *PageAlloc)</a>

```
searchKey: runtime.PageCache.Flush
tags: [method private]
```

```Go
func (c *PageCache) Flush(s *PageAlloc)
```

#### <a id="PageCache.Scav" href="#PageCache.Scav">func (c *PageCache) Scav() uint64</a>

```
searchKey: runtime.PageCache.Scav
tags: [function private]
```

```Go
func (c *PageCache) Scav() uint64
```

### <a id="PallocBits" href="#PallocBits">type PallocBits runtime.pallocBits</a>

```
searchKey: runtime.PallocBits
tags: [array number private]
```

```Go
type PallocBits pallocBits
```

Expose pallocBits for testing. 

#### <a id="PallocBits.AllocRange" href="#PallocBits.AllocRange">func (b *PallocBits) AllocRange(i, n uint)</a>

```
searchKey: runtime.PallocBits.AllocRange
tags: [method private]
```

```Go
func (b *PallocBits) AllocRange(i, n uint)
```

#### <a id="PallocBits.Find" href="#PallocBits.Find">func (b *PallocBits) Find(npages uintptr, searchIdx uint) (uint, uint)</a>

```
searchKey: runtime.PallocBits.Find
tags: [method private]
```

```Go
func (b *PallocBits) Find(npages uintptr, searchIdx uint) (uint, uint)
```

#### <a id="PallocBits.Free" href="#PallocBits.Free">func (b *PallocBits) Free(i, n uint)</a>

```
searchKey: runtime.PallocBits.Free
tags: [method private]
```

```Go
func (b *PallocBits) Free(i, n uint)
```

#### <a id="PallocBits.PopcntRange" href="#PallocBits.PopcntRange">func (b *PallocBits) PopcntRange(i, n uint) uint</a>

```
searchKey: runtime.PallocBits.PopcntRange
tags: [method private]
```

```Go
func (b *PallocBits) PopcntRange(i, n uint) uint
```

#### <a id="PallocBits.Summarize" href="#PallocBits.Summarize">func (b *PallocBits) Summarize() PallocSum</a>

```
searchKey: runtime.PallocBits.Summarize
tags: [function private]
```

```Go
func (b *PallocBits) Summarize() PallocSum
```

### <a id="PallocData" href="#PallocData">type PallocData runtime.pallocData</a>

```
searchKey: runtime.PallocData
tags: [struct private]
```

```Go
type PallocData pallocData
```

Expose pallocData for testing. 

#### <a id="PallocData.AllocRange" href="#PallocData.AllocRange">func (d *PallocData) AllocRange(i, n uint)</a>

```
searchKey: runtime.PallocData.AllocRange
tags: [method private]
```

```Go
func (d *PallocData) AllocRange(i, n uint)
```

#### <a id="PallocData.FindScavengeCandidate" href="#PallocData.FindScavengeCandidate">func (d *PallocData) FindScavengeCandidate(searchIdx uint, min, max uintptr) (uint, uint)</a>

```
searchKey: runtime.PallocData.FindScavengeCandidate
tags: [method private]
```

```Go
func (d *PallocData) FindScavengeCandidate(searchIdx uint, min, max uintptr) (uint, uint)
```

#### <a id="PallocData.PallocBits" href="#PallocData.PallocBits">func (d *PallocData) PallocBits() *PallocBits</a>

```
searchKey: runtime.PallocData.PallocBits
tags: [function private]
```

```Go
func (d *PallocData) PallocBits() *PallocBits
```

#### <a id="PallocData.Scavenged" href="#PallocData.Scavenged">func (d *PallocData) Scavenged() *PallocBits</a>

```
searchKey: runtime.PallocData.Scavenged
tags: [function private]
```

```Go
func (d *PallocData) Scavenged() *PallocBits
```

#### <a id="PallocData.ScavengedSetRange" href="#PallocData.ScavengedSetRange">func (d *PallocData) ScavengedSetRange(i, n uint)</a>

```
searchKey: runtime.PallocData.ScavengedSetRange
tags: [method private]
```

```Go
func (d *PallocData) ScavengedSetRange(i, n uint)
```

### <a id="PallocSum" href="#PallocSum">type PallocSum runtime.pallocSum</a>

```
searchKey: runtime.PallocSum
tags: [number private]
```

```Go
type PallocSum pallocSum
```

Expose pallocSum for testing. 

#### <a id="PackPallocSum" href="#PackPallocSum">func PackPallocSum(start, max, end uint) PallocSum</a>

```
searchKey: runtime.PackPallocSum
tags: [method private]
```

```Go
func PackPallocSum(start, max, end uint) PallocSum
```

#### <a id="SummarizeSlow" href="#SummarizeSlow">func SummarizeSlow(b *PallocBits) PallocSum</a>

```
searchKey: runtime.SummarizeSlow
tags: [method private]
```

```Go
func SummarizeSlow(b *PallocBits) PallocSum
```

SummarizeSlow is a slow but more obviously correct implementation of (*pallocBits).summarize. Used for testing. 

#### <a id="PallocSum.End" href="#PallocSum.End">func (m PallocSum) End() uint</a>

```
searchKey: runtime.PallocSum.End
tags: [function private]
```

```Go
func (m PallocSum) End() uint
```

#### <a id="PallocSum.Max" href="#PallocSum.Max">func (m PallocSum) Max() uint</a>

```
searchKey: runtime.PallocSum.Max
tags: [function private]
```

```Go
func (m PallocSum) Max() uint
```

#### <a id="PallocSum.Start" href="#PallocSum.Start">func (m PallocSum) Start() uint</a>

```
searchKey: runtime.PallocSum.Start
tags: [function private]
```

```Go
func (m PallocSum) Start() uint
```

### <a id="ProfBuf" href="#ProfBuf">type ProfBuf runtime.profBuf</a>

```
searchKey: runtime.ProfBuf
tags: [struct private]
```

```Go
type ProfBuf profBuf
```

#### <a id="NewProfBuf" href="#NewProfBuf">func NewProfBuf(hdrsize, bufwords, tags int) *ProfBuf</a>

```
searchKey: runtime.NewProfBuf
tags: [method private]
```

```Go
func NewProfBuf(hdrsize, bufwords, tags int) *ProfBuf
```

#### <a id="ProfBuf.Close" href="#ProfBuf.Close">func (p *ProfBuf) Close()</a>

```
searchKey: runtime.ProfBuf.Close
tags: [function private]
```

```Go
func (p *ProfBuf) Close()
```

#### <a id="ProfBuf.Read" href="#ProfBuf.Read">func (p *ProfBuf) Read(mode profBufReadMode) ([]uint64, []unsafe.Pointer, bool)</a>

```
searchKey: runtime.ProfBuf.Read
tags: [method private]
```

```Go
func (p *ProfBuf) Read(mode profBufReadMode) ([]uint64, []unsafe.Pointer, bool)
```

#### <a id="ProfBuf.Write" href="#ProfBuf.Write">func (p *ProfBuf) Write(tag *unsafe.Pointer, now int64, hdr []uint64, stk []uintptr)</a>

```
searchKey: runtime.ProfBuf.Write
tags: [method private]
```

```Go
func (p *ProfBuf) Write(tag *unsafe.Pointer, now int64, hdr []uint64, stk []uintptr)
```

### <a id="RWMutex" href="#RWMutex">type RWMutex struct</a>

```
searchKey: runtime.RWMutex
tags: [struct private]
```

```Go
type RWMutex struct {
	rw rwmutex
}
```

#### <a id="RWMutex.Lock" href="#RWMutex.Lock">func (rw *RWMutex) Lock()</a>

```
searchKey: runtime.RWMutex.Lock
tags: [function private]
```

```Go
func (rw *RWMutex) Lock()
```

#### <a id="RWMutex.RLock" href="#RWMutex.RLock">func (rw *RWMutex) RLock()</a>

```
searchKey: runtime.RWMutex.RLock
tags: [function private]
```

```Go
func (rw *RWMutex) RLock()
```

#### <a id="RWMutex.RUnlock" href="#RWMutex.RUnlock">func (rw *RWMutex) RUnlock()</a>

```
searchKey: runtime.RWMutex.RUnlock
tags: [function private]
```

```Go
func (rw *RWMutex) RUnlock()
```

#### <a id="RWMutex.Unlock" href="#RWMutex.Unlock">func (rw *RWMutex) Unlock()</a>

```
searchKey: runtime.RWMutex.Unlock
tags: [function private]
```

```Go
func (rw *RWMutex) Unlock()
```

### <a id="StackRecord" href="#StackRecord">type StackRecord struct</a>

```
searchKey: runtime.StackRecord
tags: [struct]
```

```Go
type StackRecord struct {
	Stack0 [32]uintptr // stack trace for this record; ends at first 0 entry
}
```

A StackRecord describes a single execution stack. 

#### <a id="StackRecord.Stack" href="#StackRecord.Stack">func (r *StackRecord) Stack() []uintptr</a>

```
searchKey: runtime.StackRecord.Stack
tags: [function]
```

```Go
func (r *StackRecord) Stack() []uintptr
```

Stack returns the stack trace associated with the record, a prefix of r.Stack0. 

### <a id="Sudog" href="#Sudog">type Sudog runtime.sudog</a>

```
searchKey: runtime.Sudog
tags: [struct private]
```

```Go
type Sudog = sudog
```

#### <a id="acquireSudog" href="#acquireSudog">func acquireSudog() *sudog</a>

```
searchKey: runtime.acquireSudog
tags: [function private]
```

```Go
func acquireSudog() *sudog
```

### <a id="TimeHistogram" href="#TimeHistogram">type TimeHistogram runtime.timeHistogram</a>

```
searchKey: runtime.TimeHistogram
tags: [struct private]
```

```Go
type TimeHistogram timeHistogram
```

#### <a id="TimeHistogram.Count" href="#TimeHistogram.Count">func (th *TimeHistogram) Count(bucket, subBucket uint) (uint64, bool)</a>

```
searchKey: runtime.TimeHistogram.Count
tags: [method private]
```

```Go
func (th *TimeHistogram) Count(bucket, subBucket uint) (uint64, bool)
```

Counts returns the counts for the given bucket, subBucket indices. Returns true if the bucket was valid, otherwise returns the counts for the underflow bucket and false. 

#### <a id="TimeHistogram.Record" href="#TimeHistogram.Record">func (th *TimeHistogram) Record(duration int64)</a>

```
searchKey: runtime.TimeHistogram.Record
tags: [method private]
```

```Go
func (th *TimeHistogram) Record(duration int64)
```

### <a id="TypeAssertionError" href="#TypeAssertionError">type TypeAssertionError struct</a>

```
searchKey: runtime.TypeAssertionError
tags: [struct]
```

```Go
type TypeAssertionError struct {
	_interface    *_type
	concrete      *_type
	asserted      *_type
	missingMethod string // one method needed by Interface, missing from Concrete
}
```

A TypeAssertionError explains a failed type assertion. 

#### <a id="TypeAssertionError.Error" href="#TypeAssertionError.Error">func (e *TypeAssertionError) Error() string</a>

```
searchKey: runtime.TypeAssertionError.Error
tags: [function]
```

```Go
func (e *TypeAssertionError) Error() string
```

#### <a id="TypeAssertionError.RuntimeError" href="#TypeAssertionError.RuntimeError">func (*TypeAssertionError) RuntimeError()</a>

```
searchKey: runtime.TypeAssertionError.RuntimeError
tags: [function]
```

```Go
func (*TypeAssertionError) RuntimeError()
```

### <a id="addrRange" href="#addrRange">type addrRange struct</a>

```
searchKey: runtime.addrRange
tags: [struct private]
```

```Go
type addrRange struct {
	// base and limit together represent the region of address space
	// [base, limit). That is, base is inclusive, limit is exclusive.
	// These are address over an offset view of the address space on
	// platforms with a segmented address space, that is, on platforms
	// where arenaBaseOffset != 0.
	base, limit offAddr
}
```

addrRange represents a region of address space. 

An addrRange must never span a gap in the address space. 

#### <a id="makeAddrRange" href="#makeAddrRange">func makeAddrRange(base, limit uintptr) addrRange</a>

```
searchKey: runtime.makeAddrRange
tags: [method private]
```

```Go
func makeAddrRange(base, limit uintptr) addrRange
```

makeAddrRange creates a new address range from two virtual addresses. 

Throws if the base and limit are not in the same memory segment. 

#### <a id="addrRange.contains" href="#addrRange.contains">func (a addrRange) contains(addr uintptr) bool</a>

```
searchKey: runtime.addrRange.contains
tags: [method private]
```

```Go
func (a addrRange) contains(addr uintptr) bool
```

contains returns whether or not the range contains a given address. 

#### <a id="addrRange.removeGreaterEqual" href="#addrRange.removeGreaterEqual">func (a addrRange) removeGreaterEqual(addr uintptr) addrRange</a>

```
searchKey: runtime.addrRange.removeGreaterEqual
tags: [method private]
```

```Go
func (a addrRange) removeGreaterEqual(addr uintptr) addrRange
```

removeGreaterEqual removes all addresses in a greater than or equal to addr and returns the new range. 

#### <a id="addrRange.size" href="#addrRange.size">func (a addrRange) size() uintptr</a>

```
searchKey: runtime.addrRange.size
tags: [function private]
```

```Go
func (a addrRange) size() uintptr
```

size returns the size of the range represented in bytes. 

#### <a id="addrRange.subtract" href="#addrRange.subtract">func (a addrRange) subtract(b addrRange) addrRange</a>

```
searchKey: runtime.addrRange.subtract
tags: [method private]
```

```Go
func (a addrRange) subtract(b addrRange) addrRange
```

subtract takes the addrRange toPrune and cuts out any overlap with from, then returns the new range. subtract assumes that a and b either don't overlap at all, only overlap on one side, or are equal. If b is strictly contained in a, thus forcing a split, it will throw. 

### <a id="addrRanges" href="#addrRanges">type addrRanges struct</a>

```
searchKey: runtime.addrRanges
tags: [struct private]
```

```Go
type addrRanges struct {
	// ranges is a slice of ranges sorted by base.
	ranges []addrRange

	// totalBytes is the total amount of address space in bytes counted by
	// this addrRanges.
	totalBytes uintptr

	// sysStat is the stat to track allocations by this type
	sysStat *sysMemStat
}
```

addrRanges is a data structure holding a collection of ranges of address space. 

The ranges are coalesced eagerly to reduce the number ranges it holds. 

The slice backing store for this field is persistentalloc'd and thus there is no way to free it. 

addrRanges is not thread-safe. 

#### <a id="addrRanges.add" href="#addrRanges.add">func (a *addrRanges) add(r addrRange)</a>

```
searchKey: runtime.addrRanges.add
tags: [method private]
```

```Go
func (a *addrRanges) add(r addrRange)
```

add inserts a new address range to a. 

r must not overlap with any address range in a and r.size() must be > 0. 

#### <a id="addrRanges.cloneInto" href="#addrRanges.cloneInto">func (a *addrRanges) cloneInto(b *addrRanges)</a>

```
searchKey: runtime.addrRanges.cloneInto
tags: [method private]
```

```Go
func (a *addrRanges) cloneInto(b *addrRanges)
```

cloneInto makes a deep clone of a's state into b, re-using b's ranges if able. 

#### <a id="addrRanges.contains" href="#addrRanges.contains">func (a *addrRanges) contains(addr uintptr) bool</a>

```
searchKey: runtime.addrRanges.contains
tags: [method private]
```

```Go
func (a *addrRanges) contains(addr uintptr) bool
```

contains returns true if a covers the address addr. 

#### <a id="addrRanges.findAddrGreaterEqual" href="#addrRanges.findAddrGreaterEqual">func (a *addrRanges) findAddrGreaterEqual(addr uintptr) (uintptr, bool)</a>

```
searchKey: runtime.addrRanges.findAddrGreaterEqual
tags: [method private]
```

```Go
func (a *addrRanges) findAddrGreaterEqual(addr uintptr) (uintptr, bool)
```

findAddrGreaterEqual returns the smallest address represented by a that is >= addr. Thus, if the address is represented by a, then it returns addr. The second return value indicates whether such an address exists for addr in a. That is, if addr is larger than any address known to a, the second return value will be false. 

#### <a id="addrRanges.findSucc" href="#addrRanges.findSucc">func (a *addrRanges) findSucc(addr uintptr) int</a>

```
searchKey: runtime.addrRanges.findSucc
tags: [method private]
```

```Go
func (a *addrRanges) findSucc(addr uintptr) int
```

findSucc returns the first index in a such that addr is less than the base of the addrRange at that index. 

#### <a id="addrRanges.init.mranges.go" href="#addrRanges.init.mranges.go">func (a *addrRanges) init(sysStat *sysMemStat)</a>

```
searchKey: runtime.addrRanges.init
tags: [method private]
```

```Go
func (a *addrRanges) init(sysStat *sysMemStat)
```

#### <a id="addrRanges.removeGreaterEqual" href="#addrRanges.removeGreaterEqual">func (a *addrRanges) removeGreaterEqual(addr uintptr)</a>

```
searchKey: runtime.addrRanges.removeGreaterEqual
tags: [method private]
```

```Go
func (a *addrRanges) removeGreaterEqual(addr uintptr)
```

removeGreaterEqual removes the ranges of a which are above addr, and additionally splits any range containing addr. 

#### <a id="addrRanges.removeLast" href="#addrRanges.removeLast">func (a *addrRanges) removeLast(nBytes uintptr) addrRange</a>

```
searchKey: runtime.addrRanges.removeLast
tags: [method private]
```

```Go
func (a *addrRanges) removeLast(nBytes uintptr) addrRange
```

removeLast removes and returns the highest-addressed contiguous range of a, or the last nBytes of that range, whichever is smaller. If a is empty, it returns an empty range. 

### <a id="adjustinfo" href="#adjustinfo">type adjustinfo struct</a>

```
searchKey: runtime.adjustinfo
tags: [struct private]
```

```Go
type adjustinfo struct {
	old   stack
	delta uintptr // ptr distance from old to new stack (newbase - oldbase)
	cache pcvalueCache

	// sghi is the highest sudog.elem on the stack.
	sghi uintptr
}
```

### <a id="ancestorInfo" href="#ancestorInfo">type ancestorInfo struct</a>

```
searchKey: runtime.ancestorInfo
tags: [struct private]
```

```Go
type ancestorInfo struct {
	pcs  []uintptr // pcs from the stack of this goroutine
	goid int64     // goroutine id of this goroutine; original goroutine possibly dead
	gopc uintptr   // pc of go statement that created this goroutine
}
```

ancestorInfo records details of where a goroutine was started. 

### <a id="arenaHint" href="#arenaHint">type arenaHint struct</a>

```
searchKey: runtime.arenaHint
tags: [struct private]
```

```Go
type arenaHint struct {
	addr uintptr
	down bool
	next *arenaHint
}
```

arenaHint is a hint for where to grow the heap arenas. See mheap_.arenaHints. 

### <a id="arenaIdx" href="#arenaIdx">type arenaIdx uint</a>

```
searchKey: runtime.arenaIdx
tags: [number private]
```

```Go
type arenaIdx uint
```

#### <a id="arenaIndex" href="#arenaIndex">func arenaIndex(p uintptr) arenaIdx</a>

```
searchKey: runtime.arenaIndex
tags: [method private]
```

```Go
func arenaIndex(p uintptr) arenaIdx
```

arenaIndex returns the index into mheap_.arenas of the arena containing metadata for p. This index combines of an index into the L1 map and an index into the L2 map and should be used as mheap_.arenas[ai.l1()][ai.l2()]. 

If p is outside the range of valid heap addresses, either l1() or l2() will be out of bounds. 

It is nosplit because it's called by spanOf and several other nosplit functions. 

#### <a id="arenaIdx.l1" href="#arenaIdx.l1">func (i arenaIdx) l1() uint</a>

```
searchKey: runtime.arenaIdx.l1
tags: [function private]
```

```Go
func (i arenaIdx) l1() uint
```

#### <a id="arenaIdx.l2" href="#arenaIdx.l2">func (i arenaIdx) l2() uint</a>

```
searchKey: runtime.arenaIdx.l2
tags: [function private]
```

```Go
func (i arenaIdx) l2() uint
```

### <a id="argset" href="#argset">type argset struct</a>

```
searchKey: runtime.argset
tags: [struct private]
```

```Go
type argset struct {
	args   unsafe.Pointer
	retval uintptr
}
```

argset matches runtime/cgo/linux_syscall.c:argset_t 

### <a id="arraytype" href="#arraytype">type arraytype struct</a>

```
searchKey: runtime.arraytype
tags: [struct private]
```

```Go
type arraytype struct {
	typ   _type
	elem  *_type
	slice *_type
	len   uintptr
}
```

### <a id="bitvector" href="#bitvector">type bitvector struct</a>

```
searchKey: runtime.bitvector
tags: [struct private]
```

```Go
type bitvector struct {
	n        int32 // # of bits
	bytedata *uint8
}
```

Information from the compiler about the layout of stack frames. Note: this type must agree with reflect.bitVector. 

#### <a id="getArgInfo" href="#getArgInfo">func getArgInfo(frame *stkframe, f funcInfo, needArgMap bool, ctxt *funcval) (arglen uintptr, argmap *bitvector)</a>

```
searchKey: runtime.getArgInfo
tags: [method private]
```

```Go
func getArgInfo(frame *stkframe, f funcInfo, needArgMap bool, ctxt *funcval) (arglen uintptr, argmap *bitvector)
```

getArgInfo returns the argument frame information for a call to f with call frame frame. 

This is used for both actual calls with active stack frames and for deferred calls or goroutines that are not yet executing. If this is an actual call, ctxt must be nil (getArgInfo will retrieve what it needs from the active stack frame). If this is a deferred call or unstarted goroutine, ctxt must be the function object that was deferred or go'd. 

#### <a id="getArgInfoFast" href="#getArgInfoFast">func getArgInfoFast(f funcInfo, needArgMap bool) (arglen uintptr, argmap *bitvector, ok bool)</a>

```
searchKey: runtime.getArgInfoFast
tags: [method private]
```

```Go
func getArgInfoFast(f funcInfo, needArgMap bool) (arglen uintptr, argmap *bitvector, ok bool)
```

getArgInfoFast returns the argument frame information for a call to f. It is short and inlineable. However, it does not handle all functions. If ok reports false, you must call getArgInfo instead. TODO(josharian): once we do mid-stack inlining, call getArgInfo directly from getArgInfoFast and stop returning an ok bool. 

#### <a id="getStackMap" href="#getStackMap">func getStackMap(frame *stkframe, cache *pcvalueCache, debug bool) (locals, args bitvector, objs []stackObjectRecord)</a>

```
searchKey: runtime.getStackMap
tags: [method private]
```

```Go
func getStackMap(frame *stkframe, cache *pcvalueCache, debug bool) (locals, args bitvector, objs []stackObjectRecord)
```

getStackMap returns the locals and arguments live pointer maps, and stack object list for frame. 

#### <a id="makeheapobjbv" href="#makeheapobjbv">func makeheapobjbv(p uintptr, size uintptr) bitvector</a>

```
searchKey: runtime.makeheapobjbv
tags: [method private]
```

```Go
func makeheapobjbv(p uintptr, size uintptr) bitvector
```

#### <a id="progToPointerMask" href="#progToPointerMask">func progToPointerMask(prog *byte, size uintptr) bitvector</a>

```
searchKey: runtime.progToPointerMask
tags: [method private]
```

```Go
func progToPointerMask(prog *byte, size uintptr) bitvector
```

progToPointerMask returns the 1-bit pointer mask output by the GC program prog. size the size of the region described by prog, in bytes. The resulting bitvector will have no more than size/sys.PtrSize bits. 

#### <a id="stackmapdata" href="#stackmapdata">func stackmapdata(stkmap *stackmap, n int32) bitvector</a>

```
searchKey: runtime.stackmapdata
tags: [method private]
```

```Go
func stackmapdata(stkmap *stackmap, n int32) bitvector
```

#### <a id="bitvector.ptrbit" href="#bitvector.ptrbit">func (bv *bitvector) ptrbit(i uintptr) uint8</a>

```
searchKey: runtime.bitvector.ptrbit
tags: [method private]
```

```Go
func (bv *bitvector) ptrbit(i uintptr) uint8
```

ptrbit returns the i'th bit in bv. ptrbit is less efficient than iterating directly over bitvector bits, and should only be used in non-performance-critical code. See adjustpointers for an example of a high-efficiency walk of a bitvector. 

### <a id="blockRecord" href="#blockRecord">type blockRecord struct</a>

```
searchKey: runtime.blockRecord
tags: [struct private]
```

```Go
type blockRecord struct {
	count  float64
	cycles int64
}
```

A blockRecord is the bucket data for a bucket of type blockProfile, which is used in blocking and mutex profiles. 

### <a id="bmap" href="#bmap">type bmap struct</a>

```
searchKey: runtime.bmap
tags: [struct private]
```

```Go
type bmap struct {
	// tophash generally contains the top byte of the hash value
	// for each key in this bucket. If tophash[0] < minTopHash,
	// tophash[0] is a bucket evacuation state instead.
	tophash [bucketCnt]uint8
}
```

A bucket for a Go map. 

#### <a id="makeBucketArray" href="#makeBucketArray">func makeBucketArray(t *maptype, b uint8, dirtyalloc unsafe.Pointer) (buckets unsafe.Pointer, nextOverflow *bmap)</a>

```
searchKey: runtime.makeBucketArray
tags: [method private]
```

```Go
func makeBucketArray(t *maptype, b uint8, dirtyalloc unsafe.Pointer) (buckets unsafe.Pointer, nextOverflow *bmap)
```

makeBucketArray initializes a backing array for map buckets. 1<<b is the minimum number of buckets to allocate. dirtyalloc should either be nil or a bucket array previously allocated by makeBucketArray with the same t and b parameters. If dirtyalloc is nil a new backing array will be alloced and otherwise dirtyalloc will be cleared and reused as backing array. 

#### <a id="bmap.keys" href="#bmap.keys">func (b *bmap) keys() unsafe.Pointer</a>

```
searchKey: runtime.bmap.keys
tags: [function private]
```

```Go
func (b *bmap) keys() unsafe.Pointer
```

#### <a id="bmap.overflow" href="#bmap.overflow">func (b *bmap) overflow(t *maptype) *bmap</a>

```
searchKey: runtime.bmap.overflow
tags: [method private]
```

```Go
func (b *bmap) overflow(t *maptype) *bmap
```

#### <a id="bmap.setoverflow" href="#bmap.setoverflow">func (b *bmap) setoverflow(t *maptype, ovf *bmap)</a>

```
searchKey: runtime.bmap.setoverflow
tags: [method private]
```

```Go
func (b *bmap) setoverflow(t *maptype, ovf *bmap)
```

### <a id="boundsError" href="#boundsError">type boundsError struct</a>

```
searchKey: runtime.boundsError
tags: [struct private]
```

```Go
type boundsError struct {
	x int64
	y int
	// Values in an index or slice expression can be signed or unsigned.
	// That means we'd need 65 bits to encode all possible indexes, from -2^63 to 2^64-1.
	// Instead, we keep track of whether x should be interpreted as signed or unsigned.
	// y is known to be nonnegative and to fit in an int.
	signed bool
	code   boundsErrorCode
}
```

A boundsError represents an indexing or slicing operation gone wrong. 

#### <a id="boundsError.Error" href="#boundsError.Error">func (e boundsError) Error() string</a>

```
searchKey: runtime.boundsError.Error
tags: [function private]
```

```Go
func (e boundsError) Error() string
```

#### <a id="boundsError.RuntimeError" href="#boundsError.RuntimeError">func (e boundsError) RuntimeError()</a>

```
searchKey: runtime.boundsError.RuntimeError
tags: [function private]
```

```Go
func (e boundsError) RuntimeError()
```

### <a id="boundsErrorCode" href="#boundsErrorCode">type boundsErrorCode uint8</a>

```
searchKey: runtime.boundsErrorCode
tags: [number private]
```

```Go
type boundsErrorCode uint8
```

### <a id="bucket" href="#bucket">type bucket struct</a>

```
searchKey: runtime.bucket
tags: [struct private]
```

```Go
type bucket struct {
	next    *bucket
	allnext *bucket
	typ     bucketType // memBucket or blockBucket (includes mutexProfile)
	hash    uintptr
	size    uintptr
	nstk    uintptr
}
```

A bucket holds per-call-stack profiling information. The representation is a bit sleazy, inherited from C. This struct defines the bucket header. It is followed in memory by the stack words and then the actual record data, either a memRecord or a blockRecord. 

Per-call-stack profiling information. Lookup by hashing call stack into a linked-list hash table. 

No heap pointers. 

#### <a id="newBucket" href="#newBucket">func newBucket(typ bucketType, nstk int) *bucket</a>

```
searchKey: runtime.newBucket
tags: [method private]
```

```Go
func newBucket(typ bucketType, nstk int) *bucket
```

newBucket allocates a bucket with the given type and number of stack entries. 

#### <a id="stkbucket" href="#stkbucket">func stkbucket(typ bucketType, size uintptr, stk []uintptr, alloc bool) *bucket</a>

```
searchKey: runtime.stkbucket
tags: [method private]
```

```Go
func stkbucket(typ bucketType, size uintptr, stk []uintptr, alloc bool) *bucket
```

Return the bucket for stk[0:nstk], allocating new bucket if needed. 

#### <a id="bucket.bp" href="#bucket.bp">func (b *bucket) bp() *blockRecord</a>

```
searchKey: runtime.bucket.bp
tags: [function private]
```

```Go
func (b *bucket) bp() *blockRecord
```

bp returns the blockRecord associated with the blockProfile bucket b. 

#### <a id="bucket.mp" href="#bucket.mp">func (b *bucket) mp() *memRecord</a>

```
searchKey: runtime.bucket.mp
tags: [function private]
```

```Go
func (b *bucket) mp() *memRecord
```

mp returns the memRecord associated with the memProfile bucket b. 

#### <a id="bucket.stk" href="#bucket.stk">func (b *bucket) stk() []uintptr</a>

```
searchKey: runtime.bucket.stk
tags: [function private]
```

```Go
func (b *bucket) stk() []uintptr
```

stk returns the slice in b holding the stack. 

### <a id="bucketType" href="#bucketType">type bucketType int</a>

```
searchKey: runtime.bucketType
tags: [number private]
```

```Go
type bucketType int
```

### <a id="cgoCallers" href="#cgoCallers">type cgoCallers [32]uintptr</a>

```
searchKey: runtime.cgoCallers
tags: [array number private]
```

```Go
type cgoCallers [32]uintptr
```

Addresses collected in a cgo backtrace when crashing. Length must match arg.Max in x_cgo_callers in runtime/cgo/gcc_traceback.c. 

### <a id="cgoContextArg" href="#cgoContextArg">type cgoContextArg struct</a>

```
searchKey: runtime.cgoContextArg
tags: [struct private]
```

```Go
type cgoContextArg struct {
	context uintptr
}
```

cgoContextArg is the type passed to the context function. 

### <a id="cgoSymbolizerArg" href="#cgoSymbolizerArg">type cgoSymbolizerArg struct</a>

```
searchKey: runtime.cgoSymbolizerArg
tags: [struct private]
```

```Go
type cgoSymbolizerArg struct {
	pc       uintptr
	file     *byte
	lineno   uintptr
	funcName *byte
	entry    uintptr
	more     uintptr
	data     uintptr
}
```

cgoSymbolizerArg is the type passed to cgoSymbolizer. 

### <a id="cgoTracebackArg" href="#cgoTracebackArg">type cgoTracebackArg struct</a>

```
searchKey: runtime.cgoTracebackArg
tags: [struct private]
```

```Go
type cgoTracebackArg struct {
	context    uintptr
	sigContext uintptr
	buf        *uintptr
	max        uintptr
}
```

cgoTracebackArg is the type passed to cgoTraceback. 

### <a id="cgothreadstart" href="#cgothreadstart">type cgothreadstart struct</a>

```
searchKey: runtime.cgothreadstart
tags: [struct private]
```

```Go
type cgothreadstart struct {
	g   guintptr
	tls *uint64
	fn  unsafe.Pointer
}
```

### <a id="chantype" href="#chantype">type chantype struct</a>

```
searchKey: runtime.chantype
tags: [struct private]
```

```Go
type chantype struct {
	typ  _type
	elem *_type
	dir  uintptr
}
```

### <a id="checkmarksMap" href="#checkmarksMap">type checkmarksMap [1048576]uint8</a>

```
searchKey: runtime.checkmarksMap
tags: [array number private]
```

```Go
type checkmarksMap [heapArenaBytes / sys.PtrSize / 8]uint8
```

A checkmarksMap stores the GC marks in "checkmarks" mode. It is a per-arena bitmap with a bit for every word in the arena. The mark is stored on the bit corresponding to the first word of the marked allocation. 

### <a id="childInfo" href="#childInfo">type childInfo struct</a>

```
searchKey: runtime.childInfo
tags: [struct private]
```

```Go
type childInfo struct {
	// Information passed up from the callee frame about
	// the layout of the outargs region.
	argoff uintptr   // where the arguments start in the frame
	arglen uintptr   // size of args region
	args   bitvector // if args.n >= 0, pointer map of args region
	sp     *uint8    // callee sp
	depth  uintptr   // depth in call stack (0 == most recent)
}
```

### <a id="chunkIdx" href="#chunkIdx">type chunkIdx uint</a>

```
searchKey: runtime.chunkIdx
tags: [number private]
```

```Go
type chunkIdx uint
```

Global chunk index. 

Represents an index into the leaf level of the radix tree. Similar to arenaIndex, except instead of arenas, it divides the address space into chunks. 

#### <a id="chunkIndex" href="#chunkIndex">func chunkIndex(p uintptr) chunkIdx</a>

```
searchKey: runtime.chunkIndex
tags: [method private]
```

```Go
func chunkIndex(p uintptr) chunkIdx
```

chunkIndex returns the global index of the palloc chunk containing the pointer p. 

#### <a id="chunkIdx.l1" href="#chunkIdx.l1">func (i chunkIdx) l1() uint</a>

```
searchKey: runtime.chunkIdx.l1
tags: [function private]
```

```Go
func (i chunkIdx) l1() uint
```

l1 returns the index into the first level of (*pageAlloc).chunks. 

#### <a id="chunkIdx.l2" href="#chunkIdx.l2">func (i chunkIdx) l2() uint</a>

```
searchKey: runtime.chunkIdx.l2
tags: [function private]
```

```Go
func (i chunkIdx) l2() uint
```

l2 returns the index into the second level of (*pageAlloc).chunks. 

### <a id="consistentHeapStats" href="#consistentHeapStats">type consistentHeapStats struct</a>

```
searchKey: runtime.consistentHeapStats
tags: [struct private]
```

```Go
type consistentHeapStats struct {
	// stats is a ring buffer of heapStatsDelta values.
	// Writers always atomically update the delta at index gen.
	//
	// Readers operate by rotating gen (0 -> 1 -> 2 -> 0 -> ...)
	// and synchronizing with writers by observing each P's
	// statsSeq field. If the reader observes a P not writing,
	// it can be sure that it will pick up the new gen value the
	// next time it writes.
	//
	// The reader then takes responsibility by clearing space
	// in the ring buffer for the next reader to rotate gen to
	// that space (i.e. it merges in values from index (gen-2) mod 3
	// to index (gen-1) mod 3, then clears the former).
	//
	// Note that this means only one reader can be reading at a time.
	// There is no way for readers to synchronize.
	//
	// This process is why we need a ring buffer of size 3 instead
	// of 2: one is for the writers, one contains the most recent
	// data, and the last one is clear so writers can begin writing
	// to it the moment gen is updated.
	stats [3]heapStatsDelta

	// gen represents the current index into which writers
	// are writing, and can take on the value of 0, 1, or 2.
	// This value is updated atomically.
	gen uint32

	// noPLock is intended to provide mutual exclusion for updating
	// stats when no P is available. It does not block other writers
	// with a P, only other writers without a P and the reader. Because
	// stats are usually updated when a P is available, contention on
	// this lock should be minimal.
	noPLock mutex
}
```

consistentHeapStats represents a set of various memory statistics whose updates must be viewed completely to get a consistent state of the world. 

To write updates to memory stats use the acquire and release methods. To obtain a consistent global snapshot of these statistics, use read. 

#### <a id="consistentHeapStats.acquire" href="#consistentHeapStats.acquire">func (m *consistentHeapStats) acquire() *heapStatsDelta</a>

```
searchKey: runtime.consistentHeapStats.acquire
tags: [function private]
```

```Go
func (m *consistentHeapStats) acquire() *heapStatsDelta
```

acquire returns a heapStatsDelta to be updated. In effect, it acquires the shard for writing. release must be called as soon as the relevant deltas are updated. 

The returned heapStatsDelta must be updated atomically. 

The caller's P must not change between acquire and release. This also means that the caller should not acquire a P or release its P in between. 

#### <a id="consistentHeapStats.read" href="#consistentHeapStats.read">func (m *consistentHeapStats) read(out *heapStatsDelta)</a>

```
searchKey: runtime.consistentHeapStats.read
tags: [method private]
```

```Go
func (m *consistentHeapStats) read(out *heapStatsDelta)
```

read takes a globally consistent snapshot of m and puts the aggregated value in out. Even though out is a heapStatsDelta, the resulting values should be complete and valid statistic values. 

Not safe to call concurrently. The world must be stopped or metricsSema must be held. 

#### <a id="consistentHeapStats.release" href="#consistentHeapStats.release">func (m *consistentHeapStats) release()</a>

```
searchKey: runtime.consistentHeapStats.release
tags: [function private]
```

```Go
func (m *consistentHeapStats) release()
```

release indicates that the writer is done modifying the delta. The value returned by the corresponding acquire must no longer be accessed or modified after release is called. 

The caller's P must not change between acquire and release. This also means that the caller should not acquire a P or release its P in between. 

#### <a id="consistentHeapStats.unsafeClear" href="#consistentHeapStats.unsafeClear">func (m *consistentHeapStats) unsafeClear()</a>

```
searchKey: runtime.consistentHeapStats.unsafeClear
tags: [function private]
```

```Go
func (m *consistentHeapStats) unsafeClear()
```

unsafeClear clears the shard. 

Unsafe because the world must be stopped and values should be donated elsewhere before clearing. 

#### <a id="consistentHeapStats.unsafeRead" href="#consistentHeapStats.unsafeRead">func (m *consistentHeapStats) unsafeRead(out *heapStatsDelta)</a>

```
searchKey: runtime.consistentHeapStats.unsafeRead
tags: [method private]
```

```Go
func (m *consistentHeapStats) unsafeRead(out *heapStatsDelta)
```

unsafeRead aggregates the delta for this shard into out. 

Unsafe because it does so without any synchronization. The world must be stopped. 

### <a id="cpuProfile" href="#cpuProfile">type cpuProfile struct</a>

```
searchKey: runtime.cpuProfile
tags: [struct private]
```

```Go
type cpuProfile struct {
	lock mutex
	on   bool     // profiling is on
	log  *profBuf // profile events written here

	// extra holds extra stacks accumulated in addNonGo
	// corresponding to profiling signals arriving on
	// non-Go-created threads. Those stacks are written
	// to log the next time a normal Go thread gets the
	// signal handler.
	// Assuming the stacks are 2 words each (we don't get
	// a full traceback from those threads), plus one word
	// size for framing, 100 Hz profiling would generate
	// 300 words per second.
	// Hopefully a normal Go thread will get the profiling
	// signal at least once every few seconds.
	extra      [1000]uintptr
	numExtra   int
	lostExtra  uint64 // count of frames lost because extra is full
	lostAtomic uint64 // count of frames lost because of being in atomic64 on mips/arm; updated racily
}
```

#### <a id="cpuProfile.add" href="#cpuProfile.add">func (p *cpuProfile) add(gp *g, stk []uintptr)</a>

```
searchKey: runtime.cpuProfile.add
tags: [method private]
```

```Go
func (p *cpuProfile) add(gp *g, stk []uintptr)
```

add adds the stack trace to the profile. It is called from signal handlers and other limited environments and cannot allocate memory or acquire locks that might be held at the time of the signal, nor can it use substantial amounts of stack. 

#### <a id="cpuProfile.addExtra" href="#cpuProfile.addExtra">func (p *cpuProfile) addExtra()</a>

```
searchKey: runtime.cpuProfile.addExtra
tags: [function private]
```

```Go
func (p *cpuProfile) addExtra()
```

addExtra adds the "extra" profiling events, queued by addNonGo, to the profile log. addExtra is called either from a signal handler on a Go thread or from an ordinary goroutine; either way it can use stack and has a g. The world may be stopped, though. 

#### <a id="cpuProfile.addNonGo" href="#cpuProfile.addNonGo">func (p *cpuProfile) addNonGo(stk []uintptr)</a>

```
searchKey: runtime.cpuProfile.addNonGo
tags: [method private]
```

```Go
func (p *cpuProfile) addNonGo(stk []uintptr)
```

addNonGo adds the non-Go stack trace to the profile. It is called from a non-Go thread, so we cannot use much stack at all, nor do anything that needs a g or an m. In particular, we can't call cpuprof.log.write. Instead, we copy the stack into cpuprof.extra, which will be drained the next time a Go thread gets the signal handling event. 

### <a id="dbgVar" href="#dbgVar">type dbgVar struct</a>

```
searchKey: runtime.dbgVar
tags: [struct private]
```

```Go
type dbgVar struct {
	name  string
	value *int32
}
```

### <a id="debugCallWrapArgs" href="#debugCallWrapArgs">type debugCallWrapArgs struct</a>

```
searchKey: runtime.debugCallWrapArgs
tags: [struct private]
```

```Go
type debugCallWrapArgs struct {
	dispatch uintptr
	callingG *g
}
```

### <a id="debugLogBuf" href="#debugLogBuf">type debugLogBuf [16384]byte</a>

```
searchKey: runtime.debugLogBuf
tags: [array number private]
```

```Go
type debugLogBuf [debugLogBytes]byte
```

### <a id="debugLogReader" href="#debugLogReader">type debugLogReader struct</a>

```
searchKey: runtime.debugLogReader
tags: [struct private]
```

```Go
type debugLogReader struct {
	data *debugLogBuf

	// begin and end are the positions in the log of the beginning
	// and end of the log data, modulo len(data).
	begin, end uint64

	// tick and nano are the current time base at begin.
	tick, nano uint64
}
```

#### <a id="debugLogReader.header" href="#debugLogReader.header">func (r *debugLogReader) header() (end, tick, nano uint64, p int)</a>

```
searchKey: runtime.debugLogReader.header
tags: [function private]
```

```Go
func (r *debugLogReader) header() (end, tick, nano uint64, p int)
```

#### <a id="debugLogReader.peek" href="#debugLogReader.peek">func (r *debugLogReader) peek() (tick uint64)</a>

```
searchKey: runtime.debugLogReader.peek
tags: [function private]
```

```Go
func (r *debugLogReader) peek() (tick uint64)
```

#### <a id="debugLogReader.printVal" href="#debugLogReader.printVal">func (r *debugLogReader) printVal() bool</a>

```
searchKey: runtime.debugLogReader.printVal
tags: [function private]
```

```Go
func (r *debugLogReader) printVal() bool
```

#### <a id="debugLogReader.readUint16LEAt" href="#debugLogReader.readUint16LEAt">func (r *debugLogReader) readUint16LEAt(pos uint64) uint16</a>

```
searchKey: runtime.debugLogReader.readUint16LEAt
tags: [method private]
```

```Go
func (r *debugLogReader) readUint16LEAt(pos uint64) uint16
```

#### <a id="debugLogReader.readUint64LEAt" href="#debugLogReader.readUint64LEAt">func (r *debugLogReader) readUint64LEAt(pos uint64) uint64</a>

```
searchKey: runtime.debugLogReader.readUint64LEAt
tags: [method private]
```

```Go
func (r *debugLogReader) readUint64LEAt(pos uint64) uint64
```

#### <a id="debugLogReader.skip" href="#debugLogReader.skip">func (r *debugLogReader) skip() uint64</a>

```
searchKey: runtime.debugLogReader.skip
tags: [function private]
```

```Go
func (r *debugLogReader) skip() uint64
```

#### <a id="debugLogReader.uvarint" href="#debugLogReader.uvarint">func (r *debugLogReader) uvarint() uint64</a>

```
searchKey: runtime.debugLogReader.uvarint
tags: [function private]
```

```Go
func (r *debugLogReader) uvarint() uint64
```

#### <a id="debugLogReader.varint" href="#debugLogReader.varint">func (r *debugLogReader) varint() int64</a>

```
searchKey: runtime.debugLogReader.varint
tags: [function private]
```

```Go
func (r *debugLogReader) varint() int64
```

### <a id="debugLogWriter" href="#debugLogWriter">type debugLogWriter struct</a>

```
searchKey: runtime.debugLogWriter
tags: [struct private]
```

```Go
type debugLogWriter struct {
	write uint64
	data  debugLogBuf

	// tick and nano are the time bases from the most recently
	// written sync record.
	tick, nano uint64

	// r is a reader that consumes records as they get overwritten
	// by the writer. It also acts as the initial reader state
	// when printing the log.
	r debugLogReader

	// buf is a scratch buffer for encoding. This is here to
	// reduce stack usage.
	buf [10]byte
}
```

A debugLogWriter is a ring buffer of binary debug log records. 

A log record consists of a 2-byte framing header and a sequence of fields. The framing header gives the size of the record as a little endian 16-bit value. Each field starts with a byte indicating its type, followed by type-specific data. If the size in the framing header is 0, it's a sync record consisting of two little endian 64-bit values giving a new time base. 

Because this is a ring buffer, new records will eventually overwrite old records. Hence, it maintains a reader that consumes the log as it gets overwritten. That reader state is where an actual log reader would start. 

#### <a id="debugLogWriter.byte" href="#debugLogWriter.byte">func (l *debugLogWriter) byte(x byte)</a>

```
searchKey: runtime.debugLogWriter.byte
tags: [method private]
```

```Go
func (l *debugLogWriter) byte(x byte)
```

#### <a id="debugLogWriter.bytes" href="#debugLogWriter.bytes">func (l *debugLogWriter) bytes(x []byte)</a>

```
searchKey: runtime.debugLogWriter.bytes
tags: [method private]
```

```Go
func (l *debugLogWriter) bytes(x []byte)
```

#### <a id="debugLogWriter.ensure" href="#debugLogWriter.ensure">func (l *debugLogWriter) ensure(n uint64)</a>

```
searchKey: runtime.debugLogWriter.ensure
tags: [method private]
```

```Go
func (l *debugLogWriter) ensure(n uint64)
```

#### <a id="debugLogWriter.uvarint" href="#debugLogWriter.uvarint">func (l *debugLogWriter) uvarint(u uint64)</a>

```
searchKey: runtime.debugLogWriter.uvarint
tags: [method private]
```

```Go
func (l *debugLogWriter) uvarint(u uint64)
```

#### <a id="debugLogWriter.varint" href="#debugLogWriter.varint">func (l *debugLogWriter) varint(x int64)</a>

```
searchKey: runtime.debugLogWriter.varint
tags: [method private]
```

```Go
func (l *debugLogWriter) varint(x int64)
```

#### <a id="debugLogWriter.writeFrameAt" href="#debugLogWriter.writeFrameAt">func (l *debugLogWriter) writeFrameAt(pos, size uint64) bool</a>

```
searchKey: runtime.debugLogWriter.writeFrameAt
tags: [method private]
```

```Go
func (l *debugLogWriter) writeFrameAt(pos, size uint64) bool
```

#### <a id="debugLogWriter.writeSync" href="#debugLogWriter.writeSync">func (l *debugLogWriter) writeSync(tick, nano uint64)</a>

```
searchKey: runtime.debugLogWriter.writeSync
tags: [method private]
```

```Go
func (l *debugLogWriter) writeSync(tick, nano uint64)
```

#### <a id="debugLogWriter.writeUint64LE" href="#debugLogWriter.writeUint64LE">func (l *debugLogWriter) writeUint64LE(x uint64)</a>

```
searchKey: runtime.debugLogWriter.writeUint64LE
tags: [method private]
```

```Go
func (l *debugLogWriter) writeUint64LE(x uint64)
```

### <a id="dlogPerM" href="#dlogPerM">type dlogPerM struct{}</a>

```
searchKey: runtime.dlogPerM
tags: [struct private]
```

```Go
type dlogPerM struct{}
```

### <a id="dlogger" href="#dlogger">type dlogger struct</a>

```
searchKey: runtime.dlogger
tags: [struct private]
```

```Go
type dlogger struct {
	w debugLogWriter

	// allLink is the next dlogger in the allDloggers list.
	allLink *dlogger

	// owned indicates that this dlogger is owned by an M. This is
	// accessed atomically.
	owned uint32
}
```

A dlogger writes to the debug log. 

To obtain a dlogger, call dlog(). When done with the dlogger, call end(). 

#### <a id="dlog" href="#dlog">func dlog() *dlogger</a>

```
searchKey: runtime.dlog
tags: [function private]
```

```Go
func dlog() *dlogger
```

dlog returns a debug logger. The caller can use methods on the returned logger to add values, which will be space-separated in the final output, much like println. The caller must call end() to finish the message. 

dlog can be used from highly-constrained corners of the runtime: it is safe to use in the signal handler, from within the write barrier, from within the stack implementation, and in places that must be recursively nosplit. 

This will be compiled away if built without the debuglog build tag. However, argument construction may not be. If any of the arguments are not literals or trivial expressions, consider protecting the call with "if dlogEnabled". 

#### <a id="getCachedDlogger" href="#getCachedDlogger">func getCachedDlogger() *dlogger</a>

```
searchKey: runtime.getCachedDlogger
tags: [function private]
```

```Go
func getCachedDlogger() *dlogger
```

#### <a id="dlogger.B" href="#dlogger.B">func (l *dlogger) B(x bool) *dlogger</a>

```
searchKey: runtime.dlogger.B
tags: [method private]
```

```Go
func (l *dlogger) B(x bool) *dlogger
```

#### <a id="dlogger.End" href="#dlogger.End">func (l *dlogger) End()</a>

```
searchKey: runtime.dlogger.End
tags: [function private]
```

```Go
func (l *dlogger) End()
```

#### <a id="dlogger.Hex" href="#dlogger.Hex">func (l *dlogger) Hex(x uint64) *dlogger</a>

```
searchKey: runtime.dlogger.Hex
tags: [method private]
```

```Go
func (l *dlogger) Hex(x uint64) *dlogger
```

#### <a id="dlogger.I" href="#dlogger.I">func (l *dlogger) I(x int) *dlogger</a>

```
searchKey: runtime.dlogger.I
tags: [method private]
```

```Go
func (l *dlogger) I(x int) *dlogger
```

#### <a id="dlogger.I16" href="#dlogger.I16">func (l *dlogger) I16(x int16) *dlogger</a>

```
searchKey: runtime.dlogger.I16
tags: [method private]
```

```Go
func (l *dlogger) I16(x int16) *dlogger
```

#### <a id="dlogger.P" href="#dlogger.P">func (l *dlogger) P(x interface{}) *dlogger</a>

```
searchKey: runtime.dlogger.P
tags: [method private]
```

```Go
func (l *dlogger) P(x interface{}) *dlogger
```

#### <a id="dlogger.PC" href="#dlogger.PC">func (l *dlogger) PC(x uintptr) *dlogger</a>

```
searchKey: runtime.dlogger.PC
tags: [method private]
```

```Go
func (l *dlogger) PC(x uintptr) *dlogger
```

#### <a id="dlogger.S" href="#dlogger.S">func (l *dlogger) S(x string) *dlogger</a>

```
searchKey: runtime.dlogger.S
tags: [method private]
```

```Go
func (l *dlogger) S(x string) *dlogger
```

#### <a id="dlogger.U64" href="#dlogger.U64">func (l *dlogger) U64(x uint64) *dlogger</a>

```
searchKey: runtime.dlogger.U64
tags: [method private]
```

```Go
func (l *dlogger) U64(x uint64) *dlogger
```

#### <a id="dlogger.b" href="#dlogger.b">func (l *dlogger) b(x bool) *dlogger</a>

```
searchKey: runtime.dlogger.b
tags: [method private]
```

```Go
func (l *dlogger) b(x bool) *dlogger
```

#### <a id="dlogger.end" href="#dlogger.end">func (l *dlogger) end()</a>

```
searchKey: runtime.dlogger.end
tags: [function private]
```

```Go
func (l *dlogger) end()
```

#### <a id="dlogger.hex" href="#dlogger.hex">func (l *dlogger) hex(x uint64) *dlogger</a>

```
searchKey: runtime.dlogger.hex
tags: [method private]
```

```Go
func (l *dlogger) hex(x uint64) *dlogger
```

#### <a id="dlogger.i" href="#dlogger.i">func (l *dlogger) i(x int) *dlogger</a>

```
searchKey: runtime.dlogger.i
tags: [method private]
```

```Go
func (l *dlogger) i(x int) *dlogger
```

#### <a id="dlogger.i16" href="#dlogger.i16">func (l *dlogger) i16(x int16) *dlogger</a>

```
searchKey: runtime.dlogger.i16
tags: [method private]
```

```Go
func (l *dlogger) i16(x int16) *dlogger
```

#### <a id="dlogger.i32" href="#dlogger.i32">func (l *dlogger) i32(x int32) *dlogger</a>

```
searchKey: runtime.dlogger.i32
tags: [method private]
```

```Go
func (l *dlogger) i32(x int32) *dlogger
```

#### <a id="dlogger.i64" href="#dlogger.i64">func (l *dlogger) i64(x int64) *dlogger</a>

```
searchKey: runtime.dlogger.i64
tags: [method private]
```

```Go
func (l *dlogger) i64(x int64) *dlogger
```

#### <a id="dlogger.i8" href="#dlogger.i8">func (l *dlogger) i8(x int8) *dlogger</a>

```
searchKey: runtime.dlogger.i8
tags: [method private]
```

```Go
func (l *dlogger) i8(x int8) *dlogger
```

#### <a id="dlogger.p" href="#dlogger.p">func (l *dlogger) p(x interface{}) *dlogger</a>

```
searchKey: runtime.dlogger.p
tags: [method private]
```

```Go
func (l *dlogger) p(x interface{}) *dlogger
```

#### <a id="dlogger.pc" href="#dlogger.pc">func (l *dlogger) pc(x uintptr) *dlogger</a>

```
searchKey: runtime.dlogger.pc
tags: [method private]
```

```Go
func (l *dlogger) pc(x uintptr) *dlogger
```

#### <a id="dlogger.s" href="#dlogger.s">func (l *dlogger) s(x string) *dlogger</a>

```
searchKey: runtime.dlogger.s
tags: [method private]
```

```Go
func (l *dlogger) s(x string) *dlogger
```

#### <a id="dlogger.traceback" href="#dlogger.traceback">func (l *dlogger) traceback(x []uintptr) *dlogger</a>

```
searchKey: runtime.dlogger.traceback
tags: [method private]
```

```Go
func (l *dlogger) traceback(x []uintptr) *dlogger
```

#### <a id="dlogger.u" href="#dlogger.u">func (l *dlogger) u(x uint) *dlogger</a>

```
searchKey: runtime.dlogger.u
tags: [method private]
```

```Go
func (l *dlogger) u(x uint) *dlogger
```

#### <a id="dlogger.u16" href="#dlogger.u16">func (l *dlogger) u16(x uint16) *dlogger</a>

```
searchKey: runtime.dlogger.u16
tags: [method private]
```

```Go
func (l *dlogger) u16(x uint16) *dlogger
```

#### <a id="dlogger.u32" href="#dlogger.u32">func (l *dlogger) u32(x uint32) *dlogger</a>

```
searchKey: runtime.dlogger.u32
tags: [method private]
```

```Go
func (l *dlogger) u32(x uint32) *dlogger
```

#### <a id="dlogger.u64" href="#dlogger.u64">func (l *dlogger) u64(x uint64) *dlogger</a>

```
searchKey: runtime.dlogger.u64
tags: [method private]
```

```Go
func (l *dlogger) u64(x uint64) *dlogger
```

#### <a id="dlogger.u8" href="#dlogger.u8">func (l *dlogger) u8(x uint8) *dlogger</a>

```
searchKey: runtime.dlogger.u8
tags: [method private]
```

```Go
func (l *dlogger) u8(x uint8) *dlogger
```

#### <a id="dlogger.uptr" href="#dlogger.uptr">func (l *dlogger) uptr(x uintptr) *dlogger</a>

```
searchKey: runtime.dlogger.uptr
tags: [method private]
```

```Go
func (l *dlogger) uptr(x uintptr) *dlogger
```

### <a id="eface" href="#eface">type eface struct</a>

```
searchKey: runtime.eface
tags: [struct private]
```

```Go
type eface struct {
	_type *_type
	data  unsafe.Pointer
}
```

#### <a id="convT2E" href="#convT2E">func convT2E(t *_type, elem unsafe.Pointer) (e eface)</a>

```
searchKey: runtime.convT2E
tags: [method private]
```

```Go
func convT2E(t *_type, elem unsafe.Pointer) (e eface)
```

#### <a id="convT2Enoptr" href="#convT2Enoptr">func convT2Enoptr(t *_type, elem unsafe.Pointer) (e eface)</a>

```
searchKey: runtime.convT2Enoptr
tags: [method private]
```

```Go
func convT2Enoptr(t *_type, elem unsafe.Pointer) (e eface)
```

#### <a id="efaceOf" href="#efaceOf">func efaceOf(ep *interface{}) *eface</a>

```
searchKey: runtime.efaceOf
tags: [method private]
```

```Go
func efaceOf(ep *interface{}) *eface
```

### <a id="errorAddressString" href="#errorAddressString">type errorAddressString struct</a>

```
searchKey: runtime.errorAddressString
tags: [struct private]
```

```Go
type errorAddressString struct {
	msg  string  // error message
	addr uintptr // memory address where the error occurred
}
```

#### <a id="errorAddressString.Addr" href="#errorAddressString.Addr">func (e errorAddressString) Addr() uintptr</a>

```
searchKey: runtime.errorAddressString.Addr
tags: [function private]
```

```Go
func (e errorAddressString) Addr() uintptr
```

Addr returns the memory address where a fault occurred. The address provided is best-effort. The veracity of the result may depend on the platform. Errors providing this method will only be returned as a result of using runtime/debug.SetPanicOnFault. 

#### <a id="errorAddressString.Error" href="#errorAddressString.Error">func (e errorAddressString) Error() string</a>

```
searchKey: runtime.errorAddressString.Error
tags: [function private]
```

```Go
func (e errorAddressString) Error() string
```

#### <a id="errorAddressString.RuntimeError" href="#errorAddressString.RuntimeError">func (e errorAddressString) RuntimeError()</a>

```
searchKey: runtime.errorAddressString.RuntimeError
tags: [function private]
```

```Go
func (e errorAddressString) RuntimeError()
```

### <a id="errorString" href="#errorString">type errorString string</a>

```
searchKey: runtime.errorString
tags: [string private]
```

```Go
type errorString string
```

An errorString represents a runtime error described by a single string. 

#### <a id="errorString.Error" href="#errorString.Error">func (e errorString) Error() string</a>

```
searchKey: runtime.errorString.Error
tags: [function private]
```

```Go
func (e errorString) Error() string
```

#### <a id="errorString.RuntimeError" href="#errorString.RuntimeError">func (e errorString) RuntimeError()</a>

```
searchKey: runtime.errorString.RuntimeError
tags: [function private]
```

```Go
func (e errorString) RuntimeError()
```

### <a id="evacDst" href="#evacDst">type evacDst struct</a>

```
searchKey: runtime.evacDst
tags: [struct private]
```

```Go
type evacDst struct {
	b *bmap          // current destination bucket
	i int            // key/elem index into b
	k unsafe.Pointer // pointer to current key storage
	e unsafe.Pointer // pointer to current elem storage
}
```

evacDst is an evacuation destination. 

### <a id="exceptionstate32" href="#exceptionstate32">type exceptionstate32 struct</a>

```
searchKey: runtime.exceptionstate32
tags: [struct private]
```

```Go
type exceptionstate32 struct {
	trapno     uint16
	cpu        uint16
	err        uint32
	faultvaddr uint32
}
```

### <a id="exceptionstate64" href="#exceptionstate64">type exceptionstate64 struct</a>

```
searchKey: runtime.exceptionstate64
tags: [struct private]
```

```Go
type exceptionstate64 struct {
	trapno     uint16
	cpu        uint16
	err        uint32
	faultvaddr uint64
}
```

### <a id="finalizer" href="#finalizer">type finalizer struct</a>

```
searchKey: runtime.finalizer
tags: [struct private]
```

```Go
type finalizer struct {
	fn   *funcval       // function to call (may be a heap pointer)
	arg  unsafe.Pointer // ptr to object (may be a heap pointer)
	nret uintptr        // bytes of return values from fn
	fint *_type         // type of first argument of fn
	ot   *ptrtype       // type of ptr to object (may be a heap pointer)
}
```

NOTE: Layout known to queuefinalizer. 

### <a id="finblock" href="#finblock">type finblock struct</a>

```
searchKey: runtime.finblock
tags: [struct private]
```

```Go
type finblock struct {
	alllink *finblock
	next    *finblock
	cnt     uint32
	_       int32
	fin     [(_FinBlockSize - 2*sys.PtrSize - 2*4) / unsafe.Sizeof(finalizer{})]finalizer
}
```

finblock is an array of finalizers to be executed. finblocks are arranged in a linked list for the finalizer queue. 

finblock is allocated from non-GC'd memory, so any heap pointers must be specially handled. GC currently assumes that the finalizer queue does not grow during marking (but it can shrink). 

### <a id="findfuncbucket" href="#findfuncbucket">type findfuncbucket struct</a>

```
searchKey: runtime.findfuncbucket
tags: [struct private]
```

```Go
type findfuncbucket struct {
	idx        uint32
	subbuckets [16]byte
}
```

findfunctab is an array of these structures. Each bucket represents 4096 bytes of the text segment. Each subbucket represents 256 bytes of the text segment. To find a function given a pc, locate the bucket and subbucket for that pc. Add together the idx and subbucket value to obtain a function index. Then scan the functab array starting at that index to find the target function. This table uses 20 bytes for every 4096 bytes of code, or ~0.5% overhead. 

### <a id="fixalloc" href="#fixalloc">type fixalloc struct</a>

```
searchKey: runtime.fixalloc
tags: [struct private]
```

```Go
type fixalloc struct {
	size   uintptr
	first  func(arg, p unsafe.Pointer) // called first time p is returned
	arg    unsafe.Pointer
	list   *mlink
	chunk  uintptr // use uintptr instead of unsafe.Pointer to avoid write barriers
	nchunk uint32
	inuse  uintptr // in-use bytes now
	stat   *sysMemStat
	zero   bool // zero allocations
}
```

FixAlloc is a simple free-list allocator for fixed size objects. Malloc uses a FixAlloc wrapped around sysAlloc to manage its mcache and mspan objects. 

Memory returned by fixalloc.alloc is zeroed by default, but the caller may take responsibility for zeroing allocations by setting the zero flag to false. This is only safe if the memory never contains heap pointers. 

The caller is responsible for locking around FixAlloc calls. Callers can keep state in the object but the first word is smashed by freeing and reallocating. 

Consider marking fixalloc'd types go:notinheap. 

#### <a id="fixalloc.alloc" href="#fixalloc.alloc">func (f *fixalloc) alloc() unsafe.Pointer</a>

```
searchKey: runtime.fixalloc.alloc
tags: [function private]
```

```Go
func (f *fixalloc) alloc() unsafe.Pointer
```

#### <a id="fixalloc.free" href="#fixalloc.free">func (f *fixalloc) free(p unsafe.Pointer)</a>

```
searchKey: runtime.fixalloc.free
tags: [method private]
```

```Go
func (f *fixalloc) free(p unsafe.Pointer)
```

#### <a id="fixalloc.init.mfixalloc.go" href="#fixalloc.init.mfixalloc.go">func (f *fixalloc) init(size uintptr, first func(arg, p unsafe.Pointer), arg unsafe.Pointer, stat *sysMemStat)</a>

```
searchKey: runtime.fixalloc.init
tags: [method private]
```

```Go
func (f *fixalloc) init(size uintptr, first func(arg, p unsafe.Pointer), arg unsafe.Pointer, stat *sysMemStat)
```

Initialize f to allocate objects of the given size, using the allocator to obtain chunks of memory. 

### <a id="floatstate32" href="#floatstate32">type floatstate32 struct</a>

```
searchKey: runtime.floatstate32
tags: [struct private]
```

```Go
type floatstate32 struct {
	fpu_reserved  [2]int32
	fpu_fcw       fpcontrol
	fpu_fsw       fpstatus
	fpu_ftw       uint8
	fpu_rsrv1     uint8
	fpu_fop       uint16
	fpu_ip        uint32
	fpu_cs        uint16
	fpu_rsrv2     uint16
	fpu_dp        uint32
	fpu_ds        uint16
	fpu_rsrv3     uint16
	fpu_mxcsr     uint32
	fpu_mxcsrmask uint32
	fpu_stmm0     regmmst
	fpu_stmm1     regmmst
	fpu_stmm2     regmmst
	fpu_stmm3     regmmst
	fpu_stmm4     regmmst
	fpu_stmm5     regmmst
	fpu_stmm6     regmmst
	fpu_stmm7     regmmst
	fpu_xmm0      regxmm
	fpu_xmm1      regxmm
	fpu_xmm2      regxmm
	fpu_xmm3      regxmm
	fpu_xmm4      regxmm
	fpu_xmm5      regxmm
	fpu_xmm6      regxmm
	fpu_xmm7      regxmm
	fpu_rsrv4     [224]int8
	fpu_reserved1 int32
}
```

### <a id="floatstate64" href="#floatstate64">type floatstate64 struct</a>

```
searchKey: runtime.floatstate64
tags: [struct private]
```

```Go
type floatstate64 struct {
	fpu_reserved  [2]int32
	fpu_fcw       fpcontrol
	fpu_fsw       fpstatus
	fpu_ftw       uint8
	fpu_rsrv1     uint8
	fpu_fop       uint16
	fpu_ip        uint32
	fpu_cs        uint16
	fpu_rsrv2     uint16
	fpu_dp        uint32
	fpu_ds        uint16
	fpu_rsrv3     uint16
	fpu_mxcsr     uint32
	fpu_mxcsrmask uint32
	fpu_stmm0     regmmst
	fpu_stmm1     regmmst
	fpu_stmm2     regmmst
	fpu_stmm3     regmmst
	fpu_stmm4     regmmst
	fpu_stmm5     regmmst
	fpu_stmm6     regmmst
	fpu_stmm7     regmmst
	fpu_xmm0      regxmm
	fpu_xmm1      regxmm
	fpu_xmm2      regxmm
	fpu_xmm3      regxmm
	fpu_xmm4      regxmm
	fpu_xmm5      regxmm
	fpu_xmm6      regxmm
	fpu_xmm7      regxmm
	fpu_xmm8      regxmm
	fpu_xmm9      regxmm
	fpu_xmm10     regxmm
	fpu_xmm11     regxmm
	fpu_xmm12     regxmm
	fpu_xmm13     regxmm
	fpu_xmm14     regxmm
	fpu_xmm15     regxmm
	fpu_rsrv4     [96]int8
	fpu_reserved1 int32
}
```

### <a id="forcegcstate" href="#forcegcstate">type forcegcstate struct</a>

```
searchKey: runtime.forcegcstate
tags: [struct private]
```

```Go
type forcegcstate struct {
	lock mutex
	g    *g
	idle uint32
}
```

### <a id="fpcontrol" href="#fpcontrol">type fpcontrol struct</a>

```
searchKey: runtime.fpcontrol
tags: [struct private]
```

```Go
type fpcontrol struct {
	pad_cgo_0 [2]byte
}
```

### <a id="fpstatus" href="#fpstatus">type fpstatus struct</a>

```
searchKey: runtime.fpstatus
tags: [struct private]
```

```Go
type fpstatus struct {
	pad_cgo_0 [2]byte
}
```

### <a id="funcFlag" href="#funcFlag">type funcFlag uint8</a>

```
searchKey: runtime.funcFlag
tags: [number private]
```

```Go
type funcFlag uint8
```

A FuncFlag holds bits about a function. This list must match the list in cmd/internal/objabi/funcid.go. 

### <a id="funcID" href="#funcID">type funcID uint8</a>

```
searchKey: runtime.funcID
tags: [number private]
```

```Go
type funcID uint8
```

A FuncID identifies particular functions that need to be treated specially by the runtime. Note that in some situations involving plugins, there may be multiple copies of a particular special runtime function. Note: this list must match the list in cmd/internal/objabi/funcid.go. 

### <a id="funcInfo" href="#funcInfo">type funcInfo struct</a>

```
searchKey: runtime.funcInfo
tags: [struct private]
```

```Go
type funcInfo struct {
	*_func
	datap *moduledata
}
```

#### <a id="findfunc" href="#findfunc">func findfunc(pc uintptr) funcInfo</a>

```
searchKey: runtime.findfunc
tags: [method private]
```

```Go
func findfunc(pc uintptr) funcInfo
```

findfunc looks up function metadata for a PC. 

It is nosplit because it's part of the isgoexception implementation. 

#### <a id="funcInfo.valid" href="#funcInfo.valid">func (f funcInfo) valid() bool</a>

```
searchKey: runtime.funcInfo.valid
tags: [function private]
```

```Go
func (f funcInfo) valid() bool
```

#### <a id="funcInfo._Func" href="#funcInfo._Func">func (f funcInfo) _Func() *Func</a>

```
searchKey: runtime.funcInfo._Func
tags: [function private]
```

```Go
func (f funcInfo) _Func() *Func
```

### <a id="funcinl" href="#funcinl">type funcinl struct</a>

```
searchKey: runtime.funcinl
tags: [struct private]
```

```Go
type funcinl struct {
	zero  uintptr // set to 0 to distinguish from _func
	entry uintptr // entry of the real (the "outermost") frame.
	name  string
	file  string
	line  int
}
```

Pseudo-Func that is returned for PCs that occur in inlined code. A *Func can be either a *_func or a *funcinl, and they are distinguished by the first uintptr. 

### <a id="functab" href="#functab">type functab struct</a>

```
searchKey: runtime.functab
tags: [struct private]
```

```Go
type functab struct {
	entry   uintptr
	funcoff uintptr
}
```

### <a id="functype" href="#functype">type functype struct</a>

```
searchKey: runtime.functype
tags: [struct private]
```

```Go
type functype struct {
	typ      _type
	inCount  uint16
	outCount uint16
}
```

#### <a id="functype.dotdotdot" href="#functype.dotdotdot">func (t *functype) dotdotdot() bool</a>

```
searchKey: runtime.functype.dotdotdot
tags: [function private]
```

```Go
func (t *functype) dotdotdot() bool
```

#### <a id="functype.in" href="#functype.in">func (t *functype) in() []*_type</a>

```
searchKey: runtime.functype.in
tags: [function private]
```

```Go
func (t *functype) in() []*_type
```

#### <a id="functype.out" href="#functype.out">func (t *functype) out() []*_type</a>

```
searchKey: runtime.functype.out
tags: [function private]
```

```Go
func (t *functype) out() []*_type
```

### <a id="funcval" href="#funcval">type funcval struct</a>

```
searchKey: runtime.funcval
tags: [struct private]
```

```Go
type funcval struct {
	fn uintptr
}
```

### <a id="g" href="#g">type g struct</a>

```
searchKey: runtime.g
tags: [struct private]
```

```Go
type g struct {
	// Stack parameters.
	// stack describes the actual stack memory: [stack.lo, stack.hi).
	// stackguard0 is the stack pointer compared in the Go stack growth prologue.
	// It is stack.lo+StackGuard normally, but can be StackPreempt to trigger a preemption.
	// stackguard1 is the stack pointer compared in the C stack growth prologue.
	// It is stack.lo+StackGuard on g0 and gsignal stacks.
	// It is ~0 on other goroutine stacks, to trigger a call to morestackc (and crash).
	stack       stack   // offset known to runtime/cgo
	stackguard0 uintptr // offset known to liblink
	stackguard1 uintptr // offset known to liblink

	_panic    *_panic // innermost panic - offset known to liblink
	_defer    *_defer // innermost defer
	m         *m      // current m; offset known to arm liblink
	sched     gobuf
	syscallsp uintptr // if status==Gsyscall, syscallsp = sched.sp to use during gc
	syscallpc uintptr // if status==Gsyscall, syscallpc = sched.pc to use during gc
	stktopsp  uintptr // expected sp at top of stack, to check in traceback
	// param is a generic pointer parameter field used to pass
	// values in particular contexts where other storage for the
	// parameter would be difficult to find. It is currently used
	// in three ways:
	// 1. When a channel operation wakes up a blocked goroutine, it sets param to
	//    point to the sudog of the completed blocking operation.
	// 2. By gcAssistAlloc1 to signal back to its caller that the goroutine completed
	//    the GC cycle. It is unsafe to do so in any other way, because the goroutine's
	//    stack may have moved in the meantime.
	// 3. By debugCallWrap to pass parameters to a new goroutine because allocating a
	//    closure in the runtime is forbidden.
	param        unsafe.Pointer
	atomicstatus uint32
	stackLock    uint32 // sigprof/scang lock; TODO: fold in to atomicstatus
	goid         int64
	schedlink    guintptr
	waitsince    int64      // approx time when the g become blocked
	waitreason   waitReason // if status==Gwaiting

	preempt       bool // preemption signal, duplicates stackguard0 = stackpreempt
	preemptStop   bool // transition to _Gpreempted on preemption; otherwise, just deschedule
	preemptShrink bool // shrink stack at synchronous safe point

	// asyncSafePoint is set if g is stopped at an asynchronous
	// safe point. This means there are frames on the stack
	// without precise pointer information.
	asyncSafePoint bool

	paniconfault bool // panic (instead of crash) on unexpected fault address
	gcscandone   bool // g has scanned stack; protected by _Gscan bit in status
	throwsplit   bool // must not split stack
	// activeStackChans indicates that there are unlocked channels
	// pointing into this goroutine's stack. If true, stack
	// copying needs to acquire channel locks to protect these
	// areas of the stack.
	activeStackChans bool
	// parkingOnChan indicates that the goroutine is about to
	// park on a chansend or chanrecv. Used to signal an unsafe point
	// for stack shrinking. It's a boolean value, but is updated atomically.
	parkingOnChan uint8

	raceignore     int8     // ignore race detection events
	sysblocktraced bool     // StartTrace has emitted EvGoInSyscall about this goroutine
	tracking       bool     // whether we're tracking this G for sched latency statistics
	trackingSeq    uint8    // used to decide whether to track this G
	runnableStamp  int64    // timestamp of when the G last became runnable, only used when tracking
	runnableTime   int64    // the amount of time spent runnable, cleared when running, only used when tracking
	sysexitticks   int64    // cputicks when syscall has returned (for tracing)
	traceseq       uint64   // trace event sequencer
	tracelastp     puintptr // last P emitted an event for this goroutine
	lockedm        muintptr
	sig            uint32
	writebuf       []byte
	sigcode0       uintptr
	sigcode1       uintptr
	sigpc          uintptr
	gopc           uintptr         // pc of go statement that created this goroutine
	ancestors      *[]ancestorInfo // ancestor information goroutine(s) that created this goroutine (only used if debug.tracebackancestors)
	startpc        uintptr         // pc of goroutine function
	racectx        uintptr
	waiting        *sudog         // sudog structures this g is waiting on (that have a valid elem ptr); in lock order
	cgoCtxt        []uintptr      // cgo traceback context
	labels         unsafe.Pointer // profiler labels
	timer          *timer         // cached timer for time.Sleep
	selectDone     uint32         // are we participating in a select and did someone win the race?

	// gcAssistBytes is this G's GC assist credit in terms of
	// bytes allocated. If this is positive, then the G has credit
	// to allocate gcAssistBytes bytes without assisting. If this
	// is negative, then the G must correct this by performing
	// scan work. We track this in bytes to make it fast to update
	// and check for debt in the malloc hot path. The assist ratio
	// determines how this corresponds to scan work debt.
	gcAssistBytes int64
}
```

### <a id="gList" href="#gList">type gList struct</a>

```
searchKey: runtime.gList
tags: [struct private]
```

```Go
type gList struct {
	head guintptr
}
```

A gList is a list of Gs linked through g.schedlink. A G can only be on one gQueue or gList at a time. 

#### <a id="netpoll" href="#netpoll">func netpoll(delay int64) gList</a>

```
searchKey: runtime.netpoll
tags: [method private]
```

```Go
func netpoll(delay int64) gList
```

netpoll checks for ready network connections. Returns list of goroutines that become runnable. delay < 0: blocks indefinitely delay == 0: does not block, just polls delay > 0: block for up to that many nanoseconds 

#### <a id="gList.empty" href="#gList.empty">func (l *gList) empty() bool</a>

```
searchKey: runtime.gList.empty
tags: [function private]
```

```Go
func (l *gList) empty() bool
```

empty reports whether l is empty. 

#### <a id="gList.pop" href="#gList.pop">func (l *gList) pop() *g</a>

```
searchKey: runtime.gList.pop
tags: [function private]
```

```Go
func (l *gList) pop() *g
```

pop removes and returns the head of l. If l is empty, it returns nil. 

#### <a id="gList.push" href="#gList.push">func (l *gList) push(gp *g)</a>

```
searchKey: runtime.gList.push
tags: [method private]
```

```Go
func (l *gList) push(gp *g)
```

push adds gp to the head of l. 

#### <a id="gList.pushAll" href="#gList.pushAll">func (l *gList) pushAll(q gQueue)</a>

```
searchKey: runtime.gList.pushAll
tags: [method private]
```

```Go
func (l *gList) pushAll(q gQueue)
```

pushAll prepends all Gs in q to l. 

### <a id="gQueue" href="#gQueue">type gQueue struct</a>

```
searchKey: runtime.gQueue
tags: [struct private]
```

```Go
type gQueue struct {
	head guintptr
	tail guintptr
}
```

A gQueue is a dequeue of Gs linked through g.schedlink. A G can only be on one gQueue or gList at a time. 

#### <a id="runqdrain" href="#runqdrain">func runqdrain(_p_ *p) (drainQ gQueue, n uint32)</a>

```
searchKey: runtime.runqdrain
tags: [method private]
```

```Go
func runqdrain(_p_ *p) (drainQ gQueue, n uint32)
```

runqdrain drains the local runnable queue of _p_ and returns all goroutines in it. Executed only by the owner P. 

#### <a id="gQueue.empty" href="#gQueue.empty">func (q *gQueue) empty() bool</a>

```
searchKey: runtime.gQueue.empty
tags: [function private]
```

```Go
func (q *gQueue) empty() bool
```

empty reports whether q is empty. 

#### <a id="gQueue.pop" href="#gQueue.pop">func (q *gQueue) pop() *g</a>

```
searchKey: runtime.gQueue.pop
tags: [function private]
```

```Go
func (q *gQueue) pop() *g
```

pop removes and returns the head of queue q. It returns nil if q is empty. 

#### <a id="gQueue.popList" href="#gQueue.popList">func (q *gQueue) popList() gList</a>

```
searchKey: runtime.gQueue.popList
tags: [function private]
```

```Go
func (q *gQueue) popList() gList
```

popList takes all Gs in q and returns them as a gList. 

#### <a id="gQueue.push" href="#gQueue.push">func (q *gQueue) push(gp *g)</a>

```
searchKey: runtime.gQueue.push
tags: [method private]
```

```Go
func (q *gQueue) push(gp *g)
```

push adds gp to the head of q. 

#### <a id="gQueue.pushBack" href="#gQueue.pushBack">func (q *gQueue) pushBack(gp *g)</a>

```
searchKey: runtime.gQueue.pushBack
tags: [method private]
```

```Go
func (q *gQueue) pushBack(gp *g)
```

pushBack adds gp to the tail of q. 

#### <a id="gQueue.pushBackAll" href="#gQueue.pushBackAll">func (q *gQueue) pushBackAll(q2 gQueue)</a>

```
searchKey: runtime.gQueue.pushBackAll
tags: [method private]
```

```Go
func (q *gQueue) pushBackAll(q2 gQueue)
```

pushBackAll adds all Gs in l2 to the tail of q. After this q2 must not be used. 

### <a id="gcBgMarkWorkerNode" href="#gcBgMarkWorkerNode">type gcBgMarkWorkerNode struct</a>

```
searchKey: runtime.gcBgMarkWorkerNode
tags: [struct private]
```

```Go
type gcBgMarkWorkerNode struct {
	// Unused workers are managed in a lock-free stack. This field must be first.
	node lfnode

	// The g of this worker.
	gp guintptr

	// Release this m on park. This is used to communicate with the unlock
	// function, which cannot access the G's stack. It is unused outside of
	// gcBgMarkWorker().
	m muintptr
}
```

gcBgMarkWorker is an entry in the gcBgMarkWorkerPool. It points to a single gcBgMarkWorker goroutine. 

### <a id="gcBits" href="#gcBits">type gcBits uint8</a>

```
searchKey: runtime.gcBits
tags: [number private]
```

```Go
type gcBits uint8
```

gcBits is an alloc/mark bitmap. This is always used as *gcBits. 

#### <a id="newAllocBits" href="#newAllocBits">func newAllocBits(nelems uintptr) *gcBits</a>

```
searchKey: runtime.newAllocBits
tags: [method private]
```

```Go
func newAllocBits(nelems uintptr) *gcBits
```

newAllocBits returns a pointer to 8 byte aligned bytes to be used for this span's alloc bits. newAllocBits is used to provide newly initialized spans allocation bits. For spans not being initialized the mark bits are repurposed as allocation bits when the span is swept. 

#### <a id="newMarkBits" href="#newMarkBits">func newMarkBits(nelems uintptr) *gcBits</a>

```
searchKey: runtime.newMarkBits
tags: [method private]
```

```Go
func newMarkBits(nelems uintptr) *gcBits
```

newMarkBits returns a pointer to 8 byte aligned bytes to be used for a span's mark bits. 

#### <a id="gcBits.bitp" href="#gcBits.bitp">func (b *gcBits) bitp(n uintptr) (bytep *uint8, mask uint8)</a>

```
searchKey: runtime.gcBits.bitp
tags: [method private]
```

```Go
func (b *gcBits) bitp(n uintptr) (bytep *uint8, mask uint8)
```

bitp returns a pointer to the byte containing bit n and a mask for selecting that bit from *bytep. 

#### <a id="gcBits.bytep" href="#gcBits.bytep">func (b *gcBits) bytep(n uintptr) *uint8</a>

```
searchKey: runtime.gcBits.bytep
tags: [method private]
```

```Go
func (b *gcBits) bytep(n uintptr) *uint8
```

bytep returns a pointer to the n'th byte of b. 

### <a id="gcBitsArena" href="#gcBitsArena">type gcBitsArena struct</a>

```
searchKey: runtime.gcBitsArena
tags: [struct private]
```

```Go
type gcBitsArena struct {
	// gcBitsHeader // side step recursive type bug (issue 14620) by including fields by hand.
	free uintptr // free is the index into bits of the next free byte; read/write atomically
	next *gcBitsArena
	bits [gcBitsChunkBytes - gcBitsHeaderBytes]gcBits
}
```

#### <a id="newArenaMayUnlock" href="#newArenaMayUnlock">func newArenaMayUnlock() *gcBitsArena</a>

```
searchKey: runtime.newArenaMayUnlock
tags: [function private]
```

```Go
func newArenaMayUnlock() *gcBitsArena
```

newArenaMayUnlock allocates and zeroes a gcBits arena. The caller must hold gcBitsArena.lock. This may temporarily release it. 

#### <a id="gcBitsArena.tryAlloc" href="#gcBitsArena.tryAlloc">func (b *gcBitsArena) tryAlloc(bytes uintptr) *gcBits</a>

```
searchKey: runtime.gcBitsArena.tryAlloc
tags: [method private]
```

```Go
func (b *gcBitsArena) tryAlloc(bytes uintptr) *gcBits
```

tryAlloc allocates from b or returns nil if b does not have enough room. This is safe to call concurrently. 

### <a id="gcBitsHeader" href="#gcBitsHeader">type gcBitsHeader struct</a>

```
searchKey: runtime.gcBitsHeader
tags: [struct private]
```

```Go
type gcBitsHeader struct {
	free uintptr // free is the index into bits of the next free byte.
	next uintptr // *gcBits triggers recursive type bug. (issue 14620)
}
```

### <a id="gcControllerState" href="#gcControllerState">type gcControllerState struct</a>

```
searchKey: runtime.gcControllerState
tags: [struct private]
```

```Go
type gcControllerState struct {
	// Initialized from $GOGC. GOGC=off means no GC.
	gcPercent int32

	_ uint32 // padding so following 64-bit values are 8-byte aligned

	// heapMinimum is the minimum heap size at which to trigger GC.
	// For small heaps, this overrides the usual GOGC*live set rule.
	//
	// When there is a very small live set but a lot of allocation, simply
	// collecting when the heap reaches GOGC*live results in many GC
	// cycles and high total per-GC overhead. This minimum amortizes this
	// per-GC overhead while keeping the heap reasonably small.
	//
	// During initialization this is set to 4MB*GOGC/100. In the case of
	// GOGC==0, this will set heapMinimum to 0, resulting in constant
	// collection even when the heap size is small, which is useful for
	// debugging.
	heapMinimum uint64

	// triggerRatio is the heap growth ratio that triggers marking.
	//
	// E.g., if this is 0.6, then GC should start when the live
	// heap has reached 1.6 times the heap size marked by the
	// previous cycle. This should be ≤ GOGC/100 so the trigger
	// heap size is less than the goal heap size. This is set
	// during mark termination for the next cycle's trigger.
	//
	// Protected by mheap_.lock or a STW.
	triggerRatio float64

	// trigger is the heap size that triggers marking.
	//
	// When heapLive ≥ trigger, the mark phase will start.
	// This is also the heap size by which proportional sweeping
	// must be complete.
	//
	// This is computed from triggerRatio during mark termination
	// for the next cycle's trigger.
	//
	// Protected by mheap_.lock or a STW.
	trigger uint64

	// heapGoal is the goal heapLive for when next GC ends.
	// Set to ^uint64(0) if disabled.
	//
	// Read and written atomically, unless the world is stopped.
	heapGoal uint64

	// lastHeapGoal is the value of heapGoal for the previous GC.
	// Note that this is distinct from the last value heapGoal had,
	// because it could change if e.g. gcPercent changes.
	//
	// Read and written with the world stopped or with mheap_.lock held.
	lastHeapGoal uint64

	// heapLive is the number of bytes considered live by the GC.
	// That is: retained by the most recent GC plus allocated
	// since then. heapLive ≤ memstats.heapAlloc, since heapAlloc includes
	// unmarked objects that have not yet been swept (and hence goes up as we
	// allocate and down as we sweep) while heapLive excludes these
	// objects (and hence only goes up between GCs).
	//
	// This is updated atomically without locking. To reduce
	// contention, this is updated only when obtaining a span from
	// an mcentral and at this point it counts all of the
	// unallocated slots in that span (which will be allocated
	// before that mcache obtains another span from that
	// mcentral). Hence, it slightly overestimates the "true" live
	// heap size. It's better to overestimate than to
	// underestimate because 1) this triggers the GC earlier than
	// necessary rather than potentially too late and 2) this
	// leads to a conservative GC rate rather than a GC rate that
	// is potentially too low.
	//
	// Reads should likewise be atomic (or during STW).
	//
	// Whenever this is updated, call traceHeapAlloc() and
	// this gcControllerState's revise() method.
	heapLive uint64

	// heapScan is the number of bytes of "scannable" heap. This
	// is the live heap (as counted by heapLive), but omitting
	// no-scan objects and no-scan tails of objects.
	//
	// Whenever this is updated, call this gcControllerState's
	// revise() method.
	//
	// Read and written atomically or with the world stopped.
	heapScan uint64

	// heapMarked is the number of bytes marked by the previous
	// GC. After mark termination, heapLive == heapMarked, but
	// unlike heapLive, heapMarked does not change until the
	// next mark termination.
	heapMarked uint64

	// scanWork is the total scan work performed this cycle. This
	// is updated atomically during the cycle. Updates occur in
	// bounded batches, since it is both written and read
	// throughout the cycle. At the end of the cycle, this is how
	// much of the retained heap is scannable.
	//
	// Currently this is the bytes of heap scanned. For most uses,
	// this is an opaque unit of work, but for estimation the
	// definition is important.
	scanWork int64

	// bgScanCredit is the scan work credit accumulated by the
	// concurrent background scan. This credit is accumulated by
	// the background scan and stolen by mutator assists. This is
	// updated atomically. Updates occur in bounded batches, since
	// it is both written and read throughout the cycle.
	bgScanCredit int64

	// assistTime is the nanoseconds spent in mutator assists
	// during this cycle. This is updated atomically. Updates
	// occur in bounded batches, since it is both written and read
	// throughout the cycle.
	assistTime int64

	// dedicatedMarkTime is the nanoseconds spent in dedicated
	// mark workers during this cycle. This is updated atomically
	// at the end of the concurrent mark phase.
	dedicatedMarkTime int64

	// fractionalMarkTime is the nanoseconds spent in the
	// fractional mark worker during this cycle. This is updated
	// atomically throughout the cycle and will be up-to-date if
	// the fractional mark worker is not currently running.
	fractionalMarkTime int64

	// idleMarkTime is the nanoseconds spent in idle marking
	// during this cycle. This is updated atomically throughout
	// the cycle.
	idleMarkTime int64

	// markStartTime is the absolute start time in nanoseconds
	// that assists and background mark workers started.
	markStartTime int64

	// dedicatedMarkWorkersNeeded is the number of dedicated mark
	// workers that need to be started. This is computed at the
	// beginning of each cycle and decremented atomically as
	// dedicated mark workers get started.
	dedicatedMarkWorkersNeeded int64

	// assistWorkPerByte is the ratio of scan work to allocated
	// bytes that should be performed by mutator assists. This is
	// computed at the beginning of each cycle and updated every
	// time heapScan is updated.
	//
	// Stored as a uint64, but it's actually a float64. Use
	// float64frombits to get the value.
	//
	// Read and written atomically.
	assistWorkPerByte uint64

	// assistBytesPerWork is 1/assistWorkPerByte.
	//
	// Stored as a uint64, but it's actually a float64. Use
	// float64frombits to get the value.
	//
	// Read and written atomically.
	//
	// Note that because this is read and written independently
	// from assistWorkPerByte users may notice a skew between
	// the two values, and such a state should be safe.
	assistBytesPerWork uint64

	// fractionalUtilizationGoal is the fraction of wall clock
	// time that should be spent in the fractional mark worker on
	// each P that isn't running a dedicated worker.
	//
	// For example, if the utilization goal is 25% and there are
	// no dedicated workers, this will be 0.25. If the goal is
	// 25%, there is one dedicated worker, and GOMAXPROCS is 5,
	// this will be 0.05 to make up the missing 5%.
	//
	// If this is zero, no fractional workers are needed.
	fractionalUtilizationGoal float64

	_ cpu.CacheLinePad
}
```

#### <a id="gcControllerState.commit" href="#gcControllerState.commit">func (c *gcControllerState) commit(triggerRatio float64)</a>

```
searchKey: runtime.gcControllerState.commit
tags: [method private]
```

```Go
func (c *gcControllerState) commit(triggerRatio float64)
```

commit sets the trigger ratio and updates everything derived from it: the absolute trigger, the heap goal, mark pacing, and sweep pacing. 

This can be called any time. If GC is the in the middle of a concurrent phase, it will adjust the pacing of that phase. 

This depends on gcPercent, gcController.heapMarked, and gcController.heapLive. These must be up to date. 

mheap_.lock must be held or the world must be stopped. 

#### <a id="gcControllerState.effectiveGrowthRatio" href="#gcControllerState.effectiveGrowthRatio">func (c *gcControllerState) effectiveGrowthRatio() float64</a>

```
searchKey: runtime.gcControllerState.effectiveGrowthRatio
tags: [function private]
```

```Go
func (c *gcControllerState) effectiveGrowthRatio() float64
```

effectiveGrowthRatio returns the current effective heap growth ratio (GOGC/100) based on heapMarked from the previous GC and heapGoal for the current GC. 

This may differ from gcPercent/100 because of various upper and lower bounds on gcPercent. For example, if the heap is smaller than heapMinimum, this can be higher than gcPercent/100. 

mheap_.lock must be held or the world must be stopped. 

#### <a id="gcControllerState.endCycle" href="#gcControllerState.endCycle">func (c *gcControllerState) endCycle(userForced bool) float64</a>

```
searchKey: runtime.gcControllerState.endCycle
tags: [method private]
```

```Go
func (c *gcControllerState) endCycle(userForced bool) float64
```

endCycle computes the trigger ratio for the next cycle. userForced indicates whether the current GC cycle was forced by the application. 

#### <a id="gcControllerState.enlistWorker" href="#gcControllerState.enlistWorker">func (c *gcControllerState) enlistWorker()</a>

```
searchKey: runtime.gcControllerState.enlistWorker
tags: [function private]
```

```Go
func (c *gcControllerState) enlistWorker()
```

enlistWorker encourages another dedicated mark worker to start on another P if there are spare worker slots. It is used by putfull when more work is made available. 

#### <a id="gcControllerState.findRunnableGCWorker" href="#gcControllerState.findRunnableGCWorker">func (c *gcControllerState) findRunnableGCWorker(_p_ *p) *g</a>

```
searchKey: runtime.gcControllerState.findRunnableGCWorker
tags: [method private]
```

```Go
func (c *gcControllerState) findRunnableGCWorker(_p_ *p) *g
```

findRunnableGCWorker returns a background mark worker for _p_ if it should be run. This must only be called when gcBlackenEnabled != 0. 

#### <a id="gcControllerState.init.mgcpacer.go.0xc052b2af98" href="#gcControllerState.init.mgcpacer.go.0xc052b2af98">func (c *gcControllerState) init(gcPercent int32)</a>

```
searchKey: runtime.gcControllerState.init
tags: [method private]
```

```Go
func (c *gcControllerState) init(gcPercent int32)
```

#### <a id="gcControllerState.revise" href="#gcControllerState.revise">func (c *gcControllerState) revise()</a>

```
searchKey: runtime.gcControllerState.revise
tags: [function private]
```

```Go
func (c *gcControllerState) revise()
```

revise updates the assist ratio during the GC cycle to account for improved estimates. This should be called whenever gcController.heapScan, gcController.heapLive, or gcController.heapGoal is updated. It is safe to call concurrently, but it may race with other calls to revise. 

The result of this race is that the two assist ratio values may not line up or may be stale. In practice this is OK because the assist ratio moves slowly throughout a GC cycle, and the assist ratio is a best-effort heuristic anyway. Furthermore, no part of the heuristic depends on the two assist ratio values being exact reciprocals of one another, since the two values are used to convert values from different sources. 

The worst case result of this raciness is that we may miss a larger shift in the ratio (say, if we decide to pace more aggressively against the hard heap goal) but even this "hard goal" is best-effort (see #40460). The dedicated GC should ensure we don't exceed the hard goal by too much in the rare case we do exceed it. 

It should only be called when gcBlackenEnabled != 0 (because this is when assists are enabled and the necessary statistics are available). 

#### <a id="gcControllerState.setGCPercent" href="#gcControllerState.setGCPercent">func (c *gcControllerState) setGCPercent(in int32) int32</a>

```
searchKey: runtime.gcControllerState.setGCPercent
tags: [method private]
```

```Go
func (c *gcControllerState) setGCPercent(in int32) int32
```

setGCPercent updates gcPercent and all related pacer state. Returns the old value of gcPercent. 

The world must be stopped, or mheap_.lock must be held. 

#### <a id="gcControllerState.startCycle" href="#gcControllerState.startCycle">func (c *gcControllerState) startCycle()</a>

```
searchKey: runtime.gcControllerState.startCycle
tags: [function private]
```

```Go
func (c *gcControllerState) startCycle()
```

startCycle resets the GC controller's state and computes estimates for a new GC cycle. The caller must hold worldsema and the world must be stopped. 

### <a id="gcDrainFlags" href="#gcDrainFlags">type gcDrainFlags int</a>

```
searchKey: runtime.gcDrainFlags
tags: [number private]
```

```Go
type gcDrainFlags int
```

### <a id="gcMarkWorkerMode" href="#gcMarkWorkerMode">type gcMarkWorkerMode int</a>

```
searchKey: runtime.gcMarkWorkerMode
tags: [number private]
```

```Go
type gcMarkWorkerMode int
```

gcMarkWorkerMode represents the mode that a concurrent mark worker should operate in. 

Concurrent marking happens through four different mechanisms. One is mutator assists, which happen in response to allocations and are not scheduled. The other three are variations in the per-P mark workers and are distinguished by gcMarkWorkerMode. 

### <a id="gcMode" href="#gcMode">type gcMode int</a>

```
searchKey: runtime.gcMode
tags: [number private]
```

```Go
type gcMode int
```

gcMode indicates how concurrent a GC cycle should be. 

### <a id="gcTrigger" href="#gcTrigger">type gcTrigger struct</a>

```
searchKey: runtime.gcTrigger
tags: [struct private]
```

```Go
type gcTrigger struct {
	kind gcTriggerKind
	now  int64  // gcTriggerTime: current time
	n    uint32 // gcTriggerCycle: cycle number to start
}
```

A gcTrigger is a predicate for starting a GC cycle. Specifically, it is an exit condition for the _GCoff phase. 

#### <a id="gcTrigger.test" href="#gcTrigger.test">func (t gcTrigger) test() bool</a>

```
searchKey: runtime.gcTrigger.test
tags: [function private]
```

```Go
func (t gcTrigger) test() bool
```

test reports whether the trigger condition is satisfied, meaning that the exit condition for the _GCoff phase has been met. The exit condition should be tested when allocating. 

### <a id="gcTriggerKind" href="#gcTriggerKind">type gcTriggerKind int</a>

```
searchKey: runtime.gcTriggerKind
tags: [number private]
```

```Go
type gcTriggerKind int
```

### <a id="gcWork" href="#gcWork">type gcWork struct</a>

```
searchKey: runtime.gcWork
tags: [struct private]
```

```Go
type gcWork struct {
	// wbuf1 and wbuf2 are the primary and secondary work buffers.
	//
	// This can be thought of as a stack of both work buffers'
	// pointers concatenated. When we pop the last pointer, we
	// shift the stack up by one work buffer by bringing in a new
	// full buffer and discarding an empty one. When we fill both
	// buffers, we shift the stack down by one work buffer by
	// bringing in a new empty buffer and discarding a full one.
	// This way we have one buffer's worth of hysteresis, which
	// amortizes the cost of getting or putting a work buffer over
	// at least one buffer of work and reduces contention on the
	// global work lists.
	//
	// wbuf1 is always the buffer we're currently pushing to and
	// popping from and wbuf2 is the buffer that will be discarded
	// next.
	//
	// Invariant: Both wbuf1 and wbuf2 are nil or neither are.
	wbuf1, wbuf2 *workbuf

	// Bytes marked (blackened) on this gcWork. This is aggregated
	// into work.bytesMarked by dispose.
	bytesMarked uint64

	// Scan work performed on this gcWork. This is aggregated into
	// gcController by dispose and may also be flushed by callers.
	scanWork int64

	// flushedWork indicates that a non-empty work buffer was
	// flushed to the global work list since the last gcMarkDone
	// termination check. Specifically, this indicates that this
	// gcWork may have communicated work to another gcWork.
	flushedWork bool
}
```

A gcWork provides the interface to produce and consume work for the garbage collector. 

A gcWork can be used on the stack as follows: 

```
(preemption must be disabled)
gcw := &getg().m.p.ptr().gcw
.. call gcw.put() to produce and gcw.tryGet() to consume ..

```
It's important that any use of gcWork during the mark phase prevent the garbage collector from transitioning to mark termination since gcWork may locally hold GC work buffers. This can be done by disabling preemption (systemstack or acquirem). 

#### <a id="gcWork.balance" href="#gcWork.balance">func (w *gcWork) balance()</a>

```
searchKey: runtime.gcWork.balance
tags: [function private]
```

```Go
func (w *gcWork) balance()
```

balance moves some work that's cached in this gcWork back on the global queue. 

#### <a id="gcWork.dispose" href="#gcWork.dispose">func (w *gcWork) dispose()</a>

```
searchKey: runtime.gcWork.dispose
tags: [function private]
```

```Go
func (w *gcWork) dispose()
```

dispose returns any cached pointers to the global queue. The buffers are being put on the full queue so that the write barriers will not simply reacquire them before the GC can inspect them. This helps reduce the mutator's ability to hide pointers during the concurrent mark phase. 

#### <a id="gcWork.empty" href="#gcWork.empty">func (w *gcWork) empty() bool</a>

```
searchKey: runtime.gcWork.empty
tags: [function private]
```

```Go
func (w *gcWork) empty() bool
```

empty reports whether w has no mark work available. 

#### <a id="gcWork.init.mgcwork.go.0xc0530ddb50" href="#gcWork.init.mgcwork.go.0xc0530ddb50">func (w *gcWork) init()</a>

```
searchKey: runtime.gcWork.init
tags: [function private]
```

```Go
func (w *gcWork) init()
```

#### <a id="gcWork.put" href="#gcWork.put">func (w *gcWork) put(obj uintptr)</a>

```
searchKey: runtime.gcWork.put
tags: [method private]
```

```Go
func (w *gcWork) put(obj uintptr)
```

put enqueues a pointer for the garbage collector to trace. obj must point to the beginning of a heap object or an oblet. 

#### <a id="gcWork.putBatch" href="#gcWork.putBatch">func (w *gcWork) putBatch(obj []uintptr)</a>

```
searchKey: runtime.gcWork.putBatch
tags: [method private]
```

```Go
func (w *gcWork) putBatch(obj []uintptr)
```

putBatch performs a put on every pointer in obj. See put for constraints on these pointers. 

#### <a id="gcWork.putFast" href="#gcWork.putFast">func (w *gcWork) putFast(obj uintptr) bool</a>

```
searchKey: runtime.gcWork.putFast
tags: [method private]
```

```Go
func (w *gcWork) putFast(obj uintptr) bool
```

putFast does a put and reports whether it can be done quickly otherwise it returns false and the caller needs to call put. 

#### <a id="gcWork.tryGet" href="#gcWork.tryGet">func (w *gcWork) tryGet() uintptr</a>

```
searchKey: runtime.gcWork.tryGet
tags: [function private]
```

```Go
func (w *gcWork) tryGet() uintptr
```

tryGet dequeues a pointer for the garbage collector to trace. 

If there are no pointers remaining in this gcWork or in the global queue, tryGet returns 0.  Note that there may still be pointers in other gcWork instances or other caches. 

#### <a id="gcWork.tryGetFast" href="#gcWork.tryGetFast">func (w *gcWork) tryGetFast() uintptr</a>

```
searchKey: runtime.gcWork.tryGetFast
tags: [function private]
```

```Go
func (w *gcWork) tryGetFast() uintptr
```

tryGetFast dequeues a pointer for the garbage collector to trace if one is readily available. Otherwise it returns 0 and the caller is expected to call tryGet(). 

### <a id="gclink" href="#gclink">type gclink struct</a>

```
searchKey: runtime.gclink
tags: [struct private]
```

```Go
type gclink struct {
	next gclinkptr
}
```

A gclink is a node in a linked list of blocks, like mlink, but it is opaque to the garbage collector. The GC does not trace the pointers during collection, and the compiler does not emit write barriers for assignments of gclinkptr values. Code should store references to gclinks as gclinkptr, not as *gclink. 

### <a id="gclinkptr" href="#gclinkptr">type gclinkptr uintptr</a>

```
searchKey: runtime.gclinkptr
tags: [number private]
```

```Go
type gclinkptr uintptr
```

A gclinkptr is a pointer to a gclink, but it is opaque to the garbage collector. 

#### <a id="nextFreeFast" href="#nextFreeFast">func nextFreeFast(s *mspan) gclinkptr</a>

```
searchKey: runtime.nextFreeFast
tags: [method private]
```

```Go
func nextFreeFast(s *mspan) gclinkptr
```

nextFreeFast returns the next free object if one is quickly available. Otherwise it returns 0. 

#### <a id="stackpoolalloc" href="#stackpoolalloc">func stackpoolalloc(order uint8) gclinkptr</a>

```
searchKey: runtime.stackpoolalloc
tags: [method private]
```

```Go
func stackpoolalloc(order uint8) gclinkptr
```

Allocates a stack from the free pool. Must be called with stackpool[order].item.mu held. 

#### <a id="gclinkptr.ptr" href="#gclinkptr.ptr">func (p gclinkptr) ptr() *gclink</a>

```
searchKey: runtime.gclinkptr.ptr
tags: [function private]
```

```Go
func (p gclinkptr) ptr() *gclink
```

ptr returns the *gclink form of p. The result should be used for accessing fields, not stored in other data structures. 

### <a id="gobuf" href="#gobuf">type gobuf struct</a>

```
searchKey: runtime.gobuf
tags: [struct private]
```

```Go
type gobuf struct {
	// The offsets of sp, pc, and g are known to (hard-coded in) libmach.
	//
	// ctxt is unusual with respect to GC: it may be a
	// heap-allocated funcval, so GC needs to track it, but it
	// needs to be set and cleared from assembly, where it's
	// difficult to have write barriers. However, ctxt is really a
	// saved, live register, and we only ever exchange it between
	// the real register and the gobuf. Hence, we treat it as a
	// root during stack scanning, which means assembly that saves
	// and restores it doesn't need write barriers. It's still
	// typed as a pointer so that any other writes from Go get
	// write barriers.
	sp   uintptr
	pc   uintptr
	g    guintptr
	ctxt unsafe.Pointer
	ret  uintptr
	lr   uintptr
	bp   uintptr // for framepointer-enabled architectures
}
```

### <a id="gsignalStack" href="#gsignalStack">type gsignalStack struct</a>

```
searchKey: runtime.gsignalStack
tags: [struct private]
```

```Go
type gsignalStack struct {
	stack       stack
	stackguard0 uintptr
	stackguard1 uintptr
	stktopsp    uintptr
}
```

gsignalStack saves the fields of the gsignal stack changed by setGsignalStack. 

### <a id="guintptr" href="#guintptr">type guintptr uintptr</a>

```
searchKey: runtime.guintptr
tags: [number private]
```

```Go
type guintptr uintptr
```

A guintptr holds a goroutine pointer, but typed as a uintptr to bypass write barriers. It is used in the Gobuf goroutine state and in scheduling lists that are manipulated without a P. 

The Gobuf.g goroutine pointer is almost always updated by assembly code. In one of the few places it is updated by Go code - func save - it must be treated as a uintptr to avoid a write barrier being emitted at a bad time. Instead of figuring out how to emit the write barriers missing in the assembly manipulation, we change the type of the field to uintptr, so that it does not require write barriers at all. 

Goroutine structs are published in the allg list and never freed. That will keep the goroutine structs from being collected. There is never a time that Gobuf.g's contain the only references to a goroutine: the publishing of the goroutine in allg comes first. Goroutine pointers are also kept in non-GC-visible places like TLS, so I can't see them ever moving. If we did want to start moving data in the GC, we'd need to allocate the goroutine structs from an alternate arena. Using guintptr doesn't make that problem any worse. 

#### <a id="guintptr.cas" href="#guintptr.cas">func (gp *guintptr) cas(old, new guintptr) bool</a>

```
searchKey: runtime.guintptr.cas
tags: [method private]
```

```Go
func (gp *guintptr) cas(old, new guintptr) bool
```

#### <a id="guintptr.ptr" href="#guintptr.ptr">func (gp guintptr) ptr() *g</a>

```
searchKey: runtime.guintptr.ptr
tags: [function private]
```

```Go
func (gp guintptr) ptr() *g
```

#### <a id="guintptr.set" href="#guintptr.set">func (gp *guintptr) set(g *g)</a>

```
searchKey: runtime.guintptr.set
tags: [method private]
```

```Go
func (gp *guintptr) set(g *g)
```

### <a id="hchan" href="#hchan">type hchan struct</a>

```
searchKey: runtime.hchan
tags: [struct private]
```

```Go
type hchan struct {
	qcount   uint           // total data in the queue
	dataqsiz uint           // size of the circular queue
	buf      unsafe.Pointer // points to an array of dataqsiz elements
	elemsize uint16
	closed   uint32
	elemtype *_type // element type
	sendx    uint   // send index
	recvx    uint   // receive index
	recvq    waitq  // list of recv waiters
	sendq    waitq  // list of send waiters

	// lock protects all fields in hchan, as well as several
	// fields in sudogs blocked on this channel.
	//
	// Do not change another G's status while holding this lock
	// (in particular, do not ready a G), as this can deadlock
	// with stack shrinking.
	lock mutex
}
```

#### <a id="makechan" href="#makechan">func makechan(t *chantype, size int) *hchan</a>

```
searchKey: runtime.makechan
tags: [method private]
```

```Go
func makechan(t *chantype, size int) *hchan
```

#### <a id="makechan64" href="#makechan64">func makechan64(t *chantype, size int64) *hchan</a>

```
searchKey: runtime.makechan64
tags: [method private]
```

```Go
func makechan64(t *chantype, size int64) *hchan
```

#### <a id="reflect_makechan" href="#reflect_makechan">func reflect_makechan(t *chantype, size int) *hchan</a>

```
searchKey: runtime.reflect_makechan
tags: [method private]
```

```Go
func reflect_makechan(t *chantype, size int) *hchan
```

#### <a id="hchan.raceaddr" href="#hchan.raceaddr">func (c *hchan) raceaddr() unsafe.Pointer</a>

```
searchKey: runtime.hchan.raceaddr
tags: [function private]
```

```Go
func (c *hchan) raceaddr() unsafe.Pointer
```

#### <a id="hchan.sortkey" href="#hchan.sortkey">func (c *hchan) sortkey() uintptr</a>

```
searchKey: runtime.hchan.sortkey
tags: [function private]
```

```Go
func (c *hchan) sortkey() uintptr
```

### <a id="headTailIndex" href="#headTailIndex">type headTailIndex uint64</a>

```
searchKey: runtime.headTailIndex
tags: [number private]
```

```Go
type headTailIndex uint64
```

haidTailIndex represents a combined 32-bit head and 32-bit tail of a queue into a single 64-bit value. 

#### <a id="makeHeadTailIndex" href="#makeHeadTailIndex">func makeHeadTailIndex(head, tail uint32) headTailIndex</a>

```
searchKey: runtime.makeHeadTailIndex
tags: [method private]
```

```Go
func makeHeadTailIndex(head, tail uint32) headTailIndex
```

makeHeadTailIndex creates a headTailIndex value from a separate head and tail. 

#### <a id="headTailIndex.cas" href="#headTailIndex.cas">func (h *headTailIndex) cas(old, new headTailIndex) bool</a>

```
searchKey: runtime.headTailIndex.cas
tags: [method private]
```

```Go
func (h *headTailIndex) cas(old, new headTailIndex) bool
```

cas atomically compares-and-swaps a headTailIndex value. 

#### <a id="headTailIndex.decHead" href="#headTailIndex.decHead">func (h *headTailIndex) decHead() headTailIndex</a>

```
searchKey: runtime.headTailIndex.decHead
tags: [function private]
```

```Go
func (h *headTailIndex) decHead() headTailIndex
```

decHead atomically decrements the head of a headTailIndex. 

#### <a id="headTailIndex.head" href="#headTailIndex.head">func (h headTailIndex) head() uint32</a>

```
searchKey: runtime.headTailIndex.head
tags: [function private]
```

```Go
func (h headTailIndex) head() uint32
```

head returns the head of a headTailIndex value. 

#### <a id="headTailIndex.incHead" href="#headTailIndex.incHead">func (h *headTailIndex) incHead() headTailIndex</a>

```
searchKey: runtime.headTailIndex.incHead
tags: [function private]
```

```Go
func (h *headTailIndex) incHead() headTailIndex
```

incHead atomically increments the head of a headTailIndex. 

#### <a id="headTailIndex.incTail" href="#headTailIndex.incTail">func (h *headTailIndex) incTail() headTailIndex</a>

```
searchKey: runtime.headTailIndex.incTail
tags: [function private]
```

```Go
func (h *headTailIndex) incTail() headTailIndex
```

incTail atomically increments the tail of a headTailIndex. 

#### <a id="headTailIndex.load" href="#headTailIndex.load">func (h *headTailIndex) load() headTailIndex</a>

```
searchKey: runtime.headTailIndex.load
tags: [function private]
```

```Go
func (h *headTailIndex) load() headTailIndex
```

load atomically reads a headTailIndex value. 

#### <a id="headTailIndex.reset" href="#headTailIndex.reset">func (h *headTailIndex) reset()</a>

```
searchKey: runtime.headTailIndex.reset
tags: [function private]
```

```Go
func (h *headTailIndex) reset()
```

reset clears the headTailIndex to (0, 0). 

#### <a id="headTailIndex.split" href="#headTailIndex.split">func (h headTailIndex) split() (head uint32, tail uint32)</a>

```
searchKey: runtime.headTailIndex.split
tags: [function private]
```

```Go
func (h headTailIndex) split() (head uint32, tail uint32)
```

split splits the headTailIndex value into its parts. 

#### <a id="headTailIndex.tail" href="#headTailIndex.tail">func (h headTailIndex) tail() uint32</a>

```
searchKey: runtime.headTailIndex.tail
tags: [function private]
```

```Go
func (h headTailIndex) tail() uint32
```

tail returns the tail of a headTailIndex value. 

### <a id="heapArena" href="#heapArena">type heapArena struct</a>

```
searchKey: runtime.heapArena
tags: [struct private]
```

```Go
type heapArena struct {
	// bitmap stores the pointer/scalar bitmap for the words in
	// this arena. See mbitmap.go for a description. Use the
	// heapBits type to access this.
	bitmap [heapArenaBitmapBytes]byte

	// spans maps from virtual address page ID within this arena to *mspan.
	// For allocated spans, their pages map to the span itself.
	// For free spans, only the lowest and highest pages map to the span itself.
	// Internal pages map to an arbitrary span.
	// For pages that have never been allocated, spans entries are nil.
	//
	// Modifications are protected by mheap.lock. Reads can be
	// performed without locking, but ONLY from indexes that are
	// known to contain in-use or stack spans. This means there
	// must not be a safe-point between establishing that an
	// address is live and looking it up in the spans array.
	spans [pagesPerArena]*mspan

	// pageInUse is a bitmap that indicates which spans are in
	// state mSpanInUse. This bitmap is indexed by page number,
	// but only the bit corresponding to the first page in each
	// span is used.
	//
	// Reads and writes are atomic.
	pageInUse [pagesPerArena / 8]uint8

	// pageMarks is a bitmap that indicates which spans have any
	// marked objects on them. Like pageInUse, only the bit
	// corresponding to the first page in each span is used.
	//
	// Writes are done atomically during marking. Reads are
	// non-atomic and lock-free since they only occur during
	// sweeping (and hence never race with writes).
	//
	// This is used to quickly find whole spans that can be freed.
	//
	// TODO(austin): It would be nice if this was uint64 for
	// faster scanning, but we don't have 64-bit atomic bit
	// operations.
	pageMarks [pagesPerArena / 8]uint8

	// pageSpecials is a bitmap that indicates which spans have
	// specials (finalizers or other). Like pageInUse, only the bit
	// corresponding to the first page in each span is used.
	//
	// Writes are done atomically whenever a special is added to
	// a span and whenever the last special is removed from a span.
	// Reads are done atomically to find spans containing specials
	// during marking.
	pageSpecials [pagesPerArena / 8]uint8

	// checkmarks stores the debug.gccheckmark state. It is only
	// used if debug.gccheckmark > 0.
	checkmarks *checkmarksMap

	// zeroedBase marks the first byte of the first page in this
	// arena which hasn't been used yet and is therefore already
	// zero. zeroedBase is relative to the arena base.
	// Increases monotonically until it hits heapArenaBytes.
	//
	// This field is sufficient to determine if an allocation
	// needs to be zeroed because the page allocator follows an
	// address-ordered first-fit policy.
	//
	// Read atomically and written with an atomic CAS.
	zeroedBase uintptr
}
```

A heapArena stores metadata for a heap arena. heapArenas are stored outside of the Go heap and accessed via the mheap_.arenas index. 

#### <a id="pageIndexOf" href="#pageIndexOf">func pageIndexOf(p uintptr) (arena *heapArena, pageIdx uintptr, pageMask uint8)</a>

```
searchKey: runtime.pageIndexOf
tags: [method private]
```

```Go
func pageIndexOf(p uintptr) (arena *heapArena, pageIdx uintptr, pageMask uint8)
```

pageIndexOf returns the arena, page index, and page mask for pointer p. The caller must ensure p is in the heap. 

### <a id="heapBits" href="#heapBits">type heapBits struct</a>

```
searchKey: runtime.heapBits
tags: [struct private]
```

```Go
type heapBits struct {
	bitp  *uint8
	shift uint32
	arena uint32 // Index of heap arena containing bitp
	last  *uint8 // Last byte arena's bitmap
}
```

heapBits provides access to the bitmap bits for a single heap word. The methods on heapBits take value receivers so that the compiler can more easily inline calls to those methods and registerize the struct fields independently. 

#### <a id="heapBitsForAddr" href="#heapBitsForAddr">func heapBitsForAddr(addr uintptr) (h heapBits)</a>

```
searchKey: runtime.heapBitsForAddr
tags: [method private]
```

```Go
func heapBitsForAddr(addr uintptr) (h heapBits)
```

heapBitsForAddr returns the heapBits for the address addr. The caller must ensure addr is in an allocated span. In particular, be careful not to point past the end of an object. 

nosplit because it is used during write barriers and must not be preempted. 

#### <a id="heapBits.bits" href="#heapBits.bits">func (h heapBits) bits() uint32</a>

```
searchKey: runtime.heapBits.bits
tags: [function private]
```

```Go
func (h heapBits) bits() uint32
```

The caller can test morePointers and isPointer by &-ing with bitScan and bitPointer. The result includes in its higher bits the bits for subsequent words described by the same bitmap byte. 

nosplit because it is used during write barriers and must not be preempted. 

#### <a id="heapBits.forward" href="#heapBits.forward">func (h heapBits) forward(n uintptr) heapBits</a>

```
searchKey: runtime.heapBits.forward
tags: [method private]
```

```Go
func (h heapBits) forward(n uintptr) heapBits
```

forward returns the heapBits describing n pointer-sized words ahead of h in memory. That is, if h describes address p, h.forward(n) describes p+n*ptrSize. h.forward(1) is equivalent to h.next(), just slower. Note that forward does not modify h. The caller must record the result. bits returns the heap bits for the current word. 

#### <a id="heapBits.forwardOrBoundary" href="#heapBits.forwardOrBoundary">func (h heapBits) forwardOrBoundary(n uintptr) (heapBits, uintptr)</a>

```
searchKey: runtime.heapBits.forwardOrBoundary
tags: [method private]
```

```Go
func (h heapBits) forwardOrBoundary(n uintptr) (heapBits, uintptr)
```

forwardOrBoundary is like forward, but stops at boundaries between contiguous sections of the bitmap. It returns the number of words advanced over, which will be <= n. 

#### <a id="heapBits.initSpan" href="#heapBits.initSpan">func (h heapBits) initSpan(s *mspan)</a>

```
searchKey: runtime.heapBits.initSpan
tags: [method private]
```

```Go
func (h heapBits) initSpan(s *mspan)
```

initSpan initializes the heap bitmap for a span. If this is a span of pointer-sized objects, it initializes all words to pointer/scan. Otherwise, it initializes all words to scalar/dead. 

#### <a id="heapBits.isPointer" href="#heapBits.isPointer">func (h heapBits) isPointer() bool</a>

```
searchKey: runtime.heapBits.isPointer
tags: [function private]
```

```Go
func (h heapBits) isPointer() bool
```

isPointer reports whether the heap bits describe a pointer word. 

nosplit because it is used during write barriers and must not be preempted. 

#### <a id="heapBits.morePointers" href="#heapBits.morePointers">func (h heapBits) morePointers() bool</a>

```
searchKey: runtime.heapBits.morePointers
tags: [function private]
```

```Go
func (h heapBits) morePointers() bool
```

morePointers reports whether this word and all remaining words in this object are scalars. h must not describe the second word of the object. 

#### <a id="heapBits.next" href="#heapBits.next">func (h heapBits) next() heapBits</a>

```
searchKey: runtime.heapBits.next
tags: [function private]
```

```Go
func (h heapBits) next() heapBits
```

next returns the heapBits describing the next pointer-sized word in memory. That is, if h describes address p, h.next() describes p+ptrSize. Note that next does not modify h. The caller must record the result. 

nosplit because it is used during write barriers and must not be preempted. 

#### <a id="heapBits.nextArena" href="#heapBits.nextArena">func (h heapBits) nextArena() heapBits</a>

```
searchKey: runtime.heapBits.nextArena
tags: [function private]
```

```Go
func (h heapBits) nextArena() heapBits
```

nextArena advances h to the beginning of the next heap arena. 

This is a slow-path helper to next. gc's inliner knows that heapBits.next can be inlined even though it calls this. This is marked noinline so it doesn't get inlined into next and cause next to be too big to inline. 

### <a id="heapStatsAggregate" href="#heapStatsAggregate">type heapStatsAggregate struct</a>

```
searchKey: runtime.heapStatsAggregate
tags: [struct private]
```

```Go
type heapStatsAggregate struct {
	heapStatsDelta

	// inObjects is the bytes of memory occupied by objects,
	inObjects uint64

	// numObjects is the number of live objects in the heap.
	numObjects uint64

	// totalAllocated is the total bytes of heap objects allocated
	// over the lifetime of the program.
	totalAllocated uint64

	// totalFreed is the total bytes of heap objects freed
	// over the lifetime of the program.
	totalFreed uint64

	// totalAllocs is the number of heap objects allocated over
	// the lifetime of the program.
	totalAllocs uint64

	// totalFrees is the number of heap objects freed over
	// the lifetime of the program.
	totalFrees uint64
}
```

heapStatsAggregate represents memory stats obtained from the runtime. This set of stats is grouped together because they depend on each other in some way to make sense of the runtime's current heap memory use. They're also sharded across Ps, so it makes sense to grab them all at once. 

#### <a id="heapStatsAggregate.compute" href="#heapStatsAggregate.compute">func (a *heapStatsAggregate) compute()</a>

```
searchKey: runtime.heapStatsAggregate.compute
tags: [function private]
```

```Go
func (a *heapStatsAggregate) compute()
```

compute populates the heapStatsAggregate with values from the runtime. 

### <a id="heapStatsDelta" href="#heapStatsDelta">type heapStatsDelta struct</a>

```
searchKey: runtime.heapStatsDelta
tags: [struct private]
```

```Go
type heapStatsDelta struct {
	// Memory stats.
	committed       int64 // byte delta of memory committed
	released        int64 // byte delta of released memory generated
	inHeap          int64 // byte delta of memory placed in the heap
	inStacks        int64 // byte delta of memory reserved for stacks
	inWorkBufs      int64 // byte delta of memory reserved for work bufs
	inPtrScalarBits int64 // byte delta of memory reserved for unrolled GC prog bits

	// Allocator stats.
	tinyAllocCount  uintptr                  // number of tiny allocations
	largeAlloc      uintptr                  // bytes allocated for large objects
	largeAllocCount uintptr                  // number of large object allocations
	smallAllocCount [_NumSizeClasses]uintptr // number of allocs for small objects
	largeFree       uintptr                  // bytes freed for large objects (>maxSmallSize)
	largeFreeCount  uintptr                  // number of frees for large objects (>maxSmallSize)
	smallFreeCount  [_NumSizeClasses]uintptr // number of frees for small objects (<=maxSmallSize)

	// Add a uint32 to ensure this struct is a multiple of 8 bytes in size.
	// Only necessary on 32-bit platforms.
	_ [(sys.PtrSize / 4) % 2]uint32
}
```

heapStatsDelta contains deltas of various runtime memory statistics that need to be updated together in order for them to be kept consistent with one another. 

#### <a id="heapStatsDelta.merge" href="#heapStatsDelta.merge">func (a *heapStatsDelta) merge(b *heapStatsDelta)</a>

```
searchKey: runtime.heapStatsDelta.merge
tags: [method private]
```

```Go
func (a *heapStatsDelta) merge(b *heapStatsDelta)
```

merge adds in the deltas from b into a. 

### <a id="heldLockInfo" href="#heldLockInfo">type heldLockInfo struct</a>

```
searchKey: runtime.heldLockInfo
tags: [struct private]
```

```Go
type heldLockInfo struct {
	lockAddr uintptr
	rank     lockRank
}
```

heldLockInfo gives info on a held lock and the rank of that lock 

### <a id="hex" href="#hex">type hex uint64</a>

```
searchKey: runtime.hex
tags: [number private]
```

```Go
type hex uint64
```

The compiler knows that a print of a value of this type should use printhex instead of printuint (decimal). 

### <a id="hiter" href="#hiter">type hiter struct</a>

```
searchKey: runtime.hiter
tags: [struct private]
```

```Go
type hiter struct {
	key         unsafe.Pointer // Must be in first position.  Write nil to indicate iteration end (see cmd/compile/internal/walk/range.go).
	elem        unsafe.Pointer // Must be in second position (see cmd/compile/internal/walk/range.go).
	t           *maptype
	h           *hmap
	buckets     unsafe.Pointer // bucket ptr at hash_iter initialization time
	bptr        *bmap          // current bucket
	overflow    *[]*bmap       // keeps overflow buckets of hmap.buckets alive
	oldoverflow *[]*bmap       // keeps overflow buckets of hmap.oldbuckets alive
	startBucket uintptr        // bucket iteration started at
	offset      uint8          // intra-bucket offset to start from during iteration (should be big enough to hold bucketCnt-1)
	wrapped     bool           // already wrapped around from end of bucket array to beginning
	B           uint8
	i           uint8
	bucket      uintptr
	checkBucket uintptr
}
```

A hash iteration structure. If you modify hiter, also change cmd/compile/internal/reflectdata/reflect.go to indicate the layout of this structure. 

#### <a id="reflect_mapiterinit" href="#reflect_mapiterinit">func reflect_mapiterinit(t *maptype, h *hmap) *hiter</a>

```
searchKey: runtime.reflect_mapiterinit
tags: [method private]
```

```Go
func reflect_mapiterinit(t *maptype, h *hmap) *hiter
```

### <a id="hmap" href="#hmap">type hmap struct</a>

```
searchKey: runtime.hmap
tags: [struct private]
```

```Go
type hmap struct {
	// Note: the format of the hmap is also encoded in cmd/compile/internal/reflectdata/reflect.go.
	// Make sure this stays in sync with the compiler's definition.
	count     int // # live cells == size of map.  Must be first (used by len() builtin)
	flags     uint8
	B         uint8  // log_2 of # of buckets (can hold up to loadFactor * 2^B items)
	noverflow uint16 // approximate number of overflow buckets; see incrnoverflow for details
	hash0     uint32 // hash seed

	buckets    unsafe.Pointer // array of 2^B Buckets. may be nil if count==0.
	oldbuckets unsafe.Pointer // previous bucket array of half the size, non-nil only when growing
	nevacuate  uintptr        // progress counter for evacuation (buckets less than this have been evacuated)

	extra *mapextra // optional fields
}
```

A header for a Go map. 

#### <a id="makemap" href="#makemap">func makemap(t *maptype, hint int, h *hmap) *hmap</a>

```
searchKey: runtime.makemap
tags: [method private]
```

```Go
func makemap(t *maptype, hint int, h *hmap) *hmap
```

makemap implements Go map creation for make(map[k]v, hint). If the compiler has determined that the map or the first bucket can be created on the stack, h and/or bucket may be non-nil. If h != nil, the map can be created directly in h. If h.buckets != nil, bucket pointed to can be used as the first bucket. 

#### <a id="makemap64" href="#makemap64">func makemap64(t *maptype, hint int64, h *hmap) *hmap</a>

```
searchKey: runtime.makemap64
tags: [method private]
```

```Go
func makemap64(t *maptype, hint int64, h *hmap) *hmap
```

#### <a id="makemap_small" href="#makemap_small">func makemap_small() *hmap</a>

```
searchKey: runtime.makemap_small
tags: [function private]
```

```Go
func makemap_small() *hmap
```

makemap_small implements Go map creation for make(map[k]v) and make(map[k]v, hint) when hint is known to be at most bucketCnt at compile time and the map needs to be allocated on the heap. 

#### <a id="reflect_makemap" href="#reflect_makemap">func reflect_makemap(t *maptype, cap int) *hmap</a>

```
searchKey: runtime.reflect_makemap
tags: [method private]
```

```Go
func reflect_makemap(t *maptype, cap int) *hmap
```

#### <a id="hmap.createOverflow" href="#hmap.createOverflow">func (h *hmap) createOverflow()</a>

```
searchKey: runtime.hmap.createOverflow
tags: [function private]
```

```Go
func (h *hmap) createOverflow()
```

#### <a id="hmap.growing" href="#hmap.growing">func (h *hmap) growing() bool</a>

```
searchKey: runtime.hmap.growing
tags: [function private]
```

```Go
func (h *hmap) growing() bool
```

growing reports whether h is growing. The growth may be to the same size or bigger. 

#### <a id="hmap.incrnoverflow" href="#hmap.incrnoverflow">func (h *hmap) incrnoverflow()</a>

```
searchKey: runtime.hmap.incrnoverflow
tags: [function private]
```

```Go
func (h *hmap) incrnoverflow()
```

incrnoverflow increments h.noverflow. noverflow counts the number of overflow buckets. This is used to trigger same-size map growth. See also tooManyOverflowBuckets. To keep hmap small, noverflow is a uint16. When there are few buckets, noverflow is an exact count. When there are many buckets, noverflow is an approximate count. 

#### <a id="hmap.newoverflow" href="#hmap.newoverflow">func (h *hmap) newoverflow(t *maptype, b *bmap) *bmap</a>

```
searchKey: runtime.hmap.newoverflow
tags: [method private]
```

```Go
func (h *hmap) newoverflow(t *maptype, b *bmap) *bmap
```

#### <a id="hmap.noldbuckets" href="#hmap.noldbuckets">func (h *hmap) noldbuckets() uintptr</a>

```
searchKey: runtime.hmap.noldbuckets
tags: [function private]
```

```Go
func (h *hmap) noldbuckets() uintptr
```

noldbuckets calculates the number of buckets prior to the current map growth. 

#### <a id="hmap.oldbucketmask" href="#hmap.oldbucketmask">func (h *hmap) oldbucketmask() uintptr</a>

```
searchKey: runtime.hmap.oldbucketmask
tags: [function private]
```

```Go
func (h *hmap) oldbucketmask() uintptr
```

oldbucketmask provides a mask that can be applied to calculate n % noldbuckets(). 

#### <a id="hmap.sameSizeGrow" href="#hmap.sameSizeGrow">func (h *hmap) sameSizeGrow() bool</a>

```
searchKey: runtime.hmap.sameSizeGrow
tags: [function private]
```

```Go
func (h *hmap) sameSizeGrow() bool
```

sameSizeGrow reports whether the current growth is to a map of the same size. 

### <a id="iface" href="#iface">type iface struct</a>

```
searchKey: runtime.iface
tags: [struct private]
```

```Go
type iface struct {
	tab  *itab
	data unsafe.Pointer
}
```

#### <a id="assertE2I2" href="#assertE2I2">func assertE2I2(inter *interfacetype, e eface) (r iface)</a>

```
searchKey: runtime.assertE2I2
tags: [method private]
```

```Go
func assertE2I2(inter *interfacetype, e eface) (r iface)
```

#### <a id="assertI2I2" href="#assertI2I2">func assertI2I2(inter *interfacetype, i iface) (r iface)</a>

```
searchKey: runtime.assertI2I2
tags: [method private]
```

```Go
func assertI2I2(inter *interfacetype, i iface) (r iface)
```

#### <a id="convI2I" href="#convI2I">func convI2I(inter *interfacetype, i iface) (r iface)</a>

```
searchKey: runtime.convI2I
tags: [method private]
```

```Go
func convI2I(inter *interfacetype, i iface) (r iface)
```

#### <a id="convT2I" href="#convT2I">func convT2I(tab *itab, elem unsafe.Pointer) (i iface)</a>

```
searchKey: runtime.convT2I
tags: [method private]
```

```Go
func convT2I(tab *itab, elem unsafe.Pointer) (i iface)
```

#### <a id="convT2Inoptr" href="#convT2Inoptr">func convT2Inoptr(tab *itab, elem unsafe.Pointer) (i iface)</a>

```
searchKey: runtime.convT2Inoptr
tags: [method private]
```

```Go
func convT2Inoptr(tab *itab, elem unsafe.Pointer) (i iface)
```

### <a id="imethod" href="#imethod">type imethod struct</a>

```
searchKey: runtime.imethod
tags: [struct private]
```

```Go
type imethod struct {
	name nameOff
	ityp typeOff
}
```

### <a id="initTask" href="#initTask">type initTask struct</a>

```
searchKey: runtime.initTask
tags: [struct private]
```

```Go
type initTask struct {
	// TODO: pack the first 3 fields more tightly?
	state uintptr // 0 = uninitialized, 1 = in progress, 2 = done
	ndeps uintptr
	nfns  uintptr
}
```

An initTask represents the set of initializations that need to be done for a package. Keep in sync with ../../test/initempty.go:initTask 

### <a id="inlinedCall" href="#inlinedCall">type inlinedCall struct</a>

```
searchKey: runtime.inlinedCall
tags: [struct private]
```

```Go
type inlinedCall struct {
	parent   int16  // index of parent in the inltree, or < 0
	funcID   funcID // type of the called function
	_        byte
	file     int32 // perCU file index for inlined call. See cmd/link:pcln.go
	line     int32 // line number of the call site
	func_    int32 // offset into pclntab for name of called function
	parentPc int32 // position of an instruction whose source position is the call site (offset from entry)
}
```

inlinedCall is the encoding of entries in the FUNCDATA_InlTree table. 

### <a id="interfacetype" href="#interfacetype">type interfacetype struct</a>

```
searchKey: runtime.interfacetype
tags: [struct private]
```

```Go
type interfacetype struct {
	typ     _type
	pkgpath name
	mhdr    []imethod
}
```

### <a id="itab" href="#itab">type itab struct</a>

```
searchKey: runtime.itab
tags: [struct private]
```

```Go
type itab struct {
	inter *interfacetype
	_type *_type
	hash  uint32 // copy of _type.hash. Used for type switches.
	_     [4]byte
	fun   [1]uintptr // variable sized. fun[0]==0 means _type does not implement inter.
}
```

layout of Itab known to compilers allocated in non-garbage-collected memory Needs to be in sync with ../cmd/compile/internal/gc/reflect.go:/^func.WriteTabs. 

#### <a id="assertE2I" href="#assertE2I">func assertE2I(inter *interfacetype, t *_type) *itab</a>

```
searchKey: runtime.assertE2I
tags: [method private]
```

```Go
func assertE2I(inter *interfacetype, t *_type) *itab
```

#### <a id="assertI2I" href="#assertI2I">func assertI2I(inter *interfacetype, tab *itab) *itab</a>

```
searchKey: runtime.assertI2I
tags: [method private]
```

```Go
func assertI2I(inter *interfacetype, tab *itab) *itab
```

#### <a id="getitab" href="#getitab">func getitab(inter *interfacetype, typ *_type, canfail bool) *itab</a>

```
searchKey: runtime.getitab
tags: [method private]
```

```Go
func getitab(inter *interfacetype, typ *_type, canfail bool) *itab
```

#### <a id="itab.init.iface.go" href="#itab.init.iface.go">func (m *itab) init() string</a>

```
searchKey: runtime.itab.init
tags: [function private]
```

```Go
func (m *itab) init() string
```

init fills in the m.fun array with all the code pointers for the m.inter/m._type pair. If the type does not implement the interface, it sets m.fun[0] to 0 and returns the name of an interface function that is missing. It is ok to call this multiple times on the same m, even concurrently. 

### <a id="itabTableType" href="#itabTableType">type itabTableType struct</a>

```
searchKey: runtime.itabTableType
tags: [struct private]
```

```Go
type itabTableType struct {
	size    uintptr             // length of entries array. Always a power of 2.
	count   uintptr             // current number of filled entries.
	entries [itabInitSize]*itab // really [size] large
}
```

Note: change the formula in the mallocgc call in itabAdd if you change these fields. 

#### <a id="itabTableType.add" href="#itabTableType.add">func (t *itabTableType) add(m *itab)</a>

```
searchKey: runtime.itabTableType.add
tags: [method private]
```

```Go
func (t *itabTableType) add(m *itab)
```

add adds the given itab to itab table t. itabLock must be held. 

#### <a id="itabTableType.find" href="#itabTableType.find">func (t *itabTableType) find(inter *interfacetype, typ *_type) *itab</a>

```
searchKey: runtime.itabTableType.find
tags: [method private]
```

```Go
func (t *itabTableType) find(inter *interfacetype, typ *_type) *itab
```

find finds the given interface/type pair in t. Returns nil if the given interface/type pair isn't present. 

### <a id="itimerval" href="#itimerval">type itimerval struct</a>

```
searchKey: runtime.itimerval
tags: [struct private]
```

```Go
type itimerval struct {
	it_interval timeval
	it_value    timeval
}
```

### <a id="keventt" href="#keventt">type keventt struct</a>

```
searchKey: runtime.keventt
tags: [struct private]
```

```Go
type keventt struct {
	ident  uint64
	filter int16
	flags  uint16
	fflags uint32
	data   int64
	udata  *byte
}
```

### <a id="lfnode" href="#lfnode">type lfnode struct</a>

```
searchKey: runtime.lfnode
tags: [struct private]
```

```Go
type lfnode struct {
	next    uint64
	pushcnt uintptr
}
```

Lock-free stack node. Also known to export_test.go. 

#### <a id="lfstackUnpack" href="#lfstackUnpack">func lfstackUnpack(val uint64) *lfnode</a>

```
searchKey: runtime.lfstackUnpack
tags: [method private]
```

```Go
func lfstackUnpack(val uint64) *lfnode
```

### <a id="lfstack" href="#lfstack">type lfstack uint64</a>

```
searchKey: runtime.lfstack
tags: [number private]
```

```Go
type lfstack uint64
```

lfstack is the head of a lock-free stack. 

The zero value of lfstack is an empty list. 

This stack is intrusive. Nodes must embed lfnode as the first field. 

The stack does not keep GC-visible pointers to nodes, so the caller is responsible for ensuring the nodes are not garbage collected (typically by allocating them from manually-managed memory). 

#### <a id="lfstack.empty" href="#lfstack.empty">func (head *lfstack) empty() bool</a>

```
searchKey: runtime.lfstack.empty
tags: [function private]
```

```Go
func (head *lfstack) empty() bool
```

#### <a id="lfstack.pop" href="#lfstack.pop">func (head *lfstack) pop() unsafe.Pointer</a>

```
searchKey: runtime.lfstack.pop
tags: [function private]
```

```Go
func (head *lfstack) pop() unsafe.Pointer
```

#### <a id="lfstack.push" href="#lfstack.push">func (head *lfstack) push(node *lfnode)</a>

```
searchKey: runtime.lfstack.push
tags: [method private]
```

```Go
func (head *lfstack) push(node *lfnode)
```

### <a id="libcall" href="#libcall">type libcall struct</a>

```
searchKey: runtime.libcall
tags: [struct private]
```

```Go
type libcall struct {
	fn   uintptr
	n    uintptr // number of parameters
	args uintptr // parameters
	r1   uintptr // return values
	r2   uintptr
	err  uintptr // error number
}
```

### <a id="linearAlloc" href="#linearAlloc">type linearAlloc struct</a>

```
searchKey: runtime.linearAlloc
tags: [struct private]
```

```Go
type linearAlloc struct {
	next   uintptr // next free byte
	mapped uintptr // one byte past end of mapped space
	end    uintptr // end of reserved space

	mapMemory bool // transition memory from Reserved to Ready if true
}
```

linearAlloc is a simple linear allocator that pre-reserves a region of memory and then optionally maps that region into the Ready state as needed. 

The caller is responsible for locking. 

#### <a id="linearAlloc.alloc" href="#linearAlloc.alloc">func (l *linearAlloc) alloc(size, align uintptr, sysStat *sysMemStat) unsafe.Pointer</a>

```
searchKey: runtime.linearAlloc.alloc
tags: [method private]
```

```Go
func (l *linearAlloc) alloc(size, align uintptr, sysStat *sysMemStat) unsafe.Pointer
```

#### <a id="linearAlloc.init.malloc.go" href="#linearAlloc.init.malloc.go">func (l *linearAlloc) init(base, size uintptr, mapMemory bool)</a>

```
searchKey: runtime.linearAlloc.init
tags: [method private]
```

```Go
func (l *linearAlloc) init(base, size uintptr, mapMemory bool)
```

### <a id="lockRank" href="#lockRank">type lockRank int</a>

```
searchKey: runtime.lockRank
tags: [number private]
```

```Go
type lockRank int
```

#### <a id="getLockRank" href="#getLockRank">func getLockRank(l *mutex) lockRank</a>

```
searchKey: runtime.getLockRank
tags: [method private]
```

```Go
func getLockRank(l *mutex) lockRank
```

#### <a id="lockRank.String" href="#lockRank.String">func (rank lockRank) String() string</a>

```
searchKey: runtime.lockRank.String
tags: [function private]
```

```Go
func (rank lockRank) String() string
```

### <a id="lockRankStruct" href="#lockRankStruct">type lockRankStruct struct{}</a>

```
searchKey: runtime.lockRankStruct
tags: [struct private]
```

```Go
type lockRankStruct struct {
}
```

// lockRankStruct is embedded in mutex, but is empty when staticklockranking is disabled (the default) 

### <a id="m" href="#m">type m struct</a>

```
searchKey: runtime.m
tags: [struct private]
```

```Go
type m struct {
	g0      *g     // goroutine with scheduling stack
	morebuf gobuf  // gobuf arg to morestack
	divmod  uint32 // div/mod denominator for arm - known to liblink

	// Fields not known to debuggers.
	procid        uint64            // for debuggers, but offset not hard-coded
	gsignal       *g                // signal-handling g
	goSigStack    gsignalStack      // Go-allocated signal handling stack
	sigmask       sigset            // storage for saved signal mask
	tls           [tlsSlots]uintptr // thread-local storage (for x86 extern register)
	mstartfn      func()
	curg          *g       // current running goroutine
	caughtsig     guintptr // goroutine running during fatal signal
	p             puintptr // attached p for executing go code (nil if not executing go code)
	nextp         puintptr
	oldp          puintptr // the p that was attached before executing a syscall
	id            int64
	mallocing     int32
	throwing      int32
	preemptoff    string // if != "", keep curg running on this m
	locks         int32
	dying         int32
	profilehz     int32
	spinning      bool // m is out of work and is actively looking for work
	blocked       bool // m is blocked on a note
	newSigstack   bool // minit on C thread called sigaltstack
	printlock     int8
	incgo         bool   // m is executing a cgo call
	freeWait      uint32 // if == 0, safe to free g0 and delete m (atomic)
	fastrand      [2]uint32
	needextram    bool
	traceback     uint8
	ncgocall      uint64      // number of cgo calls in total
	ncgo          int32       // number of cgo calls currently in progress
	cgoCallersUse uint32      // if non-zero, cgoCallers in use temporarily
	cgoCallers    *cgoCallers // cgo traceback if crashing in cgo call
	doesPark      bool        // non-P running threads: sysmon and newmHandoff never use .park
	park          note
	alllink       *m // on allm
	schedlink     muintptr
	lockedg       guintptr
	createstack   [32]uintptr // stack that created this thread.
	lockedExt     uint32      // tracking for external LockOSThread
	lockedInt     uint32      // tracking for internal lockOSThread
	nextwaitm     muintptr    // next m waiting for lock
	waitunlockf   func(*g, unsafe.Pointer) bool
	waitlock      unsafe.Pointer
	waittraceev   byte
	waittraceskip int
	startingtrace bool
	syscalltick   uint32
	freelink      *m // on sched.freem

	// mFixup is used to synchronize OS related m state
	// (credentials etc) use mutex to access. To avoid deadlocks
	// an atomic.Load() of used being zero in mDoFixupFn()
	// guarantees fn is nil.
	mFixup struct {
		lock mutex
		used uint32
		fn   func(bool) bool
	}

	// these are here because they are too large to be on the stack
	// of low-level NOSPLIT functions.
	libcall   libcall
	libcallpc uintptr // for cpu profiler
	libcallsp uintptr
	libcallg  guintptr
	syscall   libcall // stores syscall parameters on windows

	vdsoSP uintptr // SP for traceback while in VDSO call (0 if not in call)
	vdsoPC uintptr // PC for traceback while in VDSO call

	// preemptGen counts the number of completed preemption
	// signals. This is used to detect when a preemption is
	// requested, but fails. Accessed atomically.
	preemptGen uint32

	// Whether this is a pending preemption signal on this M.
	// Accessed atomically.
	signalPending uint32

	dlogPerM

	mOS

	// Up to 10 locks held by this m, maintained by the lock ranking code.
	locksHeldLen int
	locksHeld    [10]heldLockInfo
}
```

### <a id="mOS" href="#mOS">type mOS struct</a>

```
searchKey: runtime.mOS
tags: [struct private]
```

```Go
type mOS struct {
	initialized bool
	mutex       pthreadmutex
	cond        pthreadcond
	count       int
}
```

### <a id="mSpanList" href="#mSpanList">type mSpanList struct</a>

```
searchKey: runtime.mSpanList
tags: [struct private]
```

```Go
type mSpanList struct {
	first *mspan // first span in list, or nil if none
	last  *mspan // last span in list, or nil if none
}
```

mSpanList heads a linked list of spans. 

#### <a id="mSpanList.init.mheap.go.0xc053387380" href="#mSpanList.init.mheap.go.0xc053387380">func (list *mSpanList) init()</a>

```
searchKey: runtime.mSpanList.init
tags: [function private]
```

```Go
func (list *mSpanList) init()
```

Initialize an empty doubly-linked list. 

#### <a id="mSpanList.insert" href="#mSpanList.insert">func (list *mSpanList) insert(span *mspan)</a>

```
searchKey: runtime.mSpanList.insert
tags: [method private]
```

```Go
func (list *mSpanList) insert(span *mspan)
```

#### <a id="mSpanList.insertBack" href="#mSpanList.insertBack">func (list *mSpanList) insertBack(span *mspan)</a>

```
searchKey: runtime.mSpanList.insertBack
tags: [method private]
```

```Go
func (list *mSpanList) insertBack(span *mspan)
```

#### <a id="mSpanList.isEmpty" href="#mSpanList.isEmpty">func (list *mSpanList) isEmpty() bool</a>

```
searchKey: runtime.mSpanList.isEmpty
tags: [function private]
```

```Go
func (list *mSpanList) isEmpty() bool
```

#### <a id="mSpanList.remove" href="#mSpanList.remove">func (list *mSpanList) remove(span *mspan)</a>

```
searchKey: runtime.mSpanList.remove
tags: [method private]
```

```Go
func (list *mSpanList) remove(span *mspan)
```

#### <a id="mSpanList.takeAll" href="#mSpanList.takeAll">func (list *mSpanList) takeAll(other *mSpanList)</a>

```
searchKey: runtime.mSpanList.takeAll
tags: [method private]
```

```Go
func (list *mSpanList) takeAll(other *mSpanList)
```

takeAll removes all spans from other and inserts them at the front of list. 

### <a id="mSpanState" href="#mSpanState">type mSpanState uint8</a>

```
searchKey: runtime.mSpanState
tags: [number private]
```

```Go
type mSpanState uint8
```

An mspan representing actual memory has state mSpanInUse, mSpanManual, or mSpanFree. Transitions between these states are constrained as follows: 

* A span may transition from free to in-use or manual during any GC 

```
phase.

```
* During sweeping (gcphase == _GCoff), a span may transition from 

```
in-use to free (as a result of sweeping) or manual to free (as a
result of stacks being freed).

```
* During GC (gcphase != _GCoff), a span *must not* transition from 

```
manual or in-use to free. Because concurrent GC may read a pointer
and then look up its span, the span state must be monotonic.

```
Setting mspan.state to mSpanInUse or mSpanManual must be done atomically and only after all other span fields are valid. Likewise, if inspecting a span is contingent on it being mSpanInUse, the state should be loaded atomically and checked before depending on other fields. This allows the garbage collector to safely deal with potentially invalid pointers, since resolving such pointers may race with a span being allocated. 

### <a id="mSpanStateBox" href="#mSpanStateBox">type mSpanStateBox struct</a>

```
searchKey: runtime.mSpanStateBox
tags: [struct private]
```

```Go
type mSpanStateBox struct {
	s mSpanState
}
```

mSpanStateBox holds an mSpanState and provides atomic operations on it. This is a separate type to disallow accidental comparison or assignment with mSpanState. 

#### <a id="mSpanStateBox.get" href="#mSpanStateBox.get">func (b *mSpanStateBox) get() mSpanState</a>

```
searchKey: runtime.mSpanStateBox.get
tags: [function private]
```

```Go
func (b *mSpanStateBox) get() mSpanState
```

#### <a id="mSpanStateBox.set" href="#mSpanStateBox.set">func (b *mSpanStateBox) set(s mSpanState)</a>

```
searchKey: runtime.mSpanStateBox.set
tags: [method private]
```

```Go
func (b *mSpanStateBox) set(s mSpanState)
```

### <a id="machTimebaseInfo" href="#machTimebaseInfo">type machTimebaseInfo struct</a>

```
searchKey: runtime.machTimebaseInfo
tags: [struct private]
```

```Go
type machTimebaseInfo struct {
	numer uint32
	denom uint32
}
```

### <a id="mapextra" href="#mapextra">type mapextra struct</a>

```
searchKey: runtime.mapextra
tags: [struct private]
```

```Go
type mapextra struct {
	// If both key and elem do not contain pointers and are inline, then we mark bucket
	// type as containing no pointers. This avoids scanning such maps.
	// However, bmap.overflow is a pointer. In order to keep overflow buckets
	// alive, we store pointers to all overflow buckets in hmap.extra.overflow and hmap.extra.oldoverflow.
	// overflow and oldoverflow are only used if key and elem do not contain pointers.
	// overflow contains overflow buckets for hmap.buckets.
	// oldoverflow contains overflow buckets for hmap.oldbuckets.
	// The indirection allows to store a pointer to the slice in hiter.
	overflow    *[]*bmap
	oldoverflow *[]*bmap

	// nextOverflow holds a pointer to a free overflow bucket.
	nextOverflow *bmap
}
```

mapextra holds fields that are not present on all maps. 

### <a id="maptype" href="#maptype">type maptype struct</a>

```
searchKey: runtime.maptype
tags: [struct private]
```

```Go
type maptype struct {
	typ    _type
	key    *_type
	elem   *_type
	bucket *_type // internal type representing a hash bucket
	// function for hashing keys (ptr to key, seed) -> hash
	hasher     func(unsafe.Pointer, uintptr) uintptr
	keysize    uint8  // size of key slot
	elemsize   uint8  // size of elem slot
	bucketsize uint16 // size of bucket
	flags      uint32
}
```

#### <a id="maptype.hashMightPanic" href="#maptype.hashMightPanic">func (mt *maptype) hashMightPanic() bool</a>

```
searchKey: runtime.maptype.hashMightPanic
tags: [function private]
```

```Go
func (mt *maptype) hashMightPanic() bool
```

#### <a id="maptype.indirectelem" href="#maptype.indirectelem">func (mt *maptype) indirectelem() bool</a>

```
searchKey: runtime.maptype.indirectelem
tags: [function private]
```

```Go
func (mt *maptype) indirectelem() bool
```

#### <a id="maptype.indirectkey" href="#maptype.indirectkey">func (mt *maptype) indirectkey() bool</a>

```
searchKey: runtime.maptype.indirectkey
tags: [function private]
```

```Go
func (mt *maptype) indirectkey() bool
```

Note: flag values must match those used in the TMAP case in ../cmd/compile/internal/gc/reflect.go:writeType. 

#### <a id="maptype.needkeyupdate" href="#maptype.needkeyupdate">func (mt *maptype) needkeyupdate() bool</a>

```
searchKey: runtime.maptype.needkeyupdate
tags: [function private]
```

```Go
func (mt *maptype) needkeyupdate() bool
```

#### <a id="maptype.reflexivekey" href="#maptype.reflexivekey">func (mt *maptype) reflexivekey() bool</a>

```
searchKey: runtime.maptype.reflexivekey
tags: [function private]
```

```Go
func (mt *maptype) reflexivekey() bool
```

### <a id="markBits" href="#markBits">type markBits struct</a>

```
searchKey: runtime.markBits
tags: [struct private]
```

```Go
type markBits struct {
	bytep *uint8
	mask  uint8
	index uintptr
}
```

markBits provides access to the mark bit for an object in the heap. bytep points to the byte holding the mark bit. mask is a byte with a single bit set that can be &ed with *bytep to see if the bit has been set. *m.byte&m.mask != 0 indicates the mark bit is set. index can be used along with span information to generate the address of the object in the heap. We maintain one set of mark bits for allocation and one for marking purposes. 

#### <a id="markBitsForAddr" href="#markBitsForAddr">func markBitsForAddr(p uintptr) markBits</a>

```
searchKey: runtime.markBitsForAddr
tags: [method private]
```

```Go
func markBitsForAddr(p uintptr) markBits
```

#### <a id="markBitsForSpan" href="#markBitsForSpan">func markBitsForSpan(base uintptr) (mbits markBits)</a>

```
searchKey: runtime.markBitsForSpan
tags: [method private]
```

```Go
func markBitsForSpan(base uintptr) (mbits markBits)
```

markBitsForSpan returns the markBits for the span base address base. 

#### <a id="markBits.advance" href="#markBits.advance">func (m *markBits) advance()</a>

```
searchKey: runtime.markBits.advance
tags: [function private]
```

```Go
func (m *markBits) advance()
```

advance advances the markBits to the next object in the span. 

#### <a id="markBits.clearMarked" href="#markBits.clearMarked">func (m markBits) clearMarked()</a>

```
searchKey: runtime.markBits.clearMarked
tags: [function private]
```

```Go
func (m markBits) clearMarked()
```

clearMarked clears the marked bit in the markbits, atomically. 

#### <a id="markBits.isMarked" href="#markBits.isMarked">func (m markBits) isMarked() bool</a>

```
searchKey: runtime.markBits.isMarked
tags: [function private]
```

```Go
func (m markBits) isMarked() bool
```

isMarked reports whether mark bit m is set. 

#### <a id="markBits.setMarked" href="#markBits.setMarked">func (m markBits) setMarked()</a>

```
searchKey: runtime.markBits.setMarked
tags: [function private]
```

```Go
func (m markBits) setMarked()
```

setMarked sets the marked bit in the markbits, atomically. 

#### <a id="markBits.setMarkedNonAtomic" href="#markBits.setMarkedNonAtomic">func (m markBits) setMarkedNonAtomic()</a>

```
searchKey: runtime.markBits.setMarkedNonAtomic
tags: [function private]
```

```Go
func (m markBits) setMarkedNonAtomic()
```

setMarkedNonAtomic sets the marked bit in the markbits, non-atomically. 

### <a id="mcache" href="#mcache">type mcache struct</a>

```
searchKey: runtime.mcache
tags: [struct private]
```

```Go
type mcache struct {
	// The following members are accessed on every malloc,
	// so they are grouped here for better caching.
	nextSample uintptr // trigger heap sample after allocating this many bytes
	scanAlloc  uintptr // bytes of scannable heap allocated

	// tiny points to the beginning of the current tiny block, or
	// nil if there is no current tiny block.
	//
	// tiny is a heap pointer. Since mcache is in non-GC'd memory,
	// we handle it by clearing it in releaseAll during mark
	// termination.
	//
	// tinyAllocs is the number of tiny allocations performed
	// by the P that owns this mcache.
	tiny       uintptr
	tinyoffset uintptr
	tinyAllocs uintptr

	alloc [numSpanClasses]*mspan // spans to allocate from, indexed by spanClass

	stackcache [_NumStackOrders]stackfreelist

	// flushGen indicates the sweepgen during which this mcache
	// was last flushed. If flushGen != mheap_.sweepgen, the spans
	// in this mcache are stale and need to the flushed so they
	// can be swept. This is done in acquirep.
	flushGen uint32
}
```

Per-thread (in Go, per-P) cache for small objects. This includes a small object cache and local allocation stats. No locking needed because it is per-thread (per-P). 

mcaches are allocated from non-GC'd memory, so any heap pointers must be specially handled. 

#### <a id="allocmcache" href="#allocmcache">func allocmcache() *mcache</a>

```
searchKey: runtime.allocmcache
tags: [function private]
```

```Go
func allocmcache() *mcache
```

#### <a id="getMCache" href="#getMCache">func getMCache() *mcache</a>

```
searchKey: runtime.getMCache
tags: [function private]
```

```Go
func getMCache() *mcache
```

getMCache is a convenience function which tries to obtain an mcache. 

Returns nil if we're not bootstrapping or we don't have a P. The caller's P must not change, so we must be in a non-preemptible state. 

#### <a id="mcache.allocLarge" href="#mcache.allocLarge">func (c *mcache) allocLarge(size uintptr, needzero bool, noscan bool) (*mspan, bool)</a>

```
searchKey: runtime.mcache.allocLarge
tags: [method private]
```

```Go
func (c *mcache) allocLarge(size uintptr, needzero bool, noscan bool) (*mspan, bool)
```

allocLarge allocates a span for a large object. The boolean result indicates whether the span is known-zeroed. If it did not need to be zeroed, it may not have been zeroed; but if it came directly from the OS, it is already zeroed. 

#### <a id="mcache.nextFree" href="#mcache.nextFree">func (c *mcache) nextFree(spc spanClass) (v gclinkptr, s *mspan, shouldhelpgc bool)</a>

```
searchKey: runtime.mcache.nextFree
tags: [method private]
```

```Go
func (c *mcache) nextFree(spc spanClass) (v gclinkptr, s *mspan, shouldhelpgc bool)
```

nextFree returns the next free object from the cached span if one is available. Otherwise it refills the cache with a span with an available object and returns that object along with a flag indicating that this was a heavy weight allocation. If it is a heavy weight allocation the caller must determine whether a new GC cycle needs to be started or if the GC is active whether this goroutine needs to assist the GC. 

Must run in a non-preemptible context since otherwise the owner of c could change. 

#### <a id="mcache.prepareForSweep" href="#mcache.prepareForSweep">func (c *mcache) prepareForSweep()</a>

```
searchKey: runtime.mcache.prepareForSweep
tags: [function private]
```

```Go
func (c *mcache) prepareForSweep()
```

prepareForSweep flushes c if the system has entered a new sweep phase since c was populated. This must happen between the sweep phase starting and the first allocation from c. 

#### <a id="mcache.refill" href="#mcache.refill">func (c *mcache) refill(spc spanClass)</a>

```
searchKey: runtime.mcache.refill
tags: [method private]
```

```Go
func (c *mcache) refill(spc spanClass)
```

refill acquires a new span of span class spc for c. This span will have at least one free object. The current span in c must be full. 

Must run in a non-preemptible context since otherwise the owner of c could change. 

#### <a id="mcache.releaseAll" href="#mcache.releaseAll">func (c *mcache) releaseAll()</a>

```
searchKey: runtime.mcache.releaseAll
tags: [function private]
```

```Go
func (c *mcache) releaseAll()
```

### <a id="mcentral" href="#mcentral">type mcentral struct</a>

```
searchKey: runtime.mcentral
tags: [struct private]
```

```Go
type mcentral struct {
	spanclass spanClass

	// partial and full contain two mspan sets: one of swept in-use
	// spans, and one of unswept in-use spans. These two trade
	// roles on each GC cycle. The unswept set is drained either by
	// allocation or by the background sweeper in every GC cycle,
	// so only two roles are necessary.
	//
	// sweepgen is increased by 2 on each GC cycle, so the swept
	// spans are in partial[sweepgen/2%2] and the unswept spans are in
	// partial[1-sweepgen/2%2]. Sweeping pops spans from the
	// unswept set and pushes spans that are still in-use on the
	// swept set. Likewise, allocating an in-use span pushes it
	// on the swept set.
	//
	// Some parts of the sweeper can sweep arbitrary spans, and hence
	// can't remove them from the unswept set, but will add the span
	// to the appropriate swept list. As a result, the parts of the
	// sweeper and mcentral that do consume from the unswept list may
	// encounter swept spans, and these should be ignored.
	partial [2]spanSet // list of spans with a free object
	full    [2]spanSet // list of spans with no free objects
}
```

Central list of free objects of a given size. 

#### <a id="mcentral.cacheSpan" href="#mcentral.cacheSpan">func (c *mcentral) cacheSpan() *mspan</a>

```
searchKey: runtime.mcentral.cacheSpan
tags: [function private]
```

```Go
func (c *mcentral) cacheSpan() *mspan
```

Allocate a span to use in an mcache. 

#### <a id="mcentral.fullSwept" href="#mcentral.fullSwept">func (c *mcentral) fullSwept(sweepgen uint32) *spanSet</a>

```
searchKey: runtime.mcentral.fullSwept
tags: [method private]
```

```Go
func (c *mcentral) fullSwept(sweepgen uint32) *spanSet
```

fullSwept returns the spanSet which holds swept spans without any free slots for this sweepgen. 

#### <a id="mcentral.fullUnswept" href="#mcentral.fullUnswept">func (c *mcentral) fullUnswept(sweepgen uint32) *spanSet</a>

```
searchKey: runtime.mcentral.fullUnswept
tags: [method private]
```

```Go
func (c *mcentral) fullUnswept(sweepgen uint32) *spanSet
```

fullUnswept returns the spanSet which holds unswept spans without any free slots for this sweepgen. 

#### <a id="mcentral.grow" href="#mcentral.grow">func (c *mcentral) grow() *mspan</a>

```
searchKey: runtime.mcentral.grow
tags: [function private]
```

```Go
func (c *mcentral) grow() *mspan
```

grow allocates a new empty span from the heap and initializes it for c's size class. 

#### <a id="mcentral.init.mcentral.go" href="#mcentral.init.mcentral.go">func (c *mcentral) init(spc spanClass)</a>

```
searchKey: runtime.mcentral.init
tags: [method private]
```

```Go
func (c *mcentral) init(spc spanClass)
```

Initialize a single central free list. 

#### <a id="mcentral.partialSwept" href="#mcentral.partialSwept">func (c *mcentral) partialSwept(sweepgen uint32) *spanSet</a>

```
searchKey: runtime.mcentral.partialSwept
tags: [method private]
```

```Go
func (c *mcentral) partialSwept(sweepgen uint32) *spanSet
```

partialSwept returns the spanSet which holds partially-filled swept spans for this sweepgen. 

#### <a id="mcentral.partialUnswept" href="#mcentral.partialUnswept">func (c *mcentral) partialUnswept(sweepgen uint32) *spanSet</a>

```
searchKey: runtime.mcentral.partialUnswept
tags: [method private]
```

```Go
func (c *mcentral) partialUnswept(sweepgen uint32) *spanSet
```

partialUnswept returns the spanSet which holds partially-filled unswept spans for this sweepgen. 

#### <a id="mcentral.uncacheSpan" href="#mcentral.uncacheSpan">func (c *mcentral) uncacheSpan(s *mspan)</a>

```
searchKey: runtime.mcentral.uncacheSpan
tags: [method private]
```

```Go
func (c *mcentral) uncacheSpan(s *mspan)
```

Return span from an mcache. 

s must have a span class corresponding to this mcentral and it must not be empty. 

### <a id="mcontext32" href="#mcontext32">type mcontext32 struct</a>

```
searchKey: runtime.mcontext32
tags: [struct private]
```

```Go
type mcontext32 struct {
	es exceptionstate32
	ss regs32
	fs floatstate32
}
```

### <a id="mcontext64" href="#mcontext64">type mcontext64 struct</a>

```
searchKey: runtime.mcontext64
tags: [struct private]
```

```Go
type mcontext64 struct {
	es        exceptionstate64
	ss        regs64
	fs        floatstate64
	pad_cgo_0 [4]byte
}
```

### <a id="memRecord" href="#memRecord">type memRecord struct</a>

```
searchKey: runtime.memRecord
tags: [struct private]
```

```Go
type memRecord struct {

	// active is the currently published profile. A profiling
	// cycle can be accumulated into active once its complete.
	active memRecordCycle

	// future records the profile events we're counting for cycles
	// that have not yet been published. This is ring buffer
	// indexed by the global heap profile cycle C and stores
	// cycles C, C+1, and C+2. Unlike active, these counts are
	// only for a single cycle; they are not cumulative across
	// cycles.
	//
	// We store cycle C here because there's a window between when
	// C becomes the active cycle and when we've flushed it to
	// active.
	future [3]memRecordCycle
}
```

A memRecord is the bucket data for a bucket of type memProfile, part of the memory profile. 

### <a id="memRecordCycle" href="#memRecordCycle">type memRecordCycle struct</a>

```
searchKey: runtime.memRecordCycle
tags: [struct private]
```

```Go
type memRecordCycle struct {
	allocs, frees           uintptr
	alloc_bytes, free_bytes uintptr
}
```

memRecordCycle 

#### <a id="memRecordCycle.add" href="#memRecordCycle.add">func (a *memRecordCycle) add(b *memRecordCycle)</a>

```
searchKey: runtime.memRecordCycle.add
tags: [method private]
```

```Go
func (a *memRecordCycle) add(b *memRecordCycle)
```

add accumulates b into a. It does not zero b. 

### <a id="method" href="#method">type method struct</a>

```
searchKey: runtime.method
tags: [struct private]
```

```Go
type method struct {
	name nameOff
	mtyp typeOff
	ifn  textOff
	tfn  textOff
}
```

### <a id="metricData" href="#metricData">type metricData struct</a>

```
searchKey: runtime.metricData
tags: [struct private]
```

```Go
type metricData struct {
	// deps is the set of runtime statistics that this metric
	// depends on. Before compute is called, the statAggregate
	// which will be passed must ensure() these dependencies.
	deps statDepSet

	// compute is a function that populates a metricValue
	// given a populated statAggregate structure.
	compute func(in *statAggregate, out *metricValue)
}
```

### <a id="metricFloat64Histogram" href="#metricFloat64Histogram">type metricFloat64Histogram struct</a>

```
searchKey: runtime.metricFloat64Histogram
tags: [struct private]
```

```Go
type metricFloat64Histogram struct {
	counts  []uint64
	buckets []float64
}
```

metricFloat64Histogram is a runtime copy of runtime/metrics.Float64Histogram and must be kept structurally identical to that type. 

### <a id="metricKind" href="#metricKind">type metricKind int</a>

```
searchKey: runtime.metricKind
tags: [number private]
```

```Go
type metricKind int
```

metricValidKind is a runtime copy of runtime/metrics.ValueKind and must be kept structurally identical to that type. 

### <a id="metricSample" href="#metricSample">type metricSample struct</a>

```
searchKey: runtime.metricSample
tags: [struct private]
```

```Go
type metricSample struct {
	name  string
	value metricValue
}
```

metricSample is a runtime copy of runtime/metrics.Sample and must be kept structurally identical to that type. 

### <a id="metricValue" href="#metricValue">type metricValue struct</a>

```
searchKey: runtime.metricValue
tags: [struct private]
```

```Go
type metricValue struct {
	kind    metricKind
	scalar  uint64         // contains scalar values for scalar Kinds.
	pointer unsafe.Pointer // contains non-scalar values.
}
```

metricValue is a runtime copy of runtime/metrics.Sample and must be kept structurally identical to that type. 

#### <a id="metricValue.float64HistOrInit" href="#metricValue.float64HistOrInit">func (v *metricValue) float64HistOrInit(buckets []float64) *metricFloat64Histogram</a>

```
searchKey: runtime.metricValue.float64HistOrInit
tags: [method private]
```

```Go
func (v *metricValue) float64HistOrInit(buckets []float64) *metricFloat64Histogram
```

float64HistOrInit tries to pull out an existing float64Histogram from the value, but if none exists, then it allocates one with the given buckets. 

### <a id="mheap" href="#mheap">type mheap struct</a>

```
searchKey: runtime.mheap
tags: [struct private]
```

```Go
type mheap struct {
	// lock must only be acquired on the system stack, otherwise a g
	// could self-deadlock if its stack grows with the lock held.
	lock  mutex
	pages pageAlloc // page allocation data structure

	sweepgen     uint32 // sweep generation, see comment in mspan; written during STW
	sweepDrained uint32 // all spans are swept or are being swept
	sweepers     uint32 // number of active sweepone calls

	// allspans is a slice of all mspans ever created. Each mspan
	// appears exactly once.
	//
	// The memory for allspans is manually managed and can be
	// reallocated and move as the heap grows.
	//
	// In general, allspans is protected by mheap_.lock, which
	// prevents concurrent access as well as freeing the backing
	// store. Accesses during STW might not hold the lock, but
	// must ensure that allocation cannot happen around the
	// access (since that may free the backing store).
	allspans []*mspan // all spans out there

	_ uint32 // align uint64 fields on 32-bit for atomics

	// Proportional sweep
	//
	// These parameters represent a linear function from gcController.heapLive
	// to page sweep count. The proportional sweep system works to
	// stay in the black by keeping the current page sweep count
	// above this line at the current gcController.heapLive.
	//
	// The line has slope sweepPagesPerByte and passes through a
	// basis point at (sweepHeapLiveBasis, pagesSweptBasis). At
	// any given time, the system is at (gcController.heapLive,
	// pagesSwept) in this space.
	//
	// It's important that the line pass through a point we
	// control rather than simply starting at a (0,0) origin
	// because that lets us adjust sweep pacing at any time while
	// accounting for current progress. If we could only adjust
	// the slope, it would create a discontinuity in debt if any
	// progress has already been made.
	pagesInUse         uint64  // pages of spans in stats mSpanInUse; updated atomically
	pagesSwept         uint64  // pages swept this cycle; updated atomically
	pagesSweptBasis    uint64  // pagesSwept to use as the origin of the sweep ratio; updated atomically
	sweepHeapLiveBasis uint64  // value of gcController.heapLive to use as the origin of sweep ratio; written with lock, read without
	sweepPagesPerByte  float64 // proportional sweep ratio; written with lock, read without

	// scavengeGoal is the amount of total retained heap memory (measured by
	// heapRetained) that the runtime will try to maintain by returning memory
	// to the OS.
	scavengeGoal uint64

	// reclaimIndex is the page index in allArenas of next page to
	// reclaim. Specifically, it refers to page (i %
	// pagesPerArena) of arena allArenas[i / pagesPerArena].
	//
	// If this is >= 1<<63, the page reclaimer is done scanning
	// the page marks.
	//
	// This is accessed atomically.
	reclaimIndex uint64
	// reclaimCredit is spare credit for extra pages swept. Since
	// the page reclaimer works in large chunks, it may reclaim
	// more than requested. Any spare pages released go to this
	// credit pool.
	//
	// This is accessed atomically.
	reclaimCredit uintptr

	// arenas is the heap arena map. It points to the metadata for
	// the heap for every arena frame of the entire usable virtual
	// address space.
	//
	// Use arenaIndex to compute indexes into this array.
	//
	// For regions of the address space that are not backed by the
	// Go heap, the arena map contains nil.
	//
	// Modifications are protected by mheap_.lock. Reads can be
	// performed without locking; however, a given entry can
	// transition from nil to non-nil at any time when the lock
	// isn't held. (Entries never transitions back to nil.)
	//
	// In general, this is a two-level mapping consisting of an L1
	// map and possibly many L2 maps. This saves space when there
	// are a huge number of arena frames. However, on many
	// platforms (even 64-bit), arenaL1Bits is 0, making this
	// effectively a single-level map. In this case, arenas[0]
	// will never be nil.
	arenas [1 << arenaL1Bits]*[1 << arenaL2Bits]*heapArena

	// heapArenaAlloc is pre-reserved space for allocating heapArena
	// objects. This is only used on 32-bit, where we pre-reserve
	// this space to avoid interleaving it with the heap itself.
	heapArenaAlloc linearAlloc

	// arenaHints is a list of addresses at which to attempt to
	// add more heap arenas. This is initially populated with a
	// set of general hint addresses, and grown with the bounds of
	// actual heap arena ranges.
	arenaHints *arenaHint

	// arena is a pre-reserved space for allocating heap arenas
	// (the actual arenas). This is only used on 32-bit.
	arena linearAlloc

	// allArenas is the arenaIndex of every mapped arena. This can
	// be used to iterate through the address space.
	//
	// Access is protected by mheap_.lock. However, since this is
	// append-only and old backing arrays are never freed, it is
	// safe to acquire mheap_.lock, copy the slice header, and
	// then release mheap_.lock.
	allArenas []arenaIdx

	// sweepArenas is a snapshot of allArenas taken at the
	// beginning of the sweep cycle. This can be read safely by
	// simply blocking GC (by disabling preemption).
	sweepArenas []arenaIdx

	// markArenas is a snapshot of allArenas taken at the beginning
	// of the mark cycle. Because allArenas is append-only, neither
	// this slice nor its contents will change during the mark, so
	// it can be read safely.
	markArenas []arenaIdx

	// curArena is the arena that the heap is currently growing
	// into. This should always be physPageSize-aligned.
	curArena struct {
		base, end uintptr
	}

	_ uint32 // ensure 64-bit alignment of central

	// central free lists for small size classes.
	// the padding makes sure that the mcentrals are
	// spaced CacheLinePadSize bytes apart, so that each mcentral.lock
	// gets its own cache line.
	// central is indexed by spanClass.
	central [numSpanClasses]struct {
		mcentral mcentral
		pad      [cpu.CacheLinePadSize - unsafe.Sizeof(mcentral{})%cpu.CacheLinePadSize]byte
	}

	spanalloc             fixalloc // allocator for span*
	cachealloc            fixalloc // allocator for mcache*
	specialfinalizeralloc fixalloc // allocator for specialfinalizer*
	specialprofilealloc   fixalloc // allocator for specialprofile*
	specialReachableAlloc fixalloc // allocator for specialReachable
	speciallock           mutex    // lock for special record allocators.
	arenaHintAlloc        fixalloc // allocator for arenaHints

	unused *specialfinalizer // never set, just here to force the specialfinalizer type into DWARF
}
```

Main malloc heap. The heap itself is the "free" and "scav" treaps, but all the other global data is here too. 

mheap must not be heap-allocated because it contains mSpanLists, which must not be heap-allocated. 

#### <a id="mheap.alloc" href="#mheap.alloc">func (h *mheap) alloc(npages uintptr, spanclass spanClass, needzero bool) (*mspan, bool)</a>

```
searchKey: runtime.mheap.alloc
tags: [method private]
```

```Go
func (h *mheap) alloc(npages uintptr, spanclass spanClass, needzero bool) (*mspan, bool)
```

alloc allocates a new span of npage pages from the GC'd heap. 

spanclass indicates the span's size class and scannability. 

If needzero is true, the memory for the returned span will be zeroed. The boolean returned indicates whether the returned span contains zeroes, either because this was requested, or because it was already zeroed. 

#### <a id="mheap.allocMSpanLocked" href="#mheap.allocMSpanLocked">func (h *mheap) allocMSpanLocked() *mspan</a>

```
searchKey: runtime.mheap.allocMSpanLocked
tags: [function private]
```

```Go
func (h *mheap) allocMSpanLocked() *mspan
```

allocMSpanLocked allocates an mspan object. 

h.lock must be held. 

allocMSpanLocked must be called on the system stack because its caller holds the heap lock. See mheap for details. Running on the system stack also ensures that we won't switch Ps during this function. See tryAllocMSpan for details. 

#### <a id="mheap.allocManual" href="#mheap.allocManual">func (h *mheap) allocManual(npages uintptr, typ spanAllocType) *mspan</a>

```
searchKey: runtime.mheap.allocManual
tags: [method private]
```

```Go
func (h *mheap) allocManual(npages uintptr, typ spanAllocType) *mspan
```

allocManual allocates a manually-managed span of npage pages. allocManual returns nil if allocation fails. 

allocManual adds the bytes used to *stat, which should be a memstats in-use field. Unlike allocations in the GC'd heap, the allocation does *not* count toward heap_inuse or heap_sys. 

The memory backing the returned span may not be zeroed if span.needzero is set. 

allocManual must be called on the system stack because it may acquire the heap lock via allocSpan. See mheap for details. 

If new code is written to call allocManual, do NOT use an existing spanAllocType value and instead declare a new one. 

#### <a id="mheap.allocNeedsZero" href="#mheap.allocNeedsZero">func (h *mheap) allocNeedsZero(base, npage uintptr) (needZero bool)</a>

```
searchKey: runtime.mheap.allocNeedsZero
tags: [method private]
```

```Go
func (h *mheap) allocNeedsZero(base, npage uintptr) (needZero bool)
```

allocNeedsZero checks if the region of address space [base, base+npage*pageSize), assumed to be allocated, needs to be zeroed, updating heap arena metadata for future allocations. 

This must be called each time pages are allocated from the heap, even if the page allocator can otherwise prove the memory it's allocating is already zero because they're fresh from the operating system. It updates heapArena metadata that is critical for future page allocations. 

There are no locking constraints on this method. 

#### <a id="mheap.allocSpan" href="#mheap.allocSpan">func (h *mheap) allocSpan(npages uintptr, typ spanAllocType, spanclass spanClass) (s *mspan)</a>

```
searchKey: runtime.mheap.allocSpan
tags: [method private]
```

```Go
func (h *mheap) allocSpan(npages uintptr, typ spanAllocType, spanclass spanClass) (s *mspan)
```

allocSpan allocates an mspan which owns npages worth of memory. 

If typ.manual() == false, allocSpan allocates a heap span of class spanclass and updates heap accounting. If manual == true, allocSpan allocates a manually-managed span (spanclass is ignored), and the caller is responsible for any accounting related to its use of the span. Either way, allocSpan will atomically add the bytes in the newly allocated span to *sysStat. 

The returned span is fully initialized. 

h.lock must not be held. 

allocSpan must be called on the system stack both because it acquires the heap lock and because it must block GC transitions. 

#### <a id="mheap.freeMSpanLocked" href="#mheap.freeMSpanLocked">func (h *mheap) freeMSpanLocked(s *mspan)</a>

```
searchKey: runtime.mheap.freeMSpanLocked
tags: [method private]
```

```Go
func (h *mheap) freeMSpanLocked(s *mspan)
```

freeMSpanLocked free an mspan object. 

h.lock must be held. 

freeMSpanLocked must be called on the system stack because its caller holds the heap lock. See mheap for details. Running on the system stack also ensures that we won't switch Ps during this function. See tryAllocMSpan for details. 

#### <a id="mheap.freeManual" href="#mheap.freeManual">func (h *mheap) freeManual(s *mspan, typ spanAllocType)</a>

```
searchKey: runtime.mheap.freeManual
tags: [method private]
```

```Go
func (h *mheap) freeManual(s *mspan, typ spanAllocType)
```

freeManual frees a manually-managed span returned by allocManual. typ must be the same as the spanAllocType passed to the allocManual that allocated s. 

This must only be called when gcphase == _GCoff. See mSpanState for an explanation. 

freeManual must be called on the system stack because it acquires the heap lock. See mheap for details. 

#### <a id="mheap.freeSpan" href="#mheap.freeSpan">func (h *mheap) freeSpan(s *mspan)</a>

```
searchKey: runtime.mheap.freeSpan
tags: [method private]
```

```Go
func (h *mheap) freeSpan(s *mspan)
```

Free the span back into the heap. 

#### <a id="mheap.freeSpanLocked" href="#mheap.freeSpanLocked">func (h *mheap) freeSpanLocked(s *mspan, typ spanAllocType)</a>

```
searchKey: runtime.mheap.freeSpanLocked
tags: [method private]
```

```Go
func (h *mheap) freeSpanLocked(s *mspan, typ spanAllocType)
```

#### <a id="mheap.grow" href="#mheap.grow">func (h *mheap) grow(npage uintptr) bool</a>

```
searchKey: runtime.mheap.grow
tags: [method private]
```

```Go
func (h *mheap) grow(npage uintptr) bool
```

Try to add at least npage pages of memory to the heap, returning whether it worked. 

h.lock must be held. 

#### <a id="mheap.init.mheap.go" href="#mheap.init.mheap.go">func (h *mheap) init()</a>

```
searchKey: runtime.mheap.init
tags: [function private]
```

```Go
func (h *mheap) init()
```

Initialize the heap. 

#### <a id="mheap.nextSpanForSweep" href="#mheap.nextSpanForSweep">func (h *mheap) nextSpanForSweep() *mspan</a>

```
searchKey: runtime.mheap.nextSpanForSweep
tags: [function private]
```

```Go
func (h *mheap) nextSpanForSweep() *mspan
```

nextSpanForSweep finds and pops the next span for sweeping from the central sweep buffers. It returns ownership of the span to the caller. Returns nil if no such span exists. 

#### <a id="mheap.reclaim" href="#mheap.reclaim">func (h *mheap) reclaim(npage uintptr)</a>

```
searchKey: runtime.mheap.reclaim
tags: [method private]
```

```Go
func (h *mheap) reclaim(npage uintptr)
```

reclaim sweeps and reclaims at least npage pages into the heap. It is called before allocating npage pages to keep growth in check. 

reclaim implements the page-reclaimer half of the sweeper. 

h.lock must NOT be held. 

#### <a id="mheap.reclaimChunk" href="#mheap.reclaimChunk">func (h *mheap) reclaimChunk(arenas []arenaIdx, pageIdx, n uintptr) uintptr</a>

```
searchKey: runtime.mheap.reclaimChunk
tags: [method private]
```

```Go
func (h *mheap) reclaimChunk(arenas []arenaIdx, pageIdx, n uintptr) uintptr
```

reclaimChunk sweeps unmarked spans that start at page indexes [pageIdx, pageIdx+n). It returns the number of pages returned to the heap. 

h.lock must be held and the caller must be non-preemptible. Note: h.lock may be temporarily unlocked and re-locked in order to do sweeping or if tracing is enabled. 

#### <a id="mheap.scavengeAll" href="#mheap.scavengeAll">func (h *mheap) scavengeAll()</a>

```
searchKey: runtime.mheap.scavengeAll
tags: [function private]
```

```Go
func (h *mheap) scavengeAll()
```

scavengeAll acquires the heap lock (blocking any additional manipulation of the page allocator) and iterates over the whole heap, scavenging every free page available. 

#### <a id="mheap.setSpans" href="#mheap.setSpans">func (h *mheap) setSpans(base, npage uintptr, s *mspan)</a>

```
searchKey: runtime.mheap.setSpans
tags: [method private]
```

```Go
func (h *mheap) setSpans(base, npage uintptr, s *mspan)
```

setSpans modifies the span map so [spanOf(base), spanOf(base+npage*pageSize)) is s. 

#### <a id="mheap.sysAlloc" href="#mheap.sysAlloc">func (h *mheap) sysAlloc(n uintptr) (v unsafe.Pointer, size uintptr)</a>

```
searchKey: runtime.mheap.sysAlloc
tags: [method private]
```

```Go
func (h *mheap) sysAlloc(n uintptr) (v unsafe.Pointer, size uintptr)
```

sysAlloc allocates heap arena space for at least n bytes. The returned pointer is always heapArenaBytes-aligned and backed by h.arenas metadata. The returned size is always a multiple of heapArenaBytes. sysAlloc returns nil on failure. There is no corresponding free function. 

sysAlloc returns a memory region in the Reserved state. This region must be transitioned to Prepared and then Ready before use. 

h must be locked. 

#### <a id="mheap.tryAllocMSpan" href="#mheap.tryAllocMSpan">func (h *mheap) tryAllocMSpan() *mspan</a>

```
searchKey: runtime.mheap.tryAllocMSpan
tags: [function private]
```

```Go
func (h *mheap) tryAllocMSpan() *mspan
```

tryAllocMSpan attempts to allocate an mspan object from the P-local cache, but may fail. 

h.lock need not be held. 

This caller must ensure that its P won't change underneath it during this function. Currently to ensure that we enforce that the function is run on the system stack, because that's the only place it is used now. In the future, this requirement may be relaxed if its use is necessary elsewhere. 

### <a id="mlink" href="#mlink">type mlink struct</a>

```
searchKey: runtime.mlink
tags: [struct private]
```

```Go
type mlink struct {
	next *mlink
}
```

A generic linked list of blocks.  (Typically the block is bigger than sizeof(MLink).) Since assignments to mlink.next will result in a write barrier being performed this cannot be used by some of the internal GC structures. For example when the sweeper is placing an unmarked object on the free list it does not want the write barrier to be called since that could result in the object being reachable. 

### <a id="moduledata" href="#moduledata">type moduledata struct</a>

```
searchKey: runtime.moduledata
tags: [struct private]
```

```Go
type moduledata struct {
	pcHeader     *pcHeader
	funcnametab  []byte
	cutab        []uint32
	filetab      []byte
	pctab        []byte
	pclntable    []byte
	ftab         []functab
	findfunctab  uintptr
	minpc, maxpc uintptr

	text, etext           uintptr
	noptrdata, enoptrdata uintptr
	data, edata           uintptr
	bss, ebss             uintptr
	noptrbss, enoptrbss   uintptr
	end, gcdata, gcbss    uintptr
	types, etypes         uintptr

	textsectmap []textsect
	typelinks   []int32 // offsets from types
	itablinks   []*itab

	ptab []ptabEntry

	pluginpath string
	pkghashes  []modulehash

	modulename   string
	modulehashes []modulehash

	hasmain uint8 // 1 if module contains the main function, 0 otherwise

	gcdatamask, gcbssmask bitvector

	typemap map[typeOff]*_type // offset to *_rtype in previous module

	bad bool // module failed to load and should be ignored

	next *moduledata
}
```

moduledata records information about the layout of the executable image. It is written by the linker. Any changes here must be matched changes to the code in cmd/internal/ld/symtab.go:symtab. moduledata is stored in statically allocated non-pointer memory; none of the pointers here are visible to the garbage collector. 

#### <a id="findmoduledatap" href="#findmoduledatap">func findmoduledatap(pc uintptr) *moduledata</a>

```
searchKey: runtime.findmoduledatap
tags: [method private]
```

```Go
func findmoduledatap(pc uintptr) *moduledata
```

findmoduledatap looks up the moduledata for a PC. 

It is nosplit because it's part of the isgoexception implementation. 

### <a id="modulehash" href="#modulehash">type modulehash struct</a>

```
searchKey: runtime.modulehash
tags: [struct private]
```

```Go
type modulehash struct {
	modulename   string
	linktimehash string
	runtimehash  *string
}
```

A modulehash is used to compare the ABI of a new module or a package in a new module with the loaded program. 

For each shared library a module links against, the linker creates an entry in the moduledata.modulehashes slice containing the name of the module, the abi hash seen at link time and a pointer to the runtime abi hash. These are checked in moduledataverify1 below. 

For each loaded plugin, the pkghashes slice has a modulehash of the newly loaded package that can be used to check the plugin's version of a package against any previously loaded version of the package. This is done in plugin.lastmoduleinit. 

### <a id="mspan" href="#mspan">type mspan struct</a>

```
searchKey: runtime.mspan
tags: [struct private]
```

```Go
type mspan struct {
	next *mspan     // next span in list, or nil if none
	prev *mspan     // previous span in list, or nil if none
	list *mSpanList // For debugging. TODO: Remove.

	startAddr uintptr // address of first byte of span aka s.base()
	npages    uintptr // number of pages in span

	manualFreeList gclinkptr // list of free objects in mSpanManual spans

	// freeindex is the slot index between 0 and nelems at which to begin scanning
	// for the next free object in this span.
	// Each allocation scans allocBits starting at freeindex until it encounters a 0
	// indicating a free object. freeindex is then adjusted so that subsequent scans begin
	// just past the newly discovered free object.
	//
	// If freeindex == nelem, this span has no free objects.
	//
	// allocBits is a bitmap of objects in this span.
	// If n >= freeindex and allocBits[n/8] & (1<<(n%8)) is 0
	// then object n is free;
	// otherwise, object n is allocated. Bits starting at nelem are
	// undefined and should never be referenced.
	//
	// Object n starts at address n*elemsize + (start << pageShift).
	freeindex uintptr
	// TODO: Look up nelems from sizeclass and remove this field if it
	// helps performance.
	nelems uintptr // number of object in the span.

	// Cache of the allocBits at freeindex. allocCache is shifted
	// such that the lowest bit corresponds to the bit freeindex.
	// allocCache holds the complement of allocBits, thus allowing
	// ctz (count trailing zero) to use it directly.
	// allocCache may contain bits beyond s.nelems; the caller must ignore
	// these.
	allocCache uint64

	// allocBits and gcmarkBits hold pointers to a span's mark and
	// allocation bits. The pointers are 8 byte aligned.
	// There are three arenas where this data is held.
	// free: Dirty arenas that are no longer accessed
	//       and can be reused.
	// next: Holds information to be used in the next GC cycle.
	// current: Information being used during this GC cycle.
	// previous: Information being used during the last GC cycle.
	// A new GC cycle starts with the call to finishsweep_m.
	// finishsweep_m moves the previous arena to the free arena,
	// the current arena to the previous arena, and
	// the next arena to the current arena.
	// The next arena is populated as the spans request
	// memory to hold gcmarkBits for the next GC cycle as well
	// as allocBits for newly allocated spans.
	//
	// The pointer arithmetic is done "by hand" instead of using
	// arrays to avoid bounds checks along critical performance
	// paths.
	// The sweep will free the old allocBits and set allocBits to the
	// gcmarkBits. The gcmarkBits are replaced with a fresh zeroed
	// out memory.
	allocBits  *gcBits
	gcmarkBits *gcBits

	sweepgen    uint32
	divMul      uint32        // for divide by elemsize
	allocCount  uint16        // number of allocated objects
	spanclass   spanClass     // size class and noscan (uint8)
	state       mSpanStateBox // mSpanInUse etc; accessed atomically (get/set methods)
	needzero    uint8         // needs to be zeroed before allocation
	elemsize    uintptr       // computed from sizeclass or from npages
	limit       uintptr       // end of data in span
	speciallock mutex         // guards specials list
	specials    *special      // linked list of special records sorted by offset.
}
```

#### <a id="findObject" href="#findObject">func findObject(p, refBase, refOff uintptr) (base uintptr, s *mspan, objIndex uintptr)</a>

```
searchKey: runtime.findObject
tags: [method private]
```

```Go
func findObject(p, refBase, refOff uintptr) (base uintptr, s *mspan, objIndex uintptr)
```

findObject returns the base address for the heap object containing the address p, the object's span, and the index of the object in s. If p does not point into a heap object, it returns base == 0. 

If p points is an invalid heap pointer and debug.invalidptr != 0, findObject panics. 

refBase and refOff optionally give the base address of the object in which the pointer p was found and the byte offset at which it was found. These are used for error reporting. 

It is nosplit so it is safe for p to be a pointer to the current goroutine's stack. Since p is a uintptr, it would not be adjusted if the stack were to move. 

#### <a id="materializeGCProg" href="#materializeGCProg">func materializeGCProg(ptrdata uintptr, prog *byte) *mspan</a>

```
searchKey: runtime.materializeGCProg
tags: [method private]
```

```Go
func materializeGCProg(ptrdata uintptr, prog *byte) *mspan
```

materializeGCProg allocates space for the (1-bit) pointer bitmask for an object of size ptrdata.  Then it fills that space with the pointer bitmask specified by the program prog. The bitmask starts at s.startAddr. The result must be deallocated with dematerializeGCProg. 

#### <a id="spanOf" href="#spanOf">func spanOf(p uintptr) *mspan</a>

```
searchKey: runtime.spanOf
tags: [method private]
```

```Go
func spanOf(p uintptr) *mspan
```

spanOf returns the span of p. If p does not point into the heap arena or no span has ever contained p, spanOf returns nil. 

If p does not point to allocated memory, this may return a non-nil span that does *not* contain p. If this is a possibility, the caller should either call spanOfHeap or check the span bounds explicitly. 

Must be nosplit because it has callers that are nosplit. 

#### <a id="spanOfHeap" href="#spanOfHeap">func spanOfHeap(p uintptr) *mspan</a>

```
searchKey: runtime.spanOfHeap
tags: [method private]
```

```Go
func spanOfHeap(p uintptr) *mspan
```

spanOfHeap is like spanOf, but returns nil if p does not point to a heap object. 

Must be nosplit because it has callers that are nosplit. 

#### <a id="spanOfUnchecked" href="#spanOfUnchecked">func spanOfUnchecked(p uintptr) *mspan</a>

```
searchKey: runtime.spanOfUnchecked
tags: [method private]
```

```Go
func spanOfUnchecked(p uintptr) *mspan
```

spanOfUnchecked is equivalent to spanOf, but the caller must ensure that p points into an allocated heap arena. 

Must be nosplit because it has callers that are nosplit. 

#### <a id="mspan.allocBitsForIndex" href="#mspan.allocBitsForIndex">func (s *mspan) allocBitsForIndex(allocBitIndex uintptr) markBits</a>

```
searchKey: runtime.mspan.allocBitsForIndex
tags: [method private]
```

```Go
func (s *mspan) allocBitsForIndex(allocBitIndex uintptr) markBits
```

#### <a id="mspan.base" href="#mspan.base">func (s *mspan) base() uintptr</a>

```
searchKey: runtime.mspan.base
tags: [function private]
```

```Go
func (s *mspan) base() uintptr
```

#### <a id="mspan.countAlloc" href="#mspan.countAlloc">func (s *mspan) countAlloc() int</a>

```
searchKey: runtime.mspan.countAlloc
tags: [function private]
```

```Go
func (s *mspan) countAlloc() int
```

countAlloc returns the number of objects allocated in span s by scanning the allocation bitmap. 

#### <a id="mspan.divideByElemSize" href="#mspan.divideByElemSize">func (s *mspan) divideByElemSize(n uintptr) uintptr</a>

```
searchKey: runtime.mspan.divideByElemSize
tags: [method private]
```

```Go
func (s *mspan) divideByElemSize(n uintptr) uintptr
```

divideByElemSize returns n/s.elemsize. n must be within [0, s.npages*_PageSize), or may be exactly s.npages*_PageSize if s.elemsize is from sizeclasses.go. 

#### <a id="mspan.ensureSwept" href="#mspan.ensureSwept">func (s *mspan) ensureSwept()</a>

```
searchKey: runtime.mspan.ensureSwept
tags: [function private]
```

```Go
func (s *mspan) ensureSwept()
```

Returns only when span s has been swept. 

#### <a id="mspan.inList" href="#mspan.inList">func (span *mspan) inList() bool</a>

```
searchKey: runtime.mspan.inList
tags: [function private]
```

```Go
func (span *mspan) inList() bool
```

#### <a id="mspan.init.mheap.go.0xc053387380" href="#mspan.init.mheap.go.0xc053387380">func (span *mspan) init(base uintptr, npages uintptr)</a>

```
searchKey: runtime.mspan.init
tags: [method private]
```

```Go
func (span *mspan) init(base uintptr, npages uintptr)
```

Initialize a new span with the given start and npages. 

#### <a id="mspan.isFree" href="#mspan.isFree">func (s *mspan) isFree(index uintptr) bool</a>

```
searchKey: runtime.mspan.isFree
tags: [method private]
```

```Go
func (s *mspan) isFree(index uintptr) bool
```

isFree reports whether the index'th object in s is unallocated. 

The caller must ensure s.state is mSpanInUse, and there must have been no preemption points since ensuring this (which could allow a GC transition, which would allow the state to change). 

#### <a id="mspan.layout" href="#mspan.layout">func (s *mspan) layout() (size, n, total uintptr)</a>

```
searchKey: runtime.mspan.layout
tags: [function private]
```

```Go
func (s *mspan) layout() (size, n, total uintptr)
```

#### <a id="mspan.markBitsForBase" href="#mspan.markBitsForBase">func (s *mspan) markBitsForBase() markBits</a>

```
searchKey: runtime.mspan.markBitsForBase
tags: [function private]
```

```Go
func (s *mspan) markBitsForBase() markBits
```

#### <a id="mspan.markBitsForIndex" href="#mspan.markBitsForIndex">func (s *mspan) markBitsForIndex(objIndex uintptr) markBits</a>

```
searchKey: runtime.mspan.markBitsForIndex
tags: [method private]
```

```Go
func (s *mspan) markBitsForIndex(objIndex uintptr) markBits
```

#### <a id="mspan.nextFreeIndex" href="#mspan.nextFreeIndex">func (s *mspan) nextFreeIndex() uintptr</a>

```
searchKey: runtime.mspan.nextFreeIndex
tags: [function private]
```

```Go
func (s *mspan) nextFreeIndex() uintptr
```

nextFreeIndex returns the index of the next free object in s at or after s.freeindex. There are hardware instructions that can be used to make this faster if profiling warrants it. 

#### <a id="mspan.objIndex" href="#mspan.objIndex">func (s *mspan) objIndex(p uintptr) uintptr</a>

```
searchKey: runtime.mspan.objIndex
tags: [method private]
```

```Go
func (s *mspan) objIndex(p uintptr) uintptr
```

#### <a id="mspan.refillAllocCache" href="#mspan.refillAllocCache">func (s *mspan) refillAllocCache(whichByte uintptr)</a>

```
searchKey: runtime.mspan.refillAllocCache
tags: [method private]
```

```Go
func (s *mspan) refillAllocCache(whichByte uintptr)
```

refillAllocCache takes 8 bytes s.allocBits starting at whichByte and negates them so that ctz (count trailing zeros) instructions can be used. It then places these 8 bytes into the cached 64 bit s.allocCache. 

#### <a id="mspan.reportZombies" href="#mspan.reportZombies">func (s *mspan) reportZombies()</a>

```
searchKey: runtime.mspan.reportZombies
tags: [function private]
```

```Go
func (s *mspan) reportZombies()
```

reportZombies reports any marked but free objects in s and throws. 

This generally means one of the following: 

1. User code converted a pointer to a uintptr and then back unsafely, and a GC ran while the uintptr was the only reference to an object. 

2. User code (or a compiler bug) constructed a bad pointer that points to a free slot, often a past-the-end pointer. 

3. The GC two cycles ago missed a pointer and freed a live object, but it was still live in the last cycle, so this GC cycle found a pointer to that object and marked it. 

### <a id="mstats" href="#mstats">type mstats struct</a>

```
searchKey: runtime.mstats
tags: [struct private]
```

```Go
type mstats struct {
	// General statistics.
	alloc       uint64 // bytes allocated and not yet freed
	total_alloc uint64 // bytes allocated (even if freed)
	sys         uint64 // bytes obtained from system (should be sum of xxx_sys below, no locking, approximate)
	nlookup     uint64 // number of pointer lookups (unused)
	nmalloc     uint64 // number of mallocs
	nfree       uint64 // number of frees

	// Statistics about malloc heap.
	// Updated atomically, or with the world stopped.
	//
	// Like MemStats, heap_sys and heap_inuse do not count memory
	// in manually-managed spans.
	heap_sys      sysMemStat // virtual address space obtained from system for GC'd heap
	heap_inuse    uint64     // bytes in mSpanInUse spans
	heap_released uint64     // bytes released to the os

	// heap_objects is not used by the runtime directly and instead
	// computed on the fly by updatememstats.
	heap_objects uint64 // total number of allocated objects

	// Statistics about stacks.
	stacks_inuse uint64     // bytes in manually-managed stack spans; computed by updatememstats
	stacks_sys   sysMemStat // only counts newosproc0 stack in mstats; differs from MemStats.StackSys

	// Statistics about allocation of low-level fixed-size structures.
	// Protected by FixAlloc locks.
	mspan_inuse  uint64 // mspan structures
	mspan_sys    sysMemStat
	mcache_inuse uint64 // mcache structures
	mcache_sys   sysMemStat
	buckhash_sys sysMemStat // profiling bucket hash table

	// Statistics about GC overhead.
	gcWorkBufInUse           uint64     // computed by updatememstats
	gcProgPtrScalarBitsInUse uint64     // computed by updatememstats
	gcMiscSys                sysMemStat // updated atomically or during STW

	// Miscellaneous statistics.
	other_sys sysMemStat // updated atomically or during STW

	// Protected by mheap or stopping the world during GC.
	last_gc_unix    uint64 // last gc (in unix time)
	pause_total_ns  uint64
	pause_ns        [256]uint64 // circular buffer of recent gc pause lengths
	pause_end       [256]uint64 // circular buffer of recent gc end times (nanoseconds since 1970)
	numgc           uint32
	numforcedgc     uint32  // number of user-forced GCs
	gc_cpu_fraction float64 // fraction of CPU time used by GC
	enablegc        bool
	debuggc         bool

	by_size [_NumSizeClasses]struct {
		size    uint32
		nmalloc uint64
		nfree   uint64
	}

	// Add an uint32 for even number of size classes to align below fields
	// to 64 bits for atomic operations on 32 bit platforms.
	_ [1 - _NumSizeClasses%2]uint32

	last_gc_nanotime uint64 // last gc (monotonic time)
	last_heap_inuse  uint64 // heap_inuse at mark termination of the previous GC

	// heapStats is a set of statistics
	heapStats consistentHeapStats

	// gcPauseDist represents the distribution of all GC-related
	// application pauses in the runtime.
	//
	// Each individual pause is counted separately, unlike pause_ns.
	gcPauseDist timeHistogram
}
```

Statistics. 

For detailed descriptions see the documentation for MemStats. Fields that differ from MemStats are further documented here. 

Many of these fields are updated on the fly, while others are only updated when updatememstats is called. 

### <a id="muintptr" href="#muintptr">type muintptr uintptr</a>

```
searchKey: runtime.muintptr
tags: [number private]
```

```Go
type muintptr uintptr
```

muintptr is a *m that is not tracked by the garbage collector. 

Because we do free Ms, there are some additional constrains on muintptrs: 

1. Never hold an muintptr locally across a safe point. 

2. Any muintptr in the heap must be owned by the M itself so it can 

```
ensure it is not in use when the last true *m is released.

```
#### <a id="muintptr.ptr" href="#muintptr.ptr">func (mp muintptr) ptr() *m</a>

```
searchKey: runtime.muintptr.ptr
tags: [function private]
```

```Go
func (mp muintptr) ptr() *m
```

#### <a id="muintptr.set" href="#muintptr.set">func (mp *muintptr) set(m *m)</a>

```
searchKey: runtime.muintptr.set
tags: [method private]
```

```Go
func (mp *muintptr) set(m *m)
```

### <a id="mutex" href="#mutex">type mutex struct</a>

```
searchKey: runtime.mutex
tags: [struct private]
```

```Go
type mutex struct {
	// Empty struct if lock ranking is disabled, otherwise includes the lock rank
	lockRankStruct
	// Futex-based impl treats it as uint32 key,
	// while sema-based impl as M* waitm.
	// Used to be a union, but unions break precise GC.
	key uintptr
}
```

Mutual exclusion locks.  In the uncontended case, as fast as spin locks (just a few user-level instructions), but on the contention path they sleep in the kernel. A zeroed Mutex is unlocked (no need to initialize each lock). Initialization is helpful for static lock ranking, but not required. 

### <a id="name" href="#name">type name struct</a>

```
searchKey: runtime.name
tags: [struct private]
```

```Go
type name struct {
	bytes *byte
}
```

name is an encoded type name with optional extra data. See reflect/type.go for details. 

#### <a id="resolveNameOff" href="#resolveNameOff">func resolveNameOff(ptrInModule unsafe.Pointer, off nameOff) name</a>

```
searchKey: runtime.resolveNameOff
tags: [method private]
```

```Go
func resolveNameOff(ptrInModule unsafe.Pointer, off nameOff) name
```

#### <a id="name.data" href="#name.data">func (n name) data(off int) *byte</a>

```
searchKey: runtime.name.data
tags: [method private]
```

```Go
func (n name) data(off int) *byte
```

#### <a id="name.isBlank" href="#name.isBlank">func (n name) isBlank() bool</a>

```
searchKey: runtime.name.isBlank
tags: [function private]
```

```Go
func (n name) isBlank() bool
```

#### <a id="name.isExported" href="#name.isExported">func (n name) isExported() bool</a>

```
searchKey: runtime.name.isExported
tags: [function private]
```

```Go
func (n name) isExported() bool
```

#### <a id="name.name" href="#name.name">func (n name) name() (s string)</a>

```
searchKey: runtime.name.name
tags: [function private]
```

```Go
func (n name) name() (s string)
```

#### <a id="name.pkgPath" href="#name.pkgPath">func (n name) pkgPath() string</a>

```
searchKey: runtime.name.pkgPath
tags: [function private]
```

```Go
func (n name) pkgPath() string
```

#### <a id="name.readvarint" href="#name.readvarint">func (n name) readvarint(off int) (int, int)</a>

```
searchKey: runtime.name.readvarint
tags: [method private]
```

```Go
func (n name) readvarint(off int) (int, int)
```

#### <a id="name.tag" href="#name.tag">func (n name) tag() (s string)</a>

```
searchKey: runtime.name.tag
tags: [function private]
```

```Go
func (n name) tag() (s string)
```

### <a id="nameOff" href="#nameOff">type nameOff int32</a>

```
searchKey: runtime.nameOff
tags: [number private]
```

```Go
type nameOff int32
```

### <a id="neverCallThisFunction" href="#neverCallThisFunction">type neverCallThisFunction struct{}</a>

```
searchKey: runtime.neverCallThisFunction
tags: [struct private]
```

```Go
type neverCallThisFunction struct{}
```

### <a id="notInHeap" href="#notInHeap">type notInHeap struct{}</a>

```
searchKey: runtime.notInHeap
tags: [struct private]
```

```Go
type notInHeap struct{}
```

notInHeap is off-heap memory allocated by a lower-level allocator like sysAlloc or persistentAlloc. 

In general, it's better to use real types marked as go:notinheap, but this serves as a generic type for situations where that isn't possible (like in the allocators). 

TODO: Use this as the return type of sysAlloc, persistentAlloc, etc? 

#### <a id="persistentalloc1" href="#persistentalloc1">func persistentalloc1(size, align uintptr, sysStat *sysMemStat) *notInHeap</a>

```
searchKey: runtime.persistentalloc1
tags: [method private]
```

```Go
func persistentalloc1(size, align uintptr, sysStat *sysMemStat) *notInHeap
```

Must run on system stack because stack growth can (re)invoke it. See issue 9174. 

#### <a id="notInHeap.add" href="#notInHeap.add">func (p *notInHeap) add(bytes uintptr) *notInHeap</a>

```
searchKey: runtime.notInHeap.add
tags: [method private]
```

```Go
func (p *notInHeap) add(bytes uintptr) *notInHeap
```

### <a id="notInHeapSlice" href="#notInHeapSlice">type notInHeapSlice struct</a>

```
searchKey: runtime.notInHeapSlice
tags: [struct private]
```

```Go
type notInHeapSlice struct {
	array *notInHeap
	len   int
	cap   int
}
```

A notInHeapSlice is a slice backed by go:notinheap memory. 

### <a id="note" href="#note">type note struct</a>

```
searchKey: runtime.note
tags: [struct private]
```

```Go
type note struct {
	// Futex-based impl treats it as uint32 key,
	// while sema-based impl as M* waitm.
	// Used to be a union, but unions break precise GC.
	key uintptr
}
```

sleep and wakeup on one-time events. before any calls to notesleep or notewakeup, must call noteclear to initialize the Note. then, exactly one thread can call notesleep and exactly one thread can call notewakeup (once). once notewakeup has been called, the notesleep will return.  future notesleep will return immediately. subsequent noteclear must be called only after previous notesleep has returned, e.g. it's disallowed to call noteclear straight after notewakeup. 

notetsleep is like notesleep but wakes up after a given number of nanoseconds even if the event has not yet happened.  if a goroutine uses notetsleep to wake up early, it must wait to call noteclear until it can be sure that no other goroutine is calling notewakeup. 

notesleep/notetsleep are generally called on g0, notetsleepg is similar to notetsleep but is called on user g. 

### <a id="notifyList" href="#notifyList">type notifyList struct</a>

```
searchKey: runtime.notifyList
tags: [struct private]
```

```Go
type notifyList struct {
	// wait is the ticket number of the next waiter. It is atomically
	// incremented outside the lock.
	wait uint32

	// notify is the ticket number of the next waiter to be notified. It can
	// be read outside the lock, but is only written to with lock held.
	//
	// Both wait & notify can wrap around, and such cases will be correctly
	// handled as long as their "unwrapped" difference is bounded by 2^31.
	// For this not to be the case, we'd need to have 2^31+ goroutines
	// blocked on the same condvar, which is currently not possible.
	notify uint32

	// List of parked waiters.
	lock mutex
	head *sudog
	tail *sudog
}
```

notifyList is a ticket-based notification list used to implement sync.Cond. 

It must be kept in sync with the sync package. 

### <a id="offAddr" href="#offAddr">type offAddr struct</a>

```
searchKey: runtime.offAddr
tags: [struct private]
```

```Go
type offAddr struct {
	// a is just the virtual address, but should never be used
	// directly. Call addr() to get this value instead.
	a uintptr
}
```

offAddr represents an address in a contiguous view of the address space on systems where the address space is segmented. On other systems, it's just a normal address. 

#### <a id="levelIndexToOffAddr" href="#levelIndexToOffAddr">func levelIndexToOffAddr(level, idx int) offAddr</a>

```
searchKey: runtime.levelIndexToOffAddr
tags: [method private]
```

```Go
func levelIndexToOffAddr(level, idx int) offAddr
```

levelIndexToOffAddr converts an index into summary[level] into the corresponding address in the offset address space. 

#### <a id="offAddr.add" href="#offAddr.add">func (l offAddr) add(bytes uintptr) offAddr</a>

```
searchKey: runtime.offAddr.add
tags: [method private]
```

```Go
func (l offAddr) add(bytes uintptr) offAddr
```

add adds a uintptr offset to the offAddr. 

#### <a id="offAddr.addr" href="#offAddr.addr">func (l offAddr) addr() uintptr</a>

```
searchKey: runtime.offAddr.addr
tags: [function private]
```

```Go
func (l offAddr) addr() uintptr
```

addr returns the virtual address for this offset address. 

#### <a id="offAddr.diff" href="#offAddr.diff">func (l1 offAddr) diff(l2 offAddr) uintptr</a>

```
searchKey: runtime.offAddr.diff
tags: [method private]
```

```Go
func (l1 offAddr) diff(l2 offAddr) uintptr
```

diff returns the amount of bytes in between the two offAddrs. 

#### <a id="offAddr.equal" href="#offAddr.equal">func (l1 offAddr) equal(l2 offAddr) bool</a>

```
searchKey: runtime.offAddr.equal
tags: [method private]
```

```Go
func (l1 offAddr) equal(l2 offAddr) bool
```

equal returns true if the two offAddr values are equal. 

#### <a id="offAddr.lessEqual" href="#offAddr.lessEqual">func (l1 offAddr) lessEqual(l2 offAddr) bool</a>

```
searchKey: runtime.offAddr.lessEqual
tags: [method private]
```

```Go
func (l1 offAddr) lessEqual(l2 offAddr) bool
```

lessEqual returns true if l1 is less than or equal to l2 in the offset address space. 

#### <a id="offAddr.lessThan" href="#offAddr.lessThan">func (l1 offAddr) lessThan(l2 offAddr) bool</a>

```
searchKey: runtime.offAddr.lessThan
tags: [method private]
```

```Go
func (l1 offAddr) lessThan(l2 offAddr) bool
```

lessThan returns true if l1 is less than l2 in the offset address space. 

#### <a id="offAddr.sub" href="#offAddr.sub">func (l offAddr) sub(bytes uintptr) offAddr</a>

```
searchKey: runtime.offAddr.sub
tags: [method private]
```

```Go
func (l offAddr) sub(bytes uintptr) offAddr
```

sub subtracts a uintptr offset from the offAddr. 

### <a id="p" href="#p">type p struct</a>

```
searchKey: runtime.p
tags: [struct private]
```

```Go
type p struct {
	id          int32
	status      uint32 // one of pidle/prunning/...
	link        puintptr
	schedtick   uint32     // incremented on every scheduler call
	syscalltick uint32     // incremented on every system call
	sysmontick  sysmontick // last tick observed by sysmon
	m           muintptr   // back-link to associated m (nil if idle)
	mcache      *mcache
	pcache      pageCache
	raceprocctx uintptr

	deferpool    [5][]*_defer // pool of available defer structs of different sizes (see panic.go)
	deferpoolbuf [5][32]*_defer

	// Cache of goroutine ids, amortizes accesses to runtime·sched.goidgen.
	goidcache    uint64
	goidcacheend uint64

	// Queue of runnable goroutines. Accessed without lock.
	runqhead uint32
	runqtail uint32
	runq     [256]guintptr
	// runnext, if non-nil, is a runnable G that was ready'd by
	// the current G and should be run next instead of what's in
	// runq if there's time remaining in the running G's time
	// slice. It will inherit the time left in the current time
	// slice. If a set of goroutines is locked in a
	// communicate-and-wait pattern, this schedules that set as a
	// unit and eliminates the (potentially large) scheduling
	// latency that otherwise arises from adding the ready'd
	// goroutines to the end of the run queue.
	//
	// Note that while other P's may atomically CAS this to zero,
	// only the owner P can CAS it to a valid G.
	runnext guintptr

	// Available G's (status == Gdead)
	gFree struct {
		gList
		n int32
	}

	sudogcache []*sudog
	sudogbuf   [128]*sudog

	// Cache of mspan objects from the heap.
	mspancache struct {
		// We need an explicit length here because this field is used
		// in allocation codepaths where write barriers are not allowed,
		// and eliminating the write barrier/keeping it eliminated from
		// slice updates is tricky, moreso than just managing the length
		// ourselves.
		len int
		buf [128]*mspan
	}

	tracebuf traceBufPtr

	// traceSweep indicates the sweep events should be traced.
	// This is used to defer the sweep start event until a span
	// has actually been swept.
	traceSweep bool
	// traceSwept and traceReclaimed track the number of bytes
	// swept and reclaimed by sweeping in the current sweep loop.
	traceSwept, traceReclaimed uintptr

	palloc persistentAlloc // per-P to avoid mutex

	_ uint32 // Alignment for atomic fields below

	// The when field of the first entry on the timer heap.
	// This is updated using atomic functions.
	// This is 0 if the timer heap is empty.
	timer0When uint64

	// The earliest known nextwhen field of a timer with
	// timerModifiedEarlier status. Because the timer may have been
	// modified again, there need not be any timer with this value.
	// This is updated using atomic functions.
	// This is 0 if the value is unknown.
	timerModifiedEarliest uint64

	// Per-P GC state
	gcAssistTime         int64 // Nanoseconds in assistAlloc
	gcFractionalMarkTime int64 // Nanoseconds in fractional mark worker (atomic)

	// gcMarkWorkerMode is the mode for the next mark worker to run in.
	// That is, this is used to communicate with the worker goroutine
	// selected for immediate execution by
	// gcController.findRunnableGCWorker. When scheduling other goroutines,
	// this field must be set to gcMarkWorkerNotWorker.
	gcMarkWorkerMode gcMarkWorkerMode
	// gcMarkWorkerStartTime is the nanotime() at which the most recent
	// mark worker started.
	gcMarkWorkerStartTime int64

	// gcw is this P's GC work buffer cache. The work buffer is
	// filled by write barriers, drained by mutator assists, and
	// disposed on certain GC state transitions.
	gcw gcWork

	// wbBuf is this P's GC write barrier buffer.
	//
	// TODO: Consider caching this in the running G.
	wbBuf wbBuf

	runSafePointFn uint32 // if 1, run sched.safePointFn at next safe point

	// statsSeq is a counter indicating whether this P is currently
	// writing any stats. Its value is even when not, odd when it is.
	statsSeq uint32

	// Lock for timers. We normally access the timers while running
	// on this P, but the scheduler can also do it from a different P.
	timersLock mutex

	// Actions to take at some time. This is used to implement the
	// standard library's time package.
	// Must hold timersLock to access.
	timers []*timer

	// Number of timers in P's heap.
	// Modified using atomic instructions.
	numTimers uint32

	// Number of timerModifiedEarlier timers on P's heap.
	// This should only be modified while holding timersLock,
	// or while the timer status is in a transient state
	// such as timerModifying.
	adjustTimers uint32

	// Number of timerDeleted timers in P's heap.
	// Modified using atomic instructions.
	deletedTimers uint32

	// Race context used while executing timer functions.
	timerRaceCtx uintptr

	// preempt is set to indicate that this P should be enter the
	// scheduler ASAP (regardless of what G is running on it).
	preempt bool
}
```

#### <a id="checkRunqsNoP" href="#checkRunqsNoP">func checkRunqsNoP(allpSnapshot []*p, idlepMaskSnapshot pMask) *p</a>

```
searchKey: runtime.checkRunqsNoP
tags: [method private]
```

```Go
func checkRunqsNoP(allpSnapshot []*p, idlepMaskSnapshot pMask) *p
```

Check all Ps for a runnable G to steal. 

On entry we have no P. If a G is available to steal and a P is available, the P is returned which the caller should acquire and attempt to steal the work to. 

#### <a id="pidleget" href="#pidleget">func pidleget() *p</a>

```
searchKey: runtime.pidleget
tags: [function private]
```

```Go
func pidleget() *p
```

pidleget tries to get a p from the _Pidle list, acquiring ownership. 

sched.lock must be held. 

May run during STW, so write barriers are not allowed. 

#### <a id="procresize" href="#procresize">func procresize(nprocs int32) *p</a>

```
searchKey: runtime.procresize
tags: [method private]
```

```Go
func procresize(nprocs int32) *p
```

Change number of processors. 

sched.lock must be held, and the world must be stopped. 

gcworkbufs must not be being modified by either the GC or the write barrier code, so the GC must not be running if the number of Ps actually changes. 

Returns list of Ps with local work, they need to be scheduled by the caller. 

#### <a id="releasep" href="#releasep">func releasep() *p</a>

```
searchKey: runtime.releasep
tags: [function private]
```

```Go
func releasep() *p
```

Disassociate p and the current m. 

#### <a id="timeSleepUntil" href="#timeSleepUntil">func timeSleepUntil() (int64, *p)</a>

```
searchKey: runtime.timeSleepUntil
tags: [function private]
```

```Go
func timeSleepUntil() (int64, *p)
```

timeSleepUntil returns the time when the next timer should fire, and the P that holds the timer heap that that timer is on. This is only called by sysmon and checkdead. 

#### <a id="p.destroy" href="#p.destroy">func (pp *p) destroy()</a>

```
searchKey: runtime.p.destroy
tags: [function private]
```

```Go
func (pp *p) destroy()
```

destroy releases all of the resources associated with pp and transitions it to status _Pdead. 

sched.lock must be held and the world must be stopped. 

#### <a id="p.init.proc.go.0xc0585819a8" href="#p.init.proc.go.0xc0585819a8">func (pp *p) init(id int32)</a>

```
searchKey: runtime.p.init
tags: [method private]
```

```Go
func (pp *p) init(id int32)
```

init initializes pp, which may be a freshly allocated p or a previously destroyed p, and transitions it to status _Pgcstop. 

### <a id="pMask" href="#pMask">type pMask []uint32</a>

```
searchKey: runtime.pMask
tags: [array number private]
```

```Go
type pMask []uint32
```

pMask is an atomic bitstring with one bit per P. 

#### <a id="pMask.clear" href="#pMask.clear">func (p pMask) clear(id int32)</a>

```
searchKey: runtime.pMask.clear
tags: [method private]
```

```Go
func (p pMask) clear(id int32)
```

clear clears P id's bit. 

#### <a id="pMask.read" href="#pMask.read">func (p pMask) read(id uint32) bool</a>

```
searchKey: runtime.pMask.read
tags: [method private]
```

```Go
func (p pMask) read(id uint32) bool
```

read returns true if P id's bit is set. 

#### <a id="pMask.set" href="#pMask.set">func (p pMask) set(id int32)</a>

```
searchKey: runtime.pMask.set
tags: [method private]
```

```Go
func (p pMask) set(id int32)
```

set sets P id's bit. 

### <a id="pageAlloc" href="#pageAlloc">type pageAlloc struct</a>

```
searchKey: runtime.pageAlloc
tags: [struct private]
```

```Go
type pageAlloc struct {
	// Radix tree of summaries.
	//
	// Each slice's cap represents the whole memory reservation.
	// Each slice's len reflects the allocator's maximum known
	// mapped heap address for that level.
	//
	// The backing store of each summary level is reserved in init
	// and may or may not be committed in grow (small address spaces
	// may commit all the memory in init).
	//
	// The purpose of keeping len <= cap is to enforce bounds checks
	// on the top end of the slice so that instead of an unknown
	// runtime segmentation fault, we get a much friendlier out-of-bounds
	// error.
	//
	// To iterate over a summary level, use inUse to determine which ranges
	// are currently available. Otherwise one might try to access
	// memory which is only Reserved which may result in a hard fault.
	//
	// We may still get segmentation faults < len since some of that
	// memory may not be committed yet.
	summary [summaryLevels][]pallocSum

	// chunks is a slice of bitmap chunks.
	//
	// The total size of chunks is quite large on most 64-bit platforms
	// (O(GiB) or more) if flattened, so rather than making one large mapping
	// (which has problems on some platforms, even when PROT_NONE) we use a
	// two-level sparse array approach similar to the arena index in mheap.
	//
	// To find the chunk containing a memory address `a`, do:
	//   chunkOf(chunkIndex(a))
	//
	// Below is a table describing the configuration for chunks for various
	// heapAddrBits supported by the runtime.
	//
	// heapAddrBits | L1 Bits | L2 Bits | L2 Entry Size
	// ------------------------------------------------
	// 32           | 0       | 10      | 128 KiB
	// 33 (iOS)     | 0       | 11      | 256 KiB
	// 48           | 13      | 13      | 1 MiB
	//
	// There's no reason to use the L1 part of chunks on 32-bit, the
	// address space is small so the L2 is small. For platforms with a
	// 48-bit address space, we pick the L1 such that the L2 is 1 MiB
	// in size, which is a good balance between low granularity without
	// making the impact on BSS too high (note the L1 is stored directly
	// in pageAlloc).
	//
	// To iterate over the bitmap, use inUse to determine which ranges
	// are currently available. Otherwise one might iterate over unused
	// ranges.
	//
	// TODO(mknyszek): Consider changing the definition of the bitmap
	// such that 1 means free and 0 means in-use so that summaries and
	// the bitmaps align better on zero-values.
	chunks [1 << pallocChunksL1Bits]*[1 << pallocChunksL2Bits]pallocData

	// The address to start an allocation search with. It must never
	// point to any memory that is not contained in inUse, i.e.
	// inUse.contains(searchAddr.addr()) must always be true. The one
	// exception to this rule is that it may take on the value of
	// maxOffAddr to indicate that the heap is exhausted.
	//
	// We guarantee that all valid heap addresses below this value
	// are allocated and not worth searching.
	searchAddr offAddr

	// start and end represent the chunk indices
	// which pageAlloc knows about. It assumes
	// chunks in the range [start, end) are
	// currently ready to use.
	start, end chunkIdx

	// inUse is a slice of ranges of address space which are
	// known by the page allocator to be currently in-use (passed
	// to grow).
	//
	// This field is currently unused on 32-bit architectures but
	// is harmless to track. We care much more about having a
	// contiguous heap in these cases and take additional measures
	// to ensure that, so in nearly all cases this should have just
	// 1 element.
	//
	// All access is protected by the mheapLock.
	inUse addrRanges

	// scav stores the scavenger state.
	//
	// All fields are protected by mheapLock.
	scav struct {
		// inUse is a slice of ranges of address space which have not
		// yet been looked at by the scavenger.
		inUse addrRanges

		// gen is the scavenge generation number.
		gen uint32

		// reservationBytes is how large of a reservation should be made
		// in bytes of address space for each scavenge iteration.
		reservationBytes uintptr

		// released is the amount of memory released this generation.
		released uintptr

		// scavLWM is the lowest (offset) address that the scavenger reached this
		// scavenge generation.
		scavLWM offAddr

		// freeHWM is the highest (offset) address of a page that was freed to
		// the page allocator this scavenge generation.
		freeHWM offAddr
	}

	// mheap_.lock. This level of indirection makes it possible
	// to test pageAlloc indepedently of the runtime allocator.
	mheapLock *mutex

	// sysStat is the runtime memstat to update when new system
	// memory is committed by the pageAlloc for allocation metadata.
	sysStat *sysMemStat

	// Whether or not this struct is being used in tests.
	test bool
}
```

#### <a id="pageAlloc.alloc" href="#pageAlloc.alloc">func (p *pageAlloc) alloc(npages uintptr) (addr uintptr, scav uintptr)</a>

```
searchKey: runtime.pageAlloc.alloc
tags: [method private]
```

```Go
func (p *pageAlloc) alloc(npages uintptr) (addr uintptr, scav uintptr)
```

alloc allocates npages worth of memory from the page heap, returning the base address for the allocation and the amount of scavenged memory in bytes contained in the region [base address, base address + npages*pageSize). 

Returns a 0 base address on failure, in which case other returned values should be ignored. 

p.mheapLock must be held. 

Must run on the system stack because p.mheapLock must be held. 

#### <a id="pageAlloc.allocRange" href="#pageAlloc.allocRange">func (p *pageAlloc) allocRange(base, npages uintptr) uintptr</a>

```
searchKey: runtime.pageAlloc.allocRange
tags: [method private]
```

```Go
func (p *pageAlloc) allocRange(base, npages uintptr) uintptr
```

allocRange marks the range of memory [base, base+npages*pageSize) as allocated. It also updates the summaries to reflect the newly-updated bitmap. 

Returns the amount of scavenged memory in bytes present in the allocated range. 

p.mheapLock must be held. 

#### <a id="pageAlloc.allocToCache" href="#pageAlloc.allocToCache">func (p *pageAlloc) allocToCache() pageCache</a>

```
searchKey: runtime.pageAlloc.allocToCache
tags: [function private]
```

```Go
func (p *pageAlloc) allocToCache() pageCache
```

allocToCache acquires a pageCachePages-aligned chunk of free pages which may not be contiguous, and returns a pageCache structure which owns the chunk. 

p.mheapLock must be held. 

Must run on the system stack because p.mheapLock must be held. 

#### <a id="pageAlloc.chunkOf" href="#pageAlloc.chunkOf">func (p *pageAlloc) chunkOf(ci chunkIdx) *pallocData</a>

```
searchKey: runtime.pageAlloc.chunkOf
tags: [method private]
```

```Go
func (p *pageAlloc) chunkOf(ci chunkIdx) *pallocData
```

chunkOf returns the chunk at the given chunk index. 

The chunk index must be valid or this method may throw. 

#### <a id="pageAlloc.find" href="#pageAlloc.find">func (p *pageAlloc) find(npages uintptr) (uintptr, offAddr)</a>

```
searchKey: runtime.pageAlloc.find
tags: [method private]
```

```Go
func (p *pageAlloc) find(npages uintptr) (uintptr, offAddr)
```

find searches for the first (address-ordered) contiguous free region of npages in size and returns a base address for that region. 

It uses p.searchAddr to prune its search and assumes that no palloc chunks below chunkIndex(p.searchAddr) contain any free memory at all. 

find also computes and returns a candidate p.searchAddr, which may or may not prune more of the address space than p.searchAddr already does. This candidate is always a valid p.searchAddr. 

find represents the slow path and the full radix tree search. 

Returns a base address of 0 on failure, in which case the candidate searchAddr returned is invalid and must be ignored. 

p.mheapLock must be held. 

#### <a id="pageAlloc.findMappedAddr" href="#pageAlloc.findMappedAddr">func (p *pageAlloc) findMappedAddr(addr offAddr) offAddr</a>

```
searchKey: runtime.pageAlloc.findMappedAddr
tags: [method private]
```

```Go
func (p *pageAlloc) findMappedAddr(addr offAddr) offAddr
```

findMappedAddr returns the smallest mapped offAddr that is >= addr. That is, if addr refers to mapped memory, then it is returned. If addr is higher than any mapped region, then it returns maxOffAddr. 

p.mheapLock must be held. 

#### <a id="pageAlloc.free" href="#pageAlloc.free">func (p *pageAlloc) free(base, npages uintptr)</a>

```
searchKey: runtime.pageAlloc.free
tags: [method private]
```

```Go
func (p *pageAlloc) free(base, npages uintptr)
```

free returns npages worth of memory starting at base back to the page heap. 

p.mheapLock must be held. 

Must run on the system stack because p.mheapLock must be held. 

#### <a id="pageAlloc.grow" href="#pageAlloc.grow">func (p *pageAlloc) grow(base, size uintptr)</a>

```
searchKey: runtime.pageAlloc.grow
tags: [method private]
```

```Go
func (p *pageAlloc) grow(base, size uintptr)
```

grow sets up the metadata for the address range [base, base+size). It may allocate metadata, in which case *p.sysStat will be updated. 

p.mheapLock must be held. 

#### <a id="pageAlloc.init.mpagealloc.go" href="#pageAlloc.init.mpagealloc.go">func (p *pageAlloc) init(mheapLock *mutex, sysStat *sysMemStat)</a>

```
searchKey: runtime.pageAlloc.init
tags: [method private]
```

```Go
func (p *pageAlloc) init(mheapLock *mutex, sysStat *sysMemStat)
```

#### <a id="pageAlloc.scavenge" href="#pageAlloc.scavenge">func (p *pageAlloc) scavenge(nbytes uintptr, mayUnlock bool) uintptr</a>

```
searchKey: runtime.pageAlloc.scavenge
tags: [method private]
```

```Go
func (p *pageAlloc) scavenge(nbytes uintptr, mayUnlock bool) uintptr
```

scavenge scavenges nbytes worth of free pages, starting with the highest address first. Successive calls continue from where it left off until the heap is exhausted. Call scavengeStartGen to bring it back to the top of the heap. 

Returns the amount of memory scavenged in bytes. 

p.mheapLock must be held, but may be temporarily released if mayUnlock == true. 

Must run on the system stack because p.mheapLock must be held. 

#### <a id="pageAlloc.scavengeOne" href="#pageAlloc.scavengeOne">func (p *pageAlloc) scavengeOne(work addrRange, max uintptr, mayUnlock bool) (uintptr, addrRange)</a>

```
searchKey: runtime.pageAlloc.scavengeOne
tags: [method private]
```

```Go
func (p *pageAlloc) scavengeOne(work addrRange, max uintptr, mayUnlock bool) (uintptr, addrRange)
```

scavengeOne walks over address range work until it finds a contiguous run of pages to scavenge. It will try to scavenge at most max bytes at once, but may scavenge more to avoid breaking huge pages. Once it scavenges some memory it returns how much it scavenged in bytes. 

Returns the number of bytes scavenged and the part of work which was not yet searched. 

work's base address must be aligned to pallocChunkBytes. 

p.mheapLock must be held, but may be temporarily released if mayUnlock == true. 

Must run on the system stack because p.mheapLock must be held. 

#### <a id="pageAlloc.scavengeRangeLocked" href="#pageAlloc.scavengeRangeLocked">func (p *pageAlloc) scavengeRangeLocked(ci chunkIdx, base, npages uint) uintptr</a>

```
searchKey: runtime.pageAlloc.scavengeRangeLocked
tags: [method private]
```

```Go
func (p *pageAlloc) scavengeRangeLocked(ci chunkIdx, base, npages uint) uintptr
```

scavengeRangeLocked scavenges the given region of memory. The region of memory is described by its chunk index (ci), the starting page index of the region relative to that chunk (base), and the length of the region in pages (npages). 

Returns the base address of the scavenged region. 

p.mheapLock must be held. 

#### <a id="pageAlloc.scavengeReserve" href="#pageAlloc.scavengeReserve">func (p *pageAlloc) scavengeReserve() (addrRange, uint32)</a>

```
searchKey: runtime.pageAlloc.scavengeReserve
tags: [function private]
```

```Go
func (p *pageAlloc) scavengeReserve() (addrRange, uint32)
```

scavengeReserve reserves a contiguous range of the address space for scavenging. The maximum amount of space it reserves is proportional to the size of the heap. The ranges are reserved from the high addresses first. 

Returns the reserved range and the scavenge generation number for it. 

p.mheapLock must be held. 

Must run on the system stack because p.mheapLock must be held. 

#### <a id="pageAlloc.scavengeStartGen" href="#pageAlloc.scavengeStartGen">func (p *pageAlloc) scavengeStartGen()</a>

```
searchKey: runtime.pageAlloc.scavengeStartGen
tags: [function private]
```

```Go
func (p *pageAlloc) scavengeStartGen()
```

scavengeStartGen starts a new scavenge generation, resetting the scavenger's search space to the full in-use address space. 

p.mheapLock must be held. 

Must run on the system stack because p.mheapLock must be held. 

#### <a id="pageAlloc.scavengeUnreserve" href="#pageAlloc.scavengeUnreserve">func (p *pageAlloc) scavengeUnreserve(r addrRange, gen uint32)</a>

```
searchKey: runtime.pageAlloc.scavengeUnreserve
tags: [method private]
```

```Go
func (p *pageAlloc) scavengeUnreserve(r addrRange, gen uint32)
```

scavengeUnreserve returns an unscavenged portion of a range that was previously reserved with scavengeReserve. 

p.mheapLock must be held. 

Must run on the system stack because p.mheapLock must be held. 

#### <a id="pageAlloc.sysGrow" href="#pageAlloc.sysGrow">func (p *pageAlloc) sysGrow(base, limit uintptr)</a>

```
searchKey: runtime.pageAlloc.sysGrow
tags: [method private]
```

```Go
func (p *pageAlloc) sysGrow(base, limit uintptr)
```

sysGrow performs architecture-dependent operations on heap growth for the page allocator, such as mapping in new memory for summaries. It also updates the length of the slices in [.summary. 

base is the base of the newly-added heap memory and limit is the first address past the end of the newly-added heap memory. Both must be aligned to pallocChunkBytes. 

The caller must update p.start and p.end after calling sysGrow. 

#### <a id="pageAlloc.sysInit" href="#pageAlloc.sysInit">func (p *pageAlloc) sysInit()</a>

```
searchKey: runtime.pageAlloc.sysInit
tags: [function private]
```

```Go
func (p *pageAlloc) sysInit()
```

sysInit performs architecture-dependent initialization of fields in pageAlloc. pageAlloc should be uninitialized except for sysStat if any runtime statistic should be updated. 

#### <a id="pageAlloc.tryChunkOf" href="#pageAlloc.tryChunkOf">func (p *pageAlloc) tryChunkOf(ci chunkIdx) *pallocData</a>

```
searchKey: runtime.pageAlloc.tryChunkOf
tags: [method private]
```

```Go
func (p *pageAlloc) tryChunkOf(ci chunkIdx) *pallocData
```

tryChunkOf returns the bitmap data for the given chunk. 

Returns nil if the chunk data has not been mapped. 

#### <a id="pageAlloc.update" href="#pageAlloc.update">func (p *pageAlloc) update(base, npages uintptr, contig, alloc bool)</a>

```
searchKey: runtime.pageAlloc.update
tags: [method private]
```

```Go
func (p *pageAlloc) update(base, npages uintptr, contig, alloc bool)
```

update updates heap metadata. It must be called each time the bitmap is updated. 

If contig is true, update does some optimizations assuming that there was a contiguous allocation or free between addr and addr+npages. alloc indicates whether the operation performed was an allocation or a free. 

p.mheapLock must be held. 

### <a id="pageBits" href="#pageBits">type pageBits [8]uint64</a>

```
searchKey: runtime.pageBits
tags: [array number private]
```

```Go
type pageBits [pallocChunkPages / 64]uint64
```

pageBits is a bitmap representing one bit per page in a palloc chunk. 

#### <a id="pageBits.block64" href="#pageBits.block64">func (b *pageBits) block64(i uint) uint64</a>

```
searchKey: runtime.pageBits.block64
tags: [method private]
```

```Go
func (b *pageBits) block64(i uint) uint64
```

block64 returns the 64-bit aligned block of bits containing the i'th bit. 

#### <a id="pageBits.clear" href="#pageBits.clear">func (b *pageBits) clear(i uint)</a>

```
searchKey: runtime.pageBits.clear
tags: [method private]
```

```Go
func (b *pageBits) clear(i uint)
```

clear clears bit i of pageBits. 

#### <a id="pageBits.clearAll" href="#pageBits.clearAll">func (b *pageBits) clearAll()</a>

```
searchKey: runtime.pageBits.clearAll
tags: [function private]
```

```Go
func (b *pageBits) clearAll()
```

clearAll frees all the bits of b. 

#### <a id="pageBits.clearRange" href="#pageBits.clearRange">func (b *pageBits) clearRange(i, n uint)</a>

```
searchKey: runtime.pageBits.clearRange
tags: [method private]
```

```Go
func (b *pageBits) clearRange(i, n uint)
```

clearRange clears bits in the range [i, i+n). 

#### <a id="pageBits.get" href="#pageBits.get">func (b *pageBits) get(i uint) uint</a>

```
searchKey: runtime.pageBits.get
tags: [method private]
```

```Go
func (b *pageBits) get(i uint) uint
```

get returns the value of the i'th bit in the bitmap. 

#### <a id="pageBits.popcntRange" href="#pageBits.popcntRange">func (b *pageBits) popcntRange(i, n uint) (s uint)</a>

```
searchKey: runtime.pageBits.popcntRange
tags: [method private]
```

```Go
func (b *pageBits) popcntRange(i, n uint) (s uint)
```

popcntRange counts the number of set bits in the range [i, i+n). 

#### <a id="pageBits.set" href="#pageBits.set">func (b *pageBits) set(i uint)</a>

```
searchKey: runtime.pageBits.set
tags: [method private]
```

```Go
func (b *pageBits) set(i uint)
```

set sets bit i of pageBits. 

#### <a id="pageBits.setAll" href="#pageBits.setAll">func (b *pageBits) setAll()</a>

```
searchKey: runtime.pageBits.setAll
tags: [function private]
```

```Go
func (b *pageBits) setAll()
```

setAll sets all the bits of b. 

#### <a id="pageBits.setRange" href="#pageBits.setRange">func (b *pageBits) setRange(i, n uint)</a>

```
searchKey: runtime.pageBits.setRange
tags: [method private]
```

```Go
func (b *pageBits) setRange(i, n uint)
```

setRange sets bits in the range [i, i+n). 

### <a id="pageCache" href="#pageCache">type pageCache struct</a>

```
searchKey: runtime.pageCache
tags: [struct private]
```

```Go
type pageCache struct {
	base  uintptr // base address of the chunk
	cache uint64  // 64-bit bitmap representing free pages (1 means free)
	scav  uint64  // 64-bit bitmap representing scavenged pages (1 means scavenged)
}
```

pageCache represents a per-p cache of pages the allocator can allocate from without a lock. More specifically, it represents a pageCachePages*pageSize chunk of memory with 0 or more free pages in it. 

#### <a id="pageCache.alloc" href="#pageCache.alloc">func (c *pageCache) alloc(npages uintptr) (uintptr, uintptr)</a>

```
searchKey: runtime.pageCache.alloc
tags: [method private]
```

```Go
func (c *pageCache) alloc(npages uintptr) (uintptr, uintptr)
```

alloc allocates npages from the page cache and is the main entry point for allocation. 

Returns a base address and the amount of scavenged memory in the allocated region in bytes. 

Returns a base address of zero on failure, in which case the amount of scavenged memory should be ignored. 

#### <a id="pageCache.allocN" href="#pageCache.allocN">func (c *pageCache) allocN(npages uintptr) (uintptr, uintptr)</a>

```
searchKey: runtime.pageCache.allocN
tags: [method private]
```

```Go
func (c *pageCache) allocN(npages uintptr) (uintptr, uintptr)
```

allocN is a helper which attempts to allocate npages worth of pages from the cache. It represents the general case for allocating from the page cache. 

Returns a base address and the amount of scavenged memory in the allocated region in bytes. 

#### <a id="pageCache.empty" href="#pageCache.empty">func (c *pageCache) empty() bool</a>

```
searchKey: runtime.pageCache.empty
tags: [function private]
```

```Go
func (c *pageCache) empty() bool
```

empty returns true if the pageCache has any free pages, and false otherwise. 

#### <a id="pageCache.flush" href="#pageCache.flush">func (c *pageCache) flush(p *pageAlloc)</a>

```
searchKey: runtime.pageCache.flush
tags: [method private]
```

```Go
func (c *pageCache) flush(p *pageAlloc)
```

flush empties out unallocated free pages in the given cache into s. Then, it clears the cache, such that empty returns true. 

p.mheapLock must be held. 

Must run on the system stack because p.mheapLock must be held. 

### <a id="pallocBits" href="#pallocBits">type pallocBits runtime.pageBits</a>

```
searchKey: runtime.pallocBits
tags: [array number private]
```

```Go
type pallocBits pageBits
```

pallocBits is a bitmap that tracks page allocations for at most one palloc chunk. 

The precise representation is an implementation detail, but for the sake of documentation, 0s are free pages and 1s are allocated pages. 

#### <a id="pallocBits.allocAll" href="#pallocBits.allocAll">func (b *pallocBits) allocAll()</a>

```
searchKey: runtime.pallocBits.allocAll
tags: [function private]
```

```Go
func (b *pallocBits) allocAll()
```

allocAll allocates all the bits of b. 

#### <a id="pallocBits.allocRange" href="#pallocBits.allocRange">func (b *pallocBits) allocRange(i, n uint)</a>

```
searchKey: runtime.pallocBits.allocRange
tags: [method private]
```

```Go
func (b *pallocBits) allocRange(i, n uint)
```

allocRange allocates the range [i, i+n). 

#### <a id="pallocBits.find" href="#pallocBits.find">func (b *pallocBits) find(npages uintptr, searchIdx uint) (uint, uint)</a>

```
searchKey: runtime.pallocBits.find
tags: [method private]
```

```Go
func (b *pallocBits) find(npages uintptr, searchIdx uint) (uint, uint)
```

find searches for npages contiguous free pages in pallocBits and returns the index where that run starts, as well as the index of the first free page it found in the search. searchIdx represents the first known free page and where to begin the next search from. 

If find fails to find any free space, it returns an index of ^uint(0) and the new searchIdx should be ignored. 

Note that if npages == 1, the two returned values will always be identical. 

#### <a id="pallocBits.find1" href="#pallocBits.find1">func (b *pallocBits) find1(searchIdx uint) uint</a>

```
searchKey: runtime.pallocBits.find1
tags: [method private]
```

```Go
func (b *pallocBits) find1(searchIdx uint) uint
```

find1 is a helper for find which searches for a single free page in the pallocBits and returns the index. 

See find for an explanation of the searchIdx parameter. 

#### <a id="pallocBits.findLargeN" href="#pallocBits.findLargeN">func (b *pallocBits) findLargeN(npages uintptr, searchIdx uint) (uint, uint)</a>

```
searchKey: runtime.pallocBits.findLargeN
tags: [method private]
```

```Go
func (b *pallocBits) findLargeN(npages uintptr, searchIdx uint) (uint, uint)
```

findLargeN is a helper for find which searches for npages contiguous free pages in this pallocBits and returns the index where that run starts, as well as the index of the first free page it found it its search. 

See alloc for an explanation of the searchIdx parameter. 

Returns a ^uint(0) index on failure and the new searchIdx should be ignored. 

findLargeN assumes npages > 64, where any such run of free pages crosses at least one aligned 64-bit boundary in the bits. 

#### <a id="pallocBits.findSmallN" href="#pallocBits.findSmallN">func (b *pallocBits) findSmallN(npages uintptr, searchIdx uint) (uint, uint)</a>

```
searchKey: runtime.pallocBits.findSmallN
tags: [method private]
```

```Go
func (b *pallocBits) findSmallN(npages uintptr, searchIdx uint) (uint, uint)
```

findSmallN is a helper for find which searches for npages contiguous free pages in this pallocBits and returns the index where that run of contiguous pages starts as well as the index of the first free page it finds in its search. 

See find for an explanation of the searchIdx parameter. 

Returns a ^uint(0) index on failure and the new searchIdx should be ignored. 

findSmallN assumes npages <= 64, where any such contiguous run of pages crosses at most one aligned 64-bit boundary in the bits. 

#### <a id="pallocBits.free" href="#pallocBits.free">func (b *pallocBits) free(i, n uint)</a>

```
searchKey: runtime.pallocBits.free
tags: [method private]
```

```Go
func (b *pallocBits) free(i, n uint)
```

free frees the range [i, i+n) of pages in the pallocBits. 

#### <a id="pallocBits.free1" href="#pallocBits.free1">func (b *pallocBits) free1(i uint)</a>

```
searchKey: runtime.pallocBits.free1
tags: [method private]
```

```Go
func (b *pallocBits) free1(i uint)
```

free1 frees a single page in the pallocBits at i. 

#### <a id="pallocBits.freeAll" href="#pallocBits.freeAll">func (b *pallocBits) freeAll()</a>

```
searchKey: runtime.pallocBits.freeAll
tags: [function private]
```

```Go
func (b *pallocBits) freeAll()
```

freeAll frees all the bits of b. 

#### <a id="pallocBits.pages64" href="#pallocBits.pages64">func (b *pallocBits) pages64(i uint) uint64</a>

```
searchKey: runtime.pallocBits.pages64
tags: [method private]
```

```Go
func (b *pallocBits) pages64(i uint) uint64
```

pages64 returns a 64-bit bitmap representing a block of 64 pages aligned to 64 pages. The returned block of pages is the one containing the i'th page in this pallocBits. Each bit represents whether the page is in-use. 

#### <a id="pallocBits.summarize" href="#pallocBits.summarize">func (b *pallocBits) summarize() pallocSum</a>

```
searchKey: runtime.pallocBits.summarize
tags: [function private]
```

```Go
func (b *pallocBits) summarize() pallocSum
```

summarize returns a packed summary of the bitmap in pallocBits. 

### <a id="pallocData" href="#pallocData">type pallocData struct</a>

```
searchKey: runtime.pallocData
tags: [struct private]
```

```Go
type pallocData struct {
	pallocBits
	scavenged pageBits
}
```

pallocData encapsulates pallocBits and a bitmap for whether or not a given page is scavenged in a single structure. It's effectively a pallocBits with additional functionality. 

Update the comment on (*pageAlloc).chunks should this structure change. 

#### <a id="pallocData.allocAll" href="#pallocData.allocAll">func (m *pallocData) allocAll()</a>

```
searchKey: runtime.pallocData.allocAll
tags: [function private]
```

```Go
func (m *pallocData) allocAll()
```

allocAll sets every bit in the bitmap to 1 and updates the scavenged bits appropriately. 

#### <a id="pallocData.allocRange" href="#pallocData.allocRange">func (m *pallocData) allocRange(i, n uint)</a>

```
searchKey: runtime.pallocData.allocRange
tags: [method private]
```

```Go
func (m *pallocData) allocRange(i, n uint)
```

allocRange sets bits [i, i+n) in the bitmap to 1 and updates the scavenged bits appropriately. 

#### <a id="pallocData.findScavengeCandidate" href="#pallocData.findScavengeCandidate">func (m *pallocData) findScavengeCandidate(searchIdx uint, min, max uintptr) (uint, uint)</a>

```
searchKey: runtime.pallocData.findScavengeCandidate
tags: [method private]
```

```Go
func (m *pallocData) findScavengeCandidate(searchIdx uint, min, max uintptr) (uint, uint)
```

findScavengeCandidate returns a start index and a size for this pallocData segment which represents a contiguous region of free and unscavenged memory. 

searchIdx indicates the page index within this chunk to start the search, but note that findScavengeCandidate searches backwards through the pallocData. As a a result, it will return the highest scavenge candidate in address order. 

min indicates a hard minimum size and alignment for runs of pages. That is, findScavengeCandidate will not return a region smaller than min pages in size, or that is min pages or greater in size but not aligned to min. min must be a non-zero power of 2 <= maxPagesPerPhysPage. 

max is a hint for how big of a region is desired. If max >= pallocChunkPages, then findScavengeCandidate effectively returns entire free and unscavenged regions. If max < pallocChunkPages, it may truncate the returned region such that size is max. However, findScavengeCandidate may still return a larger region if, for example, it chooses to preserve huge pages, or if max is not aligned to min (it will round up). That is, even if max is small, the returned size is not guaranteed to be equal to max. max is allowed to be less than min, in which case it is as if max == min. 

#### <a id="pallocData.hasScavengeCandidate" href="#pallocData.hasScavengeCandidate">func (m *pallocData) hasScavengeCandidate(min uintptr) bool</a>

```
searchKey: runtime.pallocData.hasScavengeCandidate
tags: [method private]
```

```Go
func (m *pallocData) hasScavengeCandidate(min uintptr) bool
```

hasScavengeCandidate returns true if there's any min-page-aligned groups of min pages of free-and-unscavenged memory in the region represented by this pallocData. 

min must be a non-zero power of 2 <= maxPagesPerPhysPage. 

### <a id="pallocSum" href="#pallocSum">type pallocSum uint64</a>

```
searchKey: runtime.pallocSum
tags: [number private]
```

```Go
type pallocSum uint64
```

pallocSum is a packed summary type which packs three numbers: start, max, and end into a single 8-byte value. Each of these values are a summary of a bitmap and are thus counts, each of which may have a maximum value of 2^21 - 1, or all three may be equal to 2^21. The latter case is represented by just setting the 64th bit. 

#### <a id="mergeSummaries" href="#mergeSummaries">func mergeSummaries(sums []pallocSum, logMaxPagesPerSum uint) pallocSum</a>

```
searchKey: runtime.mergeSummaries
tags: [method private]
```

```Go
func mergeSummaries(sums []pallocSum, logMaxPagesPerSum uint) pallocSum
```

mergeSummaries merges consecutive summaries which may each represent at most 1 << logMaxPagesPerSum pages each together into one. 

#### <a id="packPallocSum" href="#packPallocSum">func packPallocSum(start, max, end uint) pallocSum</a>

```
searchKey: runtime.packPallocSum
tags: [method private]
```

```Go
func packPallocSum(start, max, end uint) pallocSum
```

packPallocSum takes a start, max, and end value and produces a pallocSum. 

#### <a id="pallocSum.end" href="#pallocSum.end">func (p pallocSum) end() uint</a>

```
searchKey: runtime.pallocSum.end
tags: [function private]
```

```Go
func (p pallocSum) end() uint
```

end extracts the end value from a packed sum. 

#### <a id="pallocSum.max" href="#pallocSum.max">func (p pallocSum) max() uint</a>

```
searchKey: runtime.pallocSum.max
tags: [function private]
```

```Go
func (p pallocSum) max() uint
```

max extracts the max value from a packed sum. 

#### <a id="pallocSum.start" href="#pallocSum.start">func (p pallocSum) start() uint</a>

```
searchKey: runtime.pallocSum.start
tags: [function private]
```

```Go
func (p pallocSum) start() uint
```

start extracts the start value from a packed sum. 

#### <a id="pallocSum.unpack" href="#pallocSum.unpack">func (p pallocSum) unpack() (uint, uint, uint)</a>

```
searchKey: runtime.pallocSum.unpack
tags: [function private]
```

```Go
func (p pallocSum) unpack() (uint, uint, uint)
```

unpack unpacks all three values from the summary. 

### <a id="pcHeader" href="#pcHeader">type pcHeader struct</a>

```
searchKey: runtime.pcHeader
tags: [struct private]
```

```Go
type pcHeader struct {
	magic          uint32  // 0xFFFFFFFA
	pad1, pad2     uint8   // 0,0
	minLC          uint8   // min instruction size
	ptrSize        uint8   // size of a ptr in bytes
	nfunc          int     // number of functions in the module
	nfiles         uint    // number of entries in the file tab.
	funcnameOffset uintptr // offset to the funcnametab variable from pcHeader
	cuOffset       uintptr // offset to the cutab variable from pcHeader
	filetabOffset  uintptr // offset to the filetab variable from pcHeader
	pctabOffset    uintptr // offset to the pctab varible from pcHeader
	pclnOffset     uintptr // offset to the pclntab variable from pcHeader
}
```

pcHeader holds data used by the pclntab lookups. 

### <a id="pcvalueCache" href="#pcvalueCache">type pcvalueCache struct</a>

```
searchKey: runtime.pcvalueCache
tags: [struct private]
```

```Go
type pcvalueCache struct {
	entries [2][8]pcvalueCacheEnt
}
```

### <a id="pcvalueCacheEnt" href="#pcvalueCacheEnt">type pcvalueCacheEnt struct</a>

```
searchKey: runtime.pcvalueCacheEnt
tags: [struct private]
```

```Go
type pcvalueCacheEnt struct {
	// targetpc and off together are the key of this cache entry.
	targetpc uintptr
	off      uint32
	// val is the value of this cached pcvalue entry.
	val int32
}
```

### <a id="persistentAlloc" href="#persistentAlloc">type persistentAlloc struct</a>

```
searchKey: runtime.persistentAlloc
tags: [struct private]
```

```Go
type persistentAlloc struct {
	base *notInHeap
	off  uintptr
}
```

### <a id="plainError" href="#plainError">type plainError string</a>

```
searchKey: runtime.plainError
tags: [string private]
```

```Go
type plainError string
```

plainError represents a runtime error described a string without the prefix "runtime error: " after invoking errorString.Error(). See Issue #14965. 

#### <a id="plainError.Error" href="#plainError.Error">func (e plainError) Error() string</a>

```
searchKey: runtime.plainError.Error
tags: [function private]
```

```Go
func (e plainError) Error() string
```

#### <a id="plainError.RuntimeError" href="#plainError.RuntimeError">func (e plainError) RuntimeError()</a>

```
searchKey: runtime.plainError.RuntimeError
tags: [function private]
```

```Go
func (e plainError) RuntimeError()
```

### <a id="pollCache" href="#pollCache">type pollCache struct</a>

```
searchKey: runtime.pollCache
tags: [struct private]
```

```Go
type pollCache struct {
	lock  mutex
	first *pollDesc
}
```

#### <a id="pollCache.alloc" href="#pollCache.alloc">func (c *pollCache) alloc() *pollDesc</a>

```
searchKey: runtime.pollCache.alloc
tags: [function private]
```

```Go
func (c *pollCache) alloc() *pollDesc
```

#### <a id="pollCache.free" href="#pollCache.free">func (c *pollCache) free(pd *pollDesc)</a>

```
searchKey: runtime.pollCache.free
tags: [method private]
```

```Go
func (c *pollCache) free(pd *pollDesc)
```

### <a id="pollDesc" href="#pollDesc">type pollDesc struct</a>

```
searchKey: runtime.pollDesc
tags: [struct private]
```

```Go
type pollDesc struct {
	link *pollDesc // in pollcache, protected by pollcache.lock

	// The lock protects pollOpen, pollSetDeadline, pollUnblock and deadlineimpl operations.
	// This fully covers seq, rt and wt variables. fd is constant throughout the PollDesc lifetime.
	// pollReset, pollWait, pollWaitCanceled and runtime·netpollready (IO readiness notification)
	// proceed w/o taking the lock. So closing, everr, rg, rd, wg and wd are manipulated
	// in a lock-free way by all operations.
	// NOTE(dvyukov): the following code uses uintptr to store *g (rg/wg),
	// that will blow up when GC starts moving objects.
	lock    mutex // protects the following fields
	fd      uintptr
	closing bool
	everr   bool      // marks event scanning error happened
	user    uint32    // user settable cookie
	rseq    uintptr   // protects from stale read timers
	rg      uintptr   // pdReady, pdWait, G waiting for read or nil
	rt      timer     // read deadline timer (set if rt.f != nil)
	rd      int64     // read deadline
	wseq    uintptr   // protects from stale write timers
	wg      uintptr   // pdReady, pdWait, G waiting for write or nil
	wt      timer     // write deadline timer
	wd      int64     // write deadline
	self    *pollDesc // storage for indirect interface. See (*pollDesc).makeArg.
}
```

Network poller descriptor. 

No heap pointers. 

#### <a id="poll_runtime_pollOpen" href="#poll_runtime_pollOpen">func poll_runtime_pollOpen(fd uintptr) (*pollDesc, int)</a>

```
searchKey: runtime.poll_runtime_pollOpen
tags: [method private]
```

```Go
func poll_runtime_pollOpen(fd uintptr) (*pollDesc, int)
```

#### <a id="pollDesc.makeArg" href="#pollDesc.makeArg">func (pd *pollDesc) makeArg() (i interface{})</a>

```
searchKey: runtime.pollDesc.makeArg
tags: [function private]
```

```Go
func (pd *pollDesc) makeArg() (i interface{})
```

makeArg converts pd to an interface{}. makeArg does not do any allocation. Normally, such a conversion requires an allocation because pointers to go:notinheap types (which pollDesc is) must be stored in interfaces indirectly. See issue 42076. 

### <a id="profAtomic" href="#profAtomic">type profAtomic uint64</a>

```
searchKey: runtime.profAtomic
tags: [number private]
```

```Go
type profAtomic uint64
```

A profAtomic is the atomically-accessed word holding a profIndex. 

#### <a id="profAtomic.cas" href="#profAtomic.cas">func (x *profAtomic) cas(old, new profIndex) bool</a>

```
searchKey: runtime.profAtomic.cas
tags: [method private]
```

```Go
func (x *profAtomic) cas(old, new profIndex) bool
```

#### <a id="profAtomic.load" href="#profAtomic.load">func (x *profAtomic) load() profIndex</a>

```
searchKey: runtime.profAtomic.load
tags: [function private]
```

```Go
func (x *profAtomic) load() profIndex
```

#### <a id="profAtomic.store" href="#profAtomic.store">func (x *profAtomic) store(new profIndex)</a>

```
searchKey: runtime.profAtomic.store
tags: [method private]
```

```Go
func (x *profAtomic) store(new profIndex)
```

### <a id="profBuf" href="#profBuf">type profBuf struct</a>

```
searchKey: runtime.profBuf
tags: [struct private]
```

```Go
type profBuf struct {
	// accessed atomically
	r, w         profAtomic
	overflow     uint64
	overflowTime uint64
	eof          uint32

	// immutable (excluding slice content)
	hdrsize uintptr
	data    []uint64
	tags    []unsafe.Pointer

	// owned by reader
	rNext       profIndex
	overflowBuf []uint64 // for use by reader to return overflow record
	wait        note
}
```

A profBuf is a lock-free buffer for profiling events, safe for concurrent use by one reader and one writer. The writer may be a signal handler running without a user g. The reader is assumed to be a user g. 

Each logged event corresponds to a fixed size header, a list of uintptrs (typically a stack), and exactly one unsafe.Pointer tag. The header and uintptrs are stored in the circular buffer data and the tag is stored in a circular buffer tags, running in parallel. In the circular buffer data, each event takes 2+hdrsize+len(stk) words: the value 2+hdrsize+len(stk), then the time of the event, then hdrsize words giving the fixed-size header, and then len(stk) words for the stack. 

The current effective offsets into the tags and data circular buffers for reading and writing are stored in the high 30 and low 32 bits of r and w. The bottom bits of the high 32 are additional flag bits in w, unused in r. "Effective" offsets means the total number of reads or writes, mod 2^length. The offset in the buffer is the effective offset mod the length of the buffer. To make wraparound mod 2^length match wraparound mod length of the buffer, the length of the buffer must be a power of two. 

If the reader catches up to the writer, a flag passed to read controls whether the read blocks until more data is available. A read returns a pointer to the buffer data itself; the caller is assumed to be done with that data at the next read. The read offset rNext tracks the next offset to be returned by read. By definition, r ≤ rNext ≤ w (before wraparound), and rNext is only used by the reader, so it can be accessed without atomics. 

If the writer gets ahead of the reader, so that the buffer fills, future writes are discarded and replaced in the output stream by an overflow entry, which has size 2+hdrsize+1, time set to the time of the first discarded write, a header of all zeroed words, and a "stack" containing one word, the number of discarded writes. 

Between the time the buffer fills and the buffer becomes empty enough to hold more data, the overflow entry is stored as a pending overflow entry in the fields overflow and overflowTime. The pending overflow entry can be turned into a real record by either the writer or the reader. If the writer is called to write a new record and finds that the output buffer has room for both the pending overflow entry and the new record, the writer emits the pending overflow entry and the new record into the buffer. If the reader is called to read data and finds that the output buffer is empty but that there is a pending overflow entry, the reader will return a synthesized record for the pending overflow entry. 

Only the writer can create or add to a pending overflow entry, but either the reader or the writer can clear the pending overflow entry. A pending overflow entry is indicated by the low 32 bits of 'overflow' holding the number of discarded writes, and overflowTime holding the time of the first discarded write. The high 32 bits of 'overflow' increment each time the low 32 bits transition from zero to non-zero or vice versa. This sequence number avoids ABA problems in the use of compare-and-swap to coordinate between reader and writer. The overflowTime is only written when the low 32 bits of overflow are zero, that is, only when there is no pending overflow entry, in preparation for creating a new one. The reader can therefore fetch and clear the entry atomically using 

```
for {
	overflow = load(&b.overflow)
	if uint32(overflow) == 0 {
		// no pending entry
		break
	}
	time = load(&b.overflowTime)
	if cas(&b.overflow, overflow, ((overflow>>32)+1)<<32) {
		// pending entry cleared
		break
	}
}
if uint32(overflow) > 0 {
	emit entry for uint32(overflow), time
}

```
#### <a id="newProfBuf" href="#newProfBuf">func newProfBuf(hdrsize, bufwords, tags int) *profBuf</a>

```
searchKey: runtime.newProfBuf
tags: [method private]
```

```Go
func newProfBuf(hdrsize, bufwords, tags int) *profBuf
```

newProfBuf returns a new profiling buffer with room for a header of hdrsize words and a buffer of at least bufwords words. 

#### <a id="profBuf.canWriteRecord" href="#profBuf.canWriteRecord">func (b *profBuf) canWriteRecord(nstk int) bool</a>

```
searchKey: runtime.profBuf.canWriteRecord
tags: [method private]
```

```Go
func (b *profBuf) canWriteRecord(nstk int) bool
```

canWriteRecord reports whether the buffer has room for a single contiguous record with a stack of length nstk. 

#### <a id="profBuf.canWriteTwoRecords" href="#profBuf.canWriteTwoRecords">func (b *profBuf) canWriteTwoRecords(nstk1, nstk2 int) bool</a>

```
searchKey: runtime.profBuf.canWriteTwoRecords
tags: [method private]
```

```Go
func (b *profBuf) canWriteTwoRecords(nstk1, nstk2 int) bool
```

canWriteTwoRecords reports whether the buffer has room for two records with stack lengths nstk1, nstk2, in that order. Each record must be contiguous on its own, but the two records need not be contiguous (one can be at the end of the buffer and the other can wrap around and start at the beginning of the buffer). 

#### <a id="profBuf.close" href="#profBuf.close">func (b *profBuf) close()</a>

```
searchKey: runtime.profBuf.close
tags: [function private]
```

```Go
func (b *profBuf) close()
```

close signals that there will be no more writes on the buffer. Once all the data has been read from the buffer, reads will return eof=true. 

#### <a id="profBuf.hasOverflow" href="#profBuf.hasOverflow">func (b *profBuf) hasOverflow() bool</a>

```
searchKey: runtime.profBuf.hasOverflow
tags: [function private]
```

```Go
func (b *profBuf) hasOverflow() bool
```

hasOverflow reports whether b has any overflow records pending. 

#### <a id="profBuf.incrementOverflow" href="#profBuf.incrementOverflow">func (b *profBuf) incrementOverflow(now int64)</a>

```
searchKey: runtime.profBuf.incrementOverflow
tags: [method private]
```

```Go
func (b *profBuf) incrementOverflow(now int64)
```

incrementOverflow records a single overflow at time now. It is racing against a possible takeOverflow in the reader. 

#### <a id="profBuf.read" href="#profBuf.read">func (b *profBuf) read(mode profBufReadMode) (data []uint64, tags []unsafe.Pointer, eof bool)</a>

```
searchKey: runtime.profBuf.read
tags: [method private]
```

```Go
func (b *profBuf) read(mode profBufReadMode) (data []uint64, tags []unsafe.Pointer, eof bool)
```

#### <a id="profBuf.takeOverflow" href="#profBuf.takeOverflow">func (b *profBuf) takeOverflow() (count uint32, time uint64)</a>

```
searchKey: runtime.profBuf.takeOverflow
tags: [function private]
```

```Go
func (b *profBuf) takeOverflow() (count uint32, time uint64)
```

takeOverflow consumes the pending overflow records, returning the overflow count and the time of the first overflow. When called by the reader, it is racing against incrementOverflow. 

#### <a id="profBuf.wakeupExtra" href="#profBuf.wakeupExtra">func (b *profBuf) wakeupExtra()</a>

```
searchKey: runtime.profBuf.wakeupExtra
tags: [function private]
```

```Go
func (b *profBuf) wakeupExtra()
```

wakeupExtra must be called after setting one of the "extra" atomic fields b.overflow or b.eof. It records the change in b.w and wakes up the reader if needed. 

#### <a id="profBuf.write" href="#profBuf.write">func (b *profBuf) write(tagPtr *unsafe.Pointer, now int64, hdr []uint64, stk []uintptr)</a>

```
searchKey: runtime.profBuf.write
tags: [method private]
```

```Go
func (b *profBuf) write(tagPtr *unsafe.Pointer, now int64, hdr []uint64, stk []uintptr)
```

write writes an entry to the profiling buffer b. The entry begins with a fixed hdr, which must have length b.hdrsize, followed by a variable-sized stack and a single tag pointer *tagPtr (or nil if tagPtr is nil). No write barriers allowed because this might be called from a signal handler. 

### <a id="profBufReadMode" href="#profBufReadMode">type profBufReadMode int</a>

```
searchKey: runtime.profBufReadMode
tags: [number private]
```

```Go
type profBufReadMode int
```

profBufReadMode specifies whether to block when no data is available to read. 

### <a id="profIndex" href="#profIndex">type profIndex uint64</a>

```
searchKey: runtime.profIndex
tags: [number private]
```

```Go
type profIndex uint64
```

A profIndex is the packet tag and data counts and flags bits, described above. 

#### <a id="profIndex.addCountsAndClearFlags" href="#profIndex.addCountsAndClearFlags">func (x profIndex) addCountsAndClearFlags(data, tag int) profIndex</a>

```
searchKey: runtime.profIndex.addCountsAndClearFlags
tags: [method private]
```

```Go
func (x profIndex) addCountsAndClearFlags(data, tag int) profIndex
```

addCountsAndClearFlags returns the packed form of "x + (data, tag) - all flags". 

#### <a id="profIndex.dataCount" href="#profIndex.dataCount">func (x profIndex) dataCount() uint32</a>

```
searchKey: runtime.profIndex.dataCount
tags: [function private]
```

```Go
func (x profIndex) dataCount() uint32
```

#### <a id="profIndex.tagCount" href="#profIndex.tagCount">func (x profIndex) tagCount() uint32</a>

```
searchKey: runtime.profIndex.tagCount
tags: [function private]
```

```Go
func (x profIndex) tagCount() uint32
```

### <a id="ptabEntry" href="#ptabEntry">type ptabEntry struct</a>

```
searchKey: runtime.ptabEntry
tags: [struct private]
```

```Go
type ptabEntry struct {
	name nameOff
	typ  typeOff
}
```

A ptabEntry is generated by the compiler for each exported function and global variable in the main package of a plugin. It is used to initialize the plugin module's symbol map. 

### <a id="pthread" href="#pthread">type pthread uintptr</a>

```
searchKey: runtime.pthread
tags: [number private]
```

```Go
type pthread uintptr
```

#### <a id="pthread_self" href="#pthread_self">func pthread_self() (t pthread)</a>

```
searchKey: runtime.pthread_self
tags: [function private]
```

```Go
func pthread_self() (t pthread)
```

### <a id="pthreadattr" href="#pthreadattr">type pthreadattr struct</a>

```
searchKey: runtime.pthreadattr
tags: [struct private]
```

```Go
type pthreadattr struct {
	X__sig    int64
	X__opaque [56]int8
}
```

### <a id="pthreadcond" href="#pthreadcond">type pthreadcond struct</a>

```
searchKey: runtime.pthreadcond
tags: [struct private]
```

```Go
type pthreadcond struct {
	X__sig    int64
	X__opaque [40]int8
}
```

### <a id="pthreadcondattr" href="#pthreadcondattr">type pthreadcondattr struct</a>

```
searchKey: runtime.pthreadcondattr
tags: [struct private]
```

```Go
type pthreadcondattr struct {
	X__sig    int64
	X__opaque [8]int8
}
```

### <a id="pthreadmutex" href="#pthreadmutex">type pthreadmutex struct</a>

```
searchKey: runtime.pthreadmutex
tags: [struct private]
```

```Go
type pthreadmutex struct {
	X__sig    int64
	X__opaque [56]int8
}
```

### <a id="pthreadmutexattr" href="#pthreadmutexattr">type pthreadmutexattr struct</a>

```
searchKey: runtime.pthreadmutexattr
tags: [struct private]
```

```Go
type pthreadmutexattr struct {
	X__sig    int64
	X__opaque [8]int8
}
```

### <a id="ptrtype" href="#ptrtype">type ptrtype struct</a>

```
searchKey: runtime.ptrtype
tags: [struct private]
```

```Go
type ptrtype struct {
	typ  _type
	elem *_type
}
```

### <a id="puintptr" href="#puintptr">type puintptr uintptr</a>

```
searchKey: runtime.puintptr
tags: [number private]
```

```Go
type puintptr uintptr
```

#### <a id="puintptr.ptr" href="#puintptr.ptr">func (pp puintptr) ptr() *p</a>

```
searchKey: runtime.puintptr.ptr
tags: [function private]
```

```Go
func (pp puintptr) ptr() *p
```

#### <a id="puintptr.set" href="#puintptr.set">func (pp *puintptr) set(p *p)</a>

```
searchKey: runtime.puintptr.set
tags: [method private]
```

```Go
func (pp *puintptr) set(p *p)
```

### <a id="randomEnum" href="#randomEnum">type randomEnum struct</a>

```
searchKey: runtime.randomEnum
tags: [struct private]
```

```Go
type randomEnum struct {
	i     uint32
	count uint32
	pos   uint32
	inc   uint32
}
```

#### <a id="randomEnum.done" href="#randomEnum.done">func (enum *randomEnum) done() bool</a>

```
searchKey: runtime.randomEnum.done
tags: [function private]
```

```Go
func (enum *randomEnum) done() bool
```

#### <a id="randomEnum.next" href="#randomEnum.next">func (enum *randomEnum) next()</a>

```
searchKey: runtime.randomEnum.next
tags: [function private]
```

```Go
func (enum *randomEnum) next()
```

#### <a id="randomEnum.position" href="#randomEnum.position">func (enum *randomEnum) position() uint32</a>

```
searchKey: runtime.randomEnum.position
tags: [function private]
```

```Go
func (enum *randomEnum) position() uint32
```

### <a id="randomOrder" href="#randomOrder">type randomOrder struct</a>

```
searchKey: runtime.randomOrder
tags: [struct private]
```

```Go
type randomOrder struct {
	count    uint32
	coprimes []uint32
}
```

randomOrder/randomEnum are helper types for randomized work stealing. They allow to enumerate all Ps in different pseudo-random orders without repetitions. The algorithm is based on the fact that if we have X such that X and GOMAXPROCS are coprime, then a sequences of (i + X) % GOMAXPROCS gives the required enumeration. 

#### <a id="randomOrder.reset" href="#randomOrder.reset">func (ord *randomOrder) reset(count uint32)</a>

```
searchKey: runtime.randomOrder.reset
tags: [method private]
```

```Go
func (ord *randomOrder) reset(count uint32)
```

#### <a id="randomOrder.start" href="#randomOrder.start">func (ord *randomOrder) start(i uint32) randomEnum</a>

```
searchKey: runtime.randomOrder.start
tags: [method private]
```

```Go
func (ord *randomOrder) start(i uint32) randomEnum
```

### <a id="reflectMethodValue" href="#reflectMethodValue">type reflectMethodValue struct</a>

```
searchKey: runtime.reflectMethodValue
tags: [struct private]
```

```Go
type reflectMethodValue struct {
	fn     uintptr
	stack  *bitvector // ptrmap for both args and results
	argLen uintptr    // just args
}
```

reflectMethodValue is a partial duplicate of reflect.makeFuncImpl and reflect.methodValue. 

### <a id="regmmst" href="#regmmst">type regmmst struct</a>

```
searchKey: runtime.regmmst
tags: [struct private]
```

```Go
type regmmst struct {
	mmst_reg  [10]int8
	mmst_rsrv [6]int8
}
```

### <a id="regs32" href="#regs32">type regs32 struct</a>

```
searchKey: runtime.regs32
tags: [struct private]
```

```Go
type regs32 struct {
	eax    uint32
	ebx    uint32
	ecx    uint32
	edx    uint32
	edi    uint32
	esi    uint32
	ebp    uint32
	esp    uint32
	ss     uint32
	eflags uint32
	eip    uint32
	cs     uint32
	ds     uint32
	es     uint32
	fs     uint32
	gs     uint32
}
```

### <a id="regs64" href="#regs64">type regs64 struct</a>

```
searchKey: runtime.regs64
tags: [struct private]
```

```Go
type regs64 struct {
	rax    uint64
	rbx    uint64
	rcx    uint64
	rdx    uint64
	rdi    uint64
	rsi    uint64
	rbp    uint64
	rsp    uint64
	r8     uint64
	r9     uint64
	r10    uint64
	r11    uint64
	r12    uint64
	r13    uint64
	r14    uint64
	r15    uint64
	rip    uint64
	rflags uint64
	cs     uint64
	fs     uint64
	gs     uint64
}
```

### <a id="regxmm" href="#regxmm">type regxmm struct</a>

```
searchKey: runtime.regxmm
tags: [struct private]
```

```Go
type regxmm struct {
	xmm_reg [16]int8
}
```

### <a id="runtimeSelect" href="#runtimeSelect">type runtimeSelect struct</a>

```
searchKey: runtime.runtimeSelect
tags: [struct private]
```

```Go
type runtimeSelect struct {
	dir selectDir
	typ unsafe.Pointer // channel type (not used here)
	ch  *hchan         // channel
	val unsafe.Pointer // ptr to data (SendDir) or ptr to receive buffer (RecvDir)
}
```

A runtimeSelect is a single case passed to rselect. This must match ../reflect/value.go:/runtimeSelect 

### <a id="rwmutex" href="#rwmutex">type rwmutex struct</a>

```
searchKey: runtime.rwmutex
tags: [struct private]
```

```Go
type rwmutex struct {
	rLock      mutex    // protects readers, readerPass, writer
	readers    muintptr // list of pending readers
	readerPass uint32   // number of pending readers to skip readers list

	wLock  mutex    // serializes writers
	writer muintptr // pending writer waiting for completing readers

	readerCount uint32 // number of pending readers
	readerWait  uint32 // number of departing readers
}
```

A rwmutex is a reader/writer mutual exclusion lock. The lock can be held by an arbitrary number of readers or a single writer. This is a variant of sync.RWMutex, for the runtime package. Like mutex, rwmutex blocks the calling M. It does not interact with the goroutine scheduler. 

#### <a id="rwmutex.lock" href="#rwmutex.lock">func (rw *rwmutex) lock()</a>

```
searchKey: runtime.rwmutex.lock
tags: [function private]
```

```Go
func (rw *rwmutex) lock()
```

lock locks rw for writing. 

#### <a id="rwmutex.rlock" href="#rwmutex.rlock">func (rw *rwmutex) rlock()</a>

```
searchKey: runtime.rwmutex.rlock
tags: [function private]
```

```Go
func (rw *rwmutex) rlock()
```

rlock locks rw for reading. 

#### <a id="rwmutex.runlock" href="#rwmutex.runlock">func (rw *rwmutex) runlock()</a>

```
searchKey: runtime.rwmutex.runlock
tags: [function private]
```

```Go
func (rw *rwmutex) runlock()
```

runlock undoes a single rlock call on rw. 

#### <a id="rwmutex.unlock" href="#rwmutex.unlock">func (rw *rwmutex) unlock()</a>

```
searchKey: runtime.rwmutex.unlock
tags: [function private]
```

```Go
func (rw *rwmutex) unlock()
```

unlock unlocks rw for writing. 

### <a id="scase" href="#scase">type scase struct</a>

```
searchKey: runtime.scase
tags: [struct private]
```

```Go
type scase struct {
	c    *hchan         // chan
	elem unsafe.Pointer // data element
}
```

Select case descriptor. Known to compiler. Changes here must also be made in src/cmd/internal/gc/select.go's scasetype. 

### <a id="schedt" href="#schedt">type schedt struct</a>

```
searchKey: runtime.schedt
tags: [struct private]
```

```Go
type schedt struct {
	// accessed atomically. keep at top to ensure alignment on 32-bit systems.
	goidgen   uint64
	lastpoll  uint64 // time of last network poll, 0 if currently polling
	pollUntil uint64 // time to which current poll is sleeping

	lock mutex

	midle        muintptr // idle m's waiting for work
	nmidle       int32    // number of idle m's waiting for work
	nmidlelocked int32    // number of locked m's waiting for work
	mnext        int64    // number of m's that have been created and next M ID
	maxmcount    int32    // maximum number of m's allowed (or die)
	nmsys        int32    // number of system m's not counted for deadlock
	nmfreed      int64    // cumulative number of freed m's

	ngsys uint32 // number of system goroutines; updated atomically

	pidle      puintptr // idle p's
	npidle     uint32
	nmspinning uint32 // See "Worker thread parking/unparking" comment in proc.go.

	// Global runnable queue.
	runq     gQueue
	runqsize int32

	// disable controls selective disabling of the scheduler.
	//
	// Use schedEnableUser to control this.
	//
	// disable is protected by sched.lock.
	disable struct {
		// user disables scheduling of user goroutines.
		user     bool
		runnable gQueue // pending runnable Gs
		n        int32  // length of runnable
	}

	// Global cache of dead G's.
	gFree struct {
		lock    mutex
		stack   gList // Gs with stacks
		noStack gList // Gs without stacks
		n       int32
	}

	// Central cache of sudog structs.
	sudoglock  mutex
	sudogcache *sudog

	// Central pool of available defer structs of different sizes.
	deferlock mutex
	deferpool [5]*_defer

	// freem is the list of m's waiting to be freed when their
	// m.exited is set. Linked through m.freelink.
	freem *m

	gcwaiting  uint32 // gc is waiting to run
	stopwait   int32
	stopnote   note
	sysmonwait uint32
	sysmonnote note

	// While true, sysmon not ready for mFixup calls.
	// Accessed atomically.
	sysmonStarting uint32

	// safepointFn should be called on each P at the next GC
	// safepoint if p.runSafePointFn is set.
	safePointFn   func(*p)
	safePointWait int32
	safePointNote note

	profilehz int32 // cpu profiling rate

	procresizetime int64 // nanotime() of last change to gomaxprocs
	totaltime      int64 // ∫gomaxprocs dt up to procresizetime

	// sysmonlock protects sysmon's actions on the runtime.
	//
	// Acquire and hold this mutex to block sysmon from interacting
	// with the rest of the runtime.
	sysmonlock mutex

	_ uint32 // ensure timeToRun has 8-byte alignment

	// timeToRun is a distribution of scheduling latencies, defined
	// as the sum of time a G spends in the _Grunnable state before
	// it transitions to _Grunning.
	//
	// timeToRun is protected by sched.lock.
	timeToRun timeHistogram
}
```

### <a id="selectDir" href="#selectDir">type selectDir int</a>

```
searchKey: runtime.selectDir
tags: [number private]
```

```Go
type selectDir int
```

These values must match ../reflect/value.go:/SelectDir. 

### <a id="semaProfileFlags" href="#semaProfileFlags">type semaProfileFlags int</a>

```
searchKey: runtime.semaProfileFlags
tags: [number private]
```

```Go
type semaProfileFlags int
```

### <a id="semaRoot" href="#semaRoot">type semaRoot struct</a>

```
searchKey: runtime.semaRoot
tags: [struct private]
```

```Go
type semaRoot struct {
	lock  mutex
	treap *sudog // root of balanced tree of unique waiters.
	nwait uint32 // Number of waiters. Read w/o the lock.
}
```

A semaRoot holds a balanced tree of sudog with distinct addresses (s.elem). Each of those sudog may in turn point (through s.waitlink) to a list of other sudogs waiting on the same address. The operations on the inner lists of sudogs with the same address are all O(1). The scanning of the top-level semaRoot list is O(log n), where n is the number of distinct addresses with goroutines blocked on them that hash to the given semaRoot. See golang.org/issue/17953 for a program that worked badly before we introduced the second level of list, and test/locklinear.go for a test that exercises this. 

#### <a id="semroot" href="#semroot">func semroot(addr *uint32) *semaRoot</a>

```
searchKey: runtime.semroot
tags: [method private]
```

```Go
func semroot(addr *uint32) *semaRoot
```

#### <a id="semaRoot.dequeue" href="#semaRoot.dequeue">func (root *semaRoot) dequeue(addr *uint32) (found *sudog, now int64)</a>

```
searchKey: runtime.semaRoot.dequeue
tags: [method private]
```

```Go
func (root *semaRoot) dequeue(addr *uint32) (found *sudog, now int64)
```

dequeue searches for and finds the first goroutine in semaRoot blocked on addr. If the sudog was being profiled, dequeue returns the time at which it was woken up as now. Otherwise now is 0. 

#### <a id="semaRoot.queue" href="#semaRoot.queue">func (root *semaRoot) queue(addr *uint32, s *sudog, lifo bool)</a>

```
searchKey: runtime.semaRoot.queue
tags: [method private]
```

```Go
func (root *semaRoot) queue(addr *uint32, s *sudog, lifo bool)
```

queue adds s to the blocked goroutines in semaRoot. 

#### <a id="semaRoot.rotateLeft" href="#semaRoot.rotateLeft">func (root *semaRoot) rotateLeft(x *sudog)</a>

```
searchKey: runtime.semaRoot.rotateLeft
tags: [method private]
```

```Go
func (root *semaRoot) rotateLeft(x *sudog)
```

rotateLeft rotates the tree rooted at node x. turning (x a (y b c)) into (y (x a b) c). 

#### <a id="semaRoot.rotateRight" href="#semaRoot.rotateRight">func (root *semaRoot) rotateRight(y *sudog)</a>

```
searchKey: runtime.semaRoot.rotateRight
tags: [method private]
```

```Go
func (root *semaRoot) rotateRight(y *sudog)
```

rotateRight rotates the tree rooted at node y. turning (y (x a b) c) into (x a (y b c)). 

### <a id="sigTabT" href="#sigTabT">type sigTabT struct</a>

```
searchKey: runtime.sigTabT
tags: [struct private]
```

```Go
type sigTabT struct {
	flags int32
	name  string
}
```

sigTabT is the type of an entry in the global sigtable array. sigtable is inherently system dependent, and appears in OS-specific files, but sigTabT is the same for all Unixy systems. The sigtable array is indexed by a system signal number to get the flags and printable name of each signal. 

### <a id="sigactiont" href="#sigactiont">type sigactiont struct</a>

```
searchKey: runtime.sigactiont
tags: [struct private]
```

```Go
type sigactiont struct {
	__sigaction_u [8]byte
	sa_tramp      unsafe.Pointer
	sa_mask       uint32
	sa_flags      int32
}
```

### <a id="sigctxt" href="#sigctxt">type sigctxt struct</a>

```
searchKey: runtime.sigctxt
tags: [struct private]
```

```Go
type sigctxt struct {
	info *siginfo
	ctxt unsafe.Pointer
}
```

#### <a id="sigctxt.cs" href="#sigctxt.cs">func (c *sigctxt) cs() uint64</a>

```
searchKey: runtime.sigctxt.cs
tags: [function private]
```

```Go
func (c *sigctxt) cs() uint64
```

#### <a id="sigctxt.fault" href="#sigctxt.fault">func (c *sigctxt) fault() uintptr</a>

```
searchKey: runtime.sigctxt.fault
tags: [function private]
```

```Go
func (c *sigctxt) fault() uintptr
```

#### <a id="sigctxt.fixsigcode" href="#sigctxt.fixsigcode">func (c *sigctxt) fixsigcode(sig uint32)</a>

```
searchKey: runtime.sigctxt.fixsigcode
tags: [method private]
```

```Go
func (c *sigctxt) fixsigcode(sig uint32)
```

#### <a id="sigctxt.fs" href="#sigctxt.fs">func (c *sigctxt) fs() uint64</a>

```
searchKey: runtime.sigctxt.fs
tags: [function private]
```

```Go
func (c *sigctxt) fs() uint64
```

#### <a id="sigctxt.gs" href="#sigctxt.gs">func (c *sigctxt) gs() uint64</a>

```
searchKey: runtime.sigctxt.gs
tags: [function private]
```

```Go
func (c *sigctxt) gs() uint64
```

#### <a id="sigctxt.preparePanic" href="#sigctxt.preparePanic">func (c *sigctxt) preparePanic(sig uint32, gp *g)</a>

```
searchKey: runtime.sigctxt.preparePanic
tags: [method private]
```

```Go
func (c *sigctxt) preparePanic(sig uint32, gp *g)
```

preparePanic sets up the stack to look like a call to sigpanic. 

#### <a id="sigctxt.pushCall" href="#sigctxt.pushCall">func (c *sigctxt) pushCall(targetPC, resumePC uintptr)</a>

```
searchKey: runtime.sigctxt.pushCall
tags: [method private]
```

```Go
func (c *sigctxt) pushCall(targetPC, resumePC uintptr)
```

#### <a id="sigctxt.r10" href="#sigctxt.r10">func (c *sigctxt) r10() uint64</a>

```
searchKey: runtime.sigctxt.r10
tags: [function private]
```

```Go
func (c *sigctxt) r10() uint64
```

#### <a id="sigctxt.r11" href="#sigctxt.r11">func (c *sigctxt) r11() uint64</a>

```
searchKey: runtime.sigctxt.r11
tags: [function private]
```

```Go
func (c *sigctxt) r11() uint64
```

#### <a id="sigctxt.r12" href="#sigctxt.r12">func (c *sigctxt) r12() uint64</a>

```
searchKey: runtime.sigctxt.r12
tags: [function private]
```

```Go
func (c *sigctxt) r12() uint64
```

#### <a id="sigctxt.r13" href="#sigctxt.r13">func (c *sigctxt) r13() uint64</a>

```
searchKey: runtime.sigctxt.r13
tags: [function private]
```

```Go
func (c *sigctxt) r13() uint64
```

#### <a id="sigctxt.r14" href="#sigctxt.r14">func (c *sigctxt) r14() uint64</a>

```
searchKey: runtime.sigctxt.r14
tags: [function private]
```

```Go
func (c *sigctxt) r14() uint64
```

#### <a id="sigctxt.r15" href="#sigctxt.r15">func (c *sigctxt) r15() uint64</a>

```
searchKey: runtime.sigctxt.r15
tags: [function private]
```

```Go
func (c *sigctxt) r15() uint64
```

#### <a id="sigctxt.r8" href="#sigctxt.r8">func (c *sigctxt) r8() uint64</a>

```
searchKey: runtime.sigctxt.r8
tags: [function private]
```

```Go
func (c *sigctxt) r8() uint64
```

#### <a id="sigctxt.r9" href="#sigctxt.r9">func (c *sigctxt) r9() uint64</a>

```
searchKey: runtime.sigctxt.r9
tags: [function private]
```

```Go
func (c *sigctxt) r9() uint64
```

#### <a id="sigctxt.rax" href="#sigctxt.rax">func (c *sigctxt) rax() uint64</a>

```
searchKey: runtime.sigctxt.rax
tags: [function private]
```

```Go
func (c *sigctxt) rax() uint64
```

#### <a id="sigctxt.rbp" href="#sigctxt.rbp">func (c *sigctxt) rbp() uint64</a>

```
searchKey: runtime.sigctxt.rbp
tags: [function private]
```

```Go
func (c *sigctxt) rbp() uint64
```

#### <a id="sigctxt.rbx" href="#sigctxt.rbx">func (c *sigctxt) rbx() uint64</a>

```
searchKey: runtime.sigctxt.rbx
tags: [function private]
```

```Go
func (c *sigctxt) rbx() uint64
```

#### <a id="sigctxt.rcx" href="#sigctxt.rcx">func (c *sigctxt) rcx() uint64</a>

```
searchKey: runtime.sigctxt.rcx
tags: [function private]
```

```Go
func (c *sigctxt) rcx() uint64
```

#### <a id="sigctxt.rdi" href="#sigctxt.rdi">func (c *sigctxt) rdi() uint64</a>

```
searchKey: runtime.sigctxt.rdi
tags: [function private]
```

```Go
func (c *sigctxt) rdi() uint64
```

#### <a id="sigctxt.rdx" href="#sigctxt.rdx">func (c *sigctxt) rdx() uint64</a>

```
searchKey: runtime.sigctxt.rdx
tags: [function private]
```

```Go
func (c *sigctxt) rdx() uint64
```

#### <a id="sigctxt.regs" href="#sigctxt.regs">func (c *sigctxt) regs() *regs64</a>

```
searchKey: runtime.sigctxt.regs
tags: [function private]
```

```Go
func (c *sigctxt) regs() *regs64
```

#### <a id="sigctxt.rflags" href="#sigctxt.rflags">func (c *sigctxt) rflags() uint64</a>

```
searchKey: runtime.sigctxt.rflags
tags: [function private]
```

```Go
func (c *sigctxt) rflags() uint64
```

#### <a id="sigctxt.rip" href="#sigctxt.rip">func (c *sigctxt) rip() uint64</a>

```
searchKey: runtime.sigctxt.rip
tags: [function private]
```

```Go
func (c *sigctxt) rip() uint64
```

#### <a id="sigctxt.rsi" href="#sigctxt.rsi">func (c *sigctxt) rsi() uint64</a>

```
searchKey: runtime.sigctxt.rsi
tags: [function private]
```

```Go
func (c *sigctxt) rsi() uint64
```

#### <a id="sigctxt.rsp" href="#sigctxt.rsp">func (c *sigctxt) rsp() uint64</a>

```
searchKey: runtime.sigctxt.rsp
tags: [function private]
```

```Go
func (c *sigctxt) rsp() uint64
```

#### <a id="sigctxt.set_rip" href="#sigctxt.set_rip">func (c *sigctxt) set_rip(x uint64)</a>

```
searchKey: runtime.sigctxt.set_rip
tags: [method private]
```

```Go
func (c *sigctxt) set_rip(x uint64)
```

#### <a id="sigctxt.set_rsp" href="#sigctxt.set_rsp">func (c *sigctxt) set_rsp(x uint64)</a>

```
searchKey: runtime.sigctxt.set_rsp
tags: [method private]
```

```Go
func (c *sigctxt) set_rsp(x uint64)
```

#### <a id="sigctxt.set_sigaddr" href="#sigctxt.set_sigaddr">func (c *sigctxt) set_sigaddr(x uint64)</a>

```
searchKey: runtime.sigctxt.set_sigaddr
tags: [method private]
```

```Go
func (c *sigctxt) set_sigaddr(x uint64)
```

#### <a id="sigctxt.set_sigcode" href="#sigctxt.set_sigcode">func (c *sigctxt) set_sigcode(x uint64)</a>

```
searchKey: runtime.sigctxt.set_sigcode
tags: [method private]
```

```Go
func (c *sigctxt) set_sigcode(x uint64)
```

#### <a id="sigctxt.sigaddr" href="#sigctxt.sigaddr">func (c *sigctxt) sigaddr() uint64</a>

```
searchKey: runtime.sigctxt.sigaddr
tags: [function private]
```

```Go
func (c *sigctxt) sigaddr() uint64
```

#### <a id="sigctxt.sigcode" href="#sigctxt.sigcode">func (c *sigctxt) sigcode() uint64</a>

```
searchKey: runtime.sigctxt.sigcode
tags: [function private]
```

```Go
func (c *sigctxt) sigcode() uint64
```

#### <a id="sigctxt.siglr" href="#sigctxt.siglr">func (c *sigctxt) siglr() uintptr</a>

```
searchKey: runtime.sigctxt.siglr
tags: [function private]
```

```Go
func (c *sigctxt) siglr() uintptr
```

#### <a id="sigctxt.sigpc" href="#sigctxt.sigpc">func (c *sigctxt) sigpc() uintptr</a>

```
searchKey: runtime.sigctxt.sigpc
tags: [function private]
```

```Go
func (c *sigctxt) sigpc() uintptr
```

#### <a id="sigctxt.sigsp" href="#sigctxt.sigsp">func (c *sigctxt) sigsp() uintptr</a>

```
searchKey: runtime.sigctxt.sigsp
tags: [function private]
```

```Go
func (c *sigctxt) sigsp() uintptr
```

### <a id="siginfo" href="#siginfo">type siginfo struct</a>

```
searchKey: runtime.siginfo
tags: [struct private]
```

```Go
type siginfo struct {
	si_signo  int32
	si_errno  int32
	si_code   int32
	si_pid    int32
	si_uid    uint32
	si_status int32
	si_addr   uint64
	si_value  [8]byte
	si_band   int64
	__pad     [7]uint64
}
```

### <a id="sigset" href="#sigset">type sigset uint32</a>

```
searchKey: runtime.sigset
tags: [number private]
```

```Go
type sigset uint32
```

### <a id="slice" href="#slice">type slice struct</a>

```
searchKey: runtime.slice
tags: [struct private]
```

```Go
type slice struct {
	array unsafe.Pointer
	len   int
	cap   int
}
```

#### <a id="growslice" href="#growslice">func growslice(et *_type, old slice, cap int) slice</a>

```
searchKey: runtime.growslice
tags: [method private]
```

```Go
func growslice(et *_type, old slice, cap int) slice
```

growslice handles slice growth during append. It is passed the slice element type, the old slice, and the desired new minimum capacity, and it returns a new slice with at least that capacity, with the old data copied into it. The new slice's length is set to the old slice's length, NOT to the new requested capacity. This is for codegen convenience. The old slice's length is used immediately to calculate where to write new values during an append. TODO: When the old backend is gone, reconsider this decision. The SSA backend might prefer the new length or to return only ptr/cap and save stack space. 

### <a id="sliceInterfacePtr" href="#sliceInterfacePtr">type sliceInterfacePtr []byte</a>

```
searchKey: runtime.sliceInterfacePtr
tags: [array number private]
```

```Go
type sliceInterfacePtr []byte
```

The specialized convTx routines need a type descriptor to use when calling mallocgc. We don't need the type to be exact, just to have the correct size, alignment, and pointer-ness. However, when debugging, it'd be nice to have some indication in mallocgc where the types came from, so we use named types here. We then construct interface values of these types, and then extract the type word to use as needed. 

### <a id="slicetype" href="#slicetype">type slicetype struct</a>

```
searchKey: runtime.slicetype
tags: [struct private]
```

```Go
type slicetype struct {
	typ  _type
	elem *_type
}
```

### <a id="spanAllocType" href="#spanAllocType">type spanAllocType uint8</a>

```
searchKey: runtime.spanAllocType
tags: [number private]
```

```Go
type spanAllocType uint8
```

spanAllocType represents the type of allocation to make, or the type of allocation to be freed. 

#### <a id="spanAllocType.manual" href="#spanAllocType.manual">func (s spanAllocType) manual() bool</a>

```
searchKey: runtime.spanAllocType.manual
tags: [function private]
```

```Go
func (s spanAllocType) manual() bool
```

manual returns true if the span allocation is manually managed. 

### <a id="spanClass" href="#spanClass">type spanClass uint8</a>

```
searchKey: runtime.spanClass
tags: [number private]
```

```Go
type spanClass uint8
```

A spanClass represents the size class and noscan-ness of a span. 

Each size class has a noscan spanClass and a scan spanClass. The noscan spanClass contains only noscan objects, which do not contain pointers and thus do not need to be scanned by the garbage collector. 

#### <a id="makeSpanClass" href="#makeSpanClass">func makeSpanClass(sizeclass uint8, noscan bool) spanClass</a>

```
searchKey: runtime.makeSpanClass
tags: [method private]
```

```Go
func makeSpanClass(sizeclass uint8, noscan bool) spanClass
```

#### <a id="spanClass.noscan" href="#spanClass.noscan">func (sc spanClass) noscan() bool</a>

```
searchKey: runtime.spanClass.noscan
tags: [function private]
```

```Go
func (sc spanClass) noscan() bool
```

#### <a id="spanClass.sizeclass" href="#spanClass.sizeclass">func (sc spanClass) sizeclass() int8</a>

```
searchKey: runtime.spanClass.sizeclass
tags: [function private]
```

```Go
func (sc spanClass) sizeclass() int8
```

### <a id="spanSet" href="#spanSet">type spanSet struct</a>

```
searchKey: runtime.spanSet
tags: [struct private]
```

```Go
type spanSet struct {
	spineLock mutex
	spine     unsafe.Pointer // *[N]*spanSetBlock, accessed atomically
	spineLen  uintptr        // Spine array length, accessed atomically
	spineCap  uintptr        // Spine array cap, accessed under lock

	// index is the head and tail of the spanSet in a single field.
	// The head and the tail both represent an index into the logical
	// concatenation of all blocks, with the head always behind or
	// equal to the tail (indicating an empty set). This field is
	// always accessed atomically.
	//
	// The head and the tail are only 32 bits wide, which means we
	// can only support up to 2^32 pushes before a reset. If every
	// span in the heap were stored in this set, and each span were
	// the minimum size (1 runtime page, 8 KiB), then roughly the
	// smallest heap which would be unrepresentable is 32 TiB in size.
	index headTailIndex
}
```

A spanSet is a set of *mspans. 

spanSet is safe for concurrent push and pop operations. 

#### <a id="spanSet.pop" href="#spanSet.pop">func (b *spanSet) pop() *mspan</a>

```
searchKey: runtime.spanSet.pop
tags: [function private]
```

```Go
func (b *spanSet) pop() *mspan
```

pop removes and returns a span from buffer b, or nil if b is empty. pop is safe to call concurrently with other pop and push operations. 

#### <a id="spanSet.push" href="#spanSet.push">func (b *spanSet) push(s *mspan)</a>

```
searchKey: runtime.spanSet.push
tags: [method private]
```

```Go
func (b *spanSet) push(s *mspan)
```

push adds span s to buffer b. push is safe to call concurrently with other push and pop operations. 

#### <a id="spanSet.reset" href="#spanSet.reset">func (b *spanSet) reset()</a>

```
searchKey: runtime.spanSet.reset
tags: [function private]
```

```Go
func (b *spanSet) reset()
```

reset resets a spanSet which is empty. It will also clean up any left over blocks. 

Throws if the buf is not empty. 

reset may not be called concurrently with any other operations on the span set. 

### <a id="spanSetBlock" href="#spanSetBlock">type spanSetBlock struct</a>

```
searchKey: runtime.spanSetBlock
tags: [struct private]
```

```Go
type spanSetBlock struct {
	// Free spanSetBlocks are managed via a lock-free stack.
	lfnode

	// popped is the number of pop operations that have occurred on
	// this block. This number is used to help determine when a block
	// may be safely recycled.
	popped uint32

	// spans is the set of spans in this block.
	spans [spanSetBlockEntries]*mspan
}
```

### <a id="spanSetBlockAlloc" href="#spanSetBlockAlloc">type spanSetBlockAlloc struct</a>

```
searchKey: runtime.spanSetBlockAlloc
tags: [struct private]
```

```Go
type spanSetBlockAlloc struct {
	stack lfstack
}
```

spanSetBlockAlloc represents a concurrent pool of spanSetBlocks. 

#### <a id="spanSetBlockAlloc.alloc" href="#spanSetBlockAlloc.alloc">func (p *spanSetBlockAlloc) alloc() *spanSetBlock</a>

```
searchKey: runtime.spanSetBlockAlloc.alloc
tags: [function private]
```

```Go
func (p *spanSetBlockAlloc) alloc() *spanSetBlock
```

alloc tries to grab a spanSetBlock out of the pool, and if it fails persistentallocs a new one and returns it. 

#### <a id="spanSetBlockAlloc.free" href="#spanSetBlockAlloc.free">func (p *spanSetBlockAlloc) free(block *spanSetBlock)</a>

```
searchKey: runtime.spanSetBlockAlloc.free
tags: [method private]
```

```Go
func (p *spanSetBlockAlloc) free(block *spanSetBlock)
```

free returns a spanSetBlock back to the pool. 

### <a id="special" href="#special">type special struct</a>

```
searchKey: runtime.special
tags: [struct private]
```

```Go
type special struct {
	next   *special // linked list in span
	offset uint16   // span offset of object
	kind   byte     // kind of special
}
```

#### <a id="removespecial" href="#removespecial">func removespecial(p unsafe.Pointer, kind uint8) *special</a>

```
searchKey: runtime.removespecial
tags: [method private]
```

```Go
func removespecial(p unsafe.Pointer, kind uint8) *special
```

Removes the Special record of the given kind for the object p. Returns the record if the record existed, nil otherwise. The caller must FixAlloc_Free the result. 

### <a id="specialReachable" href="#specialReachable">type specialReachable struct</a>

```
searchKey: runtime.specialReachable
tags: [struct private]
```

```Go
type specialReachable struct {
	special   special
	done      bool
	reachable bool
}
```

specialReachable tracks whether an object is reachable on the next GC cycle. This is used by testing. 

### <a id="specialfinalizer" href="#specialfinalizer">type specialfinalizer struct</a>

```
searchKey: runtime.specialfinalizer
tags: [struct private]
```

```Go
type specialfinalizer struct {
	special special
	fn      *funcval // May be a heap pointer.
	nret    uintptr
	fint    *_type   // May be a heap pointer, but always live.
	ot      *ptrtype // May be a heap pointer, but always live.
}
```

The described object has a finalizer set for it. 

specialfinalizer is allocated from non-GC'd memory, so any heap pointers must be specially handled. 

### <a id="specialprofile" href="#specialprofile">type specialprofile struct</a>

```
searchKey: runtime.specialprofile
tags: [struct private]
```

```Go
type specialprofile struct {
	special special
	b       *bucket
}
```

The described object is being heap profiled. 

### <a id="specialsIter" href="#specialsIter">type specialsIter struct</a>

```
searchKey: runtime.specialsIter
tags: [struct private]
```

```Go
type specialsIter struct {
	pprev **special
	s     *special
}
```

specialsIter helps iterate over specials lists. 

#### <a id="newSpecialsIter" href="#newSpecialsIter">func newSpecialsIter(span *mspan) specialsIter</a>

```
searchKey: runtime.newSpecialsIter
tags: [method private]
```

```Go
func newSpecialsIter(span *mspan) specialsIter
```

#### <a id="specialsIter.next" href="#specialsIter.next">func (i *specialsIter) next()</a>

```
searchKey: runtime.specialsIter.next
tags: [function private]
```

```Go
func (i *specialsIter) next()
```

#### <a id="specialsIter.unlinkAndNext" href="#specialsIter.unlinkAndNext">func (i *specialsIter) unlinkAndNext() *special</a>

```
searchKey: runtime.specialsIter.unlinkAndNext
tags: [function private]
```

```Go
func (i *specialsIter) unlinkAndNext() *special
```

unlinkAndNext removes the current special from the list and moves the iterator to the next special. It returns the unlinked special. 

#### <a id="specialsIter.valid" href="#specialsIter.valid">func (i *specialsIter) valid() bool</a>

```
searchKey: runtime.specialsIter.valid
tags: [function private]
```

```Go
func (i *specialsIter) valid() bool
```

### <a id="stack" href="#stack">type stack struct</a>

```
searchKey: runtime.stack
tags: [struct private]
```

```Go
type stack struct {
	lo uintptr
	hi uintptr
}
```

Stack describes a Go execution stack. The bounds of the stack are exactly [lo, hi), with no implicit data structures on either side. 

#### <a id="stackalloc" href="#stackalloc">func stackalloc(n uint32) stack</a>

```
searchKey: runtime.stackalloc
tags: [method private]
```

```Go
func stackalloc(n uint32) stack
```

stackalloc allocates an n byte stack. 

stackalloc must run on the system stack because it uses per-P resources and must not split the stack. 

### <a id="stackObject" href="#stackObject">type stackObject struct</a>

```
searchKey: runtime.stackObject
tags: [struct private]
```

```Go
type stackObject struct {
	off   uint32             // offset above stack.lo
	size  uint32             // size of object
	r     *stackObjectRecord // info of the object (for ptr/nonptr bits). nil if object has been scanned.
	left  *stackObject       // objects with lower addresses
	right *stackObject       // objects with higher addresses
}
```

A stackObject represents a variable on the stack that has had its address taken. 

#### <a id="binarySearchTree" href="#binarySearchTree">func binarySearchTree(x *stackObjectBuf, idx int, n int) (root *stackObject, restBuf *stackObjectBuf, restIdx int)</a>

```
searchKey: runtime.binarySearchTree
tags: [method private]
```

```Go
func binarySearchTree(x *stackObjectBuf, idx int, n int) (root *stackObject, restBuf *stackObjectBuf, restIdx int)
```

Build a binary search tree with the n objects in the list x.obj[idx], x.obj[idx+1], ..., x.next.obj[0], ... Returns the root of that tree, and the buf+idx of the nth object after x.obj[idx]. (The first object that was not included in the binary search tree.) If n == 0, returns nil, x. 

#### <a id="stackObject.setRecord" href="#stackObject.setRecord">func (obj *stackObject) setRecord(r *stackObjectRecord)</a>

```
searchKey: runtime.stackObject.setRecord
tags: [method private]
```

```Go
func (obj *stackObject) setRecord(r *stackObjectRecord)
```

obj.r = r, but with no write barrier. 

### <a id="stackObjectBuf" href="#stackObjectBuf">type stackObjectBuf struct</a>

```
searchKey: runtime.stackObjectBuf
tags: [struct private]
```

```Go
type stackObjectBuf struct {
	stackObjectBufHdr
	obj [(_WorkbufSize - unsafe.Sizeof(stackObjectBufHdr{})) / unsafe.Sizeof(stackObject{})]stackObject
}
```

Buffer for stack objects found on a goroutine stack. Must be smaller than or equal to workbuf. 

### <a id="stackObjectBufHdr" href="#stackObjectBufHdr">type stackObjectBufHdr struct</a>

```
searchKey: runtime.stackObjectBufHdr
tags: [struct private]
```

```Go
type stackObjectBufHdr struct {
	workbufhdr
	next *stackObjectBuf
}
```

### <a id="stackObjectRecord" href="#stackObjectRecord">type stackObjectRecord struct</a>

```
searchKey: runtime.stackObjectRecord
tags: [struct private]
```

```Go
type stackObjectRecord struct {
	// offset in frame
	// if negative, offset from varp
	// if non-negative, offset from argp
	off      int32
	size     int32
	_ptrdata int32 // ptrdata, or -ptrdata is GC prog is used
	gcdata   *byte // pointer map or GC prog of the type
}
```

A stackObjectRecord is generated by the compiler for each stack object in a stack frame. This record must match the generator code in cmd/compile/internal/liveness/plive.go:emitStackObjects. 

#### <a id="stackObjectRecord.ptrdata" href="#stackObjectRecord.ptrdata">func (r *stackObjectRecord) ptrdata() uintptr</a>

```
searchKey: runtime.stackObjectRecord.ptrdata
tags: [function private]
```

```Go
func (r *stackObjectRecord) ptrdata() uintptr
```

#### <a id="stackObjectRecord.useGCProg" href="#stackObjectRecord.useGCProg">func (r *stackObjectRecord) useGCProg() bool</a>

```
searchKey: runtime.stackObjectRecord.useGCProg
tags: [function private]
```

```Go
func (r *stackObjectRecord) useGCProg() bool
```

### <a id="stackScanState" href="#stackScanState">type stackScanState struct</a>

```
searchKey: runtime.stackScanState
tags: [struct private]
```

```Go
type stackScanState struct {
	cache pcvalueCache

	// stack limits
	stack stack

	// conservative indicates that the next frame must be scanned conservatively.
	// This applies only to the innermost frame at an async safe-point.
	conservative bool

	// buf contains the set of possible pointers to stack objects.
	// Organized as a LIFO linked list of buffers.
	// All buffers except possibly the head buffer are full.
	buf     *stackWorkBuf
	freeBuf *stackWorkBuf // keep around one free buffer for allocation hysteresis

	// cbuf contains conservative pointers to stack objects. If
	// all pointers to a stack object are obtained via
	// conservative scanning, then the stack object may be dead
	// and may contain dead pointers, so it must be scanned
	// defensively.
	cbuf *stackWorkBuf

	// list of stack objects
	// Objects are in increasing address order.
	head  *stackObjectBuf
	tail  *stackObjectBuf
	nobjs int

	// root of binary tree for fast object lookup by address
	// Initialized by buildIndex.
	root *stackObject
}
```

A stackScanState keeps track of the state used during the GC walk of a goroutine. 

#### <a id="stackScanState.addObject" href="#stackScanState.addObject">func (s *stackScanState) addObject(addr uintptr, r *stackObjectRecord)</a>

```
searchKey: runtime.stackScanState.addObject
tags: [method private]
```

```Go
func (s *stackScanState) addObject(addr uintptr, r *stackObjectRecord)
```

addObject adds a stack object at addr of type typ to the set of stack objects. 

#### <a id="stackScanState.buildIndex" href="#stackScanState.buildIndex">func (s *stackScanState) buildIndex()</a>

```
searchKey: runtime.stackScanState.buildIndex
tags: [function private]
```

```Go
func (s *stackScanState) buildIndex()
```

buildIndex initializes s.root to a binary search tree. It should be called after all addObject calls but before any call of findObject. 

#### <a id="stackScanState.findObject" href="#stackScanState.findObject">func (s *stackScanState) findObject(a uintptr) *stackObject</a>

```
searchKey: runtime.stackScanState.findObject
tags: [method private]
```

```Go
func (s *stackScanState) findObject(a uintptr) *stackObject
```

findObject returns the stack object containing address a, if any. Must have called buildIndex previously. 

#### <a id="stackScanState.getPtr" href="#stackScanState.getPtr">func (s *stackScanState) getPtr() (p uintptr, conservative bool)</a>

```
searchKey: runtime.stackScanState.getPtr
tags: [function private]
```

```Go
func (s *stackScanState) getPtr() (p uintptr, conservative bool)
```

Remove and return a potential pointer to a stack object. Returns 0 if there are no more pointers available. 

This prefers non-conservative pointers so we scan stack objects precisely if there are any non-conservative pointers to them. 

#### <a id="stackScanState.putPtr" href="#stackScanState.putPtr">func (s *stackScanState) putPtr(p uintptr, conservative bool)</a>

```
searchKey: runtime.stackScanState.putPtr
tags: [method private]
```

```Go
func (s *stackScanState) putPtr(p uintptr, conservative bool)
```

Add p as a potential pointer to a stack object. p must be a stack address. 

### <a id="stackWorkBuf" href="#stackWorkBuf">type stackWorkBuf struct</a>

```
searchKey: runtime.stackWorkBuf
tags: [struct private]
```

```Go
type stackWorkBuf struct {
	stackWorkBufHdr
	obj [(_WorkbufSize - unsafe.Sizeof(stackWorkBufHdr{})) / sys.PtrSize]uintptr
}
```

Buffer for pointers found during stack tracing. Must be smaller than or equal to workbuf. 

### <a id="stackWorkBufHdr" href="#stackWorkBufHdr">type stackWorkBufHdr struct</a>

```
searchKey: runtime.stackWorkBufHdr
tags: [struct private]
```

```Go
type stackWorkBufHdr struct {
	workbufhdr
	next *stackWorkBuf // linked list of workbufs

}
```

Header declaration must come after the buf declaration above, because of issue #14620. 

### <a id="stackfreelist" href="#stackfreelist">type stackfreelist struct</a>

```
searchKey: runtime.stackfreelist
tags: [struct private]
```

```Go
type stackfreelist struct {
	list gclinkptr // linked list of free stacks
	size uintptr   // total size of stacks in list
}
```

### <a id="stackmap" href="#stackmap">type stackmap struct</a>

```
searchKey: runtime.stackmap
tags: [struct private]
```

```Go
type stackmap struct {
	n        int32   // number of bitmaps
	nbit     int32   // number of bits in each bitmap
	bytedata [1]byte // bitmaps, each starting on a byte boundary
}
```

### <a id="stackpoolItem" href="#stackpoolItem">type stackpoolItem struct</a>

```
searchKey: runtime.stackpoolItem
tags: [struct private]
```

```Go
type stackpoolItem struct {
	mu   mutex
	span mSpanList
}
```

### <a id="stackt" href="#stackt">type stackt struct</a>

```
searchKey: runtime.stackt
tags: [struct private]
```

```Go
type stackt struct {
	ss_sp     *byte
	ss_size   uintptr
	ss_flags  int32
	pad_cgo_0 [4]byte
}
```

### <a id="statAggregate" href="#statAggregate">type statAggregate struct</a>

```
searchKey: runtime.statAggregate
tags: [struct private]
```

```Go
type statAggregate struct {
	ensured   statDepSet
	heapStats heapStatsAggregate
	sysStats  sysStatsAggregate
}
```

statAggregate is the main driver of the metrics implementation. 

It contains multiple aggregates of runtime statistics, as well as a set of these aggregates that it has populated. The aggergates are populated lazily by its ensure method. 

#### <a id="statAggregate.ensure" href="#statAggregate.ensure">func (a *statAggregate) ensure(deps *statDepSet)</a>

```
searchKey: runtime.statAggregate.ensure
tags: [method private]
```

```Go
func (a *statAggregate) ensure(deps *statDepSet)
```

ensure populates statistics aggregates determined by deps if they haven't yet been populated. 

### <a id="statDep" href="#statDep">type statDep uint</a>

```
searchKey: runtime.statDep
tags: [number private]
```

```Go
type statDep uint
```

statDep is a dependency on a group of statistics that a metric might have. 

### <a id="statDepSet" href="#statDepSet">type statDepSet [1]uint64</a>

```
searchKey: runtime.statDepSet
tags: [array number private]
```

```Go
type statDepSet [1]uint64
```

statDepSet represents a set of statDeps. 

Under the hood, it's a bitmap. 

#### <a id="makeStatDepSet" href="#makeStatDepSet">func makeStatDepSet(deps ...statDep) statDepSet</a>

```
searchKey: runtime.makeStatDepSet
tags: [method private]
```

```Go
func makeStatDepSet(deps ...statDep) statDepSet
```

makeStatDepSet creates a new statDepSet from a list of statDeps. 

#### <a id="statDepSet.difference" href="#statDepSet.difference">func (s statDepSet) difference(b statDepSet) statDepSet</a>

```
searchKey: runtime.statDepSet.difference
tags: [method private]
```

```Go
func (s statDepSet) difference(b statDepSet) statDepSet
```

differennce returns set difference of s from b as a new set. 

#### <a id="statDepSet.empty" href="#statDepSet.empty">func (s *statDepSet) empty() bool</a>

```
searchKey: runtime.statDepSet.empty
tags: [function private]
```

```Go
func (s *statDepSet) empty() bool
```

empty returns true if there are no dependencies in the set. 

#### <a id="statDepSet.has" href="#statDepSet.has">func (s *statDepSet) has(d statDep) bool</a>

```
searchKey: runtime.statDepSet.has
tags: [method private]
```

```Go
func (s *statDepSet) has(d statDep) bool
```

has returns true if the set contains a given statDep. 

#### <a id="statDepSet.union" href="#statDepSet.union">func (s statDepSet) union(b statDepSet) statDepSet</a>

```
searchKey: runtime.statDepSet.union
tags: [method private]
```

```Go
func (s statDepSet) union(b statDepSet) statDepSet
```

union returns the union of the two sets as a new set. 

### <a id="stkframe" href="#stkframe">type stkframe struct</a>

```
searchKey: runtime.stkframe
tags: [struct private]
```

```Go
type stkframe struct {
	fn       funcInfo   // function being run
	pc       uintptr    // program counter within fn
	continpc uintptr    // program counter where execution can continue, or 0 if not
	lr       uintptr    // program counter at caller aka link register
	sp       uintptr    // stack pointer at pc
	fp       uintptr    // stack pointer at caller aka frame pointer
	varp     uintptr    // top of local variables
	argp     uintptr    // pointer to function arguments
	arglen   uintptr    // number of bytes at argp
	argmap   *bitvector // force use of this argmap
}
```

stack traces 

### <a id="stringInterfacePtr" href="#stringInterfacePtr">type stringInterfacePtr string</a>

```
searchKey: runtime.stringInterfacePtr
tags: [string private]
```

```Go
type stringInterfacePtr string
```

The specialized convTx routines need a type descriptor to use when calling mallocgc. We don't need the type to be exact, just to have the correct size, alignment, and pointer-ness. However, when debugging, it'd be nice to have some indication in mallocgc where the types came from, so we use named types here. We then construct interface values of these types, and then extract the type word to use as needed. 

### <a id="stringStruct" href="#stringStruct">type stringStruct struct</a>

```
searchKey: runtime.stringStruct
tags: [struct private]
```

```Go
type stringStruct struct {
	str unsafe.Pointer
	len int
}
```

#### <a id="stringStructOf" href="#stringStructOf">func stringStructOf(sp *string) *stringStruct</a>

```
searchKey: runtime.stringStructOf
tags: [method private]
```

```Go
func stringStructOf(sp *string) *stringStruct
```

### <a id="stringStructDWARF" href="#stringStructDWARF">type stringStructDWARF struct</a>

```
searchKey: runtime.stringStructDWARF
tags: [struct private]
```

```Go
type stringStructDWARF struct {
	str *byte
	len int
}
```

Variant with *byte pointer type for DWARF debugging. 

### <a id="stringer" href="#stringer">type stringer interface</a>

```
searchKey: runtime.stringer
tags: [interface private]
```

```Go
type stringer interface {
	String() string
}
```

### <a id="structfield" href="#structfield">type structfield struct</a>

```
searchKey: runtime.structfield
tags: [struct private]
```

```Go
type structfield struct {
	name       name
	typ        *_type
	offsetAnon uintptr
}
```

#### <a id="structfield.offset" href="#structfield.offset">func (f *structfield) offset() uintptr</a>

```
searchKey: runtime.structfield.offset
tags: [function private]
```

```Go
func (f *structfield) offset() uintptr
```

### <a id="structtype" href="#structtype">type structtype struct</a>

```
searchKey: runtime.structtype
tags: [struct private]
```

```Go
type structtype struct {
	typ     _type
	pkgPath name
	fields  []structfield
}
```

### <a id="sudog" href="#sudog">type sudog struct</a>

```
searchKey: runtime.sudog
tags: [struct private]
```

```Go
type sudog struct {
	g *g

	next *sudog
	prev *sudog
	elem unsafe.Pointer // data element (may point to stack)

	acquiretime int64
	releasetime int64
	ticket      uint32

	// isSelect indicates g is participating in a select, so
	// g.selectDone must be CAS'd to win the wake-up race.
	isSelect bool

	// success indicates whether communication over channel c
	// succeeded. It is true if the goroutine was awoken because a
	// value was delivered over channel c, and false if awoken
	// because c was closed.
	success bool

	parent   *sudog // semaRoot binary tree
	waitlink *sudog // g.waiting list or semaRoot
	waittail *sudog // semaRoot
	c        *hchan // channel
}
```

sudog represents a g in a wait list, such as for sending/receiving on a channel. 

sudog is necessary because the g ↔ synchronization object relation is many-to-many. A g can be on many wait lists, so there may be many sudogs for one g; and many gs may be waiting on the same synchronization object, so there may be many sudogs for one object. 

sudogs are allocated from a special pool. Use acquireSudog and releaseSudog to allocate and free them. 

### <a id="suspendGState" href="#suspendGState">type suspendGState struct</a>

```
searchKey: runtime.suspendGState
tags: [struct private]
```

```Go
type suspendGState struct {
	g *g

	// dead indicates the goroutine was not suspended because it
	// is dead. This goroutine could be reused after the dead
	// state was observed, so the caller must not assume that it
	// remains dead.
	dead bool

	// stopped indicates that this suspendG transitioned the G to
	// _Gwaiting via g.preemptStop and thus is responsible for
	// readying it when done.
	stopped bool
}
```

#### <a id="suspendG" href="#suspendG">func suspendG(gp *g) suspendGState</a>

```
searchKey: runtime.suspendG
tags: [method private]
```

```Go
func suspendG(gp *g) suspendGState
```

suspendG suspends goroutine gp at a safe-point and returns the state of the suspended goroutine. The caller gets read access to the goroutine until it calls resumeG. 

It is safe for multiple callers to attempt to suspend the same goroutine at the same time. The goroutine may execute between subsequent successful suspend operations. The current implementation grants exclusive access to the goroutine, and hence multiple callers will serialize. However, the intent is to grant shared read access, so please don't depend on exclusive access. 

This must be called from the system stack and the user goroutine on the current M (if any) must be in a preemptible state. This prevents deadlocks where two goroutines attempt to suspend each other and both are in non-preemptible states. There are other ways to resolve this deadlock, but this seems simplest. 

TODO(austin): What if we instead required this to be called from a user goroutine? Then we could deschedule the goroutine while waiting instead of blocking the thread. If two goroutines tried to suspend each other, one of them would win and the other wouldn't complete the suspend until it was resumed. We would have to be careful that they couldn't actually queue up suspend for each other and then both be suspended. This would also avoid the need for a kernel context switch in the synchronous case because we could just directly schedule the waiter. The context switch is unavoidable in the signal case. 

### <a id="sweepClass" href="#sweepClass">type sweepClass uint32</a>

```
searchKey: runtime.sweepClass
tags: [number private]
```

```Go
type sweepClass uint32
```

sweepClass is a spanClass and one bit to represent whether we're currently sweeping partial or full spans. 

#### <a id="sweepClass.clear" href="#sweepClass.clear">func (s *sweepClass) clear()</a>

```
searchKey: runtime.sweepClass.clear
tags: [function private]
```

```Go
func (s *sweepClass) clear()
```

#### <a id="sweepClass.load" href="#sweepClass.load">func (s *sweepClass) load() sweepClass</a>

```
searchKey: runtime.sweepClass.load
tags: [function private]
```

```Go
func (s *sweepClass) load() sweepClass
```

#### <a id="sweepClass.split" href="#sweepClass.split">func (s sweepClass) split() (spc spanClass, full bool)</a>

```
searchKey: runtime.sweepClass.split
tags: [function private]
```

```Go
func (s sweepClass) split() (spc spanClass, full bool)
```

split returns the underlying span class as well as whether we're interested in the full or partial unswept lists for that class, indicated as a boolean (true means "full"). 

#### <a id="sweepClass.update" href="#sweepClass.update">func (s *sweepClass) update(sNew sweepClass)</a>

```
searchKey: runtime.sweepClass.update
tags: [method private]
```

```Go
func (s *sweepClass) update(sNew sweepClass)
```

### <a id="sweepLocked" href="#sweepLocked">type sweepLocked struct</a>

```
searchKey: runtime.sweepLocked
tags: [struct private]
```

```Go
type sweepLocked struct {
	*mspan
}
```

sweepLocked represents sweep ownership of a span. 

#### <a id="sweepLocked.sweep" href="#sweepLocked.sweep">func (sl *sweepLocked) sweep(preserve bool) bool</a>

```
searchKey: runtime.sweepLocked.sweep
tags: [method private]
```

```Go
func (sl *sweepLocked) sweep(preserve bool) bool
```

Sweep frees or collects finalizers for blocks not marked in the mark phase. It clears the mark bits in preparation for the next GC round. Returns true if the span was returned to heap. If preserve=true, don't return it to heap nor relink in mcentral lists; caller takes care of it. 

### <a id="sweepLocker" href="#sweepLocker">type sweepLocker struct</a>

```
searchKey: runtime.sweepLocker
tags: [struct private]
```

```Go
type sweepLocker struct {
	// sweepGen is the sweep generation of the heap.
	sweepGen uint32
	// blocking indicates that this tracker is blocking sweep
	// completion, usually as a result of acquiring sweep
	// ownership of at least one span.
	blocking bool
}
```

sweepLocker acquires sweep ownership of spans and blocks sweep completion. 

#### <a id="newSweepLocker" href="#newSweepLocker">func newSweepLocker() sweepLocker</a>

```
searchKey: runtime.newSweepLocker
tags: [function private]
```

```Go
func newSweepLocker() sweepLocker
```

#### <a id="sweepLocker.blockCompletion" href="#sweepLocker.blockCompletion">func (l *sweepLocker) blockCompletion()</a>

```
searchKey: runtime.sweepLocker.blockCompletion
tags: [function private]
```

```Go
func (l *sweepLocker) blockCompletion()
```

blockCompletion blocks sweep completion without acquiring any specific spans. 

#### <a id="sweepLocker.dispose" href="#sweepLocker.dispose">func (l *sweepLocker) dispose()</a>

```
searchKey: runtime.sweepLocker.dispose
tags: [function private]
```

```Go
func (l *sweepLocker) dispose()
```

#### <a id="sweepLocker.sweepIsDone" href="#sweepLocker.sweepIsDone">func (l *sweepLocker) sweepIsDone()</a>

```
searchKey: runtime.sweepLocker.sweepIsDone
tags: [function private]
```

```Go
func (l *sweepLocker) sweepIsDone()
```

#### <a id="sweepLocker.tryAcquire" href="#sweepLocker.tryAcquire">func (l *sweepLocker) tryAcquire(s *mspan) (sweepLocked, bool)</a>

```
searchKey: runtime.sweepLocker.tryAcquire
tags: [method private]
```

```Go
func (l *sweepLocker) tryAcquire(s *mspan) (sweepLocked, bool)
```

tryAcquire attempts to acquire sweep ownership of span s. If it successfully acquires ownership, it blocks sweep completion. 

### <a id="sweepdata" href="#sweepdata">type sweepdata struct</a>

```
searchKey: runtime.sweepdata
tags: [struct private]
```

```Go
type sweepdata struct {
	lock    mutex
	g       *g
	parked  bool
	started bool

	nbgsweep    uint32
	npausesweep uint32

	// centralIndex is the current unswept span class.
	// It represents an index into the mcentral span
	// sets. Accessed and updated via its load and
	// update methods. Not protected by a lock.
	//
	// Reset at mark termination.
	// Used by mheap.nextSpanForSweep.
	centralIndex sweepClass
}
```

State of background sweep. 

### <a id="sysMemStat" href="#sysMemStat">type sysMemStat uint64</a>

```
searchKey: runtime.sysMemStat
tags: [number private]
```

```Go
type sysMemStat uint64
```

sysMemStat represents a global system statistic that is managed atomically. 

This type must structurally be a uint64 so that mstats aligns with MemStats. 

#### <a id="sysMemStat.add" href="#sysMemStat.add">func (s *sysMemStat) add(n int64)</a>

```
searchKey: runtime.sysMemStat.add
tags: [method private]
```

```Go
func (s *sysMemStat) add(n int64)
```

add atomically adds the sysMemStat by n. 

Must be nosplit as it is called in runtime initialization, e.g. newosproc0. 

#### <a id="sysMemStat.load" href="#sysMemStat.load">func (s *sysMemStat) load() uint64</a>

```
searchKey: runtime.sysMemStat.load
tags: [function private]
```

```Go
func (s *sysMemStat) load() uint64
```

load atomically reads the value of the stat. 

Must be nosplit as it is called in runtime initialization, e.g. newosproc0. 

### <a id="sysStatsAggregate" href="#sysStatsAggregate">type sysStatsAggregate struct</a>

```
searchKey: runtime.sysStatsAggregate
tags: [struct private]
```

```Go
type sysStatsAggregate struct {
	stacksSys      uint64
	mSpanSys       uint64
	mSpanInUse     uint64
	mCacheSys      uint64
	mCacheInUse    uint64
	buckHashSys    uint64
	gcMiscSys      uint64
	otherSys       uint64
	heapGoal       uint64
	gcCyclesDone   uint64
	gcCyclesForced uint64
}
```

sysStatsAggregate represents system memory stats obtained from the runtime. This set of stats is grouped together because they're all relatively cheap to acquire and generally independent of one another and other runtime memory stats. The fact that they may be acquired at different times, especially with respect to heapStatsAggregate, means there could be some skew, but because of these stats are independent, there's no real consistency issue here. 

#### <a id="sysStatsAggregate.compute" href="#sysStatsAggregate.compute">func (a *sysStatsAggregate) compute()</a>

```
searchKey: runtime.sysStatsAggregate.compute
tags: [function private]
```

```Go
func (a *sysStatsAggregate) compute()
```

compute populates the sysStatsAggregate with values from the runtime. 

### <a id="sysmontick" href="#sysmontick">type sysmontick struct</a>

```
searchKey: runtime.sysmontick
tags: [struct private]
```

```Go
type sysmontick struct {
	schedtick   uint32
	schedwhen   int64
	syscalltick uint32
	syscallwhen int64
}
```

### <a id="textOff" href="#textOff">type textOff int32</a>

```
searchKey: runtime.textOff
tags: [number private]
```

```Go
type textOff int32
```

### <a id="textsect" href="#textsect">type textsect struct</a>

```
searchKey: runtime.textsect
tags: [struct private]
```

```Go
type textsect struct {
	vaddr    uintptr // prelinked section vaddr
	length   uintptr // section length
	baseaddr uintptr // relocated section address
}
```

### <a id="tflag" href="#tflag">type tflag uint8</a>

```
searchKey: runtime.tflag
tags: [number private]
```

```Go
type tflag uint8
```

tflag is documented in reflect/type.go. 

tflag values must be kept in sync with copies in: 

```
	cmd/compile/internal/gc/reflect.go
	cmd/link/internal/ld/decodesym.go
	reflect/type.go
     internal/reflectlite/type.go

```
### <a id="timeHistogram" href="#timeHistogram">type timeHistogram struct</a>

```
searchKey: runtime.timeHistogram
tags: [struct private]
```

```Go
type timeHistogram struct {
	counts [timeHistNumSuperBuckets * timeHistNumSubBuckets]uint64

	// underflow counts all the times we got a negative duration
	// sample. Because of how time works on some platforms, it's
	// possible to measure negative durations. We could ignore them,
	// but we record them anyway because it's better to have some
	// signal that it's happening than just missing samples.
	underflow uint64
}
```

timeHistogram represents a distribution of durations in nanoseconds. 

The accuracy and range of the histogram is defined by the timeHistSubBucketBits and timeHistNumSuperBuckets constants. 

It is an HDR histogram with exponentially-distributed buckets and linearly distributed sub-buckets. 

Counts in the histogram are updated atomically, so it is safe for concurrent use. It is also safe to read all the values atomically. 

#### <a id="timeHistogram.record" href="#timeHistogram.record">func (h *timeHistogram) record(duration int64)</a>

```
searchKey: runtime.timeHistogram.record
tags: [method private]
```

```Go
func (h *timeHistogram) record(duration int64)
```

record adds the given duration to the distribution. 

Disallow preemptions and stack growths because this function may run in sensitive locations. 

### <a id="timer" href="#timer">type timer struct</a>

```
searchKey: runtime.timer
tags: [struct private]
```

```Go
type timer struct {
	// If this timer is on a heap, which P's heap it is on.
	// puintptr rather than *p to match uintptr in the versions
	// of this struct defined in other packages.
	pp puintptr

	// Timer wakes up at when, and then at when+period, ... (period > 0 only)
	// each time calling f(arg, now) in the timer goroutine, so f must be
	// a well-behaved function and not block.
	//
	// when must be positive on an active timer.
	when   int64
	period int64
	f      func(interface{}, uintptr)
	arg    interface{}
	seq    uintptr

	// What to set the when field to in timerModifiedXX status.
	nextwhen int64

	// The status field holds one of the values below.
	status uint32
}
```

Package time knows the layout of this structure. If this struct changes, adjust ../time/sleep.go:/runtimeTimer. 

### <a id="timespec" href="#timespec">type timespec struct</a>

```
searchKey: runtime.timespec
tags: [struct private]
```

```Go
type timespec struct {
	tv_sec  int64
	tv_nsec int64
}
```

#### <a id="timespec.setNsec" href="#timespec.setNsec">func (ts *timespec) setNsec(ns int64)</a>

```
searchKey: runtime.timespec.setNsec
tags: [method private]
```

```Go
func (ts *timespec) setNsec(ns int64)
```

### <a id="timeval" href="#timeval">type timeval struct</a>

```
searchKey: runtime.timeval
tags: [struct private]
```

```Go
type timeval struct {
	tv_sec    int64
	tv_usec   int32
	pad_cgo_0 [4]byte
}
```

#### <a id="timeval.set_usec" href="#timeval.set_usec">func (tv *timeval) set_usec(x int32)</a>

```
searchKey: runtime.timeval.set_usec
tags: [method private]
```

```Go
func (tv *timeval) set_usec(x int32)
```

### <a id="tmpBuf" href="#tmpBuf">type tmpBuf [32]byte</a>

```
searchKey: runtime.tmpBuf
tags: [array number private]
```

```Go
type tmpBuf [tmpStringBufSize]byte
```

### <a id="traceAlloc" href="#traceAlloc">type traceAlloc struct</a>

```
searchKey: runtime.traceAlloc
tags: [struct private]
```

```Go
type traceAlloc struct {
	head traceAllocBlockPtr
	off  uintptr
}
```

traceAlloc is a non-thread-safe region allocator. It holds a linked list of traceAllocBlock. 

#### <a id="traceAlloc.alloc" href="#traceAlloc.alloc">func (a *traceAlloc) alloc(n uintptr) unsafe.Pointer</a>

```
searchKey: runtime.traceAlloc.alloc
tags: [method private]
```

```Go
func (a *traceAlloc) alloc(n uintptr) unsafe.Pointer
```

alloc allocates n-byte block. 

#### <a id="traceAlloc.drop" href="#traceAlloc.drop">func (a *traceAlloc) drop()</a>

```
searchKey: runtime.traceAlloc.drop
tags: [function private]
```

```Go
func (a *traceAlloc) drop()
```

drop frees all previously allocated memory and resets the allocator. 

### <a id="traceAllocBlock" href="#traceAllocBlock">type traceAllocBlock struct</a>

```
searchKey: runtime.traceAllocBlock
tags: [struct private]
```

```Go
type traceAllocBlock struct {
	next traceAllocBlockPtr
	data [64<<10 - sys.PtrSize]byte
}
```

traceAllocBlock is a block in traceAlloc. 

traceAllocBlock is allocated from non-GC'd memory, so it must not contain heap pointers. Writes to pointers to traceAllocBlocks do not need write barriers. 

### <a id="traceAllocBlockPtr" href="#traceAllocBlockPtr">type traceAllocBlockPtr uintptr</a>

```
searchKey: runtime.traceAllocBlockPtr
tags: [number private]
```

```Go
type traceAllocBlockPtr uintptr
```

TODO: Since traceAllocBlock is now go:notinheap, this isn't necessary. 

#### <a id="traceAllocBlockPtr.ptr" href="#traceAllocBlockPtr.ptr">func (p traceAllocBlockPtr) ptr() *traceAllocBlock</a>

```
searchKey: runtime.traceAllocBlockPtr.ptr
tags: [function private]
```

```Go
func (p traceAllocBlockPtr) ptr() *traceAllocBlock
```

#### <a id="traceAllocBlockPtr.set" href="#traceAllocBlockPtr.set">func (p *traceAllocBlockPtr) set(x *traceAllocBlock)</a>

```
searchKey: runtime.traceAllocBlockPtr.set
tags: [method private]
```

```Go
func (p *traceAllocBlockPtr) set(x *traceAllocBlock)
```

### <a id="traceBuf" href="#traceBuf">type traceBuf struct</a>

```
searchKey: runtime.traceBuf
tags: [struct private]
```

```Go
type traceBuf struct {
	traceBufHeader
	arr [64<<10 - unsafe.Sizeof(traceBufHeader{})]byte // underlying buffer for traceBufHeader.buf
}
```

traceBuf is per-P tracing buffer. 

#### <a id="traceBuf.byte" href="#traceBuf.byte">func (buf *traceBuf) byte(v byte)</a>

```
searchKey: runtime.traceBuf.byte
tags: [method private]
```

```Go
func (buf *traceBuf) byte(v byte)
```

byte appends v to buf. 

#### <a id="traceBuf.varint" href="#traceBuf.varint">func (buf *traceBuf) varint(v uint64)</a>

```
searchKey: runtime.traceBuf.varint
tags: [method private]
```

```Go
func (buf *traceBuf) varint(v uint64)
```

varint appends v to buf in little-endian-base-128 encoding. 

### <a id="traceBufHeader" href="#traceBufHeader">type traceBufHeader struct</a>

```
searchKey: runtime.traceBufHeader
tags: [struct private]
```

```Go
type traceBufHeader struct {
	link      traceBufPtr             // in trace.empty/full
	lastTicks uint64                  // when we wrote the last event
	pos       int                     // next write offset in arr
	stk       [traceStackSize]uintptr // scratch buffer for traceback
}
```

traceBufHeader is per-P tracing buffer. 

### <a id="traceBufPtr" href="#traceBufPtr">type traceBufPtr uintptr</a>

```
searchKey: runtime.traceBufPtr
tags: [number private]
```

```Go
type traceBufPtr uintptr
```

traceBufPtr is a *traceBuf that is not traced by the garbage collector and doesn't have write barriers. traceBufs are not allocated from the GC'd heap, so this is safe, and are often manipulated in contexts where write barriers are not allowed, so this is necessary. 

TODO: Since traceBuf is now go:notinheap, this isn't necessary. 

#### <a id="traceBufPtrOf" href="#traceBufPtrOf">func traceBufPtrOf(b *traceBuf) traceBufPtr</a>

```
searchKey: runtime.traceBufPtrOf
tags: [method private]
```

```Go
func traceBufPtrOf(b *traceBuf) traceBufPtr
```

#### <a id="traceFlush" href="#traceFlush">func traceFlush(buf traceBufPtr, pid int32) traceBufPtr</a>

```
searchKey: runtime.traceFlush
tags: [method private]
```

```Go
func traceFlush(buf traceBufPtr, pid int32) traceBufPtr
```

traceFlush puts buf onto stack of full buffers and returns an empty buffer. 

#### <a id="traceFrameForPC" href="#traceFrameForPC">func traceFrameForPC(buf traceBufPtr, pid int32, f Frame) (traceFrame, traceBufPtr)</a>

```
searchKey: runtime.traceFrameForPC
tags: [method private]
```

```Go
func traceFrameForPC(buf traceBufPtr, pid int32, f Frame) (traceFrame, traceBufPtr)
```

traceFrameForPC records the frame information. It may allocate memory. 

#### <a id="traceFullDequeue" href="#traceFullDequeue">func traceFullDequeue() traceBufPtr</a>

```
searchKey: runtime.traceFullDequeue
tags: [function private]
```

```Go
func traceFullDequeue() traceBufPtr
```

traceFullDequeue dequeues from queue of full buffers. 

#### <a id="traceString" href="#traceString">func traceString(bufp *traceBufPtr, pid int32, s string) (uint64, *traceBufPtr)</a>

```
searchKey: runtime.traceString
tags: [method private]
```

```Go
func traceString(bufp *traceBufPtr, pid int32, s string) (uint64, *traceBufPtr)
```

traceString adds a string to the trace.strings and returns the id. 

#### <a id="traceBufPtr.ptr" href="#traceBufPtr.ptr">func (tp traceBufPtr) ptr() *traceBuf</a>

```
searchKey: runtime.traceBufPtr.ptr
tags: [function private]
```

```Go
func (tp traceBufPtr) ptr() *traceBuf
```

#### <a id="traceBufPtr.set" href="#traceBufPtr.set">func (tp *traceBufPtr) set(b *traceBuf)</a>

```
searchKey: runtime.traceBufPtr.set
tags: [method private]
```

```Go
func (tp *traceBufPtr) set(b *traceBuf)
```

### <a id="traceFrame" href="#traceFrame">type traceFrame struct</a>

```
searchKey: runtime.traceFrame
tags: [struct private]
```

```Go
type traceFrame struct {
	funcID uint64
	fileID uint64
	line   uint64
}
```

### <a id="traceStack" href="#traceStack">type traceStack struct</a>

```
searchKey: runtime.traceStack
tags: [struct private]
```

```Go
type traceStack struct {
	link traceStackPtr
	hash uintptr
	id   uint32
	n    int
	stk  [0]uintptr // real type [n]uintptr
}
```

traceStack is a single stack in traceStackTable. 

#### <a id="traceStack.stack" href="#traceStack.stack">func (ts *traceStack) stack() []uintptr</a>

```
searchKey: runtime.traceStack.stack
tags: [function private]
```

```Go
func (ts *traceStack) stack() []uintptr
```

stack returns slice of PCs. 

### <a id="traceStackPtr" href="#traceStackPtr">type traceStackPtr uintptr</a>

```
searchKey: runtime.traceStackPtr
tags: [number private]
```

```Go
type traceStackPtr uintptr
```

#### <a id="traceStackPtr.ptr" href="#traceStackPtr.ptr">func (tp traceStackPtr) ptr() *traceStack</a>

```
searchKey: runtime.traceStackPtr.ptr
tags: [function private]
```

```Go
func (tp traceStackPtr) ptr() *traceStack
```

### <a id="traceStackTable" href="#traceStackTable">type traceStackTable struct</a>

```
searchKey: runtime.traceStackTable
tags: [struct private]
```

```Go
type traceStackTable struct {
	lock mutex
	seq  uint32
	mem  traceAlloc
	tab  [1 << 13]traceStackPtr
}
```

traceStackTable maps stack traces (arrays of PC's) to unique uint32 ids. It is lock-free for reading. 

#### <a id="traceStackTable.dump" href="#traceStackTable.dump">func (tab *traceStackTable) dump()</a>

```
searchKey: runtime.traceStackTable.dump
tags: [function private]
```

```Go
func (tab *traceStackTable) dump()
```

dump writes all previously cached stacks to trace buffers, releases all memory and resets state. 

#### <a id="traceStackTable.find" href="#traceStackTable.find">func (tab *traceStackTable) find(pcs []uintptr, hash uintptr) uint32</a>

```
searchKey: runtime.traceStackTable.find
tags: [method private]
```

```Go
func (tab *traceStackTable) find(pcs []uintptr, hash uintptr) uint32
```

find checks if the stack trace pcs is already present in the table. 

#### <a id="traceStackTable.newStack" href="#traceStackTable.newStack">func (tab *traceStackTable) newStack(n int) *traceStack</a>

```
searchKey: runtime.traceStackTable.newStack
tags: [method private]
```

```Go
func (tab *traceStackTable) newStack(n int) *traceStack
```

newStack allocates a new stack of size n. 

#### <a id="traceStackTable.put" href="#traceStackTable.put">func (tab *traceStackTable) put(pcs []uintptr) uint32</a>

```
searchKey: runtime.traceStackTable.put
tags: [method private]
```

```Go
func (tab *traceStackTable) put(pcs []uintptr) uint32
```

put returns a unique id for the stack trace pcs and caches it in the table, if it sees the trace for the first time. 

### <a id="tracestat" href="#tracestat">type tracestat struct</a>

```
searchKey: runtime.tracestat
tags: [struct private]
```

```Go
type tracestat struct {
	active bool   // init tracing activation status
	id     int64  // init goroutine id
	allocs uint64 // heap allocations
	bytes  uint64 // heap allocated bytes
}
```

### <a id="typeCacheBucket" href="#typeCacheBucket">type typeCacheBucket struct</a>

```
searchKey: runtime.typeCacheBucket
tags: [struct private]
```

```Go
type typeCacheBucket struct {
	t [typeCacheAssoc]*_type
}
```

### <a id="typeOff" href="#typeOff">type typeOff int32</a>

```
searchKey: runtime.typeOff
tags: [number private]
```

```Go
type typeOff int32
```

### <a id="ucontext" href="#ucontext">type ucontext struct</a>

```
searchKey: runtime.ucontext
tags: [struct private]
```

```Go
type ucontext struct {
	uc_onstack  int32
	uc_sigmask  uint32
	uc_stack    stackt
	uc_link     *ucontext
	uc_mcsize   uint64
	uc_mcontext *mcontext64
}
```

### <a id="uint16InterfacePtr" href="#uint16InterfacePtr">type uint16InterfacePtr uint16</a>

```
searchKey: runtime.uint16InterfacePtr
tags: [number private]
```

```Go
type uint16InterfacePtr uint16
```

The specialized convTx routines need a type descriptor to use when calling mallocgc. We don't need the type to be exact, just to have the correct size, alignment, and pointer-ness. However, when debugging, it'd be nice to have some indication in mallocgc where the types came from, so we use named types here. We then construct interface values of these types, and then extract the type word to use as needed. 

### <a id="uint32InterfacePtr" href="#uint32InterfacePtr">type uint32InterfacePtr uint32</a>

```
searchKey: runtime.uint32InterfacePtr
tags: [number private]
```

```Go
type uint32InterfacePtr uint32
```

The specialized convTx routines need a type descriptor to use when calling mallocgc. We don't need the type to be exact, just to have the correct size, alignment, and pointer-ness. However, when debugging, it'd be nice to have some indication in mallocgc where the types came from, so we use named types here. We then construct interface values of these types, and then extract the type word to use as needed. 

### <a id="uint64InterfacePtr" href="#uint64InterfacePtr">type uint64InterfacePtr uint64</a>

```
searchKey: runtime.uint64InterfacePtr
tags: [number private]
```

```Go
type uint64InterfacePtr uint64
```

The specialized convTx routines need a type descriptor to use when calling mallocgc. We don't need the type to be exact, just to have the correct size, alignment, and pointer-ness. However, when debugging, it'd be nice to have some indication in mallocgc where the types came from, so we use named types here. We then construct interface values of these types, and then extract the type word to use as needed. 

### <a id="uncommontype" href="#uncommontype">type uncommontype struct</a>

```
searchKey: runtime.uncommontype
tags: [struct private]
```

```Go
type uncommontype struct {
	pkgpath nameOff
	mcount  uint16 // number of methods
	xcount  uint16 // number of exported methods
	moff    uint32 // offset from this uncommontype to [mcount]method
	_       uint32 // unused
}
```

### <a id="usigactiont" href="#usigactiont">type usigactiont struct</a>

```
searchKey: runtime.usigactiont
tags: [struct private]
```

```Go
type usigactiont struct {
	__sigaction_u [8]byte
	sa_mask       uint32
	sa_flags      int32
}
```

### <a id="waitReason" href="#waitReason">type waitReason uint8</a>

```
searchKey: runtime.waitReason
tags: [number private]
```

```Go
type waitReason uint8
```

A waitReason explains why a goroutine has been stopped. See gopark. Do not re-use waitReasons, add new ones. 

#### <a id="waitReason.String" href="#waitReason.String">func (w waitReason) String() string</a>

```
searchKey: runtime.waitReason.String
tags: [function private]
```

```Go
func (w waitReason) String() string
```

### <a id="waitq" href="#waitq">type waitq struct</a>

```
searchKey: runtime.waitq
tags: [struct private]
```

```Go
type waitq struct {
	first *sudog
	last  *sudog
}
```

#### <a id="waitq.dequeue" href="#waitq.dequeue">func (q *waitq) dequeue() *sudog</a>

```
searchKey: runtime.waitq.dequeue
tags: [function private]
```

```Go
func (q *waitq) dequeue() *sudog
```

#### <a id="waitq.dequeueSudoG" href="#waitq.dequeueSudoG">func (q *waitq) dequeueSudoG(sgp *sudog)</a>

```
searchKey: runtime.waitq.dequeueSudoG
tags: [method private]
```

```Go
func (q *waitq) dequeueSudoG(sgp *sudog)
```

#### <a id="waitq.enqueue" href="#waitq.enqueue">func (q *waitq) enqueue(sgp *sudog)</a>

```
searchKey: runtime.waitq.enqueue
tags: [method private]
```

```Go
func (q *waitq) enqueue(sgp *sudog)
```

### <a id="wbBuf" href="#wbBuf">type wbBuf struct</a>

```
searchKey: runtime.wbBuf
tags: [struct private]
```

```Go
type wbBuf struct {
	// next points to the next slot in buf. It must not be a
	// pointer type because it can point past the end of buf and
	// must be updated without write barriers.
	//
	// This is a pointer rather than an index to optimize the
	// write barrier assembly.
	next uintptr

	// end points to just past the end of buf. It must not be a
	// pointer type because it points past the end of buf and must
	// be updated without write barriers.
	end uintptr

	// buf stores a series of pointers to execute write barriers
	// on. This must be a multiple of wbBufEntryPointers because
	// the write barrier only checks for overflow once per entry.
	buf [wbBufEntryPointers * wbBufEntries]uintptr
}
```

wbBuf is a per-P buffer of pointers queued by the write barrier. This buffer is flushed to the GC workbufs when it fills up and on various GC transitions. 

This is closely related to a "sequential store buffer" (SSB), except that SSBs are usually used for maintaining remembered sets, while this is used for marking. 

#### <a id="wbBuf.discard" href="#wbBuf.discard">func (b *wbBuf) discard()</a>

```
searchKey: runtime.wbBuf.discard
tags: [function private]
```

```Go
func (b *wbBuf) discard()
```

discard resets b's next pointer, but not its end pointer. 

This must be nosplit because it's called by wbBufFlush. 

#### <a id="wbBuf.empty" href="#wbBuf.empty">func (b *wbBuf) empty() bool</a>

```
searchKey: runtime.wbBuf.empty
tags: [function private]
```

```Go
func (b *wbBuf) empty() bool
```

empty reports whether b contains no pointers. 

#### <a id="wbBuf.putFast" href="#wbBuf.putFast">func (b *wbBuf) putFast(old, new uintptr) bool</a>

```
searchKey: runtime.wbBuf.putFast
tags: [method private]
```

```Go
func (b *wbBuf) putFast(old, new uintptr) bool
```

putFast adds old and new to the write barrier buffer and returns false if a flush is necessary. Callers should use this as: 

```
buf := &getg().m.p.ptr().wbBuf
if !buf.putFast(old, new) {
    wbBufFlush(...)
}
... actual memory write ...

```
The arguments to wbBufFlush depend on whether the caller is doing its own cgo pointer checks. If it is, then this can be wbBufFlush(nil, 0). Otherwise, it must pass the slot address and new. 

The caller must ensure there are no preemption points during the above sequence. There must be no preemption points while buf is in use because it is a per-P resource. There must be no preemption points between the buffer put and the write to memory because this could allow a GC phase change, which could result in missed write barriers. 

putFast must be nowritebarrierrec to because write barriers here would corrupt the write barrier buffer. It (and everything it calls, if it called anything) has to be nosplit to avoid scheduling on to a different P and a different buffer. 

#### <a id="wbBuf.reset" href="#wbBuf.reset">func (b *wbBuf) reset()</a>

```
searchKey: runtime.wbBuf.reset
tags: [function private]
```

```Go
func (b *wbBuf) reset()
```

reset empties b by resetting its next and end pointers. 

### <a id="workbuf" href="#workbuf">type workbuf struct</a>

```
searchKey: runtime.workbuf
tags: [struct private]
```

```Go
type workbuf struct {
	workbufhdr
	// account for the above fields
	obj [(_WorkbufSize - unsafe.Sizeof(workbufhdr{})) / sys.PtrSize]uintptr
}
```

#### <a id="getempty" href="#getempty">func getempty() *workbuf</a>

```
searchKey: runtime.getempty
tags: [function private]
```

```Go
func getempty() *workbuf
```

getempty pops an empty work buffer off the work.empty list, allocating new buffers if none are available. 

#### <a id="handoff" href="#handoff">func handoff(b *workbuf) *workbuf</a>

```
searchKey: runtime.handoff
tags: [method private]
```

```Go
func handoff(b *workbuf) *workbuf
```

#### <a id="trygetfull" href="#trygetfull">func trygetfull() *workbuf</a>

```
searchKey: runtime.trygetfull
tags: [function private]
```

```Go
func trygetfull() *workbuf
```

trygetfull tries to get a full or partially empty workbuffer. If one is not immediately available return nil 

#### <a id="workbuf.checkempty" href="#workbuf.checkempty">func (b *workbuf) checkempty()</a>

```
searchKey: runtime.workbuf.checkempty
tags: [function private]
```

```Go
func (b *workbuf) checkempty()
```

#### <a id="workbuf.checknonempty" href="#workbuf.checknonempty">func (b *workbuf) checknonempty()</a>

```
searchKey: runtime.workbuf.checknonempty
tags: [function private]
```

```Go
func (b *workbuf) checknonempty()
```

### <a id="workbufhdr" href="#workbufhdr">type workbufhdr struct</a>

```
searchKey: runtime.workbufhdr
tags: [struct private]
```

```Go
type workbufhdr struct {
	node lfnode // must be first
	nobj int
}
```

### <a id="_defer" href="#_defer">type _defer struct</a>

```
searchKey: runtime._defer
tags: [struct private]
```

```Go
type _defer struct {
	siz     int32 // includes both arguments and results
	started bool
	heap    bool
	// openDefer indicates that this _defer is for a frame with open-coded
	// defers. We have only one defer record for the entire frame (which may
	// currently have 0, 1, or more defers active).
	openDefer bool
	sp        uintptr  // sp at time of defer
	pc        uintptr  // pc at time of defer
	fn        *funcval // can be nil for open-coded defers
	_panic    *_panic  // panic that is running defer
	link      *_defer

	// If openDefer is true, the fields below record values about the stack
	// frame and associated function that has the open-coded defer(s). sp
	// above will be the sp for the frame, and pc will be address of the
	// deferreturn call in the function.
	fd   unsafe.Pointer // funcdata for the function associated with the frame
	varp uintptr        // value of varp for the stack frame
	// framepc is the current pc associated with the stack frame. Together,
	// with sp above (which is the sp associated with the stack frame),
	// framepc/sp can be used as pc/sp pair to continue a stack trace via
	// gentraceback().
	framepc uintptr
}
```

A _defer holds an entry on the list of deferred calls. If you add a field here, add code to clear it in freedefer and deferProcStack This struct must match the code in cmd/compile/internal/gc/reflect.go:deferstruct and cmd/compile/internal/gc/ssa.go:(*state).call. Some defers will be allocated on the stack and some on the heap. All defers are logically part of the stack, so write barriers to initialize them are not required. All defers must be manually scanned, and for heap defers, marked. 

#### <a id="newdefer" href="#newdefer">func newdefer(siz int32) *_defer</a>

```
searchKey: runtime.newdefer
tags: [method private]
```

```Go
func newdefer(siz int32) *_defer
```

Allocate a Defer, usually using per-P pool. Each defer must be released with freedefer.  The defer is not added to any defer chain yet. 

This must not grow the stack because there may be a frame without stack map information when this is called. 

### <a id="_func" href="#_func">type _func struct</a>

```
searchKey: runtime._func
tags: [struct private]
```

```Go
type _func struct {
	entry   uintptr // start pc
	nameoff int32   // function name

	args        int32  // in/out args size
	deferreturn uint32 // offset of start of a deferreturn call instruction from entry, if any.

	pcsp      uint32
	pcfile    uint32
	pcln      uint32
	npcdata   uint32
	cuOffset  uint32 // runtime.cutab offset of this function's CU
	funcID    funcID // set for certain special runtime functions
	flag      funcFlag
	_         [1]byte // pad
	nfuncdata uint8   // must be last, must end on a uint32-aligned boundary
}
```

Layout of in-memory per-function information prepared by linker See [https://golang.org/s/go12symtab](https://golang.org/s/go12symtab). Keep in sync with linker (../cmd/link/internal/ld/pcln.go:/pclntab) and with package debug/gosym and with symtab.go in package runtime. 

### <a id="_panic" href="#_panic">type _panic struct</a>

```
searchKey: runtime._panic
tags: [struct private]
```

```Go
type _panic struct {
	argp      unsafe.Pointer // pointer to arguments of deferred call run during panic; cannot move - known to liblink
	arg       interface{}    // argument to panic
	link      *_panic        // link to earlier panic
	pc        uintptr        // where to return to in runtime if this panic is bypassed
	sp        unsafe.Pointer // where to return to in runtime if this panic is bypassed
	recovered bool           // whether this panic is over
	aborted   bool           // the panic was aborted
	goexit    bool
}
```

A _panic holds information about an active panic. 

A _panic value must only ever live on the stack. 

The argp and link fields are stack pointers, but don't need special handling during stack growth: because they are pointer-typed and _panic values only live on the stack, regular stack pointer adjustment takes care of them. 

### <a id="_type" href="#_type">type _type struct</a>

```
searchKey: runtime._type
tags: [struct private]
```

```Go
type _type struct {
	size       uintptr
	ptrdata    uintptr // size of memory prefix holding all pointers
	hash       uint32
	tflag      tflag
	align      uint8
	fieldAlign uint8
	kind       uint8
	// function for comparing objects of this type
	// (ptr to object A, ptr to object B) -> ==?
	equal func(unsafe.Pointer, unsafe.Pointer) bool
	// gcdata stores the GC type data for the garbage collector.
	// If the KindGCProg bit is set in kind, gcdata is a GC program.
	// Otherwise it is a ptrmask bitmap. See mbitmap.go for details.
	gcdata    *byte
	str       nameOff
	ptrToThis typeOff
}
```

Needs to be in sync with ../cmd/link/internal/ld/decodesym.go:/^func.commonsize, ../cmd/compile/internal/gc/reflect.go:/^func.dcommontype and ../reflect/type.go:/^type.rtype. ../internal/reflectlite/type.go:/^type.rtype. 

#### <a id="resolveTypeOff" href="#resolveTypeOff">func resolveTypeOff(ptrInModule unsafe.Pointer, off typeOff) *_type</a>

```
searchKey: runtime.resolveTypeOff
tags: [method private]
```

```Go
func resolveTypeOff(ptrInModule unsafe.Pointer, off typeOff) *_type
```

#### <a id="_type.name" href="#_type.name">func (t *_type) name() string</a>

```
searchKey: runtime._type.name
tags: [function private]
```

```Go
func (t *_type) name() string
```

#### <a id="_type.nameOff" href="#_type.nameOff">func (t *_type) nameOff(off nameOff) name</a>

```
searchKey: runtime._type.nameOff
tags: [method private]
```

```Go
func (t *_type) nameOff(off nameOff) name
```

#### <a id="_type.pkgpath" href="#_type.pkgpath">func (t *_type) pkgpath() string</a>

```
searchKey: runtime._type.pkgpath
tags: [function private]
```

```Go
func (t *_type) pkgpath() string
```

pkgpath returns the path of the package where t was defined, if available. This is not the same as the reflect package's PkgPath method, in that it returns the package path for struct and interface types, not just named types. 

#### <a id="_type.string" href="#_type.string">func (t *_type) string() string</a>

```
searchKey: runtime._type.string
tags: [function private]
```

```Go
func (t *_type) string() string
```

#### <a id="_type.textOff" href="#_type.textOff">func (t *_type) textOff(off textOff) unsafe.Pointer</a>

```
searchKey: runtime._type.textOff
tags: [method private]
```

```Go
func (t *_type) textOff(off textOff) unsafe.Pointer
```

#### <a id="_type.typeOff" href="#_type.typeOff">func (t *_type) typeOff(off typeOff) *_type</a>

```
searchKey: runtime._type.typeOff
tags: [method private]
```

```Go
func (t *_type) typeOff(off typeOff) *_type
```

#### <a id="_type.uncommon" href="#_type.uncommon">func (t *_type) uncommon() *uncommontype</a>

```
searchKey: runtime._type.uncommon
tags: [function private]
```

```Go
func (t *_type) uncommon() *uncommontype
```

### <a id="_typePair" href="#_typePair">type _typePair struct</a>

```
searchKey: runtime._typePair
tags: [struct private]
```

```Go
type _typePair struct {
	t1 *_type
	t2 *_type
}
```

## <a id="func" href="#func">Functions</a>

```
tags: [package]
```

### <a id="BenchSetType" href="#BenchSetType">func BenchSetType(n int, x interface{})</a>

```
searchKey: runtime.BenchSetType
tags: [method private]
```

```Go
func BenchSetType(n int, x interface{})
```

### <a id="BlockOnSystemStack" href="#BlockOnSystemStack">func BlockOnSystemStack()</a>

```
searchKey: runtime.BlockOnSystemStack
tags: [function private]
```

```Go
func BlockOnSystemStack()
```

BlockOnSystemStack switches to the system stack, prints "x\n" to stderr, and blocks in a stack containing "runtime.blockOnSystemStackInternal". 

### <a id="BlockProfile" href="#BlockProfile">func BlockProfile(p []BlockProfileRecord) (n int, ok bool)</a>

```
searchKey: runtime.BlockProfile
tags: [method]
```

```Go
func BlockProfile(p []BlockProfileRecord) (n int, ok bool)
```

BlockProfile returns n, the number of records in the current blocking profile. If len(p) >= n, BlockProfile copies the profile into p and returns n, true. If len(p) < n, BlockProfile does not change p and returns n, false. 

Most clients should use the runtime/pprof package or the testing package's -test.blockprofile flag instead of calling BlockProfile directly. 

### <a id="Breakpoint" href="#Breakpoint">func Breakpoint()</a>

```
searchKey: runtime.Breakpoint
tags: [function]
```

```Go
func Breakpoint()
```

Breakpoint executes a breakpoint trap. 

### <a id="CPUProfile" href="#CPUProfile">func CPUProfile() []byte</a>

```
searchKey: runtime.CPUProfile
tags: [function deprecated]
```

```Go
func CPUProfile() []byte
```

CPUProfile panics. It formerly provided raw access to chunks of a pprof-format profile generated by the runtime. The details of generating that format have changed, so this functionality has been removed. 

Deprecated: Use the runtime/pprof package, or the handlers in the net/http/pprof package, or the testing package's -test.cpuprofile flag instead. 

### <a id="Caller" href="#Caller">func Caller(skip int) (pc uintptr, file string, line int, ok bool)</a>

```
searchKey: runtime.Caller
tags: [method]
```

```Go
func Caller(skip int) (pc uintptr, file string, line int, ok bool)
```

Caller reports file and line number information about function invocations on the calling goroutine's stack. The argument skip is the number of stack frames to ascend, with 0 identifying the caller of Caller.  (For historical reasons the meaning of skip differs between Caller and Callers.) The return values report the program counter, file name, and line number within the file of the corresponding call. The boolean ok is false if it was not possible to recover the information. 

### <a id="Callers" href="#Callers">func Callers(skip int, pc []uintptr) int</a>

```
searchKey: runtime.Callers
tags: [method]
```

```Go
func Callers(skip int, pc []uintptr) int
```

Callers fills the slice pc with the return program counters of function invocations on the calling goroutine's stack. The argument skip is the number of stack frames to skip before recording in pc, with 0 identifying the frame for Callers itself and 1 identifying the caller of Callers. It returns the number of entries written to pc. 

To translate these PCs into symbolic information such as function names and line numbers, use CallersFrames. CallersFrames accounts for inlined functions and adjusts the return program counters into call program counters. Iterating over the returned slice of PCs directly is discouraged, as is using FuncForPC on any of the returned PCs, since these cannot account for inlining or return program counter adjustment. 

### <a id="CheckScavengedBitsCleared" href="#CheckScavengedBitsCleared">func CheckScavengedBitsCleared(mismatches []BitsMismatch) (n int, ok bool)</a>

```
searchKey: runtime.CheckScavengedBitsCleared
tags: [method private]
```

```Go
func CheckScavengedBitsCleared(mismatches []BitsMismatch) (n int, ok bool)
```

### <a id="CountPagesInUse" href="#CountPagesInUse">func CountPagesInUse() (pagesInUse, counted uintptr)</a>

```
searchKey: runtime.CountPagesInUse
tags: [function private]
```

```Go
func CountPagesInUse() (pagesInUse, counted uintptr)
```

### <a id="DiffPallocBits" href="#DiffPallocBits">func DiffPallocBits(a, b *PallocBits) []BitRange</a>

```
searchKey: runtime.DiffPallocBits
tags: [method private]
```

```Go
func DiffPallocBits(a, b *PallocBits) []BitRange
```

Given two PallocBits, returns a set of bit ranges where they differ. 

### <a id="DumpDebugLog" href="#DumpDebugLog">func DumpDebugLog() string</a>

```
searchKey: runtime.DumpDebugLog
tags: [function private]
```

```Go
func DumpDebugLog() string
```

### <a id="Envs" href="#Envs">func Envs() []string</a>

```
searchKey: runtime.Envs
tags: [function private]
```

```Go
func Envs() []string
```

### <a id="Fastrand" href="#Fastrand">func Fastrand() uint32</a>

```
searchKey: runtime.Fastrand
tags: [function private]
```

```Go
func Fastrand() uint32
```

### <a id="Fastrandn" href="#Fastrandn">func Fastrandn(n uint32) uint32</a>

```
searchKey: runtime.Fastrandn
tags: [method private]
```

```Go
func Fastrandn(n uint32) uint32
```

### <a id="Fcntl" href="#Fcntl">func Fcntl(fd, cmd, arg uintptr) (uintptr, uintptr)</a>

```
searchKey: runtime.Fcntl
tags: [method private]
```

```Go
func Fcntl(fd, cmd, arg uintptr) (uintptr, uintptr)
```

### <a id="FillAligned" href="#FillAligned">func FillAligned(x uint64, m uint) uint64</a>

```
searchKey: runtime.FillAligned
tags: [method private]
```

```Go
func FillAligned(x uint64, m uint) uint64
```

Expose fillAligned for testing. 

### <a id="FinalizerGAsleep" href="#FinalizerGAsleep">func FinalizerGAsleep() bool</a>

```
searchKey: runtime.FinalizerGAsleep
tags: [function private]
```

```Go
func FinalizerGAsleep() bool
```

### <a id="FindBitRange64" href="#FindBitRange64">func FindBitRange64(c uint64, n uint) uint</a>

```
searchKey: runtime.FindBitRange64
tags: [method private]
```

```Go
func FindBitRange64(c uint64, n uint) uint
```

Expose non-trivial helpers for testing. 

### <a id="FreeMSpan" href="#FreeMSpan">func FreeMSpan(s *MSpan)</a>

```
searchKey: runtime.FreeMSpan
tags: [method private]
```

```Go
func FreeMSpan(s *MSpan)
```

Free an allocated mspan. 

### <a id="FreePageAlloc" href="#FreePageAlloc">func FreePageAlloc(pp *PageAlloc)</a>

```
searchKey: runtime.FreePageAlloc
tags: [method private]
```

```Go
func FreePageAlloc(pp *PageAlloc)
```

FreePageAlloc releases hard OS resources owned by the pageAlloc. Once this is called the pageAlloc may no longer be used. The object itself will be collected by the garbage collector once it is no longer live. 

### <a id="G0StackOverflow" href="#G0StackOverflow">func G0StackOverflow()</a>

```
searchKey: runtime.G0StackOverflow
tags: [function private]
```

```Go
func G0StackOverflow()
```

### <a id="GC" href="#GC">func GC()</a>

```
searchKey: runtime.GC
tags: [function]
```

```Go
func GC()
```

GC runs a garbage collection and blocks the caller until the garbage collection is complete. It may also block the entire program. 

### <a id="GCMask" href="#GCMask">func GCMask(x interface{}) (ret []byte)</a>

```
searchKey: runtime.GCMask
tags: [method private]
```

```Go
func GCMask(x interface{}) (ret []byte)
```

### <a id="GCTestIsReachable" href="#GCTestIsReachable">func GCTestIsReachable(ptrs ...unsafe.Pointer) (mask uint64)</a>

```
searchKey: runtime.GCTestIsReachable
tags: [method private]
```

```Go
func GCTestIsReachable(ptrs ...unsafe.Pointer) (mask uint64)
```

For GCTestIsReachable, it's important that we do this as a call so escape analysis can see through it. 

### <a id="GCTestPointerClass" href="#GCTestPointerClass">func GCTestPointerClass(p unsafe.Pointer) string</a>

```
searchKey: runtime.GCTestPointerClass
tags: [method private]
```

```Go
func GCTestPointerClass(p unsafe.Pointer) string
```

For GCTestPointerClass, it's important that we do this as a call so escape analysis can see through it. 

This is nosplit because gcTestPointerClass is. 

### <a id="GOMAXPROCS" href="#GOMAXPROCS">func GOMAXPROCS(n int) int</a>

```
searchKey: runtime.GOMAXPROCS
tags: [method]
```

```Go
func GOMAXPROCS(n int) int
```

GOMAXPROCS sets the maximum number of CPUs that can be executing simultaneously and returns the previous setting. It defaults to the value of runtime.NumCPU. If n < 1, it does not change the current setting. This call will go away when the scheduler improves. 

### <a id="GOROOT" href="#GOROOT">func GOROOT() string</a>

```
searchKey: runtime.GOROOT
tags: [function]
```

```Go
func GOROOT() string
```

GOROOT returns the root of the Go tree. It uses the GOROOT environment variable, if set at process start, or else the root used during the Go build. 

### <a id="GetNextArenaHint" href="#GetNextArenaHint">func GetNextArenaHint() uintptr</a>

```
searchKey: runtime.GetNextArenaHint
tags: [function private]
```

```Go
func GetNextArenaHint() uintptr
```

### <a id="GetPhysPageSize" href="#GetPhysPageSize">func GetPhysPageSize() uintptr</a>

```
searchKey: runtime.GetPhysPageSize
tags: [function private]
```

```Go
func GetPhysPageSize() uintptr
```

### <a id="Goexit" href="#Goexit">func Goexit()</a>

```
searchKey: runtime.Goexit
tags: [function]
```

```Go
func Goexit()
```

Goexit terminates the goroutine that calls it. No other goroutine is affected. Goexit runs all deferred calls before terminating the goroutine. Because Goexit is not a panic, any recover calls in those deferred functions will return nil. 

Calling Goexit from the main goroutine terminates that goroutine without func main returning. Since func main has not returned, the program continues execution of other goroutines. If all other goroutines exit, the program crashes. 

### <a id="GoroutineProfile" href="#GoroutineProfile">func GoroutineProfile(p []StackRecord) (n int, ok bool)</a>

```
searchKey: runtime.GoroutineProfile
tags: [method]
```

```Go
func GoroutineProfile(p []StackRecord) (n int, ok bool)
```

GoroutineProfile returns n, the number of records in the active goroutine stack profile. If len(p) >= n, GoroutineProfile copies the profile into p and returns n, true. If len(p) < n, GoroutineProfile does not change p and returns n, false. 

Most clients should use the runtime/pprof package instead of calling GoroutineProfile directly. 

### <a id="Gosched" href="#Gosched">func Gosched()</a>

```
searchKey: runtime.Gosched
tags: [function]
```

```Go
func Gosched()
```

Gosched yields the processor, allowing other goroutines to run. It does not suspend the current goroutine, so execution resumes automatically. 

### <a id="GostringW" href="#GostringW">func GostringW(w []uint16) (s string)</a>

```
searchKey: runtime.GostringW
tags: [method private]
```

```Go
func GostringW(w []uint16) (s string)
```

entry point for testing 

### <a id="KeepAlive" href="#KeepAlive">func KeepAlive(x interface{})</a>

```
searchKey: runtime.KeepAlive
tags: [method]
```

```Go
func KeepAlive(x interface{})
```

KeepAlive marks its argument as currently reachable. This ensures that the object is not freed, and its finalizer is not run, before the point in the program where KeepAlive is called. 

A very simplified example showing where KeepAlive is required: 

```
type File struct { d int }
d, err := syscall.Open("/file/path", syscall.O_RDONLY, 0)
// ... do something if err != nil ...
p := &File{d}
runtime.SetFinalizer(p, func(p *File) { syscall.Close(p.d) })
var buf [10]byte
n, err := syscall.Read(p.d, buf[:])
// Ensure p is not finalized until Read returns.
runtime.KeepAlive(p)
// No more uses of p after this point.

```
Without the KeepAlive call, the finalizer could run at the start of syscall.Read, closing the file descriptor before syscall.Read makes the actual system call. 

### <a id="KeepNArenaHints" href="#KeepNArenaHints">func KeepNArenaHints(n int)</a>

```
searchKey: runtime.KeepNArenaHints
tags: [method private]
```

```Go
func KeepNArenaHints(n int)
```

### <a id="LFStackPush" href="#LFStackPush">func LFStackPush(head *uint64, node *LFNode)</a>

```
searchKey: runtime.LFStackPush
tags: [method private]
```

```Go
func LFStackPush(head *uint64, node *LFNode)
```

### <a id="LockOSCounts" href="#LockOSCounts">func LockOSCounts() (external, internal uint32)</a>

```
searchKey: runtime.LockOSCounts
tags: [function private]
```

```Go
func LockOSCounts() (external, internal uint32)
```

### <a id="LockOSThread" href="#LockOSThread">func LockOSThread()</a>

```
searchKey: runtime.LockOSThread
tags: [function]
```

```Go
func LockOSThread()
```

LockOSThread wires the calling goroutine to its current operating system thread. The calling goroutine will always execute in that thread, and no other goroutine will execute in it, until the calling goroutine has made as many calls to UnlockOSThread as to LockOSThread. If the calling goroutine exits without unlocking the thread, the thread will be terminated. 

All init functions are run on the startup thread. Calling LockOSThread from an init function will cause the main function to be invoked on that thread. 

A goroutine should call LockOSThread before calling OS services or non-Go library functions that depend on per-thread state. 

### <a id="MSpanCountAlloc" href="#MSpanCountAlloc">func MSpanCountAlloc(ms *MSpan, bits []byte) int</a>

```
searchKey: runtime.MSpanCountAlloc
tags: [method private]
```

```Go
func MSpanCountAlloc(ms *MSpan, bits []byte) int
```

### <a id="MapBucketsCount" href="#MapBucketsCount">func MapBucketsCount(m map[int]int) int</a>

```
searchKey: runtime.MapBucketsCount
tags: [method private]
```

```Go
func MapBucketsCount(m map[int]int) int
```

### <a id="MapBucketsPointerIsNil" href="#MapBucketsPointerIsNil">func MapBucketsPointerIsNil(m map[int]int) bool</a>

```
searchKey: runtime.MapBucketsPointerIsNil
tags: [method private]
```

```Go
func MapBucketsPointerIsNil(m map[int]int) bool
```

### <a id="MapNextArenaHint" href="#MapNextArenaHint">func MapNextArenaHint() (start, end uintptr)</a>

```
searchKey: runtime.MapNextArenaHint
tags: [function private]
```

```Go
func MapNextArenaHint() (start, end uintptr)
```

MapNextArenaHint reserves a page at the next arena growth hint, preventing the arena from growing there, and returns the range of addresses that are no longer viable. 

### <a id="MapTombstoneCheck" href="#MapTombstoneCheck">func MapTombstoneCheck(m map[int]int)</a>

```
searchKey: runtime.MapTombstoneCheck
tags: [method private]
```

```Go
func MapTombstoneCheck(m map[int]int)
```

### <a id="MemProfile" href="#MemProfile">func MemProfile(p []MemProfileRecord, inuseZero bool) (n int, ok bool)</a>

```
searchKey: runtime.MemProfile
tags: [method]
```

```Go
func MemProfile(p []MemProfileRecord, inuseZero bool) (n int, ok bool)
```

MemProfile returns a profile of memory allocated and freed per allocation site. 

MemProfile returns n, the number of records in the current memory profile. If len(p) >= n, MemProfile copies the profile into p and returns n, true. If len(p) < n, MemProfile does not change p and returns n, false. 

If inuseZero is true, the profile includes allocation records where r.AllocBytes > 0 but r.AllocBytes == r.FreeBytes. These are sites where memory was allocated, but it has all been released back to the runtime. 

The returned profile may be up to two garbage collection cycles old. This is to avoid skewing the profile toward allocations; because allocations happen in real time but frees are delayed until the garbage collector performs sweeping, the profile only accounts for allocations that have had a chance to be freed by the garbage collector. 

Most clients should use the runtime/pprof package or the testing package's -test.memprofile flag instead of calling MemProfile directly. 

### <a id="MemclrBytes" href="#MemclrBytes">func MemclrBytes(b []byte)</a>

```
searchKey: runtime.MemclrBytes
tags: [method private]
```

```Go
func MemclrBytes(b []byte)
```

### <a id="MutexProfile" href="#MutexProfile">func MutexProfile(p []BlockProfileRecord) (n int, ok bool)</a>

```
searchKey: runtime.MutexProfile
tags: [method]
```

```Go
func MutexProfile(p []BlockProfileRecord) (n int, ok bool)
```

MutexProfile returns n, the number of records in the current mutex profile. If len(p) >= n, MutexProfile copies the profile into p and returns n, true. Otherwise, MutexProfile does not change p, and returns n, false. 

Most clients should use the runtime/pprof package instead of calling MutexProfile directly. 

### <a id="Netpoll" href="#Netpoll">func Netpoll(delta int64)</a>

```
searchKey: runtime.Netpoll
tags: [method private]
```

```Go
func Netpoll(delta int64)
```

### <a id="NumCPU" href="#NumCPU">func NumCPU() int</a>

```
searchKey: runtime.NumCPU
tags: [function]
```

```Go
func NumCPU() int
```

NumCPU returns the number of logical CPUs usable by the current process. 

The set of available CPUs is checked by querying the operating system at process startup. Changes to operating system CPU allocation after process startup are not reflected. 

### <a id="NumCgoCall" href="#NumCgoCall">func NumCgoCall() int64</a>

```
searchKey: runtime.NumCgoCall
tags: [function]
```

```Go
func NumCgoCall() int64
```

NumCgoCall returns the number of cgo calls made by the current process. 

### <a id="NumGoroutine" href="#NumGoroutine">func NumGoroutine() int</a>

```
searchKey: runtime.NumGoroutine
tags: [function]
```

```Go
func NumGoroutine() int
```

NumGoroutine returns the number of goroutines that currently exist. 

### <a id="PageBase" href="#PageBase">func PageBase(c ChunkIdx, pageIdx uint) uintptr</a>

```
searchKey: runtime.PageBase
tags: [method private]
```

```Go
func PageBase(c ChunkIdx, pageIdx uint) uintptr
```

PageBase returns an address given a chunk index and a page index relative to that chunk. 

### <a id="PageCachePagesLeaked" href="#PageCachePagesLeaked">func PageCachePagesLeaked() (leaked uintptr)</a>

```
searchKey: runtime.PageCachePagesLeaked
tags: [function private]
```

```Go
func PageCachePagesLeaked() (leaked uintptr)
```

### <a id="PanicForTesting" href="#PanicForTesting">func PanicForTesting(b []byte, i int) byte</a>

```
searchKey: runtime.PanicForTesting
tags: [method private]
```

```Go
func PanicForTesting(b []byte, i int) byte
```

### <a id="ReadMemStats" href="#ReadMemStats">func ReadMemStats(m *MemStats)</a>

```
searchKey: runtime.ReadMemStats
tags: [method]
```

```Go
func ReadMemStats(m *MemStats)
```

ReadMemStats populates m with memory allocator statistics. 

The returned memory allocator statistics are up to date as of the call to ReadMemStats. This is in contrast with a heap profile, which is a snapshot as of the most recently completed garbage collection cycle. 

### <a id="ReadMetricsSlow" href="#ReadMetricsSlow">func ReadMetricsSlow(memStats *MemStats, samplesp unsafe.Pointer, len, cap int)</a>

```
searchKey: runtime.ReadMetricsSlow
tags: [method private]
```

```Go
func ReadMetricsSlow(memStats *MemStats, samplesp unsafe.Pointer, len, cap int)
```

### <a id="ReadTrace" href="#ReadTrace">func ReadTrace() []byte</a>

```
searchKey: runtime.ReadTrace
tags: [function]
```

```Go
func ReadTrace() []byte
```

ReadTrace returns the next chunk of binary tracing data, blocking until data is available. If tracing is turned off and all the data accumulated while it was on has been returned, ReadTrace returns nil. The caller must copy the returned data before calling ReadTrace again. ReadTrace must be called from one goroutine at a time. 

### <a id="ResetDebugLog" href="#ResetDebugLog">func ResetDebugLog()</a>

```
searchKey: runtime.ResetDebugLog
tags: [function private]
```

```Go
func ResetDebugLog()
```

### <a id="RunGetgThreadSwitchTest" href="#RunGetgThreadSwitchTest">func RunGetgThreadSwitchTest()</a>

```
searchKey: runtime.RunGetgThreadSwitchTest
tags: [function private]
```

```Go
func RunGetgThreadSwitchTest()
```

### <a id="RunSchedLocalQueueEmptyTest" href="#RunSchedLocalQueueEmptyTest">func RunSchedLocalQueueEmptyTest(iters int)</a>

```
searchKey: runtime.RunSchedLocalQueueEmptyTest
tags: [method private]
```

```Go
func RunSchedLocalQueueEmptyTest(iters int)
```

### <a id="RunSchedLocalQueueStealTest" href="#RunSchedLocalQueueStealTest">func RunSchedLocalQueueStealTest()</a>

```
searchKey: runtime.RunSchedLocalQueueStealTest
tags: [function private]
```

```Go
func RunSchedLocalQueueStealTest()
```

### <a id="RunSchedLocalQueueTest" href="#RunSchedLocalQueueTest">func RunSchedLocalQueueTest()</a>

```
searchKey: runtime.RunSchedLocalQueueTest
tags: [function private]
```

```Go
func RunSchedLocalQueueTest()
```

### <a id="RunStealOrderTest" href="#RunStealOrderTest">func RunStealOrderTest()</a>

```
searchKey: runtime.RunStealOrderTest
tags: [function private]
```

```Go
func RunStealOrderTest()
```

### <a id="SemNwait" href="#SemNwait">func SemNwait(addr *uint32) uint32</a>

```
searchKey: runtime.SemNwait
tags: [method private]
```

```Go
func SemNwait(addr *uint32) uint32
```

### <a id="SendSigusr1" href="#SendSigusr1">func SendSigusr1(mp *M)</a>

```
searchKey: runtime.SendSigusr1
tags: [method private]
```

```Go
func SendSigusr1(mp *M)
```

SendSigusr1 sends SIGUSR1 to mp. 

### <a id="SetBlockProfileRate" href="#SetBlockProfileRate">func SetBlockProfileRate(rate int)</a>

```
searchKey: runtime.SetBlockProfileRate
tags: [method]
```

```Go
func SetBlockProfileRate(rate int)
```

SetBlockProfileRate controls the fraction of goroutine blocking events that are reported in the blocking profile. The profiler aims to sample an average of one blocking event per rate nanoseconds spent blocked. 

To include every blocking event in the profile, pass rate = 1. To turn off profiling entirely, pass rate <= 0. 

### <a id="SetCPUProfileRate" href="#SetCPUProfileRate">func SetCPUProfileRate(hz int)</a>

```
searchKey: runtime.SetCPUProfileRate
tags: [method]
```

```Go
func SetCPUProfileRate(hz int)
```

SetCPUProfileRate sets the CPU profiling rate to hz samples per second. If hz <= 0, SetCPUProfileRate turns off profiling. If the profiler is on, the rate cannot be changed without first turning it off. 

Most clients should use the runtime/pprof package or the testing package's -test.cpuprofile flag instead of calling SetCPUProfileRate directly. 

### <a id="SetCgoTraceback" href="#SetCgoTraceback">func SetCgoTraceback(version int, traceback, context, symbolizer unsafe.Pointer)</a>

```
searchKey: runtime.SetCgoTraceback
tags: [method]
```

```Go
func SetCgoTraceback(version int, traceback, context, symbolizer unsafe.Pointer)
```

SetCgoTraceback records three C functions to use to gather traceback information from C code and to convert that traceback information into symbolic information. These are used when printing stack traces for a program that uses cgo. 

The traceback and context functions may be called from a signal handler, and must therefore use only async-signal safe functions. The symbolizer function may be called while the program is crashing, and so must be cautious about using memory.  None of the functions may call back into Go. 

The context function will be called with a single argument, a pointer to a struct: 

```
struct {
	Context uintptr
}

```
In C syntax, this struct will be 

```
struct {
	uintptr_t Context;
};

```
If the Context field is 0, the context function is being called to record the current traceback context. It should record in the Context field whatever information is needed about the current point of execution to later produce a stack trace, probably the stack pointer and PC. In this case the context function will be called from C code. 

If the Context field is not 0, then it is a value returned by a previous call to the context function. This case is called when the context is no longer needed; that is, when the Go code is returning to its C code caller. This permits the context function to release any associated resources. 

While it would be correct for the context function to record a complete a stack trace whenever it is called, and simply copy that out in the traceback function, in a typical program the context function will be called many times without ever recording a traceback for that context. Recording a complete stack trace in a call to the context function is likely to be inefficient. 

The traceback function will be called with a single argument, a pointer to a struct: 

```
struct {
	Context    uintptr
	SigContext uintptr
	Buf        *uintptr
	Max        uintptr
}

```
In C syntax, this struct will be 

```
struct {
	uintptr_t  Context;
	uintptr_t  SigContext;
	uintptr_t* Buf;
	uintptr_t  Max;
};

```
The Context field will be zero to gather a traceback from the current program execution point. In this case, the traceback function will be called from C code. 

Otherwise Context will be a value previously returned by a call to the context function. The traceback function should gather a stack trace from that saved point in the program execution. The traceback function may be called from an execution thread other than the one that recorded the context, but only when the context is known to be valid and unchanging. The traceback function may also be called deeper in the call stack on the same thread that recorded the context. The traceback function may be called multiple times with the same Context value; it will usually be appropriate to cache the result, if possible, the first time this is called for a specific context value. 

If the traceback function is called from a signal handler on a Unix system, SigContext will be the signal context argument passed to the signal handler (a C ucontext_t* cast to uintptr_t). This may be used to start tracing at the point where the signal occurred. If the traceback function is not called from a signal handler, SigContext will be zero. 

Buf is where the traceback information should be stored. It should be PC values, such that Buf[0] is the PC of the caller, Buf[1] is the PC of that function's caller, and so on.  Max is the maximum number of entries to store.  The function should store a zero to indicate the top of the stack, or that the caller is on a different stack, presumably a Go stack. 

Unlike runtime.Callers, the PC values returned should, when passed to the symbolizer function, return the file/line of the call instruction.  No additional subtraction is required or appropriate. 

On all platforms, the traceback function is invoked when a call from Go to C to Go requests a stack trace. On linux/amd64, linux/ppc64le, and freebsd/amd64, the traceback function is also invoked when a signal is received by a thread that is executing a cgo call. The traceback function should not make assumptions about when it is called, as future versions of Go may make additional calls. 

The symbolizer function will be called with a single argument, a pointer to a struct: 

```
struct {
	PC      uintptr // program counter to fetch information for
	File    *byte   // file name (NUL terminated)
	Lineno  uintptr // line number
	Func    *byte   // function name (NUL terminated)
	Entry   uintptr // function entry point
	More    uintptr // set non-zero if more info for this PC
	Data    uintptr // unused by runtime, available for function
}

```
In C syntax, this struct will be 

```
struct {
	uintptr_t PC;
	char*     File;
	uintptr_t Lineno;
	char*     Func;
	uintptr_t Entry;
	uintptr_t More;
	uintptr_t Data;
};

```
The PC field will be a value returned by a call to the traceback function. 

The first time the function is called for a particular traceback, all the fields except PC will be 0. The function should fill in the other fields if possible, setting them to 0/nil if the information is not available. The Data field may be used to store any useful information across calls. The More field should be set to non-zero if there is more information for this PC, zero otherwise. If More is set non-zero, the function will be called again with the same PC, and may return different information (this is intended for use with inlined functions). If More is zero, the function will be called with the next PC value in the traceback. When the traceback is complete, the function will be called once more with PC set to zero; this may be used to free any information. Each call will leave the fields of the struct set to the same values they had upon return, except for the PC field when the More field is zero. The function must not keep a copy of the struct pointer between calls. 

When calling SetCgoTraceback, the version argument is the version number of the structs that the functions expect to receive. Currently this must be zero. 

The symbolizer function may be nil, in which case the results of the traceback function will be displayed as numbers. If the traceback function is nil, the symbolizer function will never be called. The context function may be nil, in which case the traceback function will only be called with the context field set to zero.  If the context function is nil, then calls from Go to C to Go will not show a traceback for the C portion of the call stack. 

SetCgoTraceback should be called only once, ideally from an init function. 

### <a id="SetEnvs" href="#SetEnvs">func SetEnvs(e []string)</a>

```
searchKey: runtime.SetEnvs
tags: [method private]
```

```Go
func SetEnvs(e []string)
```

### <a id="SetFinalizer" href="#SetFinalizer">func SetFinalizer(obj interface{}, finalizer interface{})</a>

```
searchKey: runtime.SetFinalizer
tags: [method]
```

```Go
func SetFinalizer(obj interface{}, finalizer interface{})
```

SetFinalizer sets the finalizer associated with obj to the provided finalizer function. When the garbage collector finds an unreachable block with an associated finalizer, it clears the association and runs finalizer(obj) in a separate goroutine. This makes obj reachable again, but now without an associated finalizer. Assuming that SetFinalizer is not called again, the next time the garbage collector sees that obj is unreachable, it will free obj. 

SetFinalizer(obj, nil) clears any finalizer associated with obj. 

The argument obj must be a pointer to an object allocated by calling new, by taking the address of a composite literal, or by taking the address of a local variable. The argument finalizer must be a function that takes a single argument to which obj's type can be assigned, and can have arbitrary ignored return values. If either of these is not true, SetFinalizer may abort the program. 

Finalizers are run in dependency order: if A points at B, both have finalizers, and they are otherwise unreachable, only the finalizer for A runs; once A is freed, the finalizer for B can run. If a cyclic structure includes a block with a finalizer, that cycle is not guaranteed to be garbage collected and the finalizer is not guaranteed to run, because there is no ordering that respects the dependencies. 

The finalizer is scheduled to run at some arbitrary time after the program can no longer reach the object to which obj points. There is no guarantee that finalizers will run before a program exits, so typically they are useful only for releasing non-memory resources associated with an object during a long-running program. For example, an os.File object could use a finalizer to close the associated operating system file descriptor when a program discards an os.File without calling Close, but it would be a mistake to depend on a finalizer to flush an in-memory I/O buffer such as a bufio.Writer, because the buffer would not be flushed at program exit. 

It is not guaranteed that a finalizer will run if the size of *obj is zero bytes. 

It is not guaranteed that a finalizer will run for objects allocated in initializers for package-level variables. Such objects may be linker-allocated, not heap-allocated. 

A finalizer may run as soon as an object becomes unreachable. In order to use finalizers correctly, the program must ensure that the object is reachable until it is no longer required. Objects stored in global variables, or that can be found by tracing pointers from a global variable, are reachable. For other objects, pass the object to a call of the KeepAlive function to mark the last point in the function where the object must be reachable. 

For example, if p points to a struct, such as os.File, that contains a file descriptor d, and p has a finalizer that closes that file descriptor, and if the last use of p in a function is a call to syscall.Write(p.d, buf, size), then p may be unreachable as soon as the program enters syscall.Write. The finalizer may run at that moment, closing p.d, causing syscall.Write to fail because it is writing to a closed file descriptor (or, worse, to an entirely different file descriptor opened by a different goroutine). To avoid this problem, call runtime.KeepAlive(p) after the call to syscall.Write. 

A single goroutine runs all finalizers for a program, sequentially. If a finalizer must run for a long time, it should do so by starting a new goroutine. 

### <a id="SetIntArgRegs" href="#SetIntArgRegs">func SetIntArgRegs(a int) int</a>

```
searchKey: runtime.SetIntArgRegs
tags: [method private]
```

```Go
func SetIntArgRegs(a int) int
```

### <a id="SetMutexProfileFraction" href="#SetMutexProfileFraction">func SetMutexProfileFraction(rate int) int</a>

```
searchKey: runtime.SetMutexProfileFraction
tags: [method]
```

```Go
func SetMutexProfileFraction(rate int) int
```

SetMutexProfileFraction controls the fraction of mutex contention events that are reported in the mutex profile. On average 1/rate events are reported. The previous rate is returned. 

To turn off profiling entirely, pass rate 0. To just read the current rate, pass rate < 0. (For n>1 the details of sampling may change.) 

### <a id="SetTracebackEnv" href="#SetTracebackEnv">func SetTracebackEnv(level string)</a>

```
searchKey: runtime.SetTracebackEnv
tags: [method private]
```

```Go
func SetTracebackEnv(level string)
```

SetTracebackEnv is like runtime/debug.SetTraceback, but it raises the "environment" traceback level, so later calls to debug.SetTraceback (e.g., from testing timeouts) can't lower it. 

### <a id="Sigisblocked" href="#Sigisblocked">func Sigisblocked(i int) bool</a>

```
searchKey: runtime.Sigisblocked
tags: [method private]
```

```Go
func Sigisblocked(i int) bool
```

### <a id="Stack" href="#Stack">func Stack(buf []byte, all bool) int</a>

```
searchKey: runtime.Stack
tags: [method]
```

```Go
func Stack(buf []byte, all bool) int
```

Stack formats a stack trace of the calling goroutine into buf and returns the number of bytes written to buf. If all is true, Stack formats stack traces of all other goroutines into buf after the trace for the current goroutine. 

### <a id="StartTrace" href="#StartTrace">func StartTrace() error</a>

```
searchKey: runtime.StartTrace
tags: [function]
```

```Go
func StartTrace() error
```

StartTrace enables tracing for the current process. While tracing, the data will be buffered and available via ReadTrace. StartTrace returns an error if tracing is already enabled. Most clients should use the runtime/trace package or the testing package's -test.trace flag instead of calling StartTrace directly. 

### <a id="StopTrace" href="#StopTrace">func StopTrace()</a>

```
searchKey: runtime.StopTrace
tags: [function]
```

```Go
func StopTrace()
```

StopTrace stops tracing, if it was previously enabled. StopTrace only returns after all the reads for the trace have completed. 

### <a id="StringifyPallocBits" href="#StringifyPallocBits">func StringifyPallocBits(b *PallocBits, r BitRange) string</a>

```
searchKey: runtime.StringifyPallocBits
tags: [method private]
```

```Go
func StringifyPallocBits(b *PallocBits, r BitRange) string
```

StringifyPallocBits gets the bits in the bit range r from b, and returns a string containing the bits as ASCII 0 and 1 characters. 

### <a id="ThreadCreateProfile" href="#ThreadCreateProfile">func ThreadCreateProfile(p []StackRecord) (n int, ok bool)</a>

```
searchKey: runtime.ThreadCreateProfile
tags: [method]
```

```Go
func ThreadCreateProfile(p []StackRecord) (n int, ok bool)
```

ThreadCreateProfile returns n, the number of records in the thread creation profile. If len(p) >= n, ThreadCreateProfile copies the profile into p and returns n, true. If len(p) < n, ThreadCreateProfile does not change p and returns n, false. 

Most clients should use the runtime/pprof package instead of calling ThreadCreateProfile directly. 

### <a id="TracebackSystemstack" href="#TracebackSystemstack">func TracebackSystemstack(stk []uintptr, i int) int</a>

```
searchKey: runtime.TracebackSystemstack
tags: [method private]
```

```Go
func TracebackSystemstack(stk []uintptr, i int) int
```

### <a id="UnlockOSThread" href="#UnlockOSThread">func UnlockOSThread()</a>

```
searchKey: runtime.UnlockOSThread
tags: [function]
```

```Go
func UnlockOSThread()
```

UnlockOSThread undoes an earlier call to LockOSThread. If this drops the number of active LockOSThread calls on the calling goroutine to zero, it unwires the calling goroutine from its fixed operating system thread. If there are no active LockOSThread calls, this is a no-op. 

Before calling UnlockOSThread, the caller must ensure that the OS thread is suitable for running other goroutines. If the caller made any permanent changes to the state of the thread that would affect other goroutines, it should not call this function and thus leave the goroutine locked to the OS thread until the goroutine (and hence the thread) exits. 

### <a id="Version" href="#Version">func Version() string</a>

```
searchKey: runtime.Version
tags: [function]
```

```Go
func Version() string
```

Version returns the Go tree's version string. It is either the commit hash and date at the time of the build or, when possible, a release tag like "go1.3". 

### <a id="WaitForSigusr1" href="#WaitForSigusr1">func WaitForSigusr1(r, w int32, ready func(mp *M)) (int64, int64)</a>

```
searchKey: runtime.WaitForSigusr1
tags: [method private]
```

```Go
func WaitForSigusr1(r, w int32, ready func(mp *M)) (int64, int64)
```

WaitForSigusr1 blocks until a SIGUSR1 is received. It calls ready when it is set up to receive SIGUSR1. The ready function should cause a SIGUSR1 to be sent. The r and w arguments are a pipe that the signal handler can use to report when the signal is received. 

Once SIGUSR1 is received, it returns the ID of the current M and the ID of the M the SIGUSR1 was received on. If the caller writes a non-zero byte to w, WaitForSigusr1 returns immediately with -1, -1. 

### <a id="abort" href="#abort">func abort()</a>

```
searchKey: runtime.abort
tags: [function private]
```

```Go
func abort()
```

abort crashes the runtime in situations where even throw might not work. In general it should do something a debugger will recognize (e.g., an INT3 on x86). A crash in abort is recognized by the signal handler, which will attempt to tear down the runtime immediately. 

### <a id="abs" href="#abs">func abs(x float64) float64</a>

```
searchKey: runtime.abs
tags: [method private]
```

```Go
func abs(x float64) float64
```

Abs returns the absolute value of x. 

Special cases are: 

```
Abs(±Inf) = +Inf
Abs(NaN) = NaN

```
### <a id="acquireLockRank" href="#acquireLockRank">func acquireLockRank(rank lockRank)</a>

```
searchKey: runtime.acquireLockRank
tags: [method private]
```

```Go
func acquireLockRank(rank lockRank)
```

This function may be called in nosplit context and thus must be nosplit. 

### <a id="acquirep" href="#acquirep">func acquirep(_p_ *p)</a>

```
searchKey: runtime.acquirep
tags: [method private]
```

```Go
func acquirep(_p_ *p)
```

Associate p and the current m. 

This function is allowed to have write barriers even if the caller isn't because it immediately acquires _p_. 

### <a id="activeModules" href="#activeModules">func activeModules() []*moduledata</a>

```
searchKey: runtime.activeModules
tags: [function private]
```

```Go
func activeModules() []*moduledata
```

activeModules returns a slice of active modules. 

A module is active once its gcdatamask and gcbssmask have been assembled and it is usable by the GC. 

This is nosplit/nowritebarrier because it is called by the cgo pointer checking code. 

### <a id="add" href="#add">func add(p unsafe.Pointer, x uintptr) unsafe.Pointer</a>

```
searchKey: runtime.add
tags: [method private]
```

```Go
func add(p unsafe.Pointer, x uintptr) unsafe.Pointer
```

Should be a built-in for unsafe.Pointer? 

### <a id="add1" href="#add1">func add1(p *byte) *byte</a>

```
searchKey: runtime.add1
tags: [method private]
```

```Go
func add1(p *byte) *byte
```

add1 returns the byte pointer p+1. 

### <a id="addAdjustedTimers" href="#addAdjustedTimers">func addAdjustedTimers(pp *p, moved []*timer)</a>

```
searchKey: runtime.addAdjustedTimers
tags: [method private]
```

```Go
func addAdjustedTimers(pp *p, moved []*timer)
```

addAdjustedTimers adds any timers we adjusted in adjusttimers back to the timer heap. 

### <a id="addOneOpenDeferFrame" href="#addOneOpenDeferFrame">func addOneOpenDeferFrame(gp *g, pc uintptr, sp unsafe.Pointer)</a>

```
searchKey: runtime.addOneOpenDeferFrame
tags: [method private]
```

```Go
func addOneOpenDeferFrame(gp *g, pc uintptr, sp unsafe.Pointer)
```

addOneOpenDeferFrame scans the stack for the first frame (if any) with open-coded defers and if it finds one, adds a single record to the defer chain for that frame. If sp is non-nil, it starts the stack scan from the frame specified by sp. If sp is nil, it uses the sp from the current defer record (which has just been finished). Hence, it continues the stack scan from the frame of the defer that just finished. It skips any frame that already has an open-coded _defer record, which would have been created from a previous (unrecovered) panic. 

Note: All entries of the defer chain (including this new open-coded entry) have their pointers (including sp) adjusted properly if the stack moves while running deferred functions. Also, it is safe to pass in the sp arg (which is the direct result of calling getcallersp()), because all pointer variables (including arguments) are adjusted as needed during stack copies. 

### <a id="addb" href="#addb">func addb(p *byte, n uintptr) *byte</a>

```
searchKey: runtime.addb
tags: [method private]
```

```Go
func addb(p *byte, n uintptr) *byte
```

addb returns the byte pointer p+n. 

### <a id="addfinalizer" href="#addfinalizer">func addfinalizer(p unsafe.Pointer, f *funcval, nret uintptr, fint *_type, ot *ptrtype) bool</a>

```
searchKey: runtime.addfinalizer
tags: [method private]
```

```Go
func addfinalizer(p unsafe.Pointer, f *funcval, nret uintptr, fint *_type, ot *ptrtype) bool
```

Adds a finalizer to the object p. Returns true if it succeeded. 

### <a id="addmoduledata" href="#addmoduledata">func addmoduledata()</a>

```
searchKey: runtime.addmoduledata
tags: [function private]
```

```Go
func addmoduledata()
```

Called from linker-generated .initarray; declared for go vet; do NOT call from Go. 

### <a id="addrsToSummaryRange" href="#addrsToSummaryRange">func addrsToSummaryRange(level int, base, limit uintptr) (lo int, hi int)</a>

```
searchKey: runtime.addrsToSummaryRange
tags: [method private]
```

```Go
func addrsToSummaryRange(level int, base, limit uintptr) (lo int, hi int)
```

addrsToSummaryRange converts base and limit pointers into a range of entries for the given summary level. 

The returned range is inclusive on the lower bound and exclusive on the upper bound. 

### <a id="addspecial" href="#addspecial">func addspecial(p unsafe.Pointer, s *special) bool</a>

```
searchKey: runtime.addspecial
tags: [method private]
```

```Go
func addspecial(p unsafe.Pointer, s *special) bool
```

Adds the special record s to the list of special records for the object p. All fields of s should be filled in except for offset & next, which this routine will fill in. Returns true if the special was successfully added, false otherwise. (The add will fail only if a record with the same p and s->kind 

```
already exists.)

```
### <a id="addtimer" href="#addtimer">func addtimer(t *timer)</a>

```
searchKey: runtime.addtimer
tags: [method private]
```

```Go
func addtimer(t *timer)
```

addtimer adds a timer to the current P. This should only be called with a newly created timer. That avoids the risk of changing the when field of a timer in some P's heap, which could cause the heap to become unsorted. 

### <a id="adjustSignalStack" href="#adjustSignalStack">func adjustSignalStack(sig uint32, mp *m, gsigStack *gsignalStack) bool</a>

```
searchKey: runtime.adjustSignalStack
tags: [method private]
```

```Go
func adjustSignalStack(sig uint32, mp *m, gsigStack *gsignalStack) bool
```

adjustSignalStack adjusts the current stack guard based on the stack pointer that is actually in use while handling a signal. We do this in case some non-Go code called sigaltstack. This reports whether the stack was adjusted, and if so stores the old signal stack in *gsigstack. 

### <a id="adjustctxt" href="#adjustctxt">func adjustctxt(gp *g, adjinfo *adjustinfo)</a>

```
searchKey: runtime.adjustctxt
tags: [method private]
```

```Go
func adjustctxt(gp *g, adjinfo *adjustinfo)
```

### <a id="adjustdefers" href="#adjustdefers">func adjustdefers(gp *g, adjinfo *adjustinfo)</a>

```
searchKey: runtime.adjustdefers
tags: [method private]
```

```Go
func adjustdefers(gp *g, adjinfo *adjustinfo)
```

### <a id="adjustframe" href="#adjustframe">func adjustframe(frame *stkframe, arg unsafe.Pointer) bool</a>

```
searchKey: runtime.adjustframe
tags: [method private]
```

```Go
func adjustframe(frame *stkframe, arg unsafe.Pointer) bool
```

Note: the argument/return area is adjusted by the callee. 

### <a id="adjustpanics" href="#adjustpanics">func adjustpanics(gp *g, adjinfo *adjustinfo)</a>

```
searchKey: runtime.adjustpanics
tags: [method private]
```

```Go
func adjustpanics(gp *g, adjinfo *adjustinfo)
```

### <a id="adjustpointer" href="#adjustpointer">func adjustpointer(adjinfo *adjustinfo, vpp unsafe.Pointer)</a>

```
searchKey: runtime.adjustpointer
tags: [method private]
```

```Go
func adjustpointer(adjinfo *adjustinfo, vpp unsafe.Pointer)
```

Adjustpointer checks whether *vpp is in the old stack described by adjinfo. If so, it rewrites *vpp to point into the new stack. 

### <a id="adjustpointers" href="#adjustpointers">func adjustpointers(scanp unsafe.Pointer, bv *bitvector, adjinfo *adjustinfo, f funcInfo)</a>

```
searchKey: runtime.adjustpointers
tags: [method private]
```

```Go
func adjustpointers(scanp unsafe.Pointer, bv *bitvector, adjinfo *adjustinfo, f funcInfo)
```

bv describes the memory starting at address scanp. Adjust any pointers contained therein. 

### <a id="adjustsudogs" href="#adjustsudogs">func adjustsudogs(gp *g, adjinfo *adjustinfo)</a>

```
searchKey: runtime.adjustsudogs
tags: [method private]
```

```Go
func adjustsudogs(gp *g, adjinfo *adjustinfo)
```

### <a id="adjusttimers" href="#adjusttimers">func adjusttimers(pp *p, now int64)</a>

```
searchKey: runtime.adjusttimers
tags: [method private]
```

```Go
func adjusttimers(pp *p, now int64)
```

adjusttimers looks through the timers in the current P's heap for any timers that have been modified to run earlier, and puts them in the correct place in the heap. While looking for those timers, it also moves timers that have been modified to run later, and removes deleted timers. The caller must have locked the timers for pp. 

### <a id="advanceEvacuationMark" href="#advanceEvacuationMark">func advanceEvacuationMark(h *hmap, t *maptype, newbit uintptr)</a>

```
searchKey: runtime.advanceEvacuationMark
tags: [method private]
```

```Go
func advanceEvacuationMark(h *hmap, t *maptype, newbit uintptr)
```

### <a id="afterfork" href="#afterfork">func afterfork()</a>

```
searchKey: runtime.afterfork
tags: [function private]
```

```Go
func afterfork()
```

### <a id="alginit" href="#alginit">func alginit()</a>

```
searchKey: runtime.alginit
tags: [function private]
```

```Go
func alginit()
```

### <a id="alignDown" href="#alignDown">func alignDown(n, a uintptr) uintptr</a>

```
searchKey: runtime.alignDown
tags: [method private]
```

```Go
func alignDown(n, a uintptr) uintptr
```

alignDown rounds n down to a multiple of a. a must be a power of 2. 

### <a id="alignUp" href="#alignUp">func alignUp(n, a uintptr) uintptr</a>

```
searchKey: runtime.alignUp
tags: [method private]
```

```Go
func alignUp(n, a uintptr) uintptr
```

alignUp rounds n up to a multiple of a. a must be a power of 2. 

### <a id="allFrames" href="#allFrames">func allFrames(pcs []uintptr) []Frame</a>

```
searchKey: runtime.allFrames
tags: [method private]
```

```Go
func allFrames(pcs []uintptr) []Frame
```

allFrames returns all of the Frames corresponding to pcs. 

### <a id="allgadd" href="#allgadd">func allgadd(gp *g)</a>

```
searchKey: runtime.allgadd
tags: [method private]
```

```Go
func allgadd(gp *g)
```

### <a id="appendIntStr" href="#appendIntStr">func appendIntStr(b []byte, v int64, signed bool) []byte</a>

```
searchKey: runtime.appendIntStr
tags: [method private]
```

```Go
func appendIntStr(b []byte, v int64, signed bool) []byte
```

### <a id="arenaBase" href="#arenaBase">func arenaBase(i arenaIdx) uintptr</a>

```
searchKey: runtime.arenaBase
tags: [method private]
```

```Go
func arenaBase(i arenaIdx) uintptr
```

arenaBase returns the low address of the region covered by heap arena i. 

### <a id="args" href="#args">func args(c int32, v **byte)</a>

```
searchKey: runtime.args
tags: [method private]
```

```Go
func args(c int32, v **byte)
```

### <a id="argv_index" href="#argv_index">func argv_index(argv **byte, i int32) *byte</a>

```
searchKey: runtime.argv_index
tags: [method private]
```

```Go
func argv_index(argv **byte, i int32) *byte
```

nosplit for use in linux startup sysargs 

### <a id="asmcgocall" href="#asmcgocall">func asmcgocall(fn, arg unsafe.Pointer) int32</a>

```
searchKey: runtime.asmcgocall
tags: [method private]
```

```Go
func asmcgocall(fn, arg unsafe.Pointer) int32
```

### <a id="asmcgocall_no_g" href="#asmcgocall_no_g">func asmcgocall_no_g(fn, arg unsafe.Pointer)</a>

```
searchKey: runtime.asmcgocall_no_g
tags: [method private]
```

```Go
func asmcgocall_no_g(fn, arg unsafe.Pointer)
```

### <a id="asminit" href="#asminit">func asminit()</a>

```
searchKey: runtime.asminit
tags: [function private]
```

```Go
func asminit()
```

### <a id="assertLockHeld" href="#assertLockHeld">func assertLockHeld(l *mutex)</a>

```
searchKey: runtime.assertLockHeld
tags: [method private]
```

```Go
func assertLockHeld(l *mutex)
```

### <a id="assertRankHeld" href="#assertRankHeld">func assertRankHeld(r lockRank)</a>

```
searchKey: runtime.assertRankHeld
tags: [method private]
```

```Go
func assertRankHeld(r lockRank)
```

### <a id="assertWorldStopped" href="#assertWorldStopped">func assertWorldStopped()</a>

```
searchKey: runtime.assertWorldStopped
tags: [function private]
```

```Go
func assertWorldStopped()
```

### <a id="assertWorldStoppedOrLockHeld" href="#assertWorldStoppedOrLockHeld">func assertWorldStoppedOrLockHeld(l *mutex)</a>

```
searchKey: runtime.assertWorldStoppedOrLockHeld
tags: [method private]
```

```Go
func assertWorldStoppedOrLockHeld(l *mutex)
```

### <a id="asyncPreempt" href="#asyncPreempt">func asyncPreempt()</a>

```
searchKey: runtime.asyncPreempt
tags: [function private]
```

```Go
func asyncPreempt()
```

asyncPreempt saves all user registers and calls asyncPreempt2. 

When stack scanning encounters an asyncPreempt frame, it scans that frame and its parent frame conservatively. 

asyncPreempt is implemented in assembly. 

### <a id="asyncPreempt2" href="#asyncPreempt2">func asyncPreempt2()</a>

```
searchKey: runtime.asyncPreempt2
tags: [function private]
```

```Go
func asyncPreempt2()
```

### <a id="atoi" href="#atoi">func atoi(s string) (int, bool)</a>

```
searchKey: runtime.atoi
tags: [method private]
```

```Go
func atoi(s string) (int, bool)
```

atoi parses an int from a string s. The bool result reports whether s is a number representable by a value of type int. 

### <a id="atoi32" href="#atoi32">func atoi32(s string) (int32, bool)</a>

```
searchKey: runtime.atoi32
tags: [method private]
```

```Go
func atoi32(s string) (int32, bool)
```

atoi32 is like atoi but for integers that fit into an int32. 

### <a id="atomicstorep" href="#atomicstorep">func atomicstorep(ptr unsafe.Pointer, new unsafe.Pointer)</a>

```
searchKey: runtime.atomicstorep
tags: [method private]
```

```Go
func atomicstorep(ptr unsafe.Pointer, new unsafe.Pointer)
```

atomicstorep performs *ptr = new atomically and invokes a write barrier. 

### <a id="atomicwb" href="#atomicwb">func atomicwb(ptr *unsafe.Pointer, new unsafe.Pointer)</a>

```
searchKey: runtime.atomicwb
tags: [method private]
```

```Go
func atomicwb(ptr *unsafe.Pointer, new unsafe.Pointer)
```

atomicwb performs a write barrier before an atomic pointer write. The caller should guard the call with "if writeBarrier.enabled". 

### <a id="badPointer" href="#badPointer">func badPointer(s *mspan, p, refBase, refOff uintptr)</a>

```
searchKey: runtime.badPointer
tags: [method private]
```

```Go
func badPointer(s *mspan, p, refBase, refOff uintptr)
```

badPointer throws bad pointer in heap panic. 

### <a id="badTimer" href="#badTimer">func badTimer()</a>

```
searchKey: runtime.badTimer
tags: [function private]
```

```Go
func badTimer()
```

badTimer is called if the timer data structures have been corrupted, presumably due to racy use by the program. We panic here rather than panicing due to invalid slice access while holding locks. See issue #25686. 

### <a id="badcgocallback" href="#badcgocallback">func badcgocallback()</a>

```
searchKey: runtime.badcgocallback
tags: [function private]
```

```Go
func badcgocallback()
```

called from assembly 

### <a id="badctxt" href="#badctxt">func badctxt()</a>

```
searchKey: runtime.badctxt
tags: [function private]
```

```Go
func badctxt()
```

### <a id="badmcall" href="#badmcall">func badmcall(fn func(*g))</a>

```
searchKey: runtime.badmcall
tags: [method private]
```

```Go
func badmcall(fn func(*g))
```

called from assembly 

### <a id="badmcall2" href="#badmcall2">func badmcall2(fn func(*g))</a>

```
searchKey: runtime.badmcall2
tags: [method private]
```

```Go
func badmcall2(fn func(*g))
```

### <a id="badmorestackg0" href="#badmorestackg0">func badmorestackg0()</a>

```
searchKey: runtime.badmorestackg0
tags: [function private]
```

```Go
func badmorestackg0()
```

### <a id="badmorestackgsignal" href="#badmorestackgsignal">func badmorestackgsignal()</a>

```
searchKey: runtime.badmorestackgsignal
tags: [function private]
```

```Go
func badmorestackgsignal()
```

### <a id="badreflectcall" href="#badreflectcall">func badreflectcall()</a>

```
searchKey: runtime.badreflectcall
tags: [function private]
```

```Go
func badreflectcall()
```

### <a id="badsignal" href="#badsignal">func badsignal(sig uintptr, c *sigctxt)</a>

```
searchKey: runtime.badsignal
tags: [method private]
```

```Go
func badsignal(sig uintptr, c *sigctxt)
```

This runs on a foreign stack, without an m or a g. No stack split. 

### <a id="badsystemstack" href="#badsystemstack">func badsystemstack()</a>

```
searchKey: runtime.badsystemstack
tags: [function private]
```

```Go
func badsystemstack()
```

### <a id="badunlockosthread" href="#badunlockosthread">func badunlockosthread()</a>

```
searchKey: runtime.badunlockosthread
tags: [function private]
```

```Go
func badunlockosthread()
```

### <a id="beforefork" href="#beforefork">func beforefork()</a>

```
searchKey: runtime.beforefork
tags: [function private]
```

```Go
func beforefork()
```

### <a id="bgscavenge" href="#bgscavenge">func bgscavenge()</a>

```
searchKey: runtime.bgscavenge
tags: [function private]
```

```Go
func bgscavenge()
```

Background scavenger. 

The background scavenger maintains the RSS of the application below the line described by the proportional scavenging statistics in the mheap struct. 

### <a id="bgsweep" href="#bgsweep">func bgsweep()</a>

```
searchKey: runtime.bgsweep
tags: [function private]
```

```Go
func bgsweep()
```

### <a id="block" href="#block">func block()</a>

```
searchKey: runtime.block
tags: [function private]
```

```Go
func block()
```

### <a id="blockAlignSummaryRange" href="#blockAlignSummaryRange">func blockAlignSummaryRange(level int, lo, hi int) (int, int)</a>

```
searchKey: runtime.blockAlignSummaryRange
tags: [method private]
```

```Go
func blockAlignSummaryRange(level int, lo, hi int) (int, int)
```

blockAlignSummaryRange aligns indices into the given level to that level's block width (1 << levelBits[level]). It assumes lo is inclusive and hi is exclusive, and so aligns them down and up respectively. 

### <a id="blockOnSystemStackInternal" href="#blockOnSystemStackInternal">func blockOnSystemStackInternal()</a>

```
searchKey: runtime.blockOnSystemStackInternal
tags: [function private]
```

```Go
func blockOnSystemStackInternal()
```

### <a id="blockableSig" href="#blockableSig">func blockableSig(sig uint32) bool</a>

```
searchKey: runtime.blockableSig
tags: [method private]
```

```Go
func blockableSig(sig uint32) bool
```

blockableSig reports whether sig may be blocked by the signal mask. We never want to block the signals marked _SigUnblock; these are the synchronous signals that turn into a Go panic. In a Go program--not a c-archive/c-shared--we never want to block the signals marked _SigKill or _SigThrow, as otherwise it's possible for all running threads to block them and delay their delivery until we start a new thread. When linked into a C program we let the C code decide on the disposition of those signals. 

### <a id="blockevent" href="#blockevent">func blockevent(cycles int64, skip int)</a>

```
searchKey: runtime.blockevent
tags: [method private]
```

```Go
func blockevent(cycles int64, skip int)
```

### <a id="blocksampled" href="#blocksampled">func blocksampled(cycles, rate int64) bool</a>

```
searchKey: runtime.blocksampled
tags: [method private]
```

```Go
func blocksampled(cycles, rate int64) bool
```

blocksampled returns true for all events where cycles >= rate. Shorter events have a cycles/rate random chance of returning true. 

### <a id="bool2int" href="#bool2int">func bool2int(x bool) int</a>

```
searchKey: runtime.bool2int
tags: [method private]
```

```Go
func bool2int(x bool) int
```

bool2int returns 0 if x is false or 1 if x is true. 

### <a id="breakpoint" href="#breakpoint">func breakpoint()</a>

```
searchKey: runtime.breakpoint
tags: [function private]
```

```Go
func breakpoint()
```

### <a id="bucketEvacuated" href="#bucketEvacuated">func bucketEvacuated(t *maptype, h *hmap, bucket uintptr) bool</a>

```
searchKey: runtime.bucketEvacuated
tags: [method private]
```

```Go
func bucketEvacuated(t *maptype, h *hmap, bucket uintptr) bool
```

### <a id="bucketMask" href="#bucketMask">func bucketMask(b uint8) uintptr</a>

```
searchKey: runtime.bucketMask
tags: [method private]
```

```Go
func bucketMask(b uint8) uintptr
```

bucketMask returns 1<<b - 1, optimized for code generation. 

### <a id="bucketShift" href="#bucketShift">func bucketShift(b uint8) uintptr</a>

```
searchKey: runtime.bucketShift
tags: [method private]
```

```Go
func bucketShift(b uint8) uintptr
```

bucketShift returns 1<<b, optimized for code generation. 

### <a id="bulkBarrierBitmap" href="#bulkBarrierBitmap">func bulkBarrierBitmap(dst, src, size, maskOffset uintptr, bits *uint8)</a>

```
searchKey: runtime.bulkBarrierBitmap
tags: [method private]
```

```Go
func bulkBarrierBitmap(dst, src, size, maskOffset uintptr, bits *uint8)
```

bulkBarrierBitmap executes write barriers for copying from [src, src+size) to [dst, dst+size) using a 1-bit pointer bitmap. src is assumed to start maskOffset bytes into the data covered by the bitmap in bits (which may not be a multiple of 8). 

This is used by bulkBarrierPreWrite for writes to data and BSS. 

### <a id="bulkBarrierPreWrite" href="#bulkBarrierPreWrite">func bulkBarrierPreWrite(dst, src, size uintptr)</a>

```
searchKey: runtime.bulkBarrierPreWrite
tags: [method private]
```

```Go
func bulkBarrierPreWrite(dst, src, size uintptr)
```

bulkBarrierPreWrite executes a write barrier for every pointer slot in the memory range [src, src+size), using pointer/scalar information from [dst, dst+size). This executes the write barriers necessary before a memmove. src, dst, and size must be pointer-aligned. The range [dst, dst+size) must lie within a single object. It does not perform the actual writes. 

As a special case, src == 0 indicates that this is being used for a memclr. bulkBarrierPreWrite will pass 0 for the src of each write barrier. 

Callers should call bulkBarrierPreWrite immediately before calling memmove(dst, src, size). This function is marked nosplit to avoid being preempted; the GC must not stop the goroutine between the memmove and the execution of the barriers. The caller is also responsible for cgo pointer checks if this may be writing Go pointers into non-Go memory. 

The pointer bitmap is not maintained for allocations containing no pointers at all; any caller of bulkBarrierPreWrite must first make sure the underlying allocation contains pointers, usually by checking typ.ptrdata. 

Callers must perform cgo checks if writeBarrier.cgo. 

### <a id="bulkBarrierPreWriteSrcOnly" href="#bulkBarrierPreWriteSrcOnly">func bulkBarrierPreWriteSrcOnly(dst, src, size uintptr)</a>

```
searchKey: runtime.bulkBarrierPreWriteSrcOnly
tags: [method private]
```

```Go
func bulkBarrierPreWriteSrcOnly(dst, src, size uintptr)
```

bulkBarrierPreWriteSrcOnly is like bulkBarrierPreWrite but does not execute write barriers for [dst, dst+size). 

In addition to the requirements of bulkBarrierPreWrite callers need to ensure [dst, dst+size) is zeroed. 

This is used for special cases where e.g. dst was just created and zeroed with malloc. 

### <a id="bytes" href="#bytes">func bytes(s string) (ret []byte)</a>

```
searchKey: runtime.bytes
tags: [method private]
```

```Go
func bytes(s string) (ret []byte)
```

### <a id="bytesHash" href="#bytesHash">func bytesHash(b []byte, seed uintptr) uintptr</a>

```
searchKey: runtime.bytesHash
tags: [method private]
```

```Go
func bytesHash(b []byte, seed uintptr) uintptr
```

### <a id="c128equal" href="#c128equal">func c128equal(p, q unsafe.Pointer) bool</a>

```
searchKey: runtime.c128equal
tags: [method private]
```

```Go
func c128equal(p, q unsafe.Pointer) bool
```

### <a id="c128hash" href="#c128hash">func c128hash(p unsafe.Pointer, h uintptr) uintptr</a>

```
searchKey: runtime.c128hash
tags: [method private]
```

```Go
func c128hash(p unsafe.Pointer, h uintptr) uintptr
```

### <a id="c64equal" href="#c64equal">func c64equal(p, q unsafe.Pointer) bool</a>

```
searchKey: runtime.c64equal
tags: [method private]
```

```Go
func c64equal(p, q unsafe.Pointer) bool
```

### <a id="c64hash" href="#c64hash">func c64hash(p unsafe.Pointer, h uintptr) uintptr</a>

```
searchKey: runtime.c64hash
tags: [method private]
```

```Go
func c64hash(p unsafe.Pointer, h uintptr) uintptr
```

### <a id="call1024" href="#call1024">func call1024(typ, fn, stackArgs unsafe.Pointer, stackArgsSize, stackRetOffset, frameSize uint32, regArgs *abi.RegArgs)</a>

```
searchKey: runtime.call1024
tags: [method private]
```

```Go
func call1024(typ, fn, stackArgs unsafe.Pointer, stackArgsSize, stackRetOffset, frameSize uint32, regArgs *abi.RegArgs)
```

### <a id="call1048576" href="#call1048576">func call1048576(typ, fn, stackArgs unsafe.Pointer, stackArgsSize, stackRetOffset, frameSize uint32, regArgs *abi.RegArgs)</a>

```
searchKey: runtime.call1048576
tags: [method private]
```

```Go
func call1048576(typ, fn, stackArgs unsafe.Pointer, stackArgsSize, stackRetOffset, frameSize uint32, regArgs *abi.RegArgs)
```

### <a id="call1073741824" href="#call1073741824">func call1073741824(typ, fn, stackArgs unsafe.Pointer, stackArgsSize, stackRetOffset, frameSize uint32, regArgs *abi.RegArgs)</a>

```
searchKey: runtime.call1073741824
tags: [method private]
```

```Go
func call1073741824(typ, fn, stackArgs unsafe.Pointer, stackArgsSize, stackRetOffset, frameSize uint32, regArgs *abi.RegArgs)
```

### <a id="call128" href="#call128">func call128(typ, fn, stackArgs unsafe.Pointer, stackArgsSize, stackRetOffset, frameSize uint32, regArgs *abi.RegArgs)</a>

```
searchKey: runtime.call128
tags: [method private]
```

```Go
func call128(typ, fn, stackArgs unsafe.Pointer, stackArgsSize, stackRetOffset, frameSize uint32, regArgs *abi.RegArgs)
```

### <a id="call131072" href="#call131072">func call131072(typ, fn, stackArgs unsafe.Pointer, stackArgsSize, stackRetOffset, frameSize uint32, regArgs *abi.RegArgs)</a>

```
searchKey: runtime.call131072
tags: [method private]
```

```Go
func call131072(typ, fn, stackArgs unsafe.Pointer, stackArgsSize, stackRetOffset, frameSize uint32, regArgs *abi.RegArgs)
```

### <a id="call134217728" href="#call134217728">func call134217728(typ, fn, stackArgs unsafe.Pointer, stackArgsSize, stackRetOffset, frameSize uint32, regArgs *abi.RegArgs)</a>

```
searchKey: runtime.call134217728
tags: [method private]
```

```Go
func call134217728(typ, fn, stackArgs unsafe.Pointer, stackArgsSize, stackRetOffset, frameSize uint32, regArgs *abi.RegArgs)
```

### <a id="call16" href="#call16">func call16(typ, fn, stackArgs unsafe.Pointer, stackArgsSize, stackRetOffset, frameSize uint32, regArgs *abi.RegArgs)</a>

```
searchKey: runtime.call16
tags: [method private]
```

```Go
func call16(typ, fn, stackArgs unsafe.Pointer, stackArgsSize, stackRetOffset, frameSize uint32, regArgs *abi.RegArgs)
```

in asm_*.s not called directly; definitions here supply type information for traceback. These must have the same signature (arg pointer map) as reflectcall. 

### <a id="call16384" href="#call16384">func call16384(typ, fn, stackArgs unsafe.Pointer, stackArgsSize, stackRetOffset, frameSize uint32, regArgs *abi.RegArgs)</a>

```
searchKey: runtime.call16384
tags: [method private]
```

```Go
func call16384(typ, fn, stackArgs unsafe.Pointer, stackArgsSize, stackRetOffset, frameSize uint32, regArgs *abi.RegArgs)
```

### <a id="call16777216" href="#call16777216">func call16777216(typ, fn, stackArgs unsafe.Pointer, stackArgsSize, stackRetOffset, frameSize uint32, regArgs *abi.RegArgs)</a>

```
searchKey: runtime.call16777216
tags: [method private]
```

```Go
func call16777216(typ, fn, stackArgs unsafe.Pointer, stackArgsSize, stackRetOffset, frameSize uint32, regArgs *abi.RegArgs)
```

### <a id="call2048" href="#call2048">func call2048(typ, fn, stackArgs unsafe.Pointer, stackArgsSize, stackRetOffset, frameSize uint32, regArgs *abi.RegArgs)</a>

```
searchKey: runtime.call2048
tags: [method private]
```

```Go
func call2048(typ, fn, stackArgs unsafe.Pointer, stackArgsSize, stackRetOffset, frameSize uint32, regArgs *abi.RegArgs)
```

### <a id="call2097152" href="#call2097152">func call2097152(typ, fn, stackArgs unsafe.Pointer, stackArgsSize, stackRetOffset, frameSize uint32, regArgs *abi.RegArgs)</a>

```
searchKey: runtime.call2097152
tags: [method private]
```

```Go
func call2097152(typ, fn, stackArgs unsafe.Pointer, stackArgsSize, stackRetOffset, frameSize uint32, regArgs *abi.RegArgs)
```

### <a id="call256" href="#call256">func call256(typ, fn, stackArgs unsafe.Pointer, stackArgsSize, stackRetOffset, frameSize uint32, regArgs *abi.RegArgs)</a>

```
searchKey: runtime.call256
tags: [method private]
```

```Go
func call256(typ, fn, stackArgs unsafe.Pointer, stackArgsSize, stackRetOffset, frameSize uint32, regArgs *abi.RegArgs)
```

### <a id="call262144" href="#call262144">func call262144(typ, fn, stackArgs unsafe.Pointer, stackArgsSize, stackRetOffset, frameSize uint32, regArgs *abi.RegArgs)</a>

```
searchKey: runtime.call262144
tags: [method private]
```

```Go
func call262144(typ, fn, stackArgs unsafe.Pointer, stackArgsSize, stackRetOffset, frameSize uint32, regArgs *abi.RegArgs)
```

### <a id="call268435456" href="#call268435456">func call268435456(typ, fn, stackArgs unsafe.Pointer, stackArgsSize, stackRetOffset, frameSize uint32, regArgs *abi.RegArgs)</a>

```
searchKey: runtime.call268435456
tags: [method private]
```

```Go
func call268435456(typ, fn, stackArgs unsafe.Pointer, stackArgsSize, stackRetOffset, frameSize uint32, regArgs *abi.RegArgs)
```

### <a id="call32" href="#call32">func call32(typ, fn, stackArgs unsafe.Pointer, stackArgsSize, stackRetOffset, frameSize uint32, regArgs *abi.RegArgs)</a>

```
searchKey: runtime.call32
tags: [method private]
```

```Go
func call32(typ, fn, stackArgs unsafe.Pointer, stackArgsSize, stackRetOffset, frameSize uint32, regArgs *abi.RegArgs)
```

### <a id="call32768" href="#call32768">func call32768(typ, fn, stackArgs unsafe.Pointer, stackArgsSize, stackRetOffset, frameSize uint32, regArgs *abi.RegArgs)</a>

```
searchKey: runtime.call32768
tags: [method private]
```

```Go
func call32768(typ, fn, stackArgs unsafe.Pointer, stackArgsSize, stackRetOffset, frameSize uint32, regArgs *abi.RegArgs)
```

### <a id="call33554432" href="#call33554432">func call33554432(typ, fn, stackArgs unsafe.Pointer, stackArgsSize, stackRetOffset, frameSize uint32, regArgs *abi.RegArgs)</a>

```
searchKey: runtime.call33554432
tags: [method private]
```

```Go
func call33554432(typ, fn, stackArgs unsafe.Pointer, stackArgsSize, stackRetOffset, frameSize uint32, regArgs *abi.RegArgs)
```

### <a id="call4096" href="#call4096">func call4096(typ, fn, stackArgs unsafe.Pointer, stackArgsSize, stackRetOffset, frameSize uint32, regArgs *abi.RegArgs)</a>

```
searchKey: runtime.call4096
tags: [method private]
```

```Go
func call4096(typ, fn, stackArgs unsafe.Pointer, stackArgsSize, stackRetOffset, frameSize uint32, regArgs *abi.RegArgs)
```

### <a id="call4194304" href="#call4194304">func call4194304(typ, fn, stackArgs unsafe.Pointer, stackArgsSize, stackRetOffset, frameSize uint32, regArgs *abi.RegArgs)</a>

```
searchKey: runtime.call4194304
tags: [method private]
```

```Go
func call4194304(typ, fn, stackArgs unsafe.Pointer, stackArgsSize, stackRetOffset, frameSize uint32, regArgs *abi.RegArgs)
```

### <a id="call512" href="#call512">func call512(typ, fn, stackArgs unsafe.Pointer, stackArgsSize, stackRetOffset, frameSize uint32, regArgs *abi.RegArgs)</a>

```
searchKey: runtime.call512
tags: [method private]
```

```Go
func call512(typ, fn, stackArgs unsafe.Pointer, stackArgsSize, stackRetOffset, frameSize uint32, regArgs *abi.RegArgs)
```

### <a id="call524288" href="#call524288">func call524288(typ, fn, stackArgs unsafe.Pointer, stackArgsSize, stackRetOffset, frameSize uint32, regArgs *abi.RegArgs)</a>

```
searchKey: runtime.call524288
tags: [method private]
```

```Go
func call524288(typ, fn, stackArgs unsafe.Pointer, stackArgsSize, stackRetOffset, frameSize uint32, regArgs *abi.RegArgs)
```

### <a id="call536870912" href="#call536870912">func call536870912(typ, fn, stackArgs unsafe.Pointer, stackArgsSize, stackRetOffset, frameSize uint32, regArgs *abi.RegArgs)</a>

```
searchKey: runtime.call536870912
tags: [method private]
```

```Go
func call536870912(typ, fn, stackArgs unsafe.Pointer, stackArgsSize, stackRetOffset, frameSize uint32, regArgs *abi.RegArgs)
```

### <a id="call64" href="#call64">func call64(typ, fn, stackArgs unsafe.Pointer, stackArgsSize, stackRetOffset, frameSize uint32, regArgs *abi.RegArgs)</a>

```
searchKey: runtime.call64
tags: [method private]
```

```Go
func call64(typ, fn, stackArgs unsafe.Pointer, stackArgsSize, stackRetOffset, frameSize uint32, regArgs *abi.RegArgs)
```

### <a id="call65536" href="#call65536">func call65536(typ, fn, stackArgs unsafe.Pointer, stackArgsSize, stackRetOffset, frameSize uint32, regArgs *abi.RegArgs)</a>

```
searchKey: runtime.call65536
tags: [method private]
```

```Go
func call65536(typ, fn, stackArgs unsafe.Pointer, stackArgsSize, stackRetOffset, frameSize uint32, regArgs *abi.RegArgs)
```

### <a id="call67108864" href="#call67108864">func call67108864(typ, fn, stackArgs unsafe.Pointer, stackArgsSize, stackRetOffset, frameSize uint32, regArgs *abi.RegArgs)</a>

```
searchKey: runtime.call67108864
tags: [method private]
```

```Go
func call67108864(typ, fn, stackArgs unsafe.Pointer, stackArgsSize, stackRetOffset, frameSize uint32, regArgs *abi.RegArgs)
```

### <a id="call8192" href="#call8192">func call8192(typ, fn, stackArgs unsafe.Pointer, stackArgsSize, stackRetOffset, frameSize uint32, regArgs *abi.RegArgs)</a>

```
searchKey: runtime.call8192
tags: [method private]
```

```Go
func call8192(typ, fn, stackArgs unsafe.Pointer, stackArgsSize, stackRetOffset, frameSize uint32, regArgs *abi.RegArgs)
```

### <a id="call8388608" href="#call8388608">func call8388608(typ, fn, stackArgs unsafe.Pointer, stackArgsSize, stackRetOffset, frameSize uint32, regArgs *abi.RegArgs)</a>

```
searchKey: runtime.call8388608
tags: [method private]
```

```Go
func call8388608(typ, fn, stackArgs unsafe.Pointer, stackArgsSize, stackRetOffset, frameSize uint32, regArgs *abi.RegArgs)
```

### <a id="callCgoSymbolizer" href="#callCgoSymbolizer">func callCgoSymbolizer(arg *cgoSymbolizerArg)</a>

```
searchKey: runtime.callCgoSymbolizer
tags: [method private]
```

```Go
func callCgoSymbolizer(arg *cgoSymbolizerArg)
```

callCgoSymbolizer calls the cgoSymbolizer function. 

### <a id="callers" href="#callers">func callers(skip int, pcbuf []uintptr) int</a>

```
searchKey: runtime.callers
tags: [method private]
```

```Go
func callers(skip int, pcbuf []uintptr) int
```

### <a id="canPreemptM" href="#canPreemptM">func canPreemptM(mp *m) bool</a>

```
searchKey: runtime.canPreemptM
tags: [method private]
```

```Go
func canPreemptM(mp *m) bool
```

canPreemptM reports whether mp is in a state that is safe to preempt. 

It is nosplit because it has nosplit callers. 

### <a id="canpanic" href="#canpanic">func canpanic(gp *g) bool</a>

```
searchKey: runtime.canpanic
tags: [method private]
```

```Go
func canpanic(gp *g) bool
```

canpanic returns false if a signal should throw instead of panicking. 

### <a id="cansemacquire" href="#cansemacquire">func cansemacquire(addr *uint32) bool</a>

```
searchKey: runtime.cansemacquire
tags: [method private]
```

```Go
func cansemacquire(addr *uint32) bool
```

### <a id="casGFromPreempted" href="#casGFromPreempted">func casGFromPreempted(gp *g, old, new uint32) bool</a>

```
searchKey: runtime.casGFromPreempted
tags: [method private]
```

```Go
func casGFromPreempted(gp *g, old, new uint32) bool
```

casGFromPreempted attempts to transition gp from _Gpreempted to _Gwaiting. If successful, the caller is responsible for re-scheduling gp. 

### <a id="casGToPreemptScan" href="#casGToPreemptScan">func casGToPreemptScan(gp *g, old, new uint32)</a>

```
searchKey: runtime.casGToPreemptScan
tags: [method private]
```

```Go
func casGToPreemptScan(gp *g, old, new uint32)
```

casGToPreemptScan transitions gp from _Grunning to _Gscan|_Gpreempted. 

TODO(austin): This is the only status operation that both changes the status and locks the _Gscan bit. Rethink this. 

### <a id="casfrom_Gscanstatus" href="#casfrom_Gscanstatus">func casfrom_Gscanstatus(gp *g, oldval, newval uint32)</a>

```
searchKey: runtime.casfrom_Gscanstatus
tags: [method private]
```

```Go
func casfrom_Gscanstatus(gp *g, oldval, newval uint32)
```

The Gscanstatuses are acting like locks and this releases them. If it proves to be a performance hit we should be able to make these simple atomic stores but for now we are going to throw if we see an inconsistent state. 

### <a id="casgcopystack" href="#casgcopystack">func casgcopystack(gp *g) uint32</a>

```
searchKey: runtime.casgcopystack
tags: [method private]
```

```Go
func casgcopystack(gp *g) uint32
```

casgstatus(gp, oldstatus, Gcopystack), assuming oldstatus is Gwaiting or Grunnable. Returns old status. Cannot call casgstatus directly, because we are racing with an async wakeup that might come in from netpoll. If we see Gwaiting from the readgstatus, it might have become Grunnable by the time we get to the cas. If we called casgstatus, it would loop waiting for the status to go back to Gwaiting, which it never will. 

### <a id="casgstatus" href="#casgstatus">func casgstatus(gp *g, oldval, newval uint32)</a>

```
searchKey: runtime.casgstatus
tags: [method private]
```

```Go
func casgstatus(gp *g, oldval, newval uint32)
```

If asked to move to or from a Gscanstatus this will throw. Use the castogscanstatus and casfrom_Gscanstatus instead. casgstatus will loop if the g->atomicstatus is in a Gscan status until the routine that put it in the Gscan state is finished. 

### <a id="castogscanstatus" href="#castogscanstatus">func castogscanstatus(gp *g, oldval, newval uint32) bool</a>

```
searchKey: runtime.castogscanstatus
tags: [method private]
```

```Go
func castogscanstatus(gp *g, oldval, newval uint32) bool
```

This will return false if the gp is not in the expected status and the cas fails. This acts like a lock acquire while the casfromgstatus acts like a lock release. 

### <a id="cfuncname" href="#cfuncname">func cfuncname(f funcInfo) *byte</a>

```
searchKey: runtime.cfuncname
tags: [method private]
```

```Go
func cfuncname(f funcInfo) *byte
```

### <a id="cfuncnameFromNameoff" href="#cfuncnameFromNameoff">func cfuncnameFromNameoff(f funcInfo, nameoff int32) *byte</a>

```
searchKey: runtime.cfuncnameFromNameoff
tags: [method private]
```

```Go
func cfuncnameFromNameoff(f funcInfo, nameoff int32) *byte
```

### <a id="cgoCheckArg" href="#cgoCheckArg">func cgoCheckArg(t *_type, p unsafe.Pointer, indir, top bool, msg string)</a>

```
searchKey: runtime.cgoCheckArg
tags: [method private]
```

```Go
func cgoCheckArg(t *_type, p unsafe.Pointer, indir, top bool, msg string)
```

cgoCheckArg is the real work of cgoCheckPointer. The argument p is either a pointer to the value (of type t), or the value itself, depending on indir. The top parameter is whether we are at the top level, where Go pointers are allowed. 

### <a id="cgoCheckBits" href="#cgoCheckBits">func cgoCheckBits(src unsafe.Pointer, gcbits *byte, off, size uintptr)</a>

```
searchKey: runtime.cgoCheckBits
tags: [method private]
```

```Go
func cgoCheckBits(src unsafe.Pointer, gcbits *byte, off, size uintptr)
```

cgoCheckBits checks the block of memory at src, for up to size bytes, and throws if it finds a Go pointer. The gcbits mark each pointer value. The src pointer is off bytes into the gcbits. 

### <a id="cgoCheckMemmove" href="#cgoCheckMemmove">func cgoCheckMemmove(typ *_type, dst, src unsafe.Pointer, off, size uintptr)</a>

```
searchKey: runtime.cgoCheckMemmove
tags: [method private]
```

```Go
func cgoCheckMemmove(typ *_type, dst, src unsafe.Pointer, off, size uintptr)
```

cgoCheckMemmove is called when moving a block of memory. dst and src point off bytes into the value to copy. size is the number of bytes to copy. It throws if the program is copying a block that contains a Go pointer into non-Go memory. 

### <a id="cgoCheckPointer" href="#cgoCheckPointer">func cgoCheckPointer(ptr interface{}, arg interface{})</a>

```
searchKey: runtime.cgoCheckPointer
tags: [method private]
```

```Go
func cgoCheckPointer(ptr interface{}, arg interface{})
```

cgoCheckPointer checks if the argument contains a Go pointer that points to a Go pointer, and panics if it does. 

### <a id="cgoCheckResult" href="#cgoCheckResult">func cgoCheckResult(val interface{})</a>

```
searchKey: runtime.cgoCheckResult
tags: [method private]
```

```Go
func cgoCheckResult(val interface{})
```

cgoCheckResult is called to check the result parameter of an exported Go function. It panics if the result is or contains a Go pointer. 

### <a id="cgoCheckSliceCopy" href="#cgoCheckSliceCopy">func cgoCheckSliceCopy(typ *_type, dst, src unsafe.Pointer, n int)</a>

```
searchKey: runtime.cgoCheckSliceCopy
tags: [method private]
```

```Go
func cgoCheckSliceCopy(typ *_type, dst, src unsafe.Pointer, n int)
```

cgoCheckSliceCopy is called when copying n elements of a slice. src and dst are pointers to the first element of the slice. typ is the element type of the slice. It throws if the program is copying slice elements that contain Go pointers into non-Go memory. 

### <a id="cgoCheckTypedBlock" href="#cgoCheckTypedBlock">func cgoCheckTypedBlock(typ *_type, src unsafe.Pointer, off, size uintptr)</a>

```
searchKey: runtime.cgoCheckTypedBlock
tags: [method private]
```

```Go
func cgoCheckTypedBlock(typ *_type, src unsafe.Pointer, off, size uintptr)
```

cgoCheckTypedBlock checks the block of memory at src, for up to size bytes, and throws if it finds a Go pointer. The type of the memory is typ, and src is off bytes into that type. 

### <a id="cgoCheckUnknownPointer" href="#cgoCheckUnknownPointer">func cgoCheckUnknownPointer(p unsafe.Pointer, msg string) (base, i uintptr)</a>

```
searchKey: runtime.cgoCheckUnknownPointer
tags: [method private]
```

```Go
func cgoCheckUnknownPointer(p unsafe.Pointer, msg string) (base, i uintptr)
```

cgoCheckUnknownPointer is called for an arbitrary pointer into Go memory. It checks whether that Go memory contains any other pointer into Go memory. If it does, we panic. The return values are unused but useful to see in panic tracebacks. 

### <a id="cgoCheckUsingType" href="#cgoCheckUsingType">func cgoCheckUsingType(typ *_type, src unsafe.Pointer, off, size uintptr)</a>

```
searchKey: runtime.cgoCheckUsingType
tags: [method private]
```

```Go
func cgoCheckUsingType(typ *_type, src unsafe.Pointer, off, size uintptr)
```

cgoCheckUsingType is like cgoCheckTypedBlock, but is a last ditch fall back to look for pointers in src using the type information. We only use this when looking at a value on the stack when the type uses a GC program, because otherwise it's more efficient to use the GC bits. This is called on the system stack. 

### <a id="cgoCheckWriteBarrier" href="#cgoCheckWriteBarrier">func cgoCheckWriteBarrier(dst *uintptr, src uintptr)</a>

```
searchKey: runtime.cgoCheckWriteBarrier
tags: [method private]
```

```Go
func cgoCheckWriteBarrier(dst *uintptr, src uintptr)
```

cgoCheckWriteBarrier is called whenever a pointer is stored into memory. It throws if the program is storing a Go pointer into non-Go memory. 

This is called from the write barrier, so its entire call tree must be nosplit. 

### <a id="cgoContextPCs" href="#cgoContextPCs">func cgoContextPCs(ctxt uintptr, buf []uintptr)</a>

```
searchKey: runtime.cgoContextPCs
tags: [method private]
```

```Go
func cgoContextPCs(ctxt uintptr, buf []uintptr)
```

cgoContextPCs gets the PC values from a cgo traceback. 

### <a id="cgoInRange" href="#cgoInRange">func cgoInRange(p unsafe.Pointer, start, end uintptr) bool</a>

```
searchKey: runtime.cgoInRange
tags: [method private]
```

```Go
func cgoInRange(p unsafe.Pointer, start, end uintptr) bool
```

cgoInRange reports whether p is between start and end. 

### <a id="cgoIsGoPointer" href="#cgoIsGoPointer">func cgoIsGoPointer(p unsafe.Pointer) bool</a>

```
searchKey: runtime.cgoIsGoPointer
tags: [method private]
```

```Go
func cgoIsGoPointer(p unsafe.Pointer) bool
```

cgoIsGoPointer reports whether the pointer is a Go pointer--a pointer to Go memory. We only care about Go memory that might contain pointers. 

### <a id="cgoSigtramp" href="#cgoSigtramp">func cgoSigtramp()</a>

```
searchKey: runtime.cgoSigtramp
tags: [function private]
```

```Go
func cgoSigtramp()
```

### <a id="cgoUse" href="#cgoUse">func cgoUse(interface{})</a>

```
searchKey: runtime.cgoUse
tags: [method private]
```

```Go
func cgoUse(interface{})
```

cgoUse is called by cgo-generated code (using go:linkname to get at an unexported name). The calls serve two purposes: 1) they are opaque to escape analysis, so the argument is considered to escape to the heap. 2) they keep the argument alive until the call site; the call is emitted after the end of the (presumed) use of the argument by C. cgoUse should not actually be called (see cgoAlwaysFalse). 

### <a id="cgocall" href="#cgocall">func cgocall(fn, arg unsafe.Pointer) int32</a>

```
searchKey: runtime.cgocall
tags: [method private]
```

```Go
func cgocall(fn, arg unsafe.Pointer) int32
```

Call from Go to C. 

This must be nosplit because it's used for syscalls on some platforms. Syscalls may have untyped arguments on the stack, so it's not safe to grow or scan the stack. 

### <a id="cgocallback" href="#cgocallback">func cgocallback(fn, frame, ctxt uintptr)</a>

```
searchKey: runtime.cgocallback
tags: [method private]
```

```Go
func cgocallback(fn, frame, ctxt uintptr)
```

Not all cgocallback frames are actually cgocallback, so not all have these arguments. Mark them uintptr so that the GC does not misinterpret memory when the arguments are not present. cgocallback is not called from Go, only from crosscall2. This in turn calls cgocallbackg, which is where we'll find pointer-declared arguments. 

### <a id="cgocallbackg" href="#cgocallbackg">func cgocallbackg(fn, frame unsafe.Pointer, ctxt uintptr)</a>

```
searchKey: runtime.cgocallbackg
tags: [method private]
```

```Go
func cgocallbackg(fn, frame unsafe.Pointer, ctxt uintptr)
```

Call from C back to Go. fn must point to an ABIInternal Go entry-point. 

### <a id="cgocallbackg1" href="#cgocallbackg1">func cgocallbackg1(fn, frame unsafe.Pointer, ctxt uintptr)</a>

```
searchKey: runtime.cgocallbackg1
tags: [method private]
```

```Go
func cgocallbackg1(fn, frame unsafe.Pointer, ctxt uintptr)
```

### <a id="cgounimpl" href="#cgounimpl">func cgounimpl()</a>

```
searchKey: runtime.cgounimpl
tags: [function private]
```

```Go
func cgounimpl()
```

called from (incomplete) assembly 

### <a id="chanbuf" href="#chanbuf">func chanbuf(c *hchan, i uint) unsafe.Pointer</a>

```
searchKey: runtime.chanbuf
tags: [method private]
```

```Go
func chanbuf(c *hchan, i uint) unsafe.Pointer
```

chanbuf(c, i) is pointer to the i'th slot in the buffer. 

### <a id="chanparkcommit" href="#chanparkcommit">func chanparkcommit(gp *g, chanLock unsafe.Pointer) bool</a>

```
searchKey: runtime.chanparkcommit
tags: [method private]
```

```Go
func chanparkcommit(gp *g, chanLock unsafe.Pointer) bool
```

### <a id="chanrecv" href="#chanrecv">func chanrecv(c *hchan, ep unsafe.Pointer, block bool) (selected, received bool)</a>

```
searchKey: runtime.chanrecv
tags: [method private]
```

```Go
func chanrecv(c *hchan, ep unsafe.Pointer, block bool) (selected, received bool)
```

chanrecv receives on channel c and writes the received data to ep. ep may be nil, in which case received data is ignored. If block == false and no elements are available, returns (false, false). Otherwise, if c is closed, zeros *ep and returns (true, false). Otherwise, fills in *ep with an element and returns (true, true). A non-nil ep must point to the heap or the caller's stack. 

### <a id="chanrecv1" href="#chanrecv1">func chanrecv1(c *hchan, elem unsafe.Pointer)</a>

```
searchKey: runtime.chanrecv1
tags: [method private]
```

```Go
func chanrecv1(c *hchan, elem unsafe.Pointer)
```

entry points for <- c from compiled code 

### <a id="chanrecv2" href="#chanrecv2">func chanrecv2(c *hchan, elem unsafe.Pointer) (received bool)</a>

```
searchKey: runtime.chanrecv2
tags: [method private]
```

```Go
func chanrecv2(c *hchan, elem unsafe.Pointer) (received bool)
```

### <a id="chansend" href="#chansend">func chansend(c *hchan, ep unsafe.Pointer, block bool, callerpc uintptr) bool</a>

```
searchKey: runtime.chansend
tags: [method private]
```

```Go
func chansend(c *hchan, ep unsafe.Pointer, block bool, callerpc uintptr) bool
```

* generic single channel send/recv * If block is not nil, * then the protocol will not * sleep but return if it could * not complete. * * sleep can wake up with g.param == nil * when a channel involved in the sleep has * been closed.  it is easiest to loop and re-run * the operation; we'll see that it's now closed. 

### <a id="chansend1" href="#chansend1">func chansend1(c *hchan, elem unsafe.Pointer)</a>

```
searchKey: runtime.chansend1
tags: [method private]
```

```Go
func chansend1(c *hchan, elem unsafe.Pointer)
```

entry point for c <- x from compiled code 

### <a id="check" href="#check">func check()</a>

```
searchKey: runtime.check
tags: [function private]
```

```Go
func check()
```

### <a id="checkASM" href="#checkASM">func checkASM() bool</a>

```
searchKey: runtime.checkASM
tags: [function private]
```

```Go
func checkASM() bool
```

checkASM reports whether assembly runtime checks have passed. 

### <a id="checkTimeouts" href="#checkTimeouts">func checkTimeouts()</a>

```
searchKey: runtime.checkTimeouts
tags: [function private]
```

```Go
func checkTimeouts()
```

### <a id="checkTimers" href="#checkTimers">func checkTimers(pp *p, now int64) (rnow, pollUntil int64, ran bool)</a>

```
searchKey: runtime.checkTimers
tags: [method private]
```

```Go
func checkTimers(pp *p, now int64) (rnow, pollUntil int64, ran bool)
```

checkTimers runs any timers for the P that are ready. If now is not 0 it is the current time. It returns the passed time or the current time if now was passed as 0. and the time when the next timer should run or 0 if there is no next timer, and reports whether it ran any timers. If the time when the next timer should run is not 0, it is always larger than the returned time. We pass now in and out to avoid extra calls of nanotime. 

### <a id="checkTimersNoP" href="#checkTimersNoP">func checkTimersNoP(allpSnapshot []*p, timerpMaskSnapshot pMask, pollUntil int64) int64</a>

```
searchKey: runtime.checkTimersNoP
tags: [method private]
```

```Go
func checkTimersNoP(allpSnapshot []*p, timerpMaskSnapshot pMask, pollUntil int64) int64
```

Check all Ps for a timer expiring sooner than pollUntil. 

Returns updated pollUntil value. 

### <a id="checkdead" href="#checkdead">func checkdead()</a>

```
searchKey: runtime.checkdead
tags: [function private]
```

```Go
func checkdead()
```

Check for deadlock situation. The check is based on number of running M's, if 0 -> deadlock. sched.lock must be held. 

### <a id="checkmcount" href="#checkmcount">func checkmcount()</a>

```
searchKey: runtime.checkmcount
tags: [function private]
```

```Go
func checkmcount()
```

sched.lock must be held. 

### <a id="checkptrAlignment" href="#checkptrAlignment">func checkptrAlignment(p unsafe.Pointer, elem *_type, n uintptr)</a>

```
searchKey: runtime.checkptrAlignment
tags: [method private]
```

```Go
func checkptrAlignment(p unsafe.Pointer, elem *_type, n uintptr)
```

### <a id="checkptrArithmetic" href="#checkptrArithmetic">func checkptrArithmetic(p unsafe.Pointer, originals []unsafe.Pointer)</a>

```
searchKey: runtime.checkptrArithmetic
tags: [method private]
```

```Go
func checkptrArithmetic(p unsafe.Pointer, originals []unsafe.Pointer)
```

### <a id="checkptrBase" href="#checkptrBase">func checkptrBase(p unsafe.Pointer) uintptr</a>

```
searchKey: runtime.checkptrBase
tags: [method private]
```

```Go
func checkptrBase(p unsafe.Pointer) uintptr
```

checkptrBase returns the base address for the allocation containing the address p. 

Importantly, if p1 and p2 point into the same variable, then checkptrBase(p1) == checkptrBase(p2). However, the converse/inverse is not necessarily true as allocations can have trailing padding, and multiple variables may be packed into a single allocation. 

### <a id="chunkBase" href="#chunkBase">func chunkBase(ci chunkIdx) uintptr</a>

```
searchKey: runtime.chunkBase
tags: [method private]
```

```Go
func chunkBase(ci chunkIdx) uintptr
```

chunkIndex returns the base address of the palloc chunk at index ci. 

### <a id="chunkPageIndex" href="#chunkPageIndex">func chunkPageIndex(p uintptr) uint</a>

```
searchKey: runtime.chunkPageIndex
tags: [method private]
```

```Go
func chunkPageIndex(p uintptr) uint
```

chunkPageIndex computes the index of the page that contains p, relative to the chunk which contains p. 

### <a id="cleantimers" href="#cleantimers">func cleantimers(pp *p)</a>

```
searchKey: runtime.cleantimers
tags: [method private]
```

```Go
func cleantimers(pp *p)
```

cleantimers cleans up the head of the timer queue. This speeds up programs that create and delete timers; leaving them in the heap slows down addtimer. Reports whether no timer problems were found. The caller must have locked the timers for pp. 

### <a id="clearDeletedTimers" href="#clearDeletedTimers">func clearDeletedTimers(pp *p)</a>

```
searchKey: runtime.clearDeletedTimers
tags: [method private]
```

```Go
func clearDeletedTimers(pp *p)
```

clearDeletedTimers removes all deleted timers from the P's timer heap. This is used to avoid clogging up the heap if the program starts a lot of long-running timers and then stops them. For example, this can happen via context.WithTimeout. 

This is the only function that walks through the entire timer heap, other than moveTimers which only runs when the world is stopped. 

The caller must have locked the timers for pp. 

### <a id="clearSignalHandlers" href="#clearSignalHandlers">func clearSignalHandlers()</a>

```
searchKey: runtime.clearSignalHandlers
tags: [function private]
```

```Go
func clearSignalHandlers()
```

clearSignalHandlers clears all signal handlers that are not ignored back to the default. This is called by the child after a fork, so that we can enable the signal mask for the exec without worrying about running a signal handler in the child. 

### <a id="clearpools" href="#clearpools">func clearpools()</a>

```
searchKey: runtime.clearpools
tags: [function private]
```

```Go
func clearpools()
```

### <a id="clobberfree" href="#clobberfree">func clobberfree(x unsafe.Pointer, size uintptr)</a>

```
searchKey: runtime.clobberfree
tags: [method private]
```

```Go
func clobberfree(x unsafe.Pointer, size uintptr)
```

clobberfree sets the memory content at x to bad content, for debugging purposes. 

### <a id="close_trampoline" href="#close_trampoline">func close_trampoline()</a>

```
searchKey: runtime.close_trampoline
tags: [function private]
```

```Go
func close_trampoline()
```

### <a id="closechan" href="#closechan">func closechan(c *hchan)</a>

```
searchKey: runtime.closechan
tags: [method private]
```

```Go
func closechan(c *hchan)
```

### <a id="closefd" href="#closefd">func closefd(fd int32) int32</a>

```
searchKey: runtime.closefd
tags: [method private]
```

```Go
func closefd(fd int32) int32
```

### <a id="closeonexec" href="#closeonexec">func closeonexec(fd int32)</a>

```
searchKey: runtime.closeonexec
tags: [method private]
```

```Go
func closeonexec(fd int32)
```

### <a id="complex128div" href="#complex128div">func complex128div(n complex128, m complex128) complex128</a>

```
searchKey: runtime.complex128div
tags: [method private]
```

```Go
func complex128div(n complex128, m complex128) complex128
```

### <a id="concatstring2" href="#concatstring2">func concatstring2(buf *tmpBuf, a0, a1 string) string</a>

```
searchKey: runtime.concatstring2
tags: [method private]
```

```Go
func concatstring2(buf *tmpBuf, a0, a1 string) string
```

### <a id="concatstring3" href="#concatstring3">func concatstring3(buf *tmpBuf, a0, a1, a2 string) string</a>

```
searchKey: runtime.concatstring3
tags: [method private]
```

```Go
func concatstring3(buf *tmpBuf, a0, a1, a2 string) string
```

### <a id="concatstring4" href="#concatstring4">func concatstring4(buf *tmpBuf, a0, a1, a2, a3 string) string</a>

```
searchKey: runtime.concatstring4
tags: [method private]
```

```Go
func concatstring4(buf *tmpBuf, a0, a1, a2, a3 string) string
```

### <a id="concatstring5" href="#concatstring5">func concatstring5(buf *tmpBuf, a0, a1, a2, a3, a4 string) string</a>

```
searchKey: runtime.concatstring5
tags: [method private]
```

```Go
func concatstring5(buf *tmpBuf, a0, a1, a2, a3, a4 string) string
```

### <a id="concatstrings" href="#concatstrings">func concatstrings(buf *tmpBuf, a []string) string</a>

```
searchKey: runtime.concatstrings
tags: [method private]
```

```Go
func concatstrings(buf *tmpBuf, a []string) string
```

concatstrings implements a Go string concatenation x+y+z+... The operands are passed in the slice a. If buf != nil, the compiler has determined that the result does not escape the calling function, so the string data can be stored in buf if small enough. 

### <a id="convT16" href="#convT16">func convT16(val uint16) (x unsafe.Pointer)</a>

```
searchKey: runtime.convT16
tags: [method private]
```

```Go
func convT16(val uint16) (x unsafe.Pointer)
```

### <a id="convT32" href="#convT32">func convT32(val uint32) (x unsafe.Pointer)</a>

```
searchKey: runtime.convT32
tags: [method private]
```

```Go
func convT32(val uint32) (x unsafe.Pointer)
```

### <a id="convT64" href="#convT64">func convT64(val uint64) (x unsafe.Pointer)</a>

```
searchKey: runtime.convT64
tags: [method private]
```

```Go
func convT64(val uint64) (x unsafe.Pointer)
```

### <a id="convTslice" href="#convTslice">func convTslice(val []byte) (x unsafe.Pointer)</a>

```
searchKey: runtime.convTslice
tags: [method private]
```

```Go
func convTslice(val []byte) (x unsafe.Pointer)
```

### <a id="convTstring" href="#convTstring">func convTstring(val string) (x unsafe.Pointer)</a>

```
searchKey: runtime.convTstring
tags: [method private]
```

```Go
func convTstring(val string) (x unsafe.Pointer)
```

### <a id="copysign" href="#copysign">func copysign(x, y float64) float64</a>

```
searchKey: runtime.copysign
tags: [method private]
```

```Go
func copysign(x, y float64) float64
```

copysign returns a value with the magnitude of x and the sign of y. 

### <a id="copystack" href="#copystack">func copystack(gp *g, newsize uintptr)</a>

```
searchKey: runtime.copystack
tags: [method private]
```

```Go
func copystack(gp *g, newsize uintptr)
```

Copies gp's stack to a new stack of a different size. Caller must have changed gp status to Gcopystack. 

### <a id="countSub" href="#countSub">func countSub(x, y uint32) int</a>

```
searchKey: runtime.countSub
tags: [method private]
```

```Go
func countSub(x, y uint32) int
```

countSub subtracts two counts obtained from profIndex.dataCount or profIndex.tagCount, assuming that they are no more than 2^29 apart (guaranteed since they are never more than len(data) or len(tags) apart, respectively). tagCount wraps at 2^30, while dataCount wraps at 2^32. This function works for both. 

### <a id="countrunes" href="#countrunes">func countrunes(s string) int</a>

```
searchKey: runtime.countrunes
tags: [method private]
```

```Go
func countrunes(s string) int
```

countrunes returns the number of runes in s. 

### <a id="cpuinit" href="#cpuinit">func cpuinit()</a>

```
searchKey: runtime.cpuinit
tags: [function private]
```

```Go
func cpuinit()
```

cpuinit extracts the environment variable GODEBUG from the environment on Unix-like operating systems and calls internal/cpu.Initialize. 

### <a id="cputicks" href="#cputicks">func cputicks() int64</a>

```
searchKey: runtime.cputicks
tags: [function private]
```

```Go
func cputicks() int64
```

careful: cputicks is not guaranteed to be monotonic! In particular, we have noticed drift between cpus on certain os/arch combinations. See issue 8976. 

### <a id="crash" href="#crash">func crash()</a>

```
searchKey: runtime.crash
tags: [function private]
```

```Go
func crash()
```

### <a id="createfing" href="#createfing">func createfing()</a>

```
searchKey: runtime.createfing
tags: [function private]
```

```Go
func createfing()
```

### <a id="crypto_x509_syscall" href="#crypto_x509_syscall">func crypto_x509_syscall(fn, a1, a2, a3, a4, a5, a6 uintptr) (r1 uintptr)</a>

```
searchKey: runtime.crypto_x509_syscall
tags: [method private]
```

```Go
func crypto_x509_syscall(fn, a1, a2, a3, a4, a5, a6 uintptr) (r1 uintptr)
```

### <a id="cstring" href="#cstring">func cstring(s string) unsafe.Pointer</a>

```
searchKey: runtime.cstring
tags: [method private]
```

```Go
func cstring(s string) unsafe.Pointer
```

### <a id="debugCallCheck" href="#debugCallCheck">func debugCallCheck(pc uintptr) string</a>

```
searchKey: runtime.debugCallCheck
tags: [method private]
```

```Go
func debugCallCheck(pc uintptr) string
```

debugCallCheck checks whether it is safe to inject a debugger function call with return PC pc. If not, it returns a string explaining why. 

### <a id="debugCallPanicked" href="#debugCallPanicked">func debugCallPanicked(val interface{})</a>

```
searchKey: runtime.debugCallPanicked
tags: [method private]
```

```Go
func debugCallPanicked(val interface{})
```

### <a id="debugCallV2" href="#debugCallV2">func debugCallV2()</a>

```
searchKey: runtime.debugCallV2
tags: [function private]
```

```Go
func debugCallV2()
```

### <a id="debugCallWrap" href="#debugCallWrap">func debugCallWrap(dispatch uintptr)</a>

```
searchKey: runtime.debugCallWrap
tags: [method private]
```

```Go
func debugCallWrap(dispatch uintptr)
```

debugCallWrap starts a new goroutine to run a debug call and blocks the calling goroutine. On the goroutine, it prepares to recover panics from the debug call, and then calls the call dispatching function at PC dispatch. 

This must be deeply nosplit because there are untyped values on the stack from debugCallV2. 

### <a id="debugCallWrap1" href="#debugCallWrap1">func debugCallWrap1()</a>

```
searchKey: runtime.debugCallWrap1
tags: [function private]
```

```Go
func debugCallWrap1()
```

debugCallWrap1 is the continuation of debugCallWrap on the callee goroutine. 

### <a id="debugCallWrap2" href="#debugCallWrap2">func debugCallWrap2(dispatch uintptr)</a>

```
searchKey: runtime.debugCallWrap2
tags: [method private]
```

```Go
func debugCallWrap2(dispatch uintptr)
```

### <a id="debug_modinfo" href="#debug_modinfo">func debug_modinfo() string</a>

```
searchKey: runtime.debug_modinfo
tags: [function private]
```

```Go
func debug_modinfo() string
```

### <a id="decoderune" href="#decoderune">func decoderune(s string, k int) (r rune, pos int)</a>

```
searchKey: runtime.decoderune
tags: [method private]
```

```Go
func decoderune(s string, k int) (r rune, pos int)
```

decoderune returns the non-ASCII rune at the start of s[k:] and the index after the rune in s. 

decoderune assumes that caller has checked that the to be decoded rune is a non-ASCII rune. 

If the string appears to be incomplete or decoding problems are encountered (runeerror, k + 1) is returned to ensure progress when decoderune is used to iterate over a string. 

### <a id="deductSweepCredit" href="#deductSweepCredit">func deductSweepCredit(spanBytes uintptr, callerSweepPages uintptr)</a>

```
searchKey: runtime.deductSweepCredit
tags: [method private]
```

```Go
func deductSweepCredit(spanBytes uintptr, callerSweepPages uintptr)
```

deductSweepCredit deducts sweep credit for allocating a span of size spanBytes. This must be performed *before* the span is allocated to ensure the system has enough credit. If necessary, it performs sweeping to prevent going in to debt. If the caller will also sweep pages (e.g., for a large allocation), it can pass a non-zero callerSweepPages to leave that many pages unswept. 

deductSweepCredit makes a worst-case assumption that all spanBytes bytes of the ultimately allocated span will be available for object allocation. 

deductSweepCredit is the core of the "proportional sweep" system. It uses statistics gathered by the garbage collector to perform enough sweeping so that all pages are swept during the concurrent sweep phase between GC cycles. 

mheap_ must NOT be locked. 

### <a id="defaultMemProfileRate" href="#defaultMemProfileRate">func defaultMemProfileRate(v int) int</a>

```
searchKey: runtime.defaultMemProfileRate
tags: [method private]
```

```Go
func defaultMemProfileRate(v int) int
```

defaultMemProfileRate returns 0 if disableMemoryProfiling is set. It exists primarily for the godoc rendering of MemProfileRate above. 

### <a id="deferArgs" href="#deferArgs">func deferArgs(d *_defer) unsafe.Pointer</a>

```
searchKey: runtime.deferArgs
tags: [method private]
```

```Go
func deferArgs(d *_defer) unsafe.Pointer
```

The arguments associated with a deferred call are stored immediately after the _defer header in memory. 

### <a id="deferCallSave" href="#deferCallSave">func deferCallSave(p *_panic, fn func())</a>

```
searchKey: runtime.deferCallSave
tags: [method private]
```

```Go
func deferCallSave(p *_panic, fn func())
```

deferCallSave calls fn() after saving the caller's pc and sp in the panic record. This allows the runtime to return to the Goexit defer processing loop, in the unusual case where the Goexit may be bypassed by a successful recover. 

This is marked as a wrapper by the compiler so it doesn't appear in tracebacks. 

### <a id="deferFunc" href="#deferFunc">func deferFunc(d *_defer) func()</a>

```
searchKey: runtime.deferFunc
tags: [method private]
```

```Go
func deferFunc(d *_defer) func()
```

deferFunc returns d's deferred function. This is temporary while we support both modes of GOEXPERIMENT=regabidefer. Once we commit to that experiment, we should change the type of d.fn. 

### <a id="deferclass" href="#deferclass">func deferclass(siz uintptr) uintptr</a>

```
searchKey: runtime.deferclass
tags: [method private]
```

```Go
func deferclass(siz uintptr) uintptr
```

defer size class for arg size sz 

### <a id="deferproc" href="#deferproc">func deferproc(siz int32, fn *funcval)</a>

```
searchKey: runtime.deferproc
tags: [method private]
```

```Go
func deferproc(siz int32, fn *funcval)
```

Create a new deferred function fn with siz bytes of arguments. The compiler turns a defer statement into a call to this. 

### <a id="deferprocStack" href="#deferprocStack">func deferprocStack(d *_defer)</a>

```
searchKey: runtime.deferprocStack
tags: [method private]
```

```Go
func deferprocStack(d *_defer)
```

deferprocStack queues a new deferred function with a defer record on the stack. The defer record must have its siz and fn fields initialized. All other fields can contain junk. The defer record must be immediately followed in memory by the arguments of the defer. Nosplit because the arguments on the stack won't be scanned until the defer record is spliced into the gp._defer list. 

### <a id="deferreturn" href="#deferreturn">func deferreturn()</a>

```
searchKey: runtime.deferreturn
tags: [function private]
```

```Go
func deferreturn()
```

Run a deferred function if there is one. The compiler inserts a call to this at the end of any function which calls defer. If there is a deferred function, this will call runtime·jmpdefer, which will jump to the deferred function such that it appears to have been called by the caller of deferreturn at the point just before deferreturn was called. The effect is that deferreturn is called again and again until there are no more deferred functions. 

Declared as nosplit, because the function should not be preempted once we start modifying the caller's frame in order to reuse the frame to call the deferred function. 

### <a id="deltimer" href="#deltimer">func deltimer(t *timer) bool</a>

```
searchKey: runtime.deltimer
tags: [method private]
```

```Go
func deltimer(t *timer) bool
```

deltimer deletes the timer t. It may be on some other P, so we can't actually remove it from the timers heap. We can only mark it as deleted. It will be removed in due course by the P whose heap it is on. Reports whether the timer was removed before it was run. 

### <a id="dematerializeGCProg" href="#dematerializeGCProg">func dematerializeGCProg(s *mspan)</a>

```
searchKey: runtime.dematerializeGCProg
tags: [method private]
```

```Go
func dematerializeGCProg(s *mspan)
```

### <a id="dieFromSignal" href="#dieFromSignal">func dieFromSignal(sig uint32)</a>

```
searchKey: runtime.dieFromSignal
tags: [method private]
```

```Go
func dieFromSignal(sig uint32)
```

dieFromSignal kills the program with a signal. This provides the expected exit status for the shell. This is only called with fatal signals expected to kill the process. 

### <a id="divRoundUp" href="#divRoundUp">func divRoundUp(n, a uintptr) uintptr</a>

```
searchKey: runtime.divRoundUp
tags: [method private]
```

```Go
func divRoundUp(n, a uintptr) uintptr
```

divRoundUp returns ceil(n / a). 

### <a id="divlu" href="#divlu">func divlu(u1, u0, v uint64) (q, r uint64)</a>

```
searchKey: runtime.divlu
tags: [method private]
```

```Go
func divlu(u1, u0, v uint64) (q, r uint64)
```

128/64 -> 64 quotient, 64 remainder. adapted from hacker's delight 

### <a id="doInit" href="#doInit">func doInit(t *initTask)</a>

```
searchKey: runtime.doInit
tags: [method private]
```

```Go
func doInit(t *initTask)
```

### <a id="doSigPreempt" href="#doSigPreempt">func doSigPreempt(gp *g, ctxt *sigctxt)</a>

```
searchKey: runtime.doSigPreempt
tags: [method private]
```

```Go
func doSigPreempt(gp *g, ctxt *sigctxt)
```

doSigPreempt handles a preemption signal on gp. 

### <a id="doaddtimer" href="#doaddtimer">func doaddtimer(pp *p, t *timer)</a>

```
searchKey: runtime.doaddtimer
tags: [method private]
```

```Go
func doaddtimer(pp *p, t *timer)
```

doaddtimer adds t to the current P's heap. The caller must have locked the timers for pp. 

### <a id="dodeltimer" href="#dodeltimer">func dodeltimer(pp *p, i int)</a>

```
searchKey: runtime.dodeltimer
tags: [method private]
```

```Go
func dodeltimer(pp *p, i int)
```

dodeltimer removes timer i from the current P's heap. We are locked on the P when this is called. It reports whether it saw no problems due to races. The caller must have locked the timers for pp. 

### <a id="dodeltimer0" href="#dodeltimer0">func dodeltimer0(pp *p)</a>

```
searchKey: runtime.dodeltimer0
tags: [method private]
```

```Go
func dodeltimer0(pp *p)
```

dodeltimer0 removes timer 0 from the current P's heap. We are locked on the P when this is called. It reports whether it saw no problems due to races. The caller must have locked the timers for pp. 

### <a id="dolockOSThread" href="#dolockOSThread">func dolockOSThread()</a>

```
searchKey: runtime.dolockOSThread
tags: [function private]
```

```Go
func dolockOSThread()
```

dolockOSThread is called by LockOSThread and lockOSThread below after they modify m.locked. Do not allow preemption during this call, or else the m might be different in this function than in the caller. 

### <a id="dopanic_m" href="#dopanic_m">func dopanic_m(gp *g, pc, sp uintptr) bool</a>

```
searchKey: runtime.dopanic_m
tags: [method private]
```

```Go
func dopanic_m(gp *g, pc, sp uintptr) bool
```

### <a id="dounlockOSThread" href="#dounlockOSThread">func dounlockOSThread()</a>

```
searchKey: runtime.dounlockOSThread
tags: [function private]
```

```Go
func dounlockOSThread()
```

dounlockOSThread is called by UnlockOSThread and unlockOSThread below after they update m->locked. Do not allow preemption during this call, or else the m might be in different in this function than in the caller. 

### <a id="dropg" href="#dropg">func dropg()</a>

```
searchKey: runtime.dropg
tags: [function private]
```

```Go
func dropg()
```

dropg removes the association between m and the current goroutine m->curg (gp for short). Typically a caller sets gp's status away from Grunning and then immediately calls dropg to finish the job. The caller is also responsible for arranging that gp will be restarted using ready at an appropriate time. After calling dropg and arranging for gp to be readied later, the caller can do other work but eventually should call schedule to restart the scheduling of goroutines on this m. 

### <a id="dropm" href="#dropm">func dropm()</a>

```
searchKey: runtime.dropm
tags: [function private]
```

```Go
func dropm()
```

dropm is called when a cgo callback has called needm but is now done with the callback and returning back into the non-Go thread. It puts the current m back onto the extra list. 

The main expense here is the call to signalstack to release the m's signal stack, and then the call to needm on the next callback from this thread. It is tempting to try to save the m for next time, which would eliminate both these costs, but there might not be a next time: the current thread (which Go does not control) might exit. If we saved the m for that thread, there would be an m leak each time such a thread exited. Instead, we acquire and release an m on each call. These should typically not be scheduling operations, just a few atomics, so the cost should be small. 

TODO(rsc): An alternative would be to allocate a dummy pthread per-thread variable using pthread_key_create. Unlike the pthread keys we already use on OS X, this dummy key would never be read by Go code. It would exist only so that we could register at thread-exit-time destructor. That destructor would put the m back onto the extra list. This is purely a performance optimization. The current version, in which dropm happens on each cgo call, is still correct too. We may have to keep the current version on systems with cgo but without pthreads, like Windows. 

### <a id="duffcopy" href="#duffcopy">func duffcopy()</a>

```
searchKey: runtime.duffcopy
tags: [function private]
```

```Go
func duffcopy()
```

### <a id="duffzero" href="#duffzero">func duffzero()</a>

```
searchKey: runtime.duffzero
tags: [function private]
```

```Go
func duffzero()
```

### <a id="dumpGCProg" href="#dumpGCProg">func dumpGCProg(p *byte)</a>

```
searchKey: runtime.dumpGCProg
tags: [method private]
```

```Go
func dumpGCProg(p *byte)
```

### <a id="dumpbool" href="#dumpbool">func dumpbool(b bool)</a>

```
searchKey: runtime.dumpbool
tags: [method private]
```

```Go
func dumpbool(b bool)
```

### <a id="dumpbv" href="#dumpbv">func dumpbv(cbv *bitvector, offset uintptr)</a>

```
searchKey: runtime.dumpbv
tags: [method private]
```

```Go
func dumpbv(cbv *bitvector, offset uintptr)
```

dump kinds & offsets of interesting fields in bv 

### <a id="dumpfields" href="#dumpfields">func dumpfields(bv bitvector)</a>

```
searchKey: runtime.dumpfields
tags: [method private]
```

```Go
func dumpfields(bv bitvector)
```

dumpint() the kind & offset of each field in an object. 

### <a id="dumpfinalizer" href="#dumpfinalizer">func dumpfinalizer(obj unsafe.Pointer, fn *funcval, fint *_type, ot *ptrtype)</a>

```
searchKey: runtime.dumpfinalizer
tags: [method private]
```

```Go
func dumpfinalizer(obj unsafe.Pointer, fn *funcval, fint *_type, ot *ptrtype)
```

### <a id="dumpframe" href="#dumpframe">func dumpframe(s *stkframe, arg unsafe.Pointer) bool</a>

```
searchKey: runtime.dumpframe
tags: [method private]
```

```Go
func dumpframe(s *stkframe, arg unsafe.Pointer) bool
```

### <a id="dumpgoroutine" href="#dumpgoroutine">func dumpgoroutine(gp *g)</a>

```
searchKey: runtime.dumpgoroutine
tags: [method private]
```

```Go
func dumpgoroutine(gp *g)
```

### <a id="dumpgs" href="#dumpgs">func dumpgs()</a>

```
searchKey: runtime.dumpgs
tags: [function private]
```

```Go
func dumpgs()
```

### <a id="dumpgstatus" href="#dumpgstatus">func dumpgstatus(gp *g)</a>

```
searchKey: runtime.dumpgstatus
tags: [method private]
```

```Go
func dumpgstatus(gp *g)
```

### <a id="dumpint" href="#dumpint">func dumpint(v uint64)</a>

```
searchKey: runtime.dumpint
tags: [method private]
```

```Go
func dumpint(v uint64)
```

dump a uint64 in a varint format parseable by encoding/binary 

### <a id="dumpitabs" href="#dumpitabs">func dumpitabs()</a>

```
searchKey: runtime.dumpitabs
tags: [function private]
```

```Go
func dumpitabs()
```

### <a id="dumpmemprof" href="#dumpmemprof">func dumpmemprof()</a>

```
searchKey: runtime.dumpmemprof
tags: [function private]
```

```Go
func dumpmemprof()
```

### <a id="dumpmemprof_callback" href="#dumpmemprof_callback">func dumpmemprof_callback(b *bucket, nstk uintptr, pstk *uintptr, size, allocs, frees uintptr)</a>

```
searchKey: runtime.dumpmemprof_callback
tags: [method private]
```

```Go
func dumpmemprof_callback(b *bucket, nstk uintptr, pstk *uintptr, size, allocs, frees uintptr)
```

### <a id="dumpmemrange" href="#dumpmemrange">func dumpmemrange(data unsafe.Pointer, len uintptr)</a>

```
searchKey: runtime.dumpmemrange
tags: [method private]
```

```Go
func dumpmemrange(data unsafe.Pointer, len uintptr)
```

dump varint uint64 length followed by memory contents 

### <a id="dumpmemstats" href="#dumpmemstats">func dumpmemstats(m *MemStats)</a>

```
searchKey: runtime.dumpmemstats
tags: [method private]
```

```Go
func dumpmemstats(m *MemStats)
```

### <a id="dumpms" href="#dumpms">func dumpms()</a>

```
searchKey: runtime.dumpms
tags: [function private]
```

```Go
func dumpms()
```

### <a id="dumpobj" href="#dumpobj">func dumpobj(obj unsafe.Pointer, size uintptr, bv bitvector)</a>

```
searchKey: runtime.dumpobj
tags: [method private]
```

```Go
func dumpobj(obj unsafe.Pointer, size uintptr, bv bitvector)
```

dump an object 

### <a id="dumpobjs" href="#dumpobjs">func dumpobjs()</a>

```
searchKey: runtime.dumpobjs
tags: [function private]
```

```Go
func dumpobjs()
```

### <a id="dumpotherroot" href="#dumpotherroot">func dumpotherroot(description string, to unsafe.Pointer)</a>

```
searchKey: runtime.dumpotherroot
tags: [method private]
```

```Go
func dumpotherroot(description string, to unsafe.Pointer)
```

### <a id="dumpparams" href="#dumpparams">func dumpparams()</a>

```
searchKey: runtime.dumpparams
tags: [function private]
```

```Go
func dumpparams()
```

### <a id="dumpregs" href="#dumpregs">func dumpregs(c *sigctxt)</a>

```
searchKey: runtime.dumpregs
tags: [method private]
```

```Go
func dumpregs(c *sigctxt)
```

### <a id="dumproots" href="#dumproots">func dumproots()</a>

```
searchKey: runtime.dumproots
tags: [function private]
```

```Go
func dumproots()
```

### <a id="dumpslice" href="#dumpslice">func dumpslice(b []byte)</a>

```
searchKey: runtime.dumpslice
tags: [method private]
```

```Go
func dumpslice(b []byte)
```

### <a id="dumpstr" href="#dumpstr">func dumpstr(s string)</a>

```
searchKey: runtime.dumpstr
tags: [method private]
```

```Go
func dumpstr(s string)
```

### <a id="dumptype" href="#dumptype">func dumptype(t *_type)</a>

```
searchKey: runtime.dumptype
tags: [method private]
```

```Go
func dumptype(t *_type)
```

dump information for a type 

### <a id="dwrite" href="#dwrite">func dwrite(data unsafe.Pointer, len uintptr)</a>

```
searchKey: runtime.dwrite
tags: [method private]
```

```Go
func dwrite(data unsafe.Pointer, len uintptr)
```

### <a id="dwritebyte" href="#dwritebyte">func dwritebyte(b byte)</a>

```
searchKey: runtime.dwritebyte
tags: [method private]
```

```Go
func dwritebyte(b byte)
```

### <a id="efaceHash" href="#efaceHash">func efaceHash(i interface{}, seed uintptr) uintptr</a>

```
searchKey: runtime.efaceHash
tags: [method private]
```

```Go
func efaceHash(i interface{}, seed uintptr) uintptr
```

### <a id="efaceeq" href="#efaceeq">func efaceeq(t *_type, x, y unsafe.Pointer) bool</a>

```
searchKey: runtime.efaceeq
tags: [method private]
```

```Go
func efaceeq(t *_type, x, y unsafe.Pointer) bool
```

### <a id="elideWrapperCalling" href="#elideWrapperCalling">func elideWrapperCalling(id funcID) bool</a>

```
searchKey: runtime.elideWrapperCalling
tags: [method private]
```

```Go
func elideWrapperCalling(id funcID) bool
```

elideWrapperCalling reports whether a wrapper function that called function id should be elided from stack traces. 

### <a id="empty" href="#empty">func empty(c *hchan) bool</a>

```
searchKey: runtime.empty
tags: [method private]
```

```Go
func empty(c *hchan) bool
```

empty reports whether a read from c would block (that is, the channel is empty).  It uses a single atomic read of mutable state. 

### <a id="encoderune" href="#encoderune">func encoderune(p []byte, r rune) int</a>

```
searchKey: runtime.encoderune
tags: [method private]
```

```Go
func encoderune(p []byte, r rune) int
```

encoderune writes into p (which must be large enough) the UTF-8 encoding of the rune. It returns the number of bytes written. 

### <a id="endCheckmarks" href="#endCheckmarks">func endCheckmarks()</a>

```
searchKey: runtime.endCheckmarks
tags: [function private]
```

```Go
func endCheckmarks()
```

endCheckmarks ends the checkmarks phase. 

### <a id="ensureSigM" href="#ensureSigM">func ensureSigM()</a>

```
searchKey: runtime.ensureSigM
tags: [function private]
```

```Go
func ensureSigM()
```

ensureSigM starts one global, sleeping thread to make sure at least one thread is available to catch signals enabled for os/signal. 

### <a id="entersyscall" href="#entersyscall">func entersyscall()</a>

```
searchKey: runtime.entersyscall
tags: [function private]
```

```Go
func entersyscall()
```

Standard syscall entry used by the go syscall library and normal cgo calls. 

This is exported via linkname to assembly in the syscall package. 

### <a id="entersyscall_gcwait" href="#entersyscall_gcwait">func entersyscall_gcwait()</a>

```
searchKey: runtime.entersyscall_gcwait
tags: [function private]
```

```Go
func entersyscall_gcwait()
```

### <a id="entersyscall_sysmon" href="#entersyscall_sysmon">func entersyscall_sysmon()</a>

```
searchKey: runtime.entersyscall_sysmon
tags: [function private]
```

```Go
func entersyscall_sysmon()
```

### <a id="entersyscallblock" href="#entersyscallblock">func entersyscallblock()</a>

```
searchKey: runtime.entersyscallblock
tags: [function private]
```

```Go
func entersyscallblock()
```

The same as entersyscall(), but with a hint that the syscall is blocking. 

### <a id="entersyscallblock_handoff" href="#entersyscallblock_handoff">func entersyscallblock_handoff()</a>

```
searchKey: runtime.entersyscallblock_handoff
tags: [function private]
```

```Go
func entersyscallblock_handoff()
```

### <a id="envKeyEqual" href="#envKeyEqual">func envKeyEqual(a, b string) bool</a>

```
searchKey: runtime.envKeyEqual
tags: [method private]
```

```Go
func envKeyEqual(a, b string) bool
```

envKeyEqual reports whether a == b, with ASCII-only case insensitivity on Windows. The two strings must have the same length. 

### <a id="environ" href="#environ">func environ() []string</a>

```
searchKey: runtime.environ
tags: [function private]
```

```Go
func environ() []string
```

### <a id="eqslice" href="#eqslice">func eqslice(x, y []uintptr) bool</a>

```
searchKey: runtime.eqslice
tags: [method private]
```

```Go
func eqslice(x, y []uintptr) bool
```

### <a id="evacuate" href="#evacuate">func evacuate(t *maptype, h *hmap, oldbucket uintptr)</a>

```
searchKey: runtime.evacuate
tags: [method private]
```

```Go
func evacuate(t *maptype, h *hmap, oldbucket uintptr)
```

### <a id="evacuate_fast32" href="#evacuate_fast32">func evacuate_fast32(t *maptype, h *hmap, oldbucket uintptr)</a>

```
searchKey: runtime.evacuate_fast32
tags: [method private]
```

```Go
func evacuate_fast32(t *maptype, h *hmap, oldbucket uintptr)
```

### <a id="evacuate_fast64" href="#evacuate_fast64">func evacuate_fast64(t *maptype, h *hmap, oldbucket uintptr)</a>

```
searchKey: runtime.evacuate_fast64
tags: [method private]
```

```Go
func evacuate_fast64(t *maptype, h *hmap, oldbucket uintptr)
```

### <a id="evacuate_faststr" href="#evacuate_faststr">func evacuate_faststr(t *maptype, h *hmap, oldbucket uintptr)</a>

```
searchKey: runtime.evacuate_faststr
tags: [method private]
```

```Go
func evacuate_faststr(t *maptype, h *hmap, oldbucket uintptr)
```

### <a id="evacuated" href="#evacuated">func evacuated(b *bmap) bool</a>

```
searchKey: runtime.evacuated
tags: [method private]
```

```Go
func evacuated(b *bmap) bool
```

### <a id="execute" href="#execute">func execute(gp *g, inheritTime bool)</a>

```
searchKey: runtime.execute
tags: [method private]
```

```Go
func execute(gp *g, inheritTime bool)
```

Schedules gp to run on the current M. If inheritTime is true, gp inherits the remaining time in the current time slice. Otherwise, it starts a new time slice. Never returns. 

Write barriers are allowed because this is called immediately after acquiring a P in several places. 

### <a id="exit" href="#exit">func exit(code int32)</a>

```
searchKey: runtime.exit
tags: [method private]
```

```Go
func exit(code int32)
```

This is exported via linkname to assembly in runtime/cgo. 

### <a id="exitThread" href="#exitThread">func exitThread(wait *uint32)</a>

```
searchKey: runtime.exitThread
tags: [method private]
```

```Go
func exitThread(wait *uint32)
```

Not used on Darwin, but must be defined. 

### <a id="exit_trampoline" href="#exit_trampoline">func exit_trampoline()</a>

```
searchKey: runtime.exit_trampoline
tags: [function private]
```

```Go
func exit_trampoline()
```

### <a id="exitsyscall" href="#exitsyscall">func exitsyscall()</a>

```
searchKey: runtime.exitsyscall
tags: [function private]
```

```Go
func exitsyscall()
```

The goroutine g exited its system call. Arrange for it to run on a cpu again. This is called only from the go syscall library, not from the low-level system calls used by the runtime. 

Write barriers are not allowed because our P may have been stolen. 

This is exported via linkname to assembly in the syscall package. 

### <a id="exitsyscall0" href="#exitsyscall0">func exitsyscall0(gp *g)</a>

```
searchKey: runtime.exitsyscall0
tags: [method private]
```

```Go
func exitsyscall0(gp *g)
```

exitsyscall slow path on g0. Failed to acquire P, enqueue gp as runnable. 

Called via mcall, so gp is the calling g from this M. 

### <a id="exitsyscallfast" href="#exitsyscallfast">func exitsyscallfast(oldp *p) bool</a>

```
searchKey: runtime.exitsyscallfast
tags: [method private]
```

```Go
func exitsyscallfast(oldp *p) bool
```

### <a id="exitsyscallfast_pidle" href="#exitsyscallfast_pidle">func exitsyscallfast_pidle() bool</a>

```
searchKey: runtime.exitsyscallfast_pidle
tags: [function private]
```

```Go
func exitsyscallfast_pidle() bool
```

### <a id="exitsyscallfast_reacquired" href="#exitsyscallfast_reacquired">func exitsyscallfast_reacquired()</a>

```
searchKey: runtime.exitsyscallfast_reacquired
tags: [function private]
```

```Go
func exitsyscallfast_reacquired()
```

exitsyscallfast_reacquired is the exitsyscall path on which this G has successfully reacquired the P it was running on before the syscall. 

### <a id="expandCgoFrames" href="#expandCgoFrames">func expandCgoFrames(pc uintptr) []Frame</a>

```
searchKey: runtime.expandCgoFrames
tags: [method private]
```

```Go
func expandCgoFrames(pc uintptr) []Frame
```

expandCgoFrames expands frame information for pc, known to be a non-Go function, using the cgoSymbolizer hook. expandCgoFrames returns nil if pc could not be expanded. 

### <a id="extendRandom" href="#extendRandom">func extendRandom(r []byte, n int)</a>

```
searchKey: runtime.extendRandom
tags: [method private]
```

```Go
func extendRandom(r []byte, n int)
```

extendRandom extends the random numbers in r[:n] to the whole slice r. Treats n<0 as n==0. 

### <a id="f32equal" href="#f32equal">func f32equal(p, q unsafe.Pointer) bool</a>

```
searchKey: runtime.f32equal
tags: [method private]
```

```Go
func f32equal(p, q unsafe.Pointer) bool
```

### <a id="f32hash" href="#f32hash">func f32hash(p unsafe.Pointer, h uintptr) uintptr</a>

```
searchKey: runtime.f32hash
tags: [method private]
```

```Go
func f32hash(p unsafe.Pointer, h uintptr) uintptr
```

### <a id="f32to64" href="#f32to64">func f32to64(f uint32) uint64</a>

```
searchKey: runtime.f32to64
tags: [method private]
```

```Go
func f32to64(f uint32) uint64
```

### <a id="f32toint32" href="#f32toint32">func f32toint32(x uint32) int32</a>

```
searchKey: runtime.f32toint32
tags: [method private]
```

```Go
func f32toint32(x uint32) int32
```

### <a id="f32toint64" href="#f32toint64">func f32toint64(x uint32) int64</a>

```
searchKey: runtime.f32toint64
tags: [method private]
```

```Go
func f32toint64(x uint32) int64
```

### <a id="f32touint64" href="#f32touint64">func f32touint64(x float32) uint64</a>

```
searchKey: runtime.f32touint64
tags: [method private]
```

```Go
func f32touint64(x float32) uint64
```

### <a id="f64equal" href="#f64equal">func f64equal(p, q unsafe.Pointer) bool</a>

```
searchKey: runtime.f64equal
tags: [method private]
```

```Go
func f64equal(p, q unsafe.Pointer) bool
```

### <a id="f64hash" href="#f64hash">func f64hash(p unsafe.Pointer, h uintptr) uintptr</a>

```
searchKey: runtime.f64hash
tags: [method private]
```

```Go
func f64hash(p unsafe.Pointer, h uintptr) uintptr
```

### <a id="f64to32" href="#f64to32">func f64to32(f uint64) uint32</a>

```
searchKey: runtime.f64to32
tags: [method private]
```

```Go
func f64to32(f uint64) uint32
```

### <a id="f64toint" href="#f64toint">func f64toint(f uint64) (val int64, ok bool)</a>

```
searchKey: runtime.f64toint
tags: [method private]
```

```Go
func f64toint(f uint64) (val int64, ok bool)
```

### <a id="f64toint32" href="#f64toint32">func f64toint32(x uint64) int32</a>

```
searchKey: runtime.f64toint32
tags: [method private]
```

```Go
func f64toint32(x uint64) int32
```

### <a id="f64toint64" href="#f64toint64">func f64toint64(x uint64) int64</a>

```
searchKey: runtime.f64toint64
tags: [method private]
```

```Go
func f64toint64(x uint64) int64
```

### <a id="f64touint64" href="#f64touint64">func f64touint64(x float64) uint64</a>

```
searchKey: runtime.f64touint64
tags: [method private]
```

```Go
func f64touint64(x float64) uint64
```

### <a id="fadd32" href="#fadd32">func fadd32(x, y uint32) uint32</a>

```
searchKey: runtime.fadd32
tags: [method private]
```

```Go
func fadd32(x, y uint32) uint32
```

### <a id="fadd64" href="#fadd64">func fadd64(f, g uint64) uint64</a>

```
searchKey: runtime.fadd64
tags: [method private]
```

```Go
func fadd64(f, g uint64) uint64
```

### <a id="fastexprand" href="#fastexprand">func fastexprand(mean int) int32</a>

```
searchKey: runtime.fastexprand
tags: [method private]
```

```Go
func fastexprand(mean int) int32
```

fastexprand returns a random number from an exponential distribution with the specified mean. 

### <a id="fastlog2" href="#fastlog2">func fastlog2(x float64) float64</a>

```
searchKey: runtime.fastlog2
tags: [method private]
```

```Go
func fastlog2(x float64) float64
```

fastlog2 implements a fast approximation to the base 2 log of a float64. This is used to compute a geometric distribution for heap sampling, without introducing dependencies into package math. This uses a very rough approximation using the float64 exponent and the first 25 bits of the mantissa. The top 5 bits of the mantissa are used to load limits from a table of constants and the rest are used to scale linearly between them. 

### <a id="fastrand" href="#fastrand">func fastrand() uint32</a>

```
searchKey: runtime.fastrand
tags: [function private]
```

```Go
func fastrand() uint32
```

### <a id="fastrandinit" href="#fastrandinit">func fastrandinit()</a>

```
searchKey: runtime.fastrandinit
tags: [function private]
```

```Go
func fastrandinit()
```

### <a id="fastrandn" href="#fastrandn">func fastrandn(n uint32) uint32</a>

```
searchKey: runtime.fastrandn
tags: [method private]
```

```Go
func fastrandn(n uint32) uint32
```

### <a id="fatalpanic" href="#fatalpanic">func fatalpanic(msgs *_panic)</a>

```
searchKey: runtime.fatalpanic
tags: [method private]
```

```Go
func fatalpanic(msgs *_panic)
```

fatalpanic implements an unrecoverable panic. It is like fatalthrow, except that if msgs != nil, fatalpanic also prints panic messages and decrements runningPanicDefers once main is blocked from exiting. 

### <a id="fatalthrow" href="#fatalthrow">func fatalthrow()</a>

```
searchKey: runtime.fatalthrow
tags: [function private]
```

```Go
func fatalthrow()
```

fatalthrow implements an unrecoverable runtime throw. It freezes the system, prints stack traces starting from its caller, and terminates the process. 

### <a id="fcmp64" href="#fcmp64">func fcmp64(f, g uint64) (cmp int32, isnan bool)</a>

```
searchKey: runtime.fcmp64
tags: [method private]
```

```Go
func fcmp64(f, g uint64) (cmp int32, isnan bool)
```

### <a id="fcntl" href="#fcntl">func fcntl(fd, cmd, arg int32) int32</a>

```
searchKey: runtime.fcntl
tags: [method private]
```

```Go
func fcntl(fd, cmd, arg int32) int32
```

### <a id="fcntl_trampoline" href="#fcntl_trampoline">func fcntl_trampoline()</a>

```
searchKey: runtime.fcntl_trampoline
tags: [function private]
```

```Go
func fcntl_trampoline()
```

### <a id="fdiv32" href="#fdiv32">func fdiv32(x, y uint32) uint32</a>

```
searchKey: runtime.fdiv32
tags: [method private]
```

```Go
func fdiv32(x, y uint32) uint32
```

### <a id="fdiv64" href="#fdiv64">func fdiv64(f, g uint64) uint64</a>

```
searchKey: runtime.fdiv64
tags: [method private]
```

```Go
func fdiv64(f, g uint64) uint64
```

### <a id="feq32" href="#feq32">func feq32(x, y uint32) bool</a>

```
searchKey: runtime.feq32
tags: [method private]
```

```Go
func feq32(x, y uint32) bool
```

### <a id="feq64" href="#feq64">func feq64(x, y uint64) bool</a>

```
searchKey: runtime.feq64
tags: [method private]
```

```Go
func feq64(x, y uint64) bool
```

### <a id="fge32" href="#fge32">func fge32(x, y uint32) bool</a>

```
searchKey: runtime.fge32
tags: [method private]
```

```Go
func fge32(x, y uint32) bool
```

### <a id="fge64" href="#fge64">func fge64(x, y uint64) bool</a>

```
searchKey: runtime.fge64
tags: [method private]
```

```Go
func fge64(x, y uint64) bool
```

### <a id="fgt32" href="#fgt32">func fgt32(x, y uint32) bool</a>

```
searchKey: runtime.fgt32
tags: [method private]
```

```Go
func fgt32(x, y uint32) bool
```

### <a id="fgt64" href="#fgt64">func fgt64(x, y uint64) bool</a>

```
searchKey: runtime.fgt64
tags: [method private]
```

```Go
func fgt64(x, y uint64) bool
```

### <a id="fillAligned" href="#fillAligned">func fillAligned(x uint64, m uint) uint64</a>

```
searchKey: runtime.fillAligned
tags: [method private]
```

```Go
func fillAligned(x uint64, m uint) uint64
```

fillAligned returns x but with all zeroes in m-aligned groups of m bits set to 1 if any bit in the group is non-zero. 

For example, fillAligned(0x0100a3, 8) == 0xff00ff. 

Note that if m == 1, this is a no-op. 

m must be a power of 2 <= maxPagesPerPhysPage. 

### <a id="fillstack" href="#fillstack">func fillstack(stk stack, b byte)</a>

```
searchKey: runtime.fillstack
tags: [method private]
```

```Go
func fillstack(stk stack, b byte)
```

### <a id="findBitRange64" href="#findBitRange64">func findBitRange64(c uint64, n uint) uint</a>

```
searchKey: runtime.findBitRange64
tags: [method private]
```

```Go
func findBitRange64(c uint64, n uint) uint
```

findBitRange64 returns the bit index of the first set of n consecutive 1 bits. If no consecutive set of 1 bits of size n may be found in c, then it returns an integer >= 64. n must be > 0. 

### <a id="findnull" href="#findnull">func findnull(s *byte) int</a>

```
searchKey: runtime.findnull
tags: [method private]
```

```Go
func findnull(s *byte) int
```

### <a id="findnullw" href="#findnullw">func findnullw(s *uint16) int</a>

```
searchKey: runtime.findnullw
tags: [method private]
```

```Go
func findnullw(s *uint16) int
```

### <a id="findsghi" href="#findsghi">func findsghi(gp *g, stk stack) uintptr</a>

```
searchKey: runtime.findsghi
tags: [method private]
```

```Go
func findsghi(gp *g, stk stack) uintptr
```

### <a id="finishsweep_m" href="#finishsweep_m">func finishsweep_m()</a>

```
searchKey: runtime.finishsweep_m
tags: [function private]
```

```Go
func finishsweep_m()
```

finishsweep_m ensures that all spans are swept. 

The world must be stopped. This ensures there are no sweeps in progress. 

### <a id="finq_callback" href="#finq_callback">func finq_callback(fn *funcval, obj unsafe.Pointer, nret uintptr, fint *_type, ot *ptrtype)</a>

```
searchKey: runtime.finq_callback
tags: [method private]
```

```Go
func finq_callback(fn *funcval, obj unsafe.Pointer, nret uintptr, fint *_type, ot *ptrtype)
```

### <a id="fint32to32" href="#fint32to32">func fint32to32(x int32) uint32</a>

```
searchKey: runtime.fint32to32
tags: [method private]
```

```Go
func fint32to32(x int32) uint32
```

### <a id="fint32to64" href="#fint32to64">func fint32to64(x int32) uint64</a>

```
searchKey: runtime.fint32to64
tags: [method private]
```

```Go
func fint32to64(x int32) uint64
```

### <a id="fint64to32" href="#fint64to32">func fint64to32(x int64) uint32</a>

```
searchKey: runtime.fint64to32
tags: [method private]
```

```Go
func fint64to32(x int64) uint32
```

### <a id="fint64to64" href="#fint64to64">func fint64to64(x int64) uint64</a>

```
searchKey: runtime.fint64to64
tags: [method private]
```

```Go
func fint64to64(x int64) uint64
```

### <a id="fintto64" href="#fintto64">func fintto64(val int64) (f uint64)</a>

```
searchKey: runtime.fintto64
tags: [method private]
```

```Go
func fintto64(val int64) (f uint64)
```

### <a id="float64Inf" href="#float64Inf">func float64Inf() float64</a>

```
searchKey: runtime.float64Inf
tags: [function private]
```

```Go
func float64Inf() float64
```

### <a id="float64NegInf" href="#float64NegInf">func float64NegInf() float64</a>

```
searchKey: runtime.float64NegInf
tags: [function private]
```

```Go
func float64NegInf() float64
```

### <a id="float64bits" href="#float64bits">func float64bits(f float64) uint64</a>

```
searchKey: runtime.float64bits
tags: [method private]
```

```Go
func float64bits(f float64) uint64
```

Float64bits returns the IEEE 754 binary representation of f. 

### <a id="float64frombits" href="#float64frombits">func float64frombits(b uint64) float64</a>

```
searchKey: runtime.float64frombits
tags: [method private]
```

```Go
func float64frombits(b uint64) float64
```

Float64frombits returns the floating point number corresponding the IEEE 754 binary representation b. 

### <a id="flush" href="#flush">func flush()</a>

```
searchKey: runtime.flush
tags: [function private]
```

```Go
func flush()
```

### <a id="flushallmcaches" href="#flushallmcaches">func flushallmcaches()</a>

```
searchKey: runtime.flushallmcaches
tags: [function private]
```

```Go
func flushallmcaches()
```

flushallmcaches flushes the mcaches of all Ps. 

The world must be stopped. 

### <a id="flushmcache" href="#flushmcache">func flushmcache(i int)</a>

```
searchKey: runtime.flushmcache
tags: [method private]
```

```Go
func flushmcache(i int)
```

flushmcache flushes the mcache of allp[i]. 

The world must be stopped. 

### <a id="fmtNSAsMS" href="#fmtNSAsMS">func fmtNSAsMS(buf []byte, ns uint64) []byte</a>

```
searchKey: runtime.fmtNSAsMS
tags: [method private]
```

```Go
func fmtNSAsMS(buf []byte, ns uint64) []byte
```

fmtNSAsMS nicely formats ns nanoseconds as milliseconds. 

### <a id="fmul32" href="#fmul32">func fmul32(x, y uint32) uint32</a>

```
searchKey: runtime.fmul32
tags: [method private]
```

```Go
func fmul32(x, y uint32) uint32
```

### <a id="fmul64" href="#fmul64">func fmul64(f, g uint64) uint64</a>

```
searchKey: runtime.fmul64
tags: [method private]
```

```Go
func fmul64(f, g uint64) uint64
```

### <a id="fneg64" href="#fneg64">func fneg64(f uint64) uint64</a>

```
searchKey: runtime.fneg64
tags: [method private]
```

```Go
func fneg64(f uint64) uint64
```

### <a id="forEachG" href="#forEachG">func forEachG(fn func(gp *g))</a>

```
searchKey: runtime.forEachG
tags: [method private]
```

```Go
func forEachG(fn func(gp *g))
```

forEachG calls fn on every G from allgs. 

forEachG takes a lock to exclude concurrent addition of new Gs. 

### <a id="forEachGRace" href="#forEachGRace">func forEachGRace(fn func(gp *g))</a>

```
searchKey: runtime.forEachGRace
tags: [method private]
```

```Go
func forEachGRace(fn func(gp *g))
```

forEachGRace calls fn on every G from allgs. 

forEachGRace avoids locking, but does not exclude addition of new Gs during execution, which may be missed. 

### <a id="forEachP" href="#forEachP">func forEachP(fn func(*p))</a>

```
searchKey: runtime.forEachP
tags: [method private]
```

```Go
func forEachP(fn func(*p))
```

forEachP calls fn(p) for every P p when p reaches a GC safe point. If a P is currently executing code, this will bring the P to a GC safe point and execute fn on that P. If the P is not executing code (it is idle or in a syscall), this will call fn(p) directly while preventing the P from exiting its state. This does not ensure that fn will run on every CPU executing Go code, but it acts as a global memory barrier. GC uses this as a "ragged barrier." 

The caller must hold worldsema. 

### <a id="forcegchelper" href="#forcegchelper">func forcegchelper()</a>

```
searchKey: runtime.forcegchelper
tags: [function private]
```

```Go
func forcegchelper()
```

### <a id="fpack32" href="#fpack32">func fpack32(sign, mant uint32, exp int, trunc uint32) uint32</a>

```
searchKey: runtime.fpack32
tags: [method private]
```

```Go
func fpack32(sign, mant uint32, exp int, trunc uint32) uint32
```

### <a id="fpack64" href="#fpack64">func fpack64(sign, mant uint64, exp int, trunc uint64) uint64</a>

```
searchKey: runtime.fpack64
tags: [method private]
```

```Go
func fpack64(sign, mant uint64, exp int, trunc uint64) uint64
```

### <a id="freeSomeWbufs" href="#freeSomeWbufs">func freeSomeWbufs(preemptible bool) bool</a>

```
searchKey: runtime.freeSomeWbufs
tags: [method private]
```

```Go
func freeSomeWbufs(preemptible bool) bool
```

freeSomeWbufs frees some workbufs back to the heap and returns true if it should be called again to free more. 

### <a id="freeSpecial" href="#freeSpecial">func freeSpecial(s *special, p unsafe.Pointer, size uintptr)</a>

```
searchKey: runtime.freeSpecial
tags: [method private]
```

```Go
func freeSpecial(s *special, p unsafe.Pointer, size uintptr)
```

freeSpecial performs any cleanup on special s and deallocates it. s must already be unlinked from the specials list. 

### <a id="freeStackSpans" href="#freeStackSpans">func freeStackSpans()</a>

```
searchKey: runtime.freeStackSpans
tags: [function private]
```

```Go
func freeStackSpans()
```

freeStackSpans frees unused stack spans at the end of GC. 

### <a id="freedefer" href="#freedefer">func freedefer(d *_defer)</a>

```
searchKey: runtime.freedefer
tags: [method private]
```

```Go
func freedefer(d *_defer)
```

Free the given defer. The defer cannot be used after this call. 

This must not grow the stack because there may be a frame without a stack map when this is called. 

### <a id="freedeferfn" href="#freedeferfn">func freedeferfn()</a>

```
searchKey: runtime.freedeferfn
tags: [function private]
```

```Go
func freedeferfn()
```

### <a id="freedeferpanic" href="#freedeferpanic">func freedeferpanic()</a>

```
searchKey: runtime.freedeferpanic
tags: [function private]
```

```Go
func freedeferpanic()
```

Separate function so that it can split stack. Windows otherwise runs out of stack space. 

### <a id="freemcache" href="#freemcache">func freemcache(c *mcache)</a>

```
searchKey: runtime.freemcache
tags: [method private]
```

```Go
func freemcache(c *mcache)
```

freemcache releases resources associated with this mcache and puts the object onto a free list. 

In some cases there is no way to simply release resources, such as statistics, so donate them to a different mcache (the recipient). 

### <a id="freezetheworld" href="#freezetheworld">func freezetheworld()</a>

```
searchKey: runtime.freezetheworld
tags: [function private]
```

```Go
func freezetheworld()
```

Similar to stopTheWorld but best-effort and can be called several times. There is no reverse operation, used during crashing. This function must not lock any mutexes. 

### <a id="fsub64" href="#fsub64">func fsub64(f, g uint64) uint64</a>

```
searchKey: runtime.fsub64
tags: [method private]
```

```Go
func fsub64(f, g uint64) uint64
```

### <a id="fuint64to32" href="#fuint64to32">func fuint64to32(x uint64) float32</a>

```
searchKey: runtime.fuint64to32
tags: [method private]
```

```Go
func fuint64to32(x uint64) float32
```

### <a id="fuint64to64" href="#fuint64to64">func fuint64to64(x uint64) float64</a>

```
searchKey: runtime.fuint64to64
tags: [method private]
```

```Go
func fuint64to64(x uint64) float64
```

### <a id="full" href="#full">func full(c *hchan) bool</a>

```
searchKey: runtime.full
tags: [method private]
```

```Go
func full(c *hchan) bool
```

full reports whether a send on c would block (that is, the channel is full). It uses a single word-sized read of mutable state, so although the answer is instantaneously true, the correct answer may have changed by the time the calling function receives the return value. 

### <a id="funcMaxSPDelta" href="#funcMaxSPDelta">func funcMaxSPDelta(f funcInfo) int32</a>

```
searchKey: runtime.funcMaxSPDelta
tags: [method private]
```

```Go
func funcMaxSPDelta(f funcInfo) int32
```

funcMaxSPDelta returns the maximum spdelta at any point in f. 

### <a id="funcPC" href="#funcPC">func funcPC(f interface{}) uintptr</a>

```
searchKey: runtime.funcPC
tags: [method private]
```

```Go
func funcPC(f interface{}) uintptr
```

funcPC returns the entry PC of the function f. It assumes that f is a func value. Otherwise the behavior is undefined. CAREFUL: In programs with plugins, funcPC can return different values for the same function (because there are actually multiple copies of the same function in the address space). To be safe, don't use the results of this function in any == expression. It is only safe to use the result as an address at which to start executing code. 

### <a id="funcdata" href="#funcdata">func funcdata(f funcInfo, i uint8) unsafe.Pointer</a>

```
searchKey: runtime.funcdata
tags: [method private]
```

```Go
func funcdata(f funcInfo, i uint8) unsafe.Pointer
```

### <a id="funcfile" href="#funcfile">func funcfile(f funcInfo, fileno int32) string</a>

```
searchKey: runtime.funcfile
tags: [method private]
```

```Go
func funcfile(f funcInfo, fileno int32) string
```

### <a id="funcline" href="#funcline">func funcline(f funcInfo, targetpc uintptr) (file string, line int32)</a>

```
searchKey: runtime.funcline
tags: [method private]
```

```Go
func funcline(f funcInfo, targetpc uintptr) (file string, line int32)
```

### <a id="funcline1" href="#funcline1">func funcline1(f funcInfo, targetpc uintptr, strict bool) (file string, line int32)</a>

```
searchKey: runtime.funcline1
tags: [method private]
```

```Go
func funcline1(f funcInfo, targetpc uintptr, strict bool) (file string, line int32)
```

### <a id="funcname" href="#funcname">func funcname(f funcInfo) string</a>

```
searchKey: runtime.funcname
tags: [method private]
```

```Go
func funcname(f funcInfo) string
```

### <a id="funcnameFromNameoff" href="#funcnameFromNameoff">func funcnameFromNameoff(f funcInfo, nameoff int32) string</a>

```
searchKey: runtime.funcnameFromNameoff
tags: [method private]
```

```Go
func funcnameFromNameoff(f funcInfo, nameoff int32) string
```

### <a id="funcpkgpath" href="#funcpkgpath">func funcpkgpath(f funcInfo) string</a>

```
searchKey: runtime.funcpkgpath
tags: [method private]
```

```Go
func funcpkgpath(f funcInfo) string
```

### <a id="funcspdelta" href="#funcspdelta">func funcspdelta(f funcInfo, targetpc uintptr, cache *pcvalueCache) int32</a>

```
searchKey: runtime.funcspdelta
tags: [method private]
```

```Go
func funcspdelta(f funcInfo, targetpc uintptr, cache *pcvalueCache) int32
```

### <a id="funpack32" href="#funpack32">func funpack32(f uint32) (sign, mant uint32, exp int, inf, nan bool)</a>

```
searchKey: runtime.funpack32
tags: [method private]
```

```Go
func funpack32(f uint32) (sign, mant uint32, exp int, inf, nan bool)
```

### <a id="funpack64" href="#funpack64">func funpack64(f uint64) (sign, mant uint64, exp int, inf, nan bool)</a>

```
searchKey: runtime.funpack64
tags: [method private]
```

```Go
func funpack64(f uint64) (sign, mant uint64, exp int, inf, nan bool)
```

### <a id="gcAssistAlloc" href="#gcAssistAlloc">func gcAssistAlloc(gp *g)</a>

```
searchKey: runtime.gcAssistAlloc
tags: [method private]
```

```Go
func gcAssistAlloc(gp *g)
```

gcAssistAlloc performs GC work to make gp's assist debt positive. gp must be the calling user gorountine. 

This must be called with preemption enabled. 

### <a id="gcAssistAlloc1" href="#gcAssistAlloc1">func gcAssistAlloc1(gp *g, scanWork int64)</a>

```
searchKey: runtime.gcAssistAlloc1
tags: [method private]
```

```Go
func gcAssistAlloc1(gp *g, scanWork int64)
```

gcAssistAlloc1 is the part of gcAssistAlloc that runs on the system stack. This is a separate function to make it easier to see that we're not capturing anything from the user stack, since the user stack may move while we're in this function. 

gcAssistAlloc1 indicates whether this assist completed the mark phase by setting gp.param to non-nil. This can't be communicated on the stack since it may move. 

### <a id="gcBgMarkPrepare" href="#gcBgMarkPrepare">func gcBgMarkPrepare()</a>

```
searchKey: runtime.gcBgMarkPrepare
tags: [function private]
```

```Go
func gcBgMarkPrepare()
```

gcBgMarkPrepare sets up state for background marking. Mutator assists must not yet be enabled. 

### <a id="gcBgMarkStartWorkers" href="#gcBgMarkStartWorkers">func gcBgMarkStartWorkers()</a>

```
searchKey: runtime.gcBgMarkStartWorkers
tags: [function private]
```

```Go
func gcBgMarkStartWorkers()
```

gcBgMarkStartWorkers prepares background mark worker goroutines. These goroutines will not run until the mark phase, but they must be started while the work is not stopped and from a regular G stack. The caller must hold worldsema. 

### <a id="gcBgMarkWorker" href="#gcBgMarkWorker">func gcBgMarkWorker()</a>

```
searchKey: runtime.gcBgMarkWorker
tags: [function private]
```

```Go
func gcBgMarkWorker()
```

### <a id="gcDrain" href="#gcDrain">func gcDrain(gcw *gcWork, flags gcDrainFlags)</a>

```
searchKey: runtime.gcDrain
tags: [method private]
```

```Go
func gcDrain(gcw *gcWork, flags gcDrainFlags)
```

gcDrain scans roots and objects in work buffers, blackening grey objects until it is unable to get more work. It may return before GC is done; it's the caller's responsibility to balance work from other Ps. 

If flags&gcDrainUntilPreempt != 0, gcDrain returns when g.preempt is set. 

If flags&gcDrainIdle != 0, gcDrain returns when there is other work to do. 

If flags&gcDrainFractional != 0, gcDrain self-preempts when pollFractionalWorkerExit() returns true. This implies gcDrainNoBlock. 

If flags&gcDrainFlushBgCredit != 0, gcDrain flushes scan work credit to gcController.bgScanCredit every gcCreditSlack units of scan work. 

gcDrain will always return if there is a pending STW. 

### <a id="gcDrainN" href="#gcDrainN">func gcDrainN(gcw *gcWork, scanWork int64) int64</a>

```
searchKey: runtime.gcDrainN
tags: [method private]
```

```Go
func gcDrainN(gcw *gcWork, scanWork int64) int64
```

gcDrainN blackens grey objects until it has performed roughly scanWork units of scan work or the G is preempted. This is best-effort, so it may perform less work if it fails to get a work buffer. Otherwise, it will perform at least n units of work, but may perform more because scanning is always done in whole object increments. It returns the amount of scan work performed. 

The caller goroutine must be in a preemptible state (e.g., _Gwaiting) to prevent deadlocks during stack scanning. As a consequence, this must be called on the system stack. 

### <a id="gcDumpObject" href="#gcDumpObject">func gcDumpObject(label string, obj, off uintptr)</a>

```
searchKey: runtime.gcDumpObject
tags: [method private]
```

```Go
func gcDumpObject(label string, obj, off uintptr)
```

gcDumpObject dumps the contents of obj for debugging and marks the field at byte offset off in obj. 

### <a id="gcFlushBgCredit" href="#gcFlushBgCredit">func gcFlushBgCredit(scanWork int64)</a>

```
searchKey: runtime.gcFlushBgCredit
tags: [method private]
```

```Go
func gcFlushBgCredit(scanWork int64)
```

gcFlushBgCredit flushes scanWork units of background scan work credit. This first satisfies blocked assists on the work.assistQueue and then flushes any remaining credit to gcController.bgScanCredit. 

Write barriers are disallowed because this is used by gcDrain after it has ensured that all work is drained and this must preserve that condition. 

### <a id="gcMark" href="#gcMark">func gcMark(startTime int64)</a>

```
searchKey: runtime.gcMark
tags: [method private]
```

```Go
func gcMark(startTime int64)
```

gcMark runs the mark (or, for concurrent GC, mark termination) All gcWork caches must be empty. STW is in effect at this point. 

### <a id="gcMarkDone" href="#gcMarkDone">func gcMarkDone()</a>

```
searchKey: runtime.gcMarkDone
tags: [function private]
```

```Go
func gcMarkDone()
```

gcMarkDone transitions the GC from mark to mark termination if all reachable objects have been marked (that is, there are no grey objects and can be no more in the future). Otherwise, it flushes all local work to the global queues where it can be discovered by other workers. 

This should be called when all local mark work has been drained and there are no remaining workers. Specifically, when 

```
work.nwait == work.nproc && !gcMarkWorkAvailable(p)

```
The calling context must be preemptible. 

Flushing local work is important because idle Ps may have local work queued. This is the only way to make that work visible and drive GC to completion. 

It is explicitly okay to have write barriers in this function. If it does transition to mark termination, then all reachable objects have been marked, so the write barrier cannot shade any more objects. 

### <a id="gcMarkRootCheck" href="#gcMarkRootCheck">func gcMarkRootCheck()</a>

```
searchKey: runtime.gcMarkRootCheck
tags: [function private]
```

```Go
func gcMarkRootCheck()
```

gcMarkRootCheck checks that all roots have been scanned. It is purely for debugging. 

### <a id="gcMarkRootPrepare" href="#gcMarkRootPrepare">func gcMarkRootPrepare()</a>

```
searchKey: runtime.gcMarkRootPrepare
tags: [function private]
```

```Go
func gcMarkRootPrepare()
```

gcMarkRootPrepare queues root scanning jobs (stacks, globals, and some miscellany) and initializes scanning-related state. 

The world must be stopped. 

### <a id="gcMarkTermination" href="#gcMarkTermination">func gcMarkTermination(nextTriggerRatio float64)</a>

```
searchKey: runtime.gcMarkTermination
tags: [method private]
```

```Go
func gcMarkTermination(nextTriggerRatio float64)
```

World must be stopped and mark assists and background workers must be disabled. 

### <a id="gcMarkTinyAllocs" href="#gcMarkTinyAllocs">func gcMarkTinyAllocs()</a>

```
searchKey: runtime.gcMarkTinyAllocs
tags: [function private]
```

```Go
func gcMarkTinyAllocs()
```

gcMarkTinyAllocs greys all active tiny alloc blocks. 

The world must be stopped. 

### <a id="gcMarkWorkAvailable" href="#gcMarkWorkAvailable">func gcMarkWorkAvailable(p *p) bool</a>

```
searchKey: runtime.gcMarkWorkAvailable
tags: [method private]
```

```Go
func gcMarkWorkAvailable(p *p) bool
```

gcMarkWorkAvailable reports whether executing a mark worker on p is potentially useful. p may be nil, in which case it only checks the global sources of work. 

### <a id="gcPaceScavenger" href="#gcPaceScavenger">func gcPaceScavenger()</a>

```
searchKey: runtime.gcPaceScavenger
tags: [function private]
```

```Go
func gcPaceScavenger()
```

gcPaceScavenger updates the scavenger's pacing, particularly its rate and RSS goal. 

The RSS goal is based on the current heap goal with a small overhead to accommodate non-determinism in the allocator. 

The pacing is based on scavengePageRate, which applies to both regular and huge pages. See that constant for more information. 

mheap_.lock must be held or the world must be stopped. 

### <a id="gcParkAssist" href="#gcParkAssist">func gcParkAssist() bool</a>

```
searchKey: runtime.gcParkAssist
tags: [function private]
```

```Go
func gcParkAssist() bool
```

gcParkAssist puts the current goroutine on the assist queue and parks. 

gcParkAssist reports whether the assist is now satisfied. If it returns false, the caller must retry the assist. 

### <a id="gcResetMarkState" href="#gcResetMarkState">func gcResetMarkState()</a>

```
searchKey: runtime.gcResetMarkState
tags: [function private]
```

```Go
func gcResetMarkState()
```

gcResetMarkState resets global state prior to marking (concurrent or STW) and resets the stack scan state of all Gs. 

This is safe to do without the world stopped because any Gs created during or after this will start out in the reset state. 

gcResetMarkState must be called on the system stack because it acquires the heap lock. See mheap for details. 

### <a id="gcStart" href="#gcStart">func gcStart(trigger gcTrigger)</a>

```
searchKey: runtime.gcStart
tags: [method private]
```

```Go
func gcStart(trigger gcTrigger)
```

gcStart starts the GC. It transitions from _GCoff to _GCmark (if debug.gcstoptheworld == 0) or performs all of GC (if debug.gcstoptheworld != 0). 

This may return without performing this transition in some cases, such as when called on a system stack or with locks held. 

### <a id="gcSweep" href="#gcSweep">func gcSweep(mode gcMode)</a>

```
searchKey: runtime.gcSweep
tags: [method private]
```

```Go
func gcSweep(mode gcMode)
```

gcSweep must be called on the system stack because it acquires the heap lock. See mheap for details. 

The world must be stopped. 

### <a id="gcTestIsReachable" href="#gcTestIsReachable">func gcTestIsReachable(ptrs ...unsafe.Pointer) (mask uint64)</a>

```
searchKey: runtime.gcTestIsReachable
tags: [method private]
```

```Go
func gcTestIsReachable(ptrs ...unsafe.Pointer) (mask uint64)
```

gcTestIsReachable performs a GC and returns a bit set where bit i is set if ptrs[i] is reachable. 

### <a id="gcTestMoveStackOnNextCall" href="#gcTestMoveStackOnNextCall">func gcTestMoveStackOnNextCall()</a>

```
searchKey: runtime.gcTestMoveStackOnNextCall
tags: [function private]
```

```Go
func gcTestMoveStackOnNextCall()
```

gcTestMoveStackOnNextCall causes the stack to be moved on a call immediately following the call to this. It may not work correctly if any other work appears after this call (such as returning). Typically the following call should be marked go:noinline so it performs a stack check. 

In rare cases this may not cause the stack to move, specifically if there's a preemption between this call and the next. 

### <a id="gcTestPointerClass" href="#gcTestPointerClass">func gcTestPointerClass(p unsafe.Pointer) string</a>

```
searchKey: runtime.gcTestPointerClass
tags: [method private]
```

```Go
func gcTestPointerClass(p unsafe.Pointer) string
```

gcTestPointerClass returns the category of what p points to, one of: "heap", "stack", "data", "bss", "other". This is useful for checking that a test is doing what it's intended to do. 

This is nosplit simply to avoid extra pointer shuffling that may complicate a test. 

### <a id="gcWaitOnMark" href="#gcWaitOnMark">func gcWaitOnMark(n uint32)</a>

```
searchKey: runtime.gcWaitOnMark
tags: [method private]
```

```Go
func gcWaitOnMark(n uint32)
```

gcWaitOnMark blocks until GC finishes the Nth mark phase. If GC has already completed this mark phase, it returns immediately. 

### <a id="gcWakeAllAssists" href="#gcWakeAllAssists">func gcWakeAllAssists()</a>

```
searchKey: runtime.gcWakeAllAssists
tags: [function private]
```

```Go
func gcWakeAllAssists()
```

gcWakeAllAssists wakes all currently blocked assists. This is used at the end of a GC cycle. gcBlackenEnabled must be false to prevent new assists from going to sleep after this point. 

### <a id="gcWriteBarrier" href="#gcWriteBarrier">func gcWriteBarrier()</a>

```
searchKey: runtime.gcWriteBarrier
tags: [function private]
```

```Go
func gcWriteBarrier()
```

Called from compiled code; declared for vet; do NOT call from Go. 

### <a id="gcWriteBarrierBP" href="#gcWriteBarrierBP">func gcWriteBarrierBP()</a>

```
searchKey: runtime.gcWriteBarrierBP
tags: [function private]
```

```Go
func gcWriteBarrierBP()
```

### <a id="gcWriteBarrierBX" href="#gcWriteBarrierBX">func gcWriteBarrierBX()</a>

```
searchKey: runtime.gcWriteBarrierBX
tags: [function private]
```

```Go
func gcWriteBarrierBX()
```

### <a id="gcWriteBarrierCX" href="#gcWriteBarrierCX">func gcWriteBarrierCX()</a>

```
searchKey: runtime.gcWriteBarrierCX
tags: [function private]
```

```Go
func gcWriteBarrierCX()
```

Called from compiled code; declared for vet; do NOT call from Go. 

### <a id="gcWriteBarrierDX" href="#gcWriteBarrierDX">func gcWriteBarrierDX()</a>

```
searchKey: runtime.gcWriteBarrierDX
tags: [function private]
```

```Go
func gcWriteBarrierDX()
```

### <a id="gcWriteBarrierR8" href="#gcWriteBarrierR8">func gcWriteBarrierR8()</a>

```
searchKey: runtime.gcWriteBarrierR8
tags: [function private]
```

```Go
func gcWriteBarrierR8()
```

### <a id="gcWriteBarrierR9" href="#gcWriteBarrierR9">func gcWriteBarrierR9()</a>

```
searchKey: runtime.gcWriteBarrierR9
tags: [function private]
```

```Go
func gcWriteBarrierR9()
```

### <a id="gcWriteBarrierSI" href="#gcWriteBarrierSI">func gcWriteBarrierSI()</a>

```
searchKey: runtime.gcWriteBarrierSI
tags: [function private]
```

```Go
func gcWriteBarrierSI()
```

### <a id="gcallers" href="#gcallers">func gcallers(gp *g, skip int, pcbuf []uintptr) int</a>

```
searchKey: runtime.gcallers
tags: [method private]
```

```Go
func gcallers(gp *g, skip int, pcbuf []uintptr) int
```

### <a id="gcd" href="#gcd">func gcd(a, b uint32) uint32</a>

```
searchKey: runtime.gcd
tags: [method private]
```

```Go
func gcd(a, b uint32) uint32
```

### <a id="gcenable" href="#gcenable">func gcenable()</a>

```
searchKey: runtime.gcenable
tags: [function private]
```

```Go
func gcenable()
```

gcenable is called after the bulk of the runtime initialization, just before we're about to start letting user code run. It kicks off the background sweeper goroutine, the background scavenger goroutine, and enables GC. 

### <a id="gcinit" href="#gcinit">func gcinit()</a>

```
searchKey: runtime.gcinit
tags: [function private]
```

```Go
func gcinit()
```

### <a id="gcmarknewobject" href="#gcmarknewobject">func gcmarknewobject(span *mspan, obj, size, scanSize uintptr)</a>

```
searchKey: runtime.gcmarknewobject
tags: [method private]
```

```Go
func gcmarknewobject(span *mspan, obj, size, scanSize uintptr)
```

gcmarknewobject marks a newly allocated object black. obj must not contain any non-nil pointers. 

This is nosplit so it can manipulate a gcWork without preemption. 

### <a id="gcount" href="#gcount">func gcount() int32</a>

```
searchKey: runtime.gcount
tags: [function private]
```

```Go
func gcount() int32
```

### <a id="gcstopm" href="#gcstopm">func gcstopm()</a>

```
searchKey: runtime.gcstopm
tags: [function private]
```

```Go
func gcstopm()
```

Stops the current m for stopTheWorld. Returns when the world is restarted. 

### <a id="gentraceback" href="#gentraceback">func gentraceback(pc0, sp0, lr0 uintptr, gp *g, skip int, pcbuf *uintptr, max int, callback func(*stkframe, unsafe.Pointer) bool, v unsafe.Pointer, flags uint) int</a>

```
searchKey: runtime.gentraceback
tags: [method private]
```

```Go
func gentraceback(pc0, sp0, lr0 uintptr, gp *g, skip int, pcbuf *uintptr, max int, callback func(*stkframe, unsafe.Pointer) bool, v unsafe.Pointer, flags uint) int
```

Generic traceback. Handles runtime stack prints (pcbuf == nil), the runtime.Callers function (pcbuf != nil), as well as the garbage collector (callback != nil).  A little clunky to merge these, but avoids duplicating the code and all its subtlety. 

The skip argument is only valid with pcbuf != nil and counts the number of logical frames to skip rather than physical frames (with inlining, a PC in pcbuf can represent multiple calls). 

### <a id="getPageSize" href="#getPageSize">func getPageSize() uintptr</a>

```
searchKey: runtime.getPageSize
tags: [function private]
```

```Go
func getPageSize() uintptr
```

### <a id="getRandomData" href="#getRandomData">func getRandomData(r []byte)</a>

```
searchKey: runtime.getRandomData
tags: [method private]
```

```Go
func getRandomData(r []byte)
```

### <a id="getargp" href="#getargp">func getargp() uintptr</a>

```
searchKey: runtime.getargp
tags: [function private]
```

```Go
func getargp() uintptr
```

getargp returns the location where the caller writes outgoing function call arguments. 

### <a id="getcallerpc" href="#getcallerpc">func getcallerpc() uintptr</a>

```
searchKey: runtime.getcallerpc
tags: [function private]
```

```Go
func getcallerpc() uintptr
```

### <a id="getcallersp" href="#getcallersp">func getcallersp() uintptr</a>

```
searchKey: runtime.getcallersp
tags: [function private]
```

```Go
func getcallersp() uintptr
```

### <a id="getclosureptr" href="#getclosureptr">func getclosureptr() uintptr</a>

```
searchKey: runtime.getclosureptr
tags: [function private]
```

```Go
func getclosureptr() uintptr
```

getclosureptr returns the pointer to the current closure. getclosureptr can only be used in an assignment statement at the entry of a function. Moreover, go:nosplit directive must be specified at the declaration of caller function, so that the function prolog does not clobber the closure register. for example: 

```
//go:nosplit
func f(arg1, arg2, arg3 int) {
	dx := getclosureptr()
}

```
The compiler rewrites calls to this function into instructions that fetch the pointer from a well-known register (DX on x86 architecture, etc.) directly. 

### <a id="getgcmask" href="#getgcmask">func getgcmask(ep interface{}) (mask []byte)</a>

```
searchKey: runtime.getgcmask
tags: [method private]
```

```Go
func getgcmask(ep interface{}) (mask []byte)
```

Returns GC type info for the pointer stored in ep for testing. If ep points to the stack, only static live information will be returned (i.e. not for objects which are only dynamically live stack objects). 

### <a id="getgcmaskcb" href="#getgcmaskcb">func getgcmaskcb(frame *stkframe, ctxt unsafe.Pointer) bool</a>

```
searchKey: runtime.getgcmaskcb
tags: [method private]
```

```Go
func getgcmaskcb(frame *stkframe, ctxt unsafe.Pointer) bool
```

### <a id="getm" href="#getm">func getm() uintptr</a>

```
searchKey: runtime.getm
tags: [function private]
```

```Go
func getm() uintptr
```

A helper function for EnsureDropM. 

### <a id="getncpu" href="#getncpu">func getncpu() int32</a>

```
searchKey: runtime.getncpu
tags: [function private]
```

```Go
func getncpu() int32
```

### <a id="getsig" href="#getsig">func getsig(i uint32) uintptr</a>

```
searchKey: runtime.getsig
tags: [method private]
```

```Go
func getsig(i uint32) uintptr
```

### <a id="gfpurge" href="#gfpurge">func gfpurge(_p_ *p)</a>

```
searchKey: runtime.gfpurge
tags: [method private]
```

```Go
func gfpurge(_p_ *p)
```

Purge all cached G's from gfree list to the global list. 

### <a id="gfput" href="#gfput">func gfput(_p_ *p, gp *g)</a>

```
searchKey: runtime.gfput
tags: [method private]
```

```Go
func gfput(_p_ *p, gp *g)
```

Put on gfree list. If local list is too long, transfer a batch to the global list. 

### <a id="globrunqput" href="#globrunqput">func globrunqput(gp *g)</a>

```
searchKey: runtime.globrunqput
tags: [method private]
```

```Go
func globrunqput(gp *g)
```

Put gp on the global runnable queue. sched.lock must be held. May run during STW, so write barriers are not allowed. 

### <a id="globrunqputbatch" href="#globrunqputbatch">func globrunqputbatch(batch *gQueue, n int32)</a>

```
searchKey: runtime.globrunqputbatch
tags: [method private]
```

```Go
func globrunqputbatch(batch *gQueue, n int32)
```

Put a batch of runnable goroutines on the global runnable queue. This clears *batch. sched.lock must be held. May run during STW, so write barriers are not allowed. 

### <a id="globrunqputhead" href="#globrunqputhead">func globrunqputhead(gp *g)</a>

```
searchKey: runtime.globrunqputhead
tags: [method private]
```

```Go
func globrunqputhead(gp *g)
```

Put gp at the head of the global runnable queue. sched.lock must be held. May run during STW, so write barriers are not allowed. 

### <a id="goPanicIndex" href="#goPanicIndex">func goPanicIndex(x int, y int)</a>

```
searchKey: runtime.goPanicIndex
tags: [method private]
```

```Go
func goPanicIndex(x int, y int)
```

failures in the comparisons for s[x], 0 <= x < y (y == len(s)) 

### <a id="goPanicIndexU" href="#goPanicIndexU">func goPanicIndexU(x uint, y int)</a>

```
searchKey: runtime.goPanicIndexU
tags: [method private]
```

```Go
func goPanicIndexU(x uint, y int)
```

### <a id="goPanicSlice3Acap" href="#goPanicSlice3Acap">func goPanicSlice3Acap(x int, y int)</a>

```
searchKey: runtime.goPanicSlice3Acap
tags: [method private]
```

```Go
func goPanicSlice3Acap(x int, y int)
```

### <a id="goPanicSlice3AcapU" href="#goPanicSlice3AcapU">func goPanicSlice3AcapU(x uint, y int)</a>

```
searchKey: runtime.goPanicSlice3AcapU
tags: [method private]
```

```Go
func goPanicSlice3AcapU(x uint, y int)
```

### <a id="goPanicSlice3Alen" href="#goPanicSlice3Alen">func goPanicSlice3Alen(x int, y int)</a>

```
searchKey: runtime.goPanicSlice3Alen
tags: [method private]
```

```Go
func goPanicSlice3Alen(x int, y int)
```

failures in the comparisons for s[::x], 0 <= x <= y (y == len(s) or cap(s)) 

### <a id="goPanicSlice3AlenU" href="#goPanicSlice3AlenU">func goPanicSlice3AlenU(x uint, y int)</a>

```
searchKey: runtime.goPanicSlice3AlenU
tags: [method private]
```

```Go
func goPanicSlice3AlenU(x uint, y int)
```

### <a id="goPanicSlice3B" href="#goPanicSlice3B">func goPanicSlice3B(x int, y int)</a>

```
searchKey: runtime.goPanicSlice3B
tags: [method private]
```

```Go
func goPanicSlice3B(x int, y int)
```

failures in the comparisons for s[:x:y], 0 <= x <= y 

### <a id="goPanicSlice3BU" href="#goPanicSlice3BU">func goPanicSlice3BU(x uint, y int)</a>

```
searchKey: runtime.goPanicSlice3BU
tags: [method private]
```

```Go
func goPanicSlice3BU(x uint, y int)
```

### <a id="goPanicSlice3C" href="#goPanicSlice3C">func goPanicSlice3C(x int, y int)</a>

```
searchKey: runtime.goPanicSlice3C
tags: [method private]
```

```Go
func goPanicSlice3C(x int, y int)
```

failures in the comparisons for s[x:y:], 0 <= x <= y 

### <a id="goPanicSlice3CU" href="#goPanicSlice3CU">func goPanicSlice3CU(x uint, y int)</a>

```
searchKey: runtime.goPanicSlice3CU
tags: [method private]
```

```Go
func goPanicSlice3CU(x uint, y int)
```

### <a id="goPanicSliceAcap" href="#goPanicSliceAcap">func goPanicSliceAcap(x int, y int)</a>

```
searchKey: runtime.goPanicSliceAcap
tags: [method private]
```

```Go
func goPanicSliceAcap(x int, y int)
```

### <a id="goPanicSliceAcapU" href="#goPanicSliceAcapU">func goPanicSliceAcapU(x uint, y int)</a>

```
searchKey: runtime.goPanicSliceAcapU
tags: [method private]
```

```Go
func goPanicSliceAcapU(x uint, y int)
```

### <a id="goPanicSliceAlen" href="#goPanicSliceAlen">func goPanicSliceAlen(x int, y int)</a>

```
searchKey: runtime.goPanicSliceAlen
tags: [method private]
```

```Go
func goPanicSliceAlen(x int, y int)
```

failures in the comparisons for s[:x], 0 <= x <= y (y == len(s) or cap(s)) 

### <a id="goPanicSliceAlenU" href="#goPanicSliceAlenU">func goPanicSliceAlenU(x uint, y int)</a>

```
searchKey: runtime.goPanicSliceAlenU
tags: [method private]
```

```Go
func goPanicSliceAlenU(x uint, y int)
```

### <a id="goPanicSliceB" href="#goPanicSliceB">func goPanicSliceB(x int, y int)</a>

```
searchKey: runtime.goPanicSliceB
tags: [method private]
```

```Go
func goPanicSliceB(x int, y int)
```

failures in the comparisons for s[x:y], 0 <= x <= y 

### <a id="goPanicSliceBU" href="#goPanicSliceBU">func goPanicSliceBU(x uint, y int)</a>

```
searchKey: runtime.goPanicSliceBU
tags: [method private]
```

```Go
func goPanicSliceBU(x uint, y int)
```

### <a id="goPanicSliceConvert" href="#goPanicSliceConvert">func goPanicSliceConvert(x int, y int)</a>

```
searchKey: runtime.goPanicSliceConvert
tags: [method private]
```

```Go
func goPanicSliceConvert(x int, y int)
```

failures in the conversion (*[x]T)s, 0 <= x <= y, x == cap(s) 

### <a id="goargs" href="#goargs">func goargs()</a>

```
searchKey: runtime.goargs
tags: [function private]
```

```Go
func goargs()
```

### <a id="gobytes" href="#gobytes">func gobytes(p *byte, n int) (b []byte)</a>

```
searchKey: runtime.gobytes
tags: [method private]
```

```Go
func gobytes(p *byte, n int) (b []byte)
```

used by cmd/cgo 

### <a id="goenvs" href="#goenvs">func goenvs()</a>

```
searchKey: runtime.goenvs
tags: [function private]
```

```Go
func goenvs()
```

### <a id="goenvs_unix" href="#goenvs_unix">func goenvs_unix()</a>

```
searchKey: runtime.goenvs_unix
tags: [function private]
```

```Go
func goenvs_unix()
```

### <a id="goexit" href="#goexit">func goexit(neverCallThisFunction)</a>

```
searchKey: runtime.goexit
tags: [method private]
```

```Go
func goexit(neverCallThisFunction)
```

goexit is the return stub at the top of every goroutine call stack. Each goroutine stack is constructed as if goexit called the goroutine's entry point function, so that when the entry point function returns, it will return to goexit, which will call goexit1 to perform the actual exit. 

This function must never be called directly. Call goexit1 instead. gentraceback assumes that goexit terminates the stack. A direct call on the stack will cause gentraceback to stop walking the stack prematurely and if there is leftover state it may panic. 

### <a id="goexit0" href="#goexit0">func goexit0(gp *g)</a>

```
searchKey: runtime.goexit0
tags: [method private]
```

```Go
func goexit0(gp *g)
```

goexit continuation on g0. 

### <a id="goexit1" href="#goexit1">func goexit1()</a>

```
searchKey: runtime.goexit1
tags: [function private]
```

```Go
func goexit1()
```

Finishes execution of the current goroutine. 

### <a id="gogetenv" href="#gogetenv">func gogetenv(key string) string</a>

```
searchKey: runtime.gogetenv
tags: [method private]
```

```Go
func gogetenv(key string) string
```

### <a id="gogo" href="#gogo">func gogo(buf *gobuf)</a>

```
searchKey: runtime.gogo
tags: [method private]
```

```Go
func gogo(buf *gobuf)
```

### <a id="gopanic" href="#gopanic">func gopanic(e interface{})</a>

```
searchKey: runtime.gopanic
tags: [method private]
```

```Go
func gopanic(e interface{})
```

The implementation of the predeclared function panic. 

### <a id="gopark" href="#gopark">func gopark(unlockf func(*g, unsafe.Pointer) bool, lock unsafe.Pointer, reason waitReason, traceEv byte, traceskip int)</a>

```
searchKey: runtime.gopark
tags: [method private]
```

```Go
func gopark(unlockf func(*g, unsafe.Pointer) bool, lock unsafe.Pointer, reason waitReason, traceEv byte, traceskip int)
```

Puts the current goroutine into a waiting state and calls unlockf on the system stack. 

If unlockf returns false, the goroutine is resumed. 

unlockf must not access this G's stack, as it may be moved between the call to gopark and the call to unlockf. 

Note that because unlockf is called after putting the G into a waiting state, the G may have already been readied by the time unlockf is called unless there is external synchronization preventing the G from being readied. If unlockf returns false, it must guarantee that the G cannot be externally readied. 

Reason explains why the goroutine has been parked. It is displayed in stack traces and heap dumps. Reasons should be unique and descriptive. Do not re-use reasons, add new ones. 

### <a id="goparkunlock" href="#goparkunlock">func goparkunlock(lock *mutex, reason waitReason, traceEv byte, traceskip int)</a>

```
searchKey: runtime.goparkunlock
tags: [method private]
```

```Go
func goparkunlock(lock *mutex, reason waitReason, traceEv byte, traceskip int)
```

Puts the current goroutine into a waiting state and unlocks the lock. The goroutine can be made runnable again by calling goready(gp). 

### <a id="gopreempt_m" href="#gopreempt_m">func gopreempt_m(gp *g)</a>

```
searchKey: runtime.gopreempt_m
tags: [method private]
```

```Go
func gopreempt_m(gp *g)
```

### <a id="goready" href="#goready">func goready(gp *g, traceskip int)</a>

```
searchKey: runtime.goready
tags: [method private]
```

```Go
func goready(gp *g, traceskip int)
```

### <a id="gorecover" href="#gorecover">func gorecover(argp uintptr) interface{}</a>

```
searchKey: runtime.gorecover
tags: [method private]
```

```Go
func gorecover(argp uintptr) interface{}
```

The implementation of the predeclared function recover. Cannot split the stack because it needs to reliably find the stack segment of its caller. 

TODO(rsc): Once we commit to CopyStackAlways, this doesn't need to be nosplit. 

### <a id="goroutineProfileWithLabels" href="#goroutineProfileWithLabels">func goroutineProfileWithLabels(p []StackRecord, labels []unsafe.Pointer) (n int, ok bool)</a>

```
searchKey: runtime.goroutineProfileWithLabels
tags: [method private]
```

```Go
func goroutineProfileWithLabels(p []StackRecord, labels []unsafe.Pointer) (n int, ok bool)
```

labels may be nil. If labels is non-nil, it must have the same length as p. 

### <a id="goroutineReady" href="#goroutineReady">func goroutineReady(arg interface{}, seq uintptr)</a>

```
searchKey: runtime.goroutineReady
tags: [method private]
```

```Go
func goroutineReady(arg interface{}, seq uintptr)
```

Ready the goroutine arg. 

### <a id="goroutineheader" href="#goroutineheader">func goroutineheader(gp *g)</a>

```
searchKey: runtime.goroutineheader
tags: [method private]
```

```Go
func goroutineheader(gp *g)
```

### <a id="goschedImpl" href="#goschedImpl">func goschedImpl(gp *g)</a>

```
searchKey: runtime.goschedImpl
tags: [method private]
```

```Go
func goschedImpl(gp *g)
```

### <a id="gosched_m" href="#gosched_m">func gosched_m(gp *g)</a>

```
searchKey: runtime.gosched_m
tags: [method private]
```

```Go
func gosched_m(gp *g)
```

Gosched continuation on g0. 

### <a id="goschedguarded" href="#goschedguarded">func goschedguarded()</a>

```
searchKey: runtime.goschedguarded
tags: [function private]
```

```Go
func goschedguarded()
```

goschedguarded yields the processor like gosched, but also checks for forbidden states and opts out of the yield in those cases. 

### <a id="goschedguarded_m" href="#goschedguarded_m">func goschedguarded_m(gp *g)</a>

```
searchKey: runtime.goschedguarded_m
tags: [method private]
```

```Go
func goschedguarded_m(gp *g)
```

goschedguarded is a forbidden-states-avoided version of gosched_m 

### <a id="gostartcall" href="#gostartcall">func gostartcall(buf *gobuf, fn, ctxt unsafe.Pointer)</a>

```
searchKey: runtime.gostartcall
tags: [method private]
```

```Go
func gostartcall(buf *gobuf, fn, ctxt unsafe.Pointer)
```

adjust Gobuf as if it executed a call to fn with context ctxt and then stopped before the first instruction in fn. 

### <a id="gostartcallfn" href="#gostartcallfn">func gostartcallfn(gobuf *gobuf, fv *funcval)</a>

```
searchKey: runtime.gostartcallfn
tags: [method private]
```

```Go
func gostartcallfn(gobuf *gobuf, fv *funcval)
```

adjust Gobuf as if it executed a call to fn and then stopped before the first instruction in fn. 

### <a id="gostring" href="#gostring">func gostring(p *byte) string</a>

```
searchKey: runtime.gostring
tags: [method private]
```

```Go
func gostring(p *byte) string
```

This is exported via linkname to assembly in syscall (for Plan9). 

### <a id="gostringn" href="#gostringn">func gostringn(p *byte, l int) string</a>

```
searchKey: runtime.gostringn
tags: [method private]
```

```Go
func gostringn(p *byte, l int) string
```

### <a id="gostringnocopy" href="#gostringnocopy">func gostringnocopy(str *byte) string</a>

```
searchKey: runtime.gostringnocopy
tags: [method private]
```

```Go
func gostringnocopy(str *byte) string
```

### <a id="gostringw" href="#gostringw">func gostringw(strw *uint16) string</a>

```
searchKey: runtime.gostringw
tags: [method private]
```

```Go
func gostringw(strw *uint16) string
```

### <a id="gotraceback" href="#gotraceback">func gotraceback() (level int32, all, crash bool)</a>

```
searchKey: runtime.gotraceback
tags: [function private]
```

```Go
func gotraceback() (level int32, all, crash bool)
```

gotraceback returns the current traceback settings. 

If level is 0, suppress all tracebacks. If level is 1, show tracebacks, but exclude runtime frames. If level is 2, show tracebacks including runtime frames. If all is set, print all goroutine stacks. Otherwise, print just the current goroutine. If crash is set, crash (core dump, etc) after tracebacking. 

### <a id="goyield" href="#goyield">func goyield()</a>

```
searchKey: runtime.goyield
tags: [function private]
```

```Go
func goyield()
```

goyield is like Gosched, but it: - emits a GoPreempt trace event instead of a GoSched trace event - puts the current G on the runq of the current P instead of the globrunq 

### <a id="goyield_m" href="#goyield_m">func goyield_m(gp *g)</a>

```
searchKey: runtime.goyield_m
tags: [method private]
```

```Go
func goyield_m(gp *g)
```

### <a id="greyobject" href="#greyobject">func greyobject(obj, base, off uintptr, span *mspan, gcw *gcWork, objIndex uintptr)</a>

```
searchKey: runtime.greyobject
tags: [method private]
```

```Go
func greyobject(obj, base, off uintptr, span *mspan, gcw *gcWork, objIndex uintptr)
```

obj is the start of an object with mark mbits. If it isn't already marked, mark it and enqueue into gcw. base and off are for debugging only and could be removed. 

See also wbBufFlush1, which partially duplicates this logic. 

### <a id="growWork" href="#growWork">func growWork(t *maptype, h *hmap, bucket uintptr)</a>

```
searchKey: runtime.growWork
tags: [method private]
```

```Go
func growWork(t *maptype, h *hmap, bucket uintptr)
```

### <a id="growWork_fast32" href="#growWork_fast32">func growWork_fast32(t *maptype, h *hmap, bucket uintptr)</a>

```
searchKey: runtime.growWork_fast32
tags: [method private]
```

```Go
func growWork_fast32(t *maptype, h *hmap, bucket uintptr)
```

### <a id="growWork_fast64" href="#growWork_fast64">func growWork_fast64(t *maptype, h *hmap, bucket uintptr)</a>

```
searchKey: runtime.growWork_fast64
tags: [method private]
```

```Go
func growWork_fast64(t *maptype, h *hmap, bucket uintptr)
```

### <a id="growWork_faststr" href="#growWork_faststr">func growWork_faststr(t *maptype, h *hmap, bucket uintptr)</a>

```
searchKey: runtime.growWork_faststr
tags: [method private]
```

```Go
func growWork_faststr(t *maptype, h *hmap, bucket uintptr)
```

### <a id="gwrite" href="#gwrite">func gwrite(b []byte)</a>

```
searchKey: runtime.gwrite
tags: [method private]
```

```Go
func gwrite(b []byte)
```

write to goroutine-local buffer if diverting output, or else standard error. 

### <a id="handoffp" href="#handoffp">func handoffp(_p_ *p)</a>

```
searchKey: runtime.handoffp
tags: [method private]
```

```Go
func handoffp(_p_ *p)
```

Hands off P from syscall or locked M. Always runs without a P, so write barriers are not allowed. 

### <a id="hasPrefix" href="#hasPrefix">func hasPrefix(s, prefix string) bool</a>

```
searchKey: runtime.hasPrefix
tags: [method private]
```

```Go
func hasPrefix(s, prefix string) bool
```

### <a id="hashGrow" href="#hashGrow">func hashGrow(t *maptype, h *hmap)</a>

```
searchKey: runtime.hashGrow
tags: [method private]
```

```Go
func hashGrow(t *maptype, h *hmap)
```

### <a id="heapBitsSetType" href="#heapBitsSetType">func heapBitsSetType(x, size, dataSize uintptr, typ *_type)</a>

```
searchKey: runtime.heapBitsSetType
tags: [method private]
```

```Go
func heapBitsSetType(x, size, dataSize uintptr, typ *_type)
```

heapBitsSetType records that the new allocation [x, x+size) holds in [x, x+dataSize) one or more values of type typ. (The number of values is given by dataSize / typ.size.) If dataSize < size, the fragment [x+dataSize, x+size) is recorded as non-pointer data. It is known that the type has pointers somewhere; malloc does not call heapBitsSetType when there are no pointers, because all free objects are marked as noscan during heapBitsSweepSpan. 

There can only be one allocation from a given span active at a time, and the bitmap for a span always falls on byte boundaries, so there are no write-write races for access to the heap bitmap. Hence, heapBitsSetType can access the bitmap without atomics. 

There can be read-write races between heapBitsSetType and things that read the heap bitmap like scanobject. However, since heapBitsSetType is only used for objects that have not yet been made reachable, readers will ignore bits being modified by this function. This does mean this function cannot transiently modify bits that belong to neighboring objects. Also, on weakly-ordered machines, callers must execute a store/store (publication) barrier between calling this function and making the object reachable. 

### <a id="heapBitsSetTypeGCProg" href="#heapBitsSetTypeGCProg">func heapBitsSetTypeGCProg(h heapBits, progSize, elemSize, dataSize, allocSize uintptr, prog *byte)</a>

```
searchKey: runtime.heapBitsSetTypeGCProg
tags: [method private]
```

```Go
func heapBitsSetTypeGCProg(h heapBits, progSize, elemSize, dataSize, allocSize uintptr, prog *byte)
```

heapBitsSetTypeGCProg implements heapBitsSetType using a GC program. progSize is the size of the memory described by the program. elemSize is the size of the element that the GC program describes (a prefix of). dataSize is the total size of the intended data, a multiple of elemSize. allocSize is the total size of the allocated memory. 

GC programs are only used for large allocations. heapBitsSetType requires that allocSize is a multiple of 4 words, so that the relevant bitmap bytes are not shared with surrounding objects. 

### <a id="heapRetained" href="#heapRetained">func heapRetained() uint64</a>

```
searchKey: runtime.heapRetained
tags: [function private]
```

```Go
func heapRetained() uint64
```

heapRetained returns an estimate of the current heap RSS. 

### <a id="hexdumpWords" href="#hexdumpWords">func hexdumpWords(p, end uintptr, mark func(uintptr) byte)</a>

```
searchKey: runtime.hexdumpWords
tags: [method private]
```

```Go
func hexdumpWords(p, end uintptr, mark func(uintptr) byte)
```

hexdumpWords prints a word-oriented hex dump of [p, end). 

If mark != nil, it will be called with each printed word's address and should return a character mark to appear just before that word's value. It can return 0 to indicate no mark. 

### <a id="ifaceHash" href="#ifaceHash">func ifaceHash(i interface {...</a>

```
searchKey: runtime.ifaceHash
tags: [method private]
```

```Go
func ifaceHash(i interface {
	F()
}, seed uintptr) uintptr
```

### <a id="ifaceeq" href="#ifaceeq">func ifaceeq(tab *itab, x, y unsafe.Pointer) bool</a>

```
searchKey: runtime.ifaceeq
tags: [method private]
```

```Go
func ifaceeq(tab *itab, x, y unsafe.Pointer) bool
```

### <a id="inHeapOrStack" href="#inHeapOrStack">func inHeapOrStack(b uintptr) bool</a>

```
searchKey: runtime.inHeapOrStack
tags: [method private]
```

```Go
func inHeapOrStack(b uintptr) bool
```

inHeapOrStack is a variant of inheap that returns true for pointers into any allocated heap span. 

### <a id="inPersistentAlloc" href="#inPersistentAlloc">func inPersistentAlloc(p uintptr) bool</a>

```
searchKey: runtime.inPersistentAlloc
tags: [method private]
```

```Go
func inPersistentAlloc(p uintptr) bool
```

inPersistentAlloc reports whether p points to memory allocated by persistentalloc. This must be nosplit because it is called by the cgo checker code, which is called by the write barrier code. 

### <a id="inRange" href="#inRange">func inRange(r0, r1, v0, v1 uintptr) bool</a>

```
searchKey: runtime.inRange
tags: [method private]
```

```Go
func inRange(r0, r1, v0, v1 uintptr) bool
```

inRange reports whether v0 or v1 are in the range [r0, r1]. 

### <a id="inVDSOPage" href="#inVDSOPage">func inVDSOPage(pc uintptr) bool</a>

```
searchKey: runtime.inVDSOPage
tags: [method private]
```

```Go
func inVDSOPage(pc uintptr) bool
```

### <a id="incidlelocked" href="#incidlelocked">func incidlelocked(v int32)</a>

```
searchKey: runtime.incidlelocked
tags: [method private]
```

```Go
func incidlelocked(v int32)
```

### <a id="inf2one" href="#inf2one">func inf2one(f float64) float64</a>

```
searchKey: runtime.inf2one
tags: [method private]
```

```Go
func inf2one(f float64) float64
```

inf2one returns a signed 1 if f is an infinity and a signed 0 otherwise. The sign of the result is the sign of f. 

### <a id="inheap" href="#inheap">func inheap(b uintptr) bool</a>

```
searchKey: runtime.inheap
tags: [method private]
```

```Go
func inheap(b uintptr) bool
```

inheap reports whether b is a pointer into a (potentially dead) heap object. It returns false for pointers into mSpanManual spans. Non-preemptible because it is used by write barriers. 

### <a id="init.cpuflags_amd64.go" href="#init.cpuflags_amd64.go">func init()</a>

```
searchKey: runtime.init
tags: [function private]
```

```Go
func init()
```

### <a id="init.mgcpacer.go" href="#init.mgcpacer.go">func init()</a>

```
searchKey: runtime.init
tags: [function private]
```

```Go
func init()
```

### <a id="init.mgcstack.go" href="#init.mgcstack.go">func init()</a>

```
searchKey: runtime.init
tags: [function private]
```

```Go
func init()
```

### <a id="init.mgcwork.go" href="#init.mgcwork.go">func init()</a>

```
searchKey: runtime.init
tags: [function private]
```

```Go
func init()
```

### <a id="init.mstats.go" href="#init.mstats.go">func init()</a>

```
searchKey: runtime.init
tags: [function private]
```

```Go
func init()
```

### <a id="init.panic.go" href="#init.panic.go">func init()</a>

```
searchKey: runtime.init
tags: [function private]
```

```Go
func init()
```

### <a id="init.preempt.go" href="#init.preempt.go">func init()</a>

```
searchKey: runtime.init
tags: [function private]
```

```Go
func init()
```

### <a id="init.proc.go" href="#init.proc.go">func init()</a>

```
searchKey: runtime.init
tags: [function private]
```

```Go
func init()
```

start forcegc helper goroutine 

### <a id="init.signal_unix.go" href="#init.signal_unix.go">func init()</a>

```
searchKey: runtime.init
tags: [function private]
```

```Go
func init()
```

### <a id="init.stack.go" href="#init.stack.go">func init()</a>

```
searchKey: runtime.init
tags: [function private]
```

```Go
func init()
```

### <a id="initAlgAES" href="#initAlgAES">func initAlgAES()</a>

```
searchKey: runtime.initAlgAES
tags: [function private]
```

```Go
func initAlgAES()
```

### <a id="initMetrics" href="#initMetrics">func initMetrics()</a>

```
searchKey: runtime.initMetrics
tags: [function private]
```

```Go
func initMetrics()
```

initMetrics initializes the metrics map if it hasn't been yet. 

metricsSema must be held. 

### <a id="initsig" href="#initsig">func initsig(preinit bool)</a>

```
searchKey: runtime.initsig
tags: [method private]
```

```Go
func initsig(preinit bool)
```

Initialize signals. Called by libpreinit so runtime may not be initialized. 

### <a id="injectglist" href="#injectglist">func injectglist(glist *gList)</a>

```
searchKey: runtime.injectglist
tags: [method private]
```

```Go
func injectglist(glist *gList)
```

injectglist adds each runnable G on the list to some run queue, and clears glist. If there is no current P, they are added to the global queue, and up to npidle M's are started to run them. Otherwise, for each idle P, this adds a G to the global queue and starts an M. Any remaining G's are added to the current P's local run queue. This may temporarily acquire sched.lock. Can run concurrently with GC. 

### <a id="int32Hash" href="#int32Hash">func int32Hash(i uint32, seed uintptr) uintptr</a>

```
searchKey: runtime.int32Hash
tags: [method private]
```

```Go
func int32Hash(i uint32, seed uintptr) uintptr
```

### <a id="int64Hash" href="#int64Hash">func int64Hash(i uint64, seed uintptr) uintptr</a>

```
searchKey: runtime.int64Hash
tags: [method private]
```

```Go
func int64Hash(i uint64, seed uintptr) uintptr
```

### <a id="interequal" href="#interequal">func interequal(p, q unsafe.Pointer) bool</a>

```
searchKey: runtime.interequal
tags: [method private]
```

```Go
func interequal(p, q unsafe.Pointer) bool
```

### <a id="interhash" href="#interhash">func interhash(p unsafe.Pointer, h uintptr) uintptr</a>

```
searchKey: runtime.interhash
tags: [method private]
```

```Go
func interhash(p unsafe.Pointer, h uintptr) uintptr
```

### <a id="internal_cpu_getsysctlbyname" href="#internal_cpu_getsysctlbyname">func internal_cpu_getsysctlbyname(name []byte) (int32, int32)</a>

```
searchKey: runtime.internal_cpu_getsysctlbyname
tags: [method private]
```

```Go
func internal_cpu_getsysctlbyname(name []byte) (int32, int32)
```

### <a id="intstring" href="#intstring">func intstring(buf *[4]byte, v int64) (s string)</a>

```
searchKey: runtime.intstring
tags: [method private]
```

```Go
func intstring(buf *[4]byte, v int64) (s string)
```

### <a id="isAbortPC" href="#isAbortPC">func isAbortPC(pc uintptr) bool</a>

```
searchKey: runtime.isAbortPC
tags: [method private]
```

```Go
func isAbortPC(pc uintptr) bool
```

isAbortPC reports whether pc is the program counter at which runtime.abort raises a signal. 

It is nosplit because it's part of the isgoexception implementation. 

### <a id="isAsyncSafePoint" href="#isAsyncSafePoint">func isAsyncSafePoint(gp *g, pc, sp, lr uintptr) (bool, uintptr)</a>

```
searchKey: runtime.isAsyncSafePoint
tags: [method private]
```

```Go
func isAsyncSafePoint(gp *g, pc, sp, lr uintptr) (bool, uintptr)
```

isAsyncSafePoint reports whether gp at instruction PC is an asynchronous safe point. This indicates that: 

1. It's safe to suspend gp and conservatively scan its stack and registers. There are no potentially hidden pointer values and it's not in the middle of an atomic sequence like a write barrier. 

2. gp has enough stack space to inject the asyncPreempt call. 

3. It's generally safe to interact with the runtime, even if we're in a signal handler stopped here. For example, there are no runtime locks held, so acquiring a runtime lock won't self-deadlock. 

In some cases the PC is safe for asynchronous preemption but it also needs to adjust the resumption PC. The new PC is returned in the second result. 

### <a id="isDirectIface" href="#isDirectIface">func isDirectIface(t *_type) bool</a>

```
searchKey: runtime.isDirectIface
tags: [method private]
```

```Go
func isDirectIface(t *_type) bool
```

isDirectIface reports whether t is stored directly in an interface value. 

### <a id="isEmpty" href="#isEmpty">func isEmpty(x uint8) bool</a>

```
searchKey: runtime.isEmpty
tags: [method private]
```

```Go
func isEmpty(x uint8) bool
```

isEmpty reports whether the given tophash array entry represents an empty bucket entry. 

### <a id="isExportedRuntime" href="#isExportedRuntime">func isExportedRuntime(name string) bool</a>

```
searchKey: runtime.isExportedRuntime
tags: [method private]
```

```Go
func isExportedRuntime(name string) bool
```

isExportedRuntime reports whether name is an exported runtime function. It is only for runtime functions, so ASCII A-Z is fine. 

### <a id="isFinite" href="#isFinite">func isFinite(f float64) bool</a>

```
searchKey: runtime.isFinite
tags: [method private]
```

```Go
func isFinite(f float64) bool
```

isFinite reports whether f is neither NaN nor an infinity. 

### <a id="isInf" href="#isInf">func isInf(f float64) bool</a>

```
searchKey: runtime.isInf
tags: [method private]
```

```Go
func isInf(f float64) bool
```

isInf reports whether f is an infinity. 

### <a id="isNaN" href="#isNaN">func isNaN(f float64) (is bool)</a>

```
searchKey: runtime.isNaN
tags: [method private]
```

```Go
func isNaN(f float64) (is bool)
```

isNaN reports whether f is an IEEE 754 `not-a-number' value. 

### <a id="isPowerOfTwo" href="#isPowerOfTwo">func isPowerOfTwo(x uintptr) bool</a>

```
searchKey: runtime.isPowerOfTwo
tags: [method private]
```

```Go
func isPowerOfTwo(x uintptr) bool
```

### <a id="isShrinkStackSafe" href="#isShrinkStackSafe">func isShrinkStackSafe(gp *g) bool</a>

```
searchKey: runtime.isShrinkStackSafe
tags: [method private]
```

```Go
func isShrinkStackSafe(gp *g) bool
```

isShrinkStackSafe returns whether it's safe to attempt to shrink gp's stack. Shrinking the stack is only safe when we have precise pointer maps for all frames on the stack. 

### <a id="isSweepDone" href="#isSweepDone">func isSweepDone() bool</a>

```
searchKey: runtime.isSweepDone
tags: [function private]
```

```Go
func isSweepDone() bool
```

isSweepDone reports whether all spans are swept. 

Note that this condition may transition from false to true at any time as the sweeper runs. It may transition from true to false if a GC runs; to prevent that the caller must be non-preemptible or must somehow block GC progress. 

### <a id="isSystemGoroutine" href="#isSystemGoroutine">func isSystemGoroutine(gp *g, fixed bool) bool</a>

```
searchKey: runtime.isSystemGoroutine
tags: [method private]
```

```Go
func isSystemGoroutine(gp *g, fixed bool) bool
```

isSystemGoroutine reports whether the goroutine g must be omitted in stack dumps and deadlock detector. This is any goroutine that starts at a runtime.* entry point, except for runtime.main, runtime.handleAsyncEvent (wasm only) and sometimes runtime.runfinq. 

If fixed is true, any goroutine that can vary between user and system (that is, the finalizer goroutine) is considered a user goroutine. 

### <a id="itabAdd" href="#itabAdd">func itabAdd(m *itab)</a>

```
searchKey: runtime.itabAdd
tags: [method private]
```

```Go
func itabAdd(m *itab)
```

itabAdd adds the given itab to the itab hash table. itabLock must be held. 

### <a id="itabHashFunc" href="#itabHashFunc">func itabHashFunc(inter *interfacetype, typ *_type) uintptr</a>

```
searchKey: runtime.itabHashFunc
tags: [method private]
```

```Go
func itabHashFunc(inter *interfacetype, typ *_type) uintptr
```

### <a id="itab_callback" href="#itab_callback">func itab_callback(tab *itab)</a>

```
searchKey: runtime.itab_callback
tags: [method private]
```

```Go
func itab_callback(tab *itab)
```

### <a id="itabsinit" href="#itabsinit">func itabsinit()</a>

```
searchKey: runtime.itabsinit
tags: [function private]
```

```Go
func itabsinit()
```

### <a id="iterate_finq" href="#iterate_finq">func iterate_finq(callback func(*funcval, unsafe.Pointer, uintptr, *_type, *ptrtype))</a>

```
searchKey: runtime.iterate_finq
tags: [method private]
```

```Go
func iterate_finq(callback func(*funcval, unsafe.Pointer, uintptr, *_type, *ptrtype))
```

### <a id="iterate_itabs" href="#iterate_itabs">func iterate_itabs(fn func(*itab))</a>

```
searchKey: runtime.iterate_itabs
tags: [method private]
```

```Go
func iterate_itabs(fn func(*itab))
```

### <a id="iterate_memprof" href="#iterate_memprof">func iterate_memprof(fn func(*bucket, uintptr, *uintptr, uintptr, uintptr, uintptr))</a>

```
searchKey: runtime.iterate_memprof
tags: [method private]
```

```Go
func iterate_memprof(fn func(*bucket, uintptr, *uintptr, uintptr, uintptr, uintptr))
```

### <a id="itoa" href="#itoa">func itoa(buf []byte, val uint64) []byte</a>

```
searchKey: runtime.itoa
tags: [method private]
```

```Go
func itoa(buf []byte, val uint64) []byte
```

itoa converts val to a decimal representation. The result is written somewhere within buf and the location of the result is returned. buf must be at least 20 bytes. 

### <a id="itoaDiv" href="#itoaDiv">func itoaDiv(buf []byte, val uint64, dec int) []byte</a>

```
searchKey: runtime.itoaDiv
tags: [method private]
```

```Go
func itoaDiv(buf []byte, val uint64, dec int) []byte
```

itoaDiv formats val/(10**dec) into buf. 

### <a id="jmpdefer" href="#jmpdefer">func jmpdefer(fv *funcval, argp uintptr)</a>

```
searchKey: runtime.jmpdefer
tags: [method private]
```

```Go
func jmpdefer(fv *funcval, argp uintptr)
```

### <a id="kevent" href="#kevent">func kevent(kq int32, ch *keventt, nch int32, ev *keventt, nev int32, ts *timespec) int32</a>

```
searchKey: runtime.kevent
tags: [method private]
```

```Go
func kevent(kq int32, ch *keventt, nch int32, ev *keventt, nev int32, ts *timespec) int32
```

### <a id="kevent_trampoline" href="#kevent_trampoline">func kevent_trampoline()</a>

```
searchKey: runtime.kevent_trampoline
tags: [function private]
```

```Go
func kevent_trampoline()
```

### <a id="kqueue" href="#kqueue">func kqueue() int32</a>

```
searchKey: runtime.kqueue
tags: [function private]
```

```Go
func kqueue() int32
```

### <a id="kqueue_trampoline" href="#kqueue_trampoline">func kqueue_trampoline()</a>

```
searchKey: runtime.kqueue_trampoline
tags: [function private]
```

```Go
func kqueue_trampoline()
```

### <a id="less" href="#less">func less(a, b uint32) bool</a>

```
searchKey: runtime.less
tags: [method private]
```

```Go
func less(a, b uint32) bool
```

less checks if a < b, considering a & b running counts that may overflow the 32-bit range, and that their "unwrapped" difference is always less than 2^31. 

### <a id="lfnodeValidate" href="#lfnodeValidate">func lfnodeValidate(node *lfnode)</a>

```
searchKey: runtime.lfnodeValidate
tags: [method private]
```

```Go
func lfnodeValidate(node *lfnode)
```

lfnodeValidate panics if node is not a valid address for use with lfstack.push. This only needs to be called when node is allocated. 

### <a id="lfstackPack" href="#lfstackPack">func lfstackPack(node *lfnode, cnt uintptr) uint64</a>

```
searchKey: runtime.lfstackPack
tags: [method private]
```

```Go
func lfstackPack(node *lfnode, cnt uintptr) uint64
```

### <a id="libcCall" href="#libcCall">func libcCall(fn, arg unsafe.Pointer) int32</a>

```
searchKey: runtime.libcCall
tags: [method private]
```

```Go
func libcCall(fn, arg unsafe.Pointer) int32
```

Call fn with arg as its argument. Return what fn returns. fn is the raw pc value of the entry point of the desired function. Switches to the system stack, if not already there. Preserves the calling point as the location where a profiler traceback will begin. 

### <a id="libpreinit" href="#libpreinit">func libpreinit()</a>

```
searchKey: runtime.libpreinit
tags: [function private]
```

```Go
func libpreinit()
```

Called to do synchronous initialization of Go code built with -buildmode=c-archive or -buildmode=c-shared. None of the Go runtime is initialized. 

### <a id="lock" href="#lock">func lock(l *mutex)</a>

```
searchKey: runtime.lock
tags: [method private]
```

```Go
func lock(l *mutex)
```

### <a id="lock2" href="#lock2">func lock2(l *mutex)</a>

```
searchKey: runtime.lock2
tags: [method private]
```

```Go
func lock2(l *mutex)
```

### <a id="lockInit" href="#lockInit">func lockInit(l *mutex, rank lockRank)</a>

```
searchKey: runtime.lockInit
tags: [method private]
```

```Go
func lockInit(l *mutex, rank lockRank)
```

### <a id="lockOSThread" href="#lockOSThread">func lockOSThread()</a>

```
searchKey: runtime.lockOSThread
tags: [function private]
```

```Go
func lockOSThread()
```

### <a id="lockWithRank" href="#lockWithRank">func lockWithRank(l *mutex, rank lockRank)</a>

```
searchKey: runtime.lockWithRank
tags: [method private]
```

```Go
func lockWithRank(l *mutex, rank lockRank)
```

### <a id="lockWithRankMayAcquire" href="#lockWithRankMayAcquire">func lockWithRankMayAcquire(l *mutex, rank lockRank)</a>

```
searchKey: runtime.lockWithRankMayAcquire
tags: [method private]
```

```Go
func lockWithRankMayAcquire(l *mutex, rank lockRank)
```

### <a id="lockedOSThread" href="#lockedOSThread">func lockedOSThread() bool</a>

```
searchKey: runtime.lockedOSThread
tags: [function private]
```

```Go
func lockedOSThread() bool
```

### <a id="lowerASCII" href="#lowerASCII">func lowerASCII(c byte) byte</a>

```
searchKey: runtime.lowerASCII
tags: [method private]
```

```Go
func lowerASCII(c byte) byte
```

### <a id="mDoFixup" href="#mDoFixup">func mDoFixup() bool</a>

```
searchKey: runtime.mDoFixup
tags: [function private]
```

```Go
func mDoFixup() bool
```

mDoFixup runs any outstanding fixup function for the running m. Returns true if a fixup was outstanding and actually executed. 

Note: to avoid deadlocks, and the need for the fixup function itself to be async safe, signals are blocked for the working m while it holds the mFixup lock. (See golang.org/issue/44193) 

### <a id="mDoFixupAndOSYield" href="#mDoFixupAndOSYield">func mDoFixupAndOSYield()</a>

```
searchKey: runtime.mDoFixupAndOSYield
tags: [function private]
```

```Go
func mDoFixupAndOSYield()
```

mDoFixupAndOSYield is called when an m is unable to send a signal because the allThreadsSyscall mechanism is in progress. That is, an mPark() has been interrupted with this signal handler so we need to ensure the fixup is executed from this context. 

### <a id="mPark" href="#mPark">func mPark()</a>

```
searchKey: runtime.mPark
tags: [function private]
```

```Go
func mPark()
```

mPark causes a thread to park itself - temporarily waking for fixups but otherwise waiting to be fully woken. This is the only way that m's should park themselves. 

### <a id="mProf_Flush" href="#mProf_Flush">func mProf_Flush()</a>

```
searchKey: runtime.mProf_Flush
tags: [function private]
```

```Go
func mProf_Flush()
```

mProf_Flush flushes the events from the current heap profiling cycle into the active profile. After this it is safe to start a new heap profiling cycle with mProf_NextCycle. 

This is called by GC after mark termination starts the world. In contrast with mProf_NextCycle, this is somewhat expensive, but safe to do concurrently. 

### <a id="mProf_FlushLocked" href="#mProf_FlushLocked">func mProf_FlushLocked()</a>

```
searchKey: runtime.mProf_FlushLocked
tags: [function private]
```

```Go
func mProf_FlushLocked()
```

### <a id="mProf_Free" href="#mProf_Free">func mProf_Free(b *bucket, size uintptr)</a>

```
searchKey: runtime.mProf_Free
tags: [method private]
```

```Go
func mProf_Free(b *bucket, size uintptr)
```

Called when freeing a profiled block. 

### <a id="mProf_Malloc" href="#mProf_Malloc">func mProf_Malloc(p unsafe.Pointer, size uintptr)</a>

```
searchKey: runtime.mProf_Malloc
tags: [method private]
```

```Go
func mProf_Malloc(p unsafe.Pointer, size uintptr)
```

Called by malloc to record a profiled block. 

### <a id="mProf_NextCycle" href="#mProf_NextCycle">func mProf_NextCycle()</a>

```
searchKey: runtime.mProf_NextCycle
tags: [function private]
```

```Go
func mProf_NextCycle()
```

mProf_NextCycle publishes the next heap profile cycle and creates a fresh heap profile cycle. This operation is fast and can be done during STW. The caller must call mProf_Flush before calling mProf_NextCycle again. 

This is called by mark termination during STW so allocations and frees after the world is started again count towards a new heap profiling cycle. 

### <a id="mProf_PostSweep" href="#mProf_PostSweep">func mProf_PostSweep()</a>

```
searchKey: runtime.mProf_PostSweep
tags: [function private]
```

```Go
func mProf_PostSweep()
```

mProf_PostSweep records that all sweep frees for this GC cycle have completed. This has the effect of publishing the heap profile snapshot as of the last mark termination without advancing the heap profile cycle. 

### <a id="mReserveID" href="#mReserveID">func mReserveID() int64</a>

```
searchKey: runtime.mReserveID
tags: [function private]
```

```Go
func mReserveID() int64
```

mReserveID returns the next ID to use for a new m. This new m is immediately considered 'running' by checkdead. 

sched.lock must be held. 

### <a id="mStackIsSystemAllocated" href="#mStackIsSystemAllocated">func mStackIsSystemAllocated() bool</a>

```
searchKey: runtime.mStackIsSystemAllocated
tags: [function private]
```

```Go
func mStackIsSystemAllocated() bool
```

mStackIsSystemAllocated indicates whether this runtime starts on a system-allocated stack. 

### <a id="madvise" href="#madvise">func madvise(addr unsafe.Pointer, n uintptr, flags int32)</a>

```
searchKey: runtime.madvise
tags: [method private]
```

```Go
func madvise(addr unsafe.Pointer, n uintptr, flags int32)
```

### <a id="madvise_trampoline" href="#madvise_trampoline">func madvise_trampoline()</a>

```
searchKey: runtime.madvise_trampoline
tags: [function private]
```

```Go
func madvise_trampoline()
```

### <a id="main" href="#main">func main()</a>

```
searchKey: runtime.main
tags: [function private]
```

```Go
func main()
```

The main goroutine. 

### <a id="main_main" href="#main_main">func main_main()</a>

```
searchKey: runtime.main_main
tags: [function private]
```

```Go
func main_main()
```

### <a id="makeslice" href="#makeslice">func makeslice(et *_type, len, cap int) unsafe.Pointer</a>

```
searchKey: runtime.makeslice
tags: [method private]
```

```Go
func makeslice(et *_type, len, cap int) unsafe.Pointer
```

### <a id="makeslice64" href="#makeslice64">func makeslice64(et *_type, len64, cap64 int64) unsafe.Pointer</a>

```
searchKey: runtime.makeslice64
tags: [method private]
```

```Go
func makeslice64(et *_type, len64, cap64 int64) unsafe.Pointer
```

### <a id="makeslicecopy" href="#makeslicecopy">func makeslicecopy(et *_type, tolen int, fromlen int, from unsafe.Pointer) unsafe.Pointer</a>

```
searchKey: runtime.makeslicecopy
tags: [method private]
```

```Go
func makeslicecopy(et *_type, tolen int, fromlen int, from unsafe.Pointer) unsafe.Pointer
```

makeslicecopy allocates a slice of "tolen" elements of type "et", then copies "fromlen" elements of type "et" into that new allocation from "from". 

### <a id="mallocgc" href="#mallocgc">func mallocgc(size uintptr, typ *_type, needzero bool) unsafe.Pointer</a>

```
searchKey: runtime.mallocgc
tags: [method private]
```

```Go
func mallocgc(size uintptr, typ *_type, needzero bool) unsafe.Pointer
```

Allocate an object of size bytes. Small objects are allocated from the per-P cache's free lists. Large objects (> 32 kB) are allocated straight from the heap. 

### <a id="mallocinit" href="#mallocinit">func mallocinit()</a>

```
searchKey: runtime.mallocinit
tags: [function private]
```

```Go
func mallocinit()
```

### <a id="mapaccess1" href="#mapaccess1">func mapaccess1(t *maptype, h *hmap, key unsafe.Pointer) unsafe.Pointer</a>

```
searchKey: runtime.mapaccess1
tags: [method private]
```

```Go
func mapaccess1(t *maptype, h *hmap, key unsafe.Pointer) unsafe.Pointer
```

mapaccess1 returns a pointer to h[key].  Never returns nil, instead it will return a reference to the zero object for the elem type if the key is not in the map. NOTE: The returned pointer may keep the whole map live, so don't hold onto it for very long. 

### <a id="mapaccess1_fast32" href="#mapaccess1_fast32">func mapaccess1_fast32(t *maptype, h *hmap, key uint32) unsafe.Pointer</a>

```
searchKey: runtime.mapaccess1_fast32
tags: [method private]
```

```Go
func mapaccess1_fast32(t *maptype, h *hmap, key uint32) unsafe.Pointer
```

### <a id="mapaccess1_fast64" href="#mapaccess1_fast64">func mapaccess1_fast64(t *maptype, h *hmap, key uint64) unsafe.Pointer</a>

```
searchKey: runtime.mapaccess1_fast64
tags: [method private]
```

```Go
func mapaccess1_fast64(t *maptype, h *hmap, key uint64) unsafe.Pointer
```

### <a id="mapaccess1_faststr" href="#mapaccess1_faststr">func mapaccess1_faststr(t *maptype, h *hmap, ky string) unsafe.Pointer</a>

```
searchKey: runtime.mapaccess1_faststr
tags: [method private]
```

```Go
func mapaccess1_faststr(t *maptype, h *hmap, ky string) unsafe.Pointer
```

### <a id="mapaccess1_fat" href="#mapaccess1_fat">func mapaccess1_fat(t *maptype, h *hmap, key, zero unsafe.Pointer) unsafe.Pointer</a>

```
searchKey: runtime.mapaccess1_fat
tags: [method private]
```

```Go
func mapaccess1_fat(t *maptype, h *hmap, key, zero unsafe.Pointer) unsafe.Pointer
```

### <a id="mapaccess2" href="#mapaccess2">func mapaccess2(t *maptype, h *hmap, key unsafe.Pointer) (unsafe.Pointer, bool)</a>

```
searchKey: runtime.mapaccess2
tags: [method private]
```

```Go
func mapaccess2(t *maptype, h *hmap, key unsafe.Pointer) (unsafe.Pointer, bool)
```

### <a id="mapaccess2_fast32" href="#mapaccess2_fast32">func mapaccess2_fast32(t *maptype, h *hmap, key uint32) (unsafe.Pointer, bool)</a>

```
searchKey: runtime.mapaccess2_fast32
tags: [method private]
```

```Go
func mapaccess2_fast32(t *maptype, h *hmap, key uint32) (unsafe.Pointer, bool)
```

### <a id="mapaccess2_fast64" href="#mapaccess2_fast64">func mapaccess2_fast64(t *maptype, h *hmap, key uint64) (unsafe.Pointer, bool)</a>

```
searchKey: runtime.mapaccess2_fast64
tags: [method private]
```

```Go
func mapaccess2_fast64(t *maptype, h *hmap, key uint64) (unsafe.Pointer, bool)
```

### <a id="mapaccess2_faststr" href="#mapaccess2_faststr">func mapaccess2_faststr(t *maptype, h *hmap, ky string) (unsafe.Pointer, bool)</a>

```
searchKey: runtime.mapaccess2_faststr
tags: [method private]
```

```Go
func mapaccess2_faststr(t *maptype, h *hmap, ky string) (unsafe.Pointer, bool)
```

### <a id="mapaccess2_fat" href="#mapaccess2_fat">func mapaccess2_fat(t *maptype, h *hmap, key, zero unsafe.Pointer) (unsafe.Pointer, bool)</a>

```
searchKey: runtime.mapaccess2_fat
tags: [method private]
```

```Go
func mapaccess2_fat(t *maptype, h *hmap, key, zero unsafe.Pointer) (unsafe.Pointer, bool)
```

### <a id="mapaccessK" href="#mapaccessK">func mapaccessK(t *maptype, h *hmap, key unsafe.Pointer) (unsafe.Pointer, unsafe.Pointer)</a>

```
searchKey: runtime.mapaccessK
tags: [method private]
```

```Go
func mapaccessK(t *maptype, h *hmap, key unsafe.Pointer) (unsafe.Pointer, unsafe.Pointer)
```

returns both key and elem. Used by map iterator 

### <a id="mapassign" href="#mapassign">func mapassign(t *maptype, h *hmap, key unsafe.Pointer) unsafe.Pointer</a>

```
searchKey: runtime.mapassign
tags: [method private]
```

```Go
func mapassign(t *maptype, h *hmap, key unsafe.Pointer) unsafe.Pointer
```

Like mapaccess, but allocates a slot for the key if it is not present in the map. 

### <a id="mapassign_fast32" href="#mapassign_fast32">func mapassign_fast32(t *maptype, h *hmap, key uint32) unsafe.Pointer</a>

```
searchKey: runtime.mapassign_fast32
tags: [method private]
```

```Go
func mapassign_fast32(t *maptype, h *hmap, key uint32) unsafe.Pointer
```

### <a id="mapassign_fast32ptr" href="#mapassign_fast32ptr">func mapassign_fast32ptr(t *maptype, h *hmap, key unsafe.Pointer) unsafe.Pointer</a>

```
searchKey: runtime.mapassign_fast32ptr
tags: [method private]
```

```Go
func mapassign_fast32ptr(t *maptype, h *hmap, key unsafe.Pointer) unsafe.Pointer
```

### <a id="mapassign_fast64" href="#mapassign_fast64">func mapassign_fast64(t *maptype, h *hmap, key uint64) unsafe.Pointer</a>

```
searchKey: runtime.mapassign_fast64
tags: [method private]
```

```Go
func mapassign_fast64(t *maptype, h *hmap, key uint64) unsafe.Pointer
```

### <a id="mapassign_fast64ptr" href="#mapassign_fast64ptr">func mapassign_fast64ptr(t *maptype, h *hmap, key unsafe.Pointer) unsafe.Pointer</a>

```
searchKey: runtime.mapassign_fast64ptr
tags: [method private]
```

```Go
func mapassign_fast64ptr(t *maptype, h *hmap, key unsafe.Pointer) unsafe.Pointer
```

### <a id="mapassign_faststr" href="#mapassign_faststr">func mapassign_faststr(t *maptype, h *hmap, s string) unsafe.Pointer</a>

```
searchKey: runtime.mapassign_faststr
tags: [method private]
```

```Go
func mapassign_faststr(t *maptype, h *hmap, s string) unsafe.Pointer
```

### <a id="mapclear" href="#mapclear">func mapclear(t *maptype, h *hmap)</a>

```
searchKey: runtime.mapclear
tags: [method private]
```

```Go
func mapclear(t *maptype, h *hmap)
```

mapclear deletes all keys from a map. 

### <a id="mapdelete" href="#mapdelete">func mapdelete(t *maptype, h *hmap, key unsafe.Pointer)</a>

```
searchKey: runtime.mapdelete
tags: [method private]
```

```Go
func mapdelete(t *maptype, h *hmap, key unsafe.Pointer)
```

### <a id="mapdelete_fast32" href="#mapdelete_fast32">func mapdelete_fast32(t *maptype, h *hmap, key uint32)</a>

```
searchKey: runtime.mapdelete_fast32
tags: [method private]
```

```Go
func mapdelete_fast32(t *maptype, h *hmap, key uint32)
```

### <a id="mapdelete_fast64" href="#mapdelete_fast64">func mapdelete_fast64(t *maptype, h *hmap, key uint64)</a>

```
searchKey: runtime.mapdelete_fast64
tags: [method private]
```

```Go
func mapdelete_fast64(t *maptype, h *hmap, key uint64)
```

### <a id="mapdelete_faststr" href="#mapdelete_faststr">func mapdelete_faststr(t *maptype, h *hmap, ky string)</a>

```
searchKey: runtime.mapdelete_faststr
tags: [method private]
```

```Go
func mapdelete_faststr(t *maptype, h *hmap, ky string)
```

### <a id="mapiterinit" href="#mapiterinit">func mapiterinit(t *maptype, h *hmap, it *hiter)</a>

```
searchKey: runtime.mapiterinit
tags: [method private]
```

```Go
func mapiterinit(t *maptype, h *hmap, it *hiter)
```

mapiterinit initializes the hiter struct used for ranging over maps. The hiter struct pointed to by 'it' is allocated on the stack by the compilers order pass or on the heap by reflect_mapiterinit. Both need to have zeroed hiter since the struct contains pointers. 

### <a id="mapiternext" href="#mapiternext">func mapiternext(it *hiter)</a>

```
searchKey: runtime.mapiternext
tags: [method private]
```

```Go
func mapiternext(it *hiter)
```

### <a id="markroot" href="#markroot">func markroot(gcw *gcWork, i uint32)</a>

```
searchKey: runtime.markroot
tags: [method private]
```

```Go
func markroot(gcw *gcWork, i uint32)
```

markroot scans the i'th root. 

Preemption must be disabled (because this uses a gcWork). 

nowritebarrier is only advisory here. 

### <a id="markrootBlock" href="#markrootBlock">func markrootBlock(b0, n0 uintptr, ptrmask0 *uint8, gcw *gcWork, shard int)</a>

```
searchKey: runtime.markrootBlock
tags: [method private]
```

```Go
func markrootBlock(b0, n0 uintptr, ptrmask0 *uint8, gcw *gcWork, shard int)
```

markrootBlock scans the shard'th shard of the block of memory [b0, b0+n0), with the given pointer mask. 

### <a id="markrootFreeGStacks" href="#markrootFreeGStacks">func markrootFreeGStacks()</a>

```
searchKey: runtime.markrootFreeGStacks
tags: [function private]
```

```Go
func markrootFreeGStacks()
```

markrootFreeGStacks frees stacks of dead Gs. 

This does not free stacks of dead Gs cached on Ps, but having a few cached stacks around isn't a problem. 

### <a id="markrootSpans" href="#markrootSpans">func markrootSpans(gcw *gcWork, shard int)</a>

```
searchKey: runtime.markrootSpans
tags: [method private]
```

```Go
func markrootSpans(gcw *gcWork, shard int)
```

markrootSpans marks roots for one shard of markArenas. 

### <a id="mcall" href="#mcall">func mcall(fn func(*g))</a>

```
searchKey: runtime.mcall
tags: [method private]
```

```Go
func mcall(fn func(*g))
```

mcall switches from the g to the g0 stack and invokes fn(g), where g is the goroutine that made the call. mcall saves g's current PC/SP in g->sched so that it can be restored later. It is up to fn to arrange for that later execution, typically by recording g in a data structure, causing something to call ready(g) later. mcall returns to the original goroutine g later, when g has been rescheduled. fn must not return at all; typically it ends by calling schedule, to let the m run other goroutines. 

mcall can only be called from g stacks (not g0, not gsignal). 

This must NOT be go:noescape: if fn is a stack-allocated closure, fn puts g on a run queue, and g executes before fn returns, the closure will be invalidated while it is still executing. 

### <a id="mcommoninit" href="#mcommoninit">func mcommoninit(mp *m, id int64)</a>

```
searchKey: runtime.mcommoninit
tags: [method private]
```

```Go
func mcommoninit(mp *m, id int64)
```

Pre-allocated ID may be passed as 'id', or omitted by passing -1. 

### <a id="mcount" href="#mcount">func mcount() int32</a>

```
searchKey: runtime.mcount
tags: [function private]
```

```Go
func mcount() int32
```

### <a id="mdestroy" href="#mdestroy">func mdestroy(mp *m)</a>

```
searchKey: runtime.mdestroy
tags: [method private]
```

```Go
func mdestroy(mp *m)
```

Called from exitm, but not from drop, to undo the effect of thread-owned resources in minit, semacreate, or elsewhere. Do not take locks after calling this. 

### <a id="mdump" href="#mdump">func mdump(m *MemStats)</a>

```
searchKey: runtime.mdump
tags: [method private]
```

```Go
func mdump(m *MemStats)
```

### <a id="memclrHasPointers" href="#memclrHasPointers">func memclrHasPointers(ptr unsafe.Pointer, n uintptr)</a>

```
searchKey: runtime.memclrHasPointers
tags: [method private]
```

```Go
func memclrHasPointers(ptr unsafe.Pointer, n uintptr)
```

memclrHasPointers clears n bytes of typed memory starting at ptr. The caller must ensure that the type of the object at ptr has pointers, usually by checking typ.ptrdata. However, ptr does not have to point to the start of the allocation. 

### <a id="memclrNoHeapPointers" href="#memclrNoHeapPointers">func memclrNoHeapPointers(ptr unsafe.Pointer, n uintptr)</a>

```
searchKey: runtime.memclrNoHeapPointers
tags: [method private]
```

```Go
func memclrNoHeapPointers(ptr unsafe.Pointer, n uintptr)
```

memclrNoHeapPointers clears n bytes starting at ptr. 

Usually you should use typedmemclr. memclrNoHeapPointers should be used only when the caller knows that *ptr contains no heap pointers because either: 

*ptr is initialized memory and its type is pointer-free, or 

*ptr is uninitialized memory (e.g., memory that's being reused for a new allocation) and hence contains only "junk". 

memclrNoHeapPointers ensures that if ptr is pointer-aligned, and n is a multiple of the pointer size, then any pointer-aligned, pointer-sized portion is cleared atomically. Despite the function name, this is necessary because this function is the underlying implementation of typedmemclr and memclrHasPointers. See the doc of memmove for more details. 

The (CPU-specific) implementations of this function are in memclr_*.s. 

### <a id="memclrNoHeapPointersChunked" href="#memclrNoHeapPointersChunked">func memclrNoHeapPointersChunked(size uintptr, x unsafe.Pointer)</a>

```
searchKey: runtime.memclrNoHeapPointersChunked
tags: [method private]
```

```Go
func memclrNoHeapPointersChunked(size uintptr, x unsafe.Pointer)
```

memclrNoHeapPointersChunked repeatedly calls memclrNoHeapPointers on chunks of the buffer to be zeroed, with opportunities for preemption along the way.  memclrNoHeapPointers contains no safepoints and also cannot be preemptively scheduled, so this provides a still-efficient block copy that can also be preempted on a reasonable granularity. 

Use this with care; if the data being cleared is tagged to contain pointers, this allows the GC to run before it is all cleared. 

### <a id="memequal" href="#memequal">func memequal(a, b unsafe.Pointer, size uintptr) bool</a>

```
searchKey: runtime.memequal
tags: [method private]
```

```Go
func memequal(a, b unsafe.Pointer, size uintptr) bool
```

in internal/bytealg/equal_*.s 

### <a id="memequal0" href="#memequal0">func memequal0(p, q unsafe.Pointer) bool</a>

```
searchKey: runtime.memequal0
tags: [method private]
```

```Go
func memequal0(p, q unsafe.Pointer) bool
```

### <a id="memequal128" href="#memequal128">func memequal128(p, q unsafe.Pointer) bool</a>

```
searchKey: runtime.memequal128
tags: [method private]
```

```Go
func memequal128(p, q unsafe.Pointer) bool
```

### <a id="memequal16" href="#memequal16">func memequal16(p, q unsafe.Pointer) bool</a>

```
searchKey: runtime.memequal16
tags: [method private]
```

```Go
func memequal16(p, q unsafe.Pointer) bool
```

### <a id="memequal32" href="#memequal32">func memequal32(p, q unsafe.Pointer) bool</a>

```
searchKey: runtime.memequal32
tags: [method private]
```

```Go
func memequal32(p, q unsafe.Pointer) bool
```

### <a id="memequal64" href="#memequal64">func memequal64(p, q unsafe.Pointer) bool</a>

```
searchKey: runtime.memequal64
tags: [method private]
```

```Go
func memequal64(p, q unsafe.Pointer) bool
```

### <a id="memequal8" href="#memequal8">func memequal8(p, q unsafe.Pointer) bool</a>

```
searchKey: runtime.memequal8
tags: [method private]
```

```Go
func memequal8(p, q unsafe.Pointer) bool
```

### <a id="memequal_varlen" href="#memequal_varlen">func memequal_varlen(a, b unsafe.Pointer) bool</a>

```
searchKey: runtime.memequal_varlen
tags: [method private]
```

```Go
func memequal_varlen(a, b unsafe.Pointer) bool
```

### <a id="memhash" href="#memhash">func memhash(p unsafe.Pointer, h, s uintptr) uintptr</a>

```
searchKey: runtime.memhash
tags: [method private]
```

```Go
func memhash(p unsafe.Pointer, h, s uintptr) uintptr
```

in asm_*.s 

### <a id="memhash0" href="#memhash0">func memhash0(p unsafe.Pointer, h uintptr) uintptr</a>

```
searchKey: runtime.memhash0
tags: [method private]
```

```Go
func memhash0(p unsafe.Pointer, h uintptr) uintptr
```

### <a id="memhash128" href="#memhash128">func memhash128(p unsafe.Pointer, h uintptr) uintptr</a>

```
searchKey: runtime.memhash128
tags: [method private]
```

```Go
func memhash128(p unsafe.Pointer, h uintptr) uintptr
```

### <a id="memhash16" href="#memhash16">func memhash16(p unsafe.Pointer, h uintptr) uintptr</a>

```
searchKey: runtime.memhash16
tags: [method private]
```

```Go
func memhash16(p unsafe.Pointer, h uintptr) uintptr
```

### <a id="memhash32" href="#memhash32">func memhash32(p unsafe.Pointer, h uintptr) uintptr</a>

```
searchKey: runtime.memhash32
tags: [method private]
```

```Go
func memhash32(p unsafe.Pointer, h uintptr) uintptr
```

### <a id="memhash32Fallback" href="#memhash32Fallback">func memhash32Fallback(p unsafe.Pointer, seed uintptr) uintptr</a>

```
searchKey: runtime.memhash32Fallback
tags: [method private]
```

```Go
func memhash32Fallback(p unsafe.Pointer, seed uintptr) uintptr
```

### <a id="memhash64" href="#memhash64">func memhash64(p unsafe.Pointer, h uintptr) uintptr</a>

```
searchKey: runtime.memhash64
tags: [method private]
```

```Go
func memhash64(p unsafe.Pointer, h uintptr) uintptr
```

### <a id="memhash64Fallback" href="#memhash64Fallback">func memhash64Fallback(p unsafe.Pointer, seed uintptr) uintptr</a>

```
searchKey: runtime.memhash64Fallback
tags: [method private]
```

```Go
func memhash64Fallback(p unsafe.Pointer, seed uintptr) uintptr
```

### <a id="memhash8" href="#memhash8">func memhash8(p unsafe.Pointer, h uintptr) uintptr</a>

```
searchKey: runtime.memhash8
tags: [method private]
```

```Go
func memhash8(p unsafe.Pointer, h uintptr) uintptr
```

### <a id="memhashFallback" href="#memhashFallback">func memhashFallback(p unsafe.Pointer, seed, s uintptr) uintptr</a>

```
searchKey: runtime.memhashFallback
tags: [method private]
```

```Go
func memhashFallback(p unsafe.Pointer, seed, s uintptr) uintptr
```

### <a id="memhash_varlen" href="#memhash_varlen">func memhash_varlen(p unsafe.Pointer, h uintptr) uintptr</a>

```
searchKey: runtime.memhash_varlen
tags: [method private]
```

```Go
func memhash_varlen(p unsafe.Pointer, h uintptr) uintptr
```

### <a id="memmove" href="#memmove">func memmove(to, from unsafe.Pointer, n uintptr)</a>

```
searchKey: runtime.memmove
tags: [method private]
```

```Go
func memmove(to, from unsafe.Pointer, n uintptr)
```

memmove copies n bytes from "from" to "to". 

memmove ensures that any pointer in "from" is written to "to" with an indivisible write, so that racy reads cannot observe a half-written pointer. This is necessary to prevent the garbage collector from observing invalid pointers, and differs from memmove in unmanaged languages. However, memmove is only required to do this if "from" and "to" may contain pointers, which can only be the case if "from", "to", and "n" are all be word-aligned. 

Implementations are in memmove_*.s. 

### <a id="mexit" href="#mexit">func mexit(osStack bool)</a>

```
searchKey: runtime.mexit
tags: [method private]
```

```Go
func mexit(osStack bool)
```

mexit tears down and exits the current thread. 

Don't call this directly to exit the thread, since it must run at the top of the thread stack. Instead, use gogo(&_g_.m.g0.sched) to unwind the stack to the point that exits the thread. 

It is entered with m.p != nil, so write barriers are allowed. It will release the P before exiting. 

### <a id="minit" href="#minit">func minit()</a>

```
searchKey: runtime.minit
tags: [function private]
```

```Go
func minit()
```

Called to initialize a new m (including the bootstrap m). Called on the new thread, cannot allocate memory. 

### <a id="minitSignalMask" href="#minitSignalMask">func minitSignalMask()</a>

```
searchKey: runtime.minitSignalMask
tags: [function private]
```

```Go
func minitSignalMask()
```

minitSignalMask is called when initializing a new m to set the thread's signal mask. When this is called all signals have been blocked for the thread.  This starts with m.sigmask, which was set either from initSigmask for a newly created thread or by calling sigsave if this is a non-Go thread calling a Go function. It removes all essential signals from the mask, thus causing those signals to not be blocked. Then it sets the thread's signal mask. After this is called the thread can receive signals. 

### <a id="minitSignalStack" href="#minitSignalStack">func minitSignalStack()</a>

```
searchKey: runtime.minitSignalStack
tags: [function private]
```

```Go
func minitSignalStack()
```

minitSignalStack is called when initializing a new m to set the alternate signal stack. If the alternate signal stack is not set for the thread (the normal case) then set the alternate signal stack to the gsignal stack. If the alternate signal stack is set for the thread (the case when a non-Go thread sets the alternate signal stack and then calls a Go function) then set the gsignal stack to the alternate signal stack. We also set the alternate signal stack to the gsignal stack if cgo is not used (regardless of whether it is already set). Record which choice was made in newSigstack, so that it can be undone in unminit. 

### <a id="minitSignals" href="#minitSignals">func minitSignals()</a>

```
searchKey: runtime.minitSignals
tags: [function private]
```

```Go
func minitSignals()
```

minitSignals is called when initializing a new m to set the thread's alternate signal stack and signal mask. 

### <a id="mix" href="#mix">func mix(a, b uintptr) uintptr</a>

```
searchKey: runtime.mix
tags: [method private]
```

```Go
func mix(a, b uintptr) uintptr
```

### <a id="mlock" href="#mlock">func mlock(addr unsafe.Pointer, n uintptr)</a>

```
searchKey: runtime.mlock
tags: [method private]
```

```Go
func mlock(addr unsafe.Pointer, n uintptr)
```

### <a id="mlock_trampoline" href="#mlock_trampoline">func mlock_trampoline()</a>

```
searchKey: runtime.mlock_trampoline
tags: [function private]
```

```Go
func mlock_trampoline()
```

### <a id="mmap" href="#mmap">func mmap(addr unsafe.Pointer, n uintptr, prot, flags, fd int32, off uint32) (unsafe.Pointer, int)</a>

```
searchKey: runtime.mmap
tags: [method private]
```

```Go
func mmap(addr unsafe.Pointer, n uintptr, prot, flags, fd int32, off uint32) (unsafe.Pointer, int)
```

mmap is used to do low-level memory allocation via mmap. Don't allow stack splits, since this function (used by sysAlloc) is called in a lot of low-level parts of the runtime and callers often assume it won't acquire any locks. go:nosplit 

### <a id="mmap_trampoline" href="#mmap_trampoline">func mmap_trampoline()</a>

```
searchKey: runtime.mmap_trampoline
tags: [function private]
```

```Go
func mmap_trampoline()
```

### <a id="modTimer" href="#modTimer">func modTimer(t *timer, when, period int64, f func(interface{}, uintptr), arg interface{}, seq uintptr)</a>

```
searchKey: runtime.modTimer
tags: [method private]
```

```Go
func modTimer(t *timer, when, period int64, f func(interface{}, uintptr), arg interface{}, seq uintptr)
```

modTimer modifies an existing timer. 

### <a id="modtimer" href="#modtimer">func modtimer(t *timer, when, period int64, f func(interface{}, uintptr), arg interface{}, seq uintptr) bool</a>

```
searchKey: runtime.modtimer
tags: [method private]
```

```Go
func modtimer(t *timer, when, period int64, f func(interface{}, uintptr), arg interface{}, seq uintptr) bool
```

modtimer modifies an existing timer. This is called by the netpoll code or time.Ticker.Reset or time.Timer.Reset. Reports whether the timer was modified before it was run. 

### <a id="moduledataverify" href="#moduledataverify">func moduledataverify()</a>

```
searchKey: runtime.moduledataverify
tags: [function private]
```

```Go
func moduledataverify()
```

### <a id="moduledataverify1" href="#moduledataverify1">func moduledataverify1(datap *moduledata)</a>

```
searchKey: runtime.moduledataverify1
tags: [method private]
```

```Go
func moduledataverify1(datap *moduledata)
```

### <a id="modulesinit" href="#modulesinit">func modulesinit()</a>

```
searchKey: runtime.modulesinit
tags: [function private]
```

```Go
func modulesinit()
```

modulesinit creates the active modules slice out of all loaded modules. 

When a module is first loaded by the dynamic linker, an .init_array function (written by cmd/link) is invoked to call addmoduledata, appending to the module to the linked list that starts with firstmoduledata. 

There are two times this can happen in the lifecycle of a Go program. First, if compiled with -linkshared, a number of modules built with -buildmode=shared can be loaded at program initialization. Second, a Go program can load a module while running that was built with -buildmode=plugin. 

After loading, this function is called which initializes the moduledata so it is usable by the GC and creates a new activeModules list. 

Only one goroutine may call modulesinit at a time. 

### <a id="morestack" href="#morestack">func morestack()</a>

```
searchKey: runtime.morestack
tags: [function private]
```

```Go
func morestack()
```

### <a id="morestack_noctxt" href="#morestack_noctxt">func morestack_noctxt()</a>

```
searchKey: runtime.morestack_noctxt
tags: [function private]
```

```Go
func morestack_noctxt()
```

### <a id="morestackc" href="#morestackc">func morestackc()</a>

```
searchKey: runtime.morestackc
tags: [function private]
```

```Go
func morestackc()
```

This is exported as ABI0 via linkname so obj can call it. 

### <a id="moveTimers" href="#moveTimers">func moveTimers(pp *p, timers []*timer)</a>

```
searchKey: runtime.moveTimers
tags: [method private]
```

```Go
func moveTimers(pp *p, timers []*timer)
```

moveTimers moves a slice of timers to pp. The slice has been taken from a different P. This is currently called when the world is stopped, but the caller is expected to have locked the timers for pp. 

### <a id="mpreinit" href="#mpreinit">func mpreinit(mp *m)</a>

```
searchKey: runtime.mpreinit
tags: [method private]
```

```Go
func mpreinit(mp *m)
```

Called to initialize a new m (including the bootstrap m). Called on the parent thread (main thread in case of bootstrap), can allocate memory. 

### <a id="mput" href="#mput">func mput(mp *m)</a>

```
searchKey: runtime.mput
tags: [method private]
```

```Go
func mput(mp *m)
```

Put mp on midle list. sched.lock must be held. May run during STW, so write barriers are not allowed. 

### <a id="msanfree" href="#msanfree">func msanfree(addr unsafe.Pointer, sz uintptr)</a>

```
searchKey: runtime.msanfree
tags: [method private]
```

```Go
func msanfree(addr unsafe.Pointer, sz uintptr)
```

### <a id="msanmalloc" href="#msanmalloc">func msanmalloc(addr unsafe.Pointer, sz uintptr)</a>

```
searchKey: runtime.msanmalloc
tags: [method private]
```

```Go
func msanmalloc(addr unsafe.Pointer, sz uintptr)
```

### <a id="msanmove" href="#msanmove">func msanmove(dst, src unsafe.Pointer, sz uintptr)</a>

```
searchKey: runtime.msanmove
tags: [method private]
```

```Go
func msanmove(dst, src unsafe.Pointer, sz uintptr)
```

### <a id="msanread" href="#msanread">func msanread(addr unsafe.Pointer, sz uintptr)</a>

```
searchKey: runtime.msanread
tags: [method private]
```

```Go
func msanread(addr unsafe.Pointer, sz uintptr)
```

### <a id="msanwrite" href="#msanwrite">func msanwrite(addr unsafe.Pointer, sz uintptr)</a>

```
searchKey: runtime.msanwrite
tags: [method private]
```

```Go
func msanwrite(addr unsafe.Pointer, sz uintptr)
```

### <a id="msigrestore" href="#msigrestore">func msigrestore(sigmask sigset)</a>

```
searchKey: runtime.msigrestore
tags: [method private]
```

```Go
func msigrestore(sigmask sigset)
```

msigrestore sets the current thread's signal mask to sigmask. This is used to restore the non-Go signal mask when a non-Go thread calls a Go function. This is nosplit and nowritebarrierrec because it is called by dropm after g has been cleared. 

### <a id="mspinning" href="#mspinning">func mspinning()</a>

```
searchKey: runtime.mspinning
tags: [function private]
```

```Go
func mspinning()
```

### <a id="mstart" href="#mstart">func mstart()</a>

```
searchKey: runtime.mstart
tags: [function private]
```

```Go
func mstart()
```

mstart is the entry-point for new Ms. It is written in assembly, uses ABI0, is marked TOPFRAME, and calls mstart0. 

### <a id="mstart0" href="#mstart0">func mstart0()</a>

```
searchKey: runtime.mstart0
tags: [function private]
```

```Go
func mstart0()
```

mstart0 is the Go entry-point for new Ms. This must not split the stack because we may not even have stack bounds set up yet. 

May run during STW (because it doesn't have a P yet), so write barriers are not allowed. 

### <a id="mstart1" href="#mstart1">func mstart1()</a>

```
searchKey: runtime.mstart1
tags: [function private]
```

```Go
func mstart1()
```

The go:noinline is to guarantee the getcallerpc/getcallersp below are safe, so that we can set up g0.sched to return to the call of mstart1 above. 

### <a id="mstart_stub" href="#mstart_stub">func mstart_stub()</a>

```
searchKey: runtime.mstart_stub
tags: [function private]
```

```Go
func mstart_stub()
```

glue code to call mstart from pthread_create. 

### <a id="mstartm0" href="#mstartm0">func mstartm0()</a>

```
searchKey: runtime.mstartm0
tags: [function private]
```

```Go
func mstartm0()
```

mstartm0 implements part of mstart1 that only runs on the m0. 

Write barriers are allowed here because we know the GC can't be running yet, so they'll be no-ops. 

### <a id="mullu" href="#mullu">func mullu(u, v uint64) (lo, hi uint64)</a>

```
searchKey: runtime.mullu
tags: [method private]
```

```Go
func mullu(u, v uint64) (lo, hi uint64)
```

64x64 -> 128 multiply. adapted from hacker's delight. 

### <a id="munmap" href="#munmap">func munmap(addr unsafe.Pointer, n uintptr)</a>

```
searchKey: runtime.munmap
tags: [method private]
```

```Go
func munmap(addr unsafe.Pointer, n uintptr)
```

### <a id="munmap_trampoline" href="#munmap_trampoline">func munmap_trampoline()</a>

```
searchKey: runtime.munmap_trampoline
tags: [function private]
```

```Go
func munmap_trampoline()
```

### <a id="mutexevent" href="#mutexevent">func mutexevent(cycles int64, skip int)</a>

```
searchKey: runtime.mutexevent
tags: [method private]
```

```Go
func mutexevent(cycles int64, skip int)
```

### <a id="nanotime" href="#nanotime">func nanotime() int64</a>

```
searchKey: runtime.nanotime
tags: [function private]
```

```Go
func nanotime() int64
```

### <a id="nanotime1" href="#nanotime1">func nanotime1() int64</a>

```
searchKey: runtime.nanotime1
tags: [function private]
```

```Go
func nanotime1() int64
```

### <a id="nanotime_trampoline" href="#nanotime_trampoline">func nanotime_trampoline()</a>

```
searchKey: runtime.nanotime_trampoline
tags: [function private]
```

```Go
func nanotime_trampoline()
```

### <a id="needm" href="#needm">func needm()</a>

```
searchKey: runtime.needm
tags: [function private]
```

```Go
func needm()
```

needm is called when a cgo callback happens on a thread without an m (a thread not created by Go). In this case, needm is expected to find an m to use and return with m, g initialized correctly. Since m and g are not set now (likely nil, but see below) needm is limited in what routines it can call. In particular it can only call nosplit functions (textflag 7) and cannot do any scheduling that requires an m. 

In order to avoid needing heavy lifting here, we adopt the following strategy: there is a stack of available m's that can be stolen. Using compare-and-swap to pop from the stack has ABA races, so we simulate a lock by doing an exchange (via Casuintptr) to steal the stack head and replace the top pointer with MLOCKED (1). This serves as a simple spin lock that we can use even without an m. The thread that locks the stack in this way unlocks the stack by storing a valid stack head pointer. 

In order to make sure that there is always an m structure available to be stolen, we maintain the invariant that there is always one more than needed. At the beginning of the program (if cgo is in use) the list is seeded with a single m. If needm finds that it has taken the last m off the list, its job is - once it has installed its own m so that it can do things like allocate memory - to create a spare m and put it on the list. 

Each of these extra m's also has a g0 and a curg that are pressed into service as the scheduling stack and current goroutine for the duration of the cgo callback. 

When the callback is done with the m, it calls dropm to put the m back on the list. 

### <a id="net_fastrand" href="#net_fastrand">func net_fastrand() uint32</a>

```
searchKey: runtime.net_fastrand
tags: [function private]
```

```Go
func net_fastrand() uint32
```

### <a id="netpollBreak" href="#netpollBreak">func netpollBreak()</a>

```
searchKey: runtime.netpollBreak
tags: [function private]
```

```Go
func netpollBreak()
```

netpollBreak interrupts a kevent. 

### <a id="netpollDeadline" href="#netpollDeadline">func netpollDeadline(arg interface{}, seq uintptr)</a>

```
searchKey: runtime.netpollDeadline
tags: [method private]
```

```Go
func netpollDeadline(arg interface{}, seq uintptr)
```

### <a id="netpollGenericInit" href="#netpollGenericInit">func netpollGenericInit()</a>

```
searchKey: runtime.netpollGenericInit
tags: [function private]
```

```Go
func netpollGenericInit()
```

### <a id="netpollIsPollDescriptor" href="#netpollIsPollDescriptor">func netpollIsPollDescriptor(fd uintptr) bool</a>

```
searchKey: runtime.netpollIsPollDescriptor
tags: [method private]
```

```Go
func netpollIsPollDescriptor(fd uintptr) bool
```

### <a id="netpollReadDeadline" href="#netpollReadDeadline">func netpollReadDeadline(arg interface{}, seq uintptr)</a>

```
searchKey: runtime.netpollReadDeadline
tags: [method private]
```

```Go
func netpollReadDeadline(arg interface{}, seq uintptr)
```

### <a id="netpollWriteDeadline" href="#netpollWriteDeadline">func netpollWriteDeadline(arg interface{}, seq uintptr)</a>

```
searchKey: runtime.netpollWriteDeadline
tags: [method private]
```

```Go
func netpollWriteDeadline(arg interface{}, seq uintptr)
```

### <a id="netpollarm" href="#netpollarm">func netpollarm(pd *pollDesc, mode int)</a>

```
searchKey: runtime.netpollarm
tags: [method private]
```

```Go
func netpollarm(pd *pollDesc, mode int)
```

### <a id="netpollblock" href="#netpollblock">func netpollblock(pd *pollDesc, mode int32, waitio bool) bool</a>

```
searchKey: runtime.netpollblock
tags: [method private]
```

```Go
func netpollblock(pd *pollDesc, mode int32, waitio bool) bool
```

returns true if IO is ready, or false if timedout or closed waitio - wait only for completed IO, ignore errors 

### <a id="netpollblockcommit" href="#netpollblockcommit">func netpollblockcommit(gp *g, gpp unsafe.Pointer) bool</a>

```
searchKey: runtime.netpollblockcommit
tags: [method private]
```

```Go
func netpollblockcommit(gp *g, gpp unsafe.Pointer) bool
```

### <a id="netpollcheckerr" href="#netpollcheckerr">func netpollcheckerr(pd *pollDesc, mode int32) int</a>

```
searchKey: runtime.netpollcheckerr
tags: [method private]
```

```Go
func netpollcheckerr(pd *pollDesc, mode int32) int
```

### <a id="netpollclose" href="#netpollclose">func netpollclose(fd uintptr) int32</a>

```
searchKey: runtime.netpollclose
tags: [method private]
```

```Go
func netpollclose(fd uintptr) int32
```

### <a id="netpolldeadlineimpl" href="#netpolldeadlineimpl">func netpolldeadlineimpl(pd *pollDesc, seq uintptr, read, write bool)</a>

```
searchKey: runtime.netpolldeadlineimpl
tags: [method private]
```

```Go
func netpolldeadlineimpl(pd *pollDesc, seq uintptr, read, write bool)
```

### <a id="netpollgoready" href="#netpollgoready">func netpollgoready(gp *g, traceskip int)</a>

```
searchKey: runtime.netpollgoready
tags: [method private]
```

```Go
func netpollgoready(gp *g, traceskip int)
```

### <a id="netpollinit" href="#netpollinit">func netpollinit()</a>

```
searchKey: runtime.netpollinit
tags: [function private]
```

```Go
func netpollinit()
```

### <a id="netpollinited" href="#netpollinited">func netpollinited() bool</a>

```
searchKey: runtime.netpollinited
tags: [function private]
```

```Go
func netpollinited() bool
```

### <a id="netpollopen" href="#netpollopen">func netpollopen(fd uintptr, pd *pollDesc) int32</a>

```
searchKey: runtime.netpollopen
tags: [method private]
```

```Go
func netpollopen(fd uintptr, pd *pollDesc) int32
```

### <a id="netpollready" href="#netpollready">func netpollready(toRun *gList, pd *pollDesc, mode int32)</a>

```
searchKey: runtime.netpollready
tags: [method private]
```

```Go
func netpollready(toRun *gList, pd *pollDesc, mode int32)
```

netpollready is called by the platform-specific netpoll function. It declares that the fd associated with pd is ready for I/O. The toRun argument is used to build a list of goroutines to return from netpoll. The mode argument is 'r', 'w', or 'r'+'w' to indicate whether the fd is ready for reading or writing or both. 

This may run while the world is stopped, so write barriers are not allowed. 

### <a id="newarray" href="#newarray">func newarray(typ *_type, n int) unsafe.Pointer</a>

```
searchKey: runtime.newarray
tags: [method private]
```

```Go
func newarray(typ *_type, n int) unsafe.Pointer
```

newarray allocates an array of n elements of type typ. 

### <a id="newextram" href="#newextram">func newextram()</a>

```
searchKey: runtime.newextram
tags: [function private]
```

```Go
func newextram()
```

newextram allocates m's and puts them on the extra list. It is called with a working local m, so that it can do things like call schedlock and allocate. 

### <a id="newm" href="#newm">func newm(fn func(), _p_ *p, id int64)</a>

```
searchKey: runtime.newm
tags: [method private]
```

```Go
func newm(fn func(), _p_ *p, id int64)
```

Create a new m. It will start off with a call to fn, or else the scheduler. fn needs to be static and not a heap allocated closure. May run with m.p==nil, so write barriers are not allowed. 

id is optional pre-allocated m ID. Omit by passing -1. 

### <a id="newm1" href="#newm1">func newm1(mp *m)</a>

```
searchKey: runtime.newm1
tags: [method private]
```

```Go
func newm1(mp *m)
```

### <a id="newobject" href="#newobject">func newobject(typ *_type) unsafe.Pointer</a>

```
searchKey: runtime.newobject
tags: [method private]
```

```Go
func newobject(typ *_type) unsafe.Pointer
```

implementation of new builtin compiler (both frontend and SSA backend) knows the signature of this function 

### <a id="newosproc" href="#newosproc">func newosproc(mp *m)</a>

```
searchKey: runtime.newosproc
tags: [method private]
```

```Go
func newosproc(mp *m)
```

May run with m.p==nil, so write barriers are not allowed. 

### <a id="newosproc0" href="#newosproc0">func newosproc0(stacksize uintptr, fn uintptr)</a>

```
searchKey: runtime.newosproc0
tags: [method private]
```

```Go
func newosproc0(stacksize uintptr, fn uintptr)
```

newosproc0 is a version of newosproc that can be called before the runtime is initialized. 

This function is not safe to use after initialization as it does not pass an M as fnarg. 

### <a id="newproc" href="#newproc">func newproc(siz int32, fn *funcval)</a>

```
searchKey: runtime.newproc
tags: [method private]
```

```Go
func newproc(siz int32, fn *funcval)
```

Create a new g running fn with siz bytes of arguments. Put it on the queue of g's waiting to run. The compiler turns a go statement into a call to this. 

The stack layout of this call is unusual: it assumes that the arguments to pass to fn are on the stack sequentially immediately after &fn. Hence, they are logically part of newproc's argument frame, even though they don't appear in its signature (and can't because their types differ between call sites). 

This must be nosplit because this stack layout means there are untyped arguments in newproc's argument frame. Stack copies won't be able to adjust them and stack splits won't be able to copy them. 

### <a id="newstack" href="#newstack">func newstack()</a>

```
searchKey: runtime.newstack
tags: [function private]
```

```Go
func newstack()
```

Called from runtime·morestack when more stack is needed. Allocate larger stack and relocate to new stack. Stack growth is multiplicative, for constant amortized cost. 

g->atomicstatus will be Grunning or Gscanrunning upon entry. If the scheduler is trying to stop this g, then it will set preemptStop. 

This must be nowritebarrierrec because it can be called as part of stack growth from other nowritebarrierrec functions, but the compiler doesn't check this. 

### <a id="nextMarkBitArenaEpoch" href="#nextMarkBitArenaEpoch">func nextMarkBitArenaEpoch()</a>

```
searchKey: runtime.nextMarkBitArenaEpoch
tags: [function private]
```

```Go
func nextMarkBitArenaEpoch()
```

nextMarkBitArenaEpoch establishes a new epoch for the arenas holding the mark bits. The arenas are named relative to the current GC cycle which is demarcated by the call to finishweep_m. 

All current spans have been swept. During that sweep each span allocated room for its gcmarkBits in gcBitsArenas.next block. gcBitsArenas.next becomes the gcBitsArenas.current where the GC will mark objects and after each span is swept these bits will be used to allocate objects. gcBitsArenas.current becomes gcBitsArenas.previous where the span's gcAllocBits live until all the spans have been swept during this GC cycle. The span's sweep extinguishes all the references to gcBitsArenas.previous by pointing gcAllocBits into the gcBitsArenas.current. The gcBitsArenas.previous is released to the gcBitsArenas.free list. 

### <a id="nextSample" href="#nextSample">func nextSample() uintptr</a>

```
searchKey: runtime.nextSample
tags: [function private]
```

```Go
func nextSample() uintptr
```

nextSample returns the next sampling point for heap profiling. The goal is to sample allocations on average every MemProfileRate bytes, but with a completely random distribution over the allocation timeline; this corresponds to a Poisson process with parameter MemProfileRate. In Poisson processes, the distance between two samples follows the exponential distribution (exp(MemProfileRate)), so the best return value is a random number taken from an exponential distribution whose mean is MemProfileRate. 

### <a id="nextSampleNoFP" href="#nextSampleNoFP">func nextSampleNoFP() uintptr</a>

```
searchKey: runtime.nextSampleNoFP
tags: [function private]
```

```Go
func nextSampleNoFP() uintptr
```

nextSampleNoFP is similar to nextSample, but uses older, simpler code to avoid floating point. 

### <a id="nilfunc" href="#nilfunc">func nilfunc()</a>

```
searchKey: runtime.nilfunc
tags: [function private]
```

```Go
func nilfunc()
```

### <a id="nilinterequal" href="#nilinterequal">func nilinterequal(p, q unsafe.Pointer) bool</a>

```
searchKey: runtime.nilinterequal
tags: [method private]
```

```Go
func nilinterequal(p, q unsafe.Pointer) bool
```

### <a id="nilinterhash" href="#nilinterhash">func nilinterhash(p unsafe.Pointer, h uintptr) uintptr</a>

```
searchKey: runtime.nilinterhash
tags: [method private]
```

```Go
func nilinterhash(p unsafe.Pointer, h uintptr) uintptr
```

### <a id="noSignalStack" href="#noSignalStack">func noSignalStack(sig uint32)</a>

```
searchKey: runtime.noSignalStack
tags: [method private]
```

```Go
func noSignalStack(sig uint32)
```

This is called when we receive a signal when there is no signal stack. This can only happen if non-Go code calls sigaltstack to disable the signal stack. 

### <a id="nobarrierWakeTime" href="#nobarrierWakeTime">func nobarrierWakeTime(pp *p) int64</a>

```
searchKey: runtime.nobarrierWakeTime
tags: [method private]
```

```Go
func nobarrierWakeTime(pp *p) int64
```

nobarrierWakeTime looks at P's timers and returns the time when we should wake up the netpoller. It returns 0 if there are no timers. This function is invoked when dropping a P, and must run without any write barriers. 

### <a id="noescape" href="#noescape">func noescape(p unsafe.Pointer) unsafe.Pointer</a>

```
searchKey: runtime.noescape
tags: [method private]
```

```Go
func noescape(p unsafe.Pointer) unsafe.Pointer
```

noescape hides a pointer from escape analysis.  noescape is the identity function but escape analysis doesn't think the output depends on the input.  noescape is inlined and currently compiles down to zero instructions. USE CAREFULLY! 

### <a id="nonblockingPipe" href="#nonblockingPipe">func nonblockingPipe() (r, w int32, errno int32)</a>

```
searchKey: runtime.nonblockingPipe
tags: [function private]
```

```Go
func nonblockingPipe() (r, w int32, errno int32)
```

### <a id="noteclear" href="#noteclear">func noteclear(n *note)</a>

```
searchKey: runtime.noteclear
tags: [method private]
```

```Go
func noteclear(n *note)
```

One-time notifications. 

### <a id="notesleep" href="#notesleep">func notesleep(n *note)</a>

```
searchKey: runtime.notesleep
tags: [method private]
```

```Go
func notesleep(n *note)
```

### <a id="notetsleep" href="#notetsleep">func notetsleep(n *note, ns int64) bool</a>

```
searchKey: runtime.notetsleep
tags: [method private]
```

```Go
func notetsleep(n *note, ns int64) bool
```

### <a id="notetsleep_internal" href="#notetsleep_internal">func notetsleep_internal(n *note, ns int64, gp *g, deadline int64) bool</a>

```
searchKey: runtime.notetsleep_internal
tags: [method private]
```

```Go
func notetsleep_internal(n *note, ns int64, gp *g, deadline int64) bool
```

### <a id="notetsleepg" href="#notetsleepg">func notetsleepg(n *note, ns int64) bool</a>

```
searchKey: runtime.notetsleepg
tags: [method private]
```

```Go
func notetsleepg(n *note, ns int64) bool
```

same as runtime·notetsleep, but called on user g (not g0) calls only nosplit functions between entersyscallblock/exitsyscall 

### <a id="notewakeup" href="#notewakeup">func notewakeup(n *note)</a>

```
searchKey: runtime.notewakeup
tags: [method private]
```

```Go
func notewakeup(n *note)
```

### <a id="notifyListAdd" href="#notifyListAdd">func notifyListAdd(l *notifyList) uint32</a>

```
searchKey: runtime.notifyListAdd
tags: [method private]
```

```Go
func notifyListAdd(l *notifyList) uint32
```

notifyListAdd adds the caller to a notify list such that it can receive notifications. The caller must eventually call notifyListWait to wait for such a notification, passing the returned ticket number. 

### <a id="notifyListCheck" href="#notifyListCheck">func notifyListCheck(sz uintptr)</a>

```
searchKey: runtime.notifyListCheck
tags: [method private]
```

```Go
func notifyListCheck(sz uintptr)
```

### <a id="notifyListNotifyAll" href="#notifyListNotifyAll">func notifyListNotifyAll(l *notifyList)</a>

```
searchKey: runtime.notifyListNotifyAll
tags: [method private]
```

```Go
func notifyListNotifyAll(l *notifyList)
```

notifyListNotifyAll notifies all entries in the list. 

### <a id="notifyListNotifyOne" href="#notifyListNotifyOne">func notifyListNotifyOne(l *notifyList)</a>

```
searchKey: runtime.notifyListNotifyOne
tags: [method private]
```

```Go
func notifyListNotifyOne(l *notifyList)
```

notifyListNotifyOne notifies one entry in the list. 

### <a id="notifyListWait" href="#notifyListWait">func notifyListWait(l *notifyList, t uint32)</a>

```
searchKey: runtime.notifyListWait
tags: [method private]
```

```Go
func notifyListWait(l *notifyList, t uint32)
```

notifyListWait waits for a notification. If one has been sent since notifyListAdd was called, it returns immediately. Otherwise, it blocks. 

### <a id="offAddrToLevelIndex" href="#offAddrToLevelIndex">func offAddrToLevelIndex(level int, addr offAddr) int</a>

```
searchKey: runtime.offAddrToLevelIndex
tags: [method private]
```

```Go
func offAddrToLevelIndex(level int, addr offAddr) int
```

offAddrToLevelIndex converts an address in the offset address space to the index into summary[level] containing addr. 

### <a id="oneNewExtraM" href="#oneNewExtraM">func oneNewExtraM()</a>

```
searchKey: runtime.oneNewExtraM
tags: [function private]
```

```Go
func oneNewExtraM()
```

oneNewExtraM allocates an m and puts it on the extra list. 

### <a id="open" href="#open">func open(name *byte, mode, perm int32) (ret int32)</a>

```
searchKey: runtime.open
tags: [method private]
```

```Go
func open(name *byte, mode, perm int32) (ret int32)
```

### <a id="open_trampoline" href="#open_trampoline">func open_trampoline()</a>

```
searchKey: runtime.open_trampoline
tags: [function private]
```

```Go
func open_trampoline()
```

### <a id="osPreemptExtEnter" href="#osPreemptExtEnter">func osPreemptExtEnter(mp *m)</a>

```
searchKey: runtime.osPreemptExtEnter
tags: [method private]
```

```Go
func osPreemptExtEnter(mp *m)
```

### <a id="osPreemptExtExit" href="#osPreemptExtExit">func osPreemptExtExit(mp *m)</a>

```
searchKey: runtime.osPreemptExtExit
tags: [method private]
```

```Go
func osPreemptExtExit(mp *m)
```

### <a id="osRelax" href="#osRelax">func osRelax(relax bool)</a>

```
searchKey: runtime.osRelax
tags: [method private]
```

```Go
func osRelax(relax bool)
```

osRelax is called by the scheduler when transitioning to and from all Ps being idle. 

### <a id="osSetupTLS" href="#osSetupTLS">func osSetupTLS(mp *m)</a>

```
searchKey: runtime.osSetupTLS
tags: [method private]
```

```Go
func osSetupTLS(mp *m)
```

### <a id="osStackAlloc" href="#osStackAlloc">func osStackAlloc(s *mspan)</a>

```
searchKey: runtime.osStackAlloc
tags: [method private]
```

```Go
func osStackAlloc(s *mspan)
```

osStackAlloc performs OS-specific initialization before s is used as stack memory. 

### <a id="osStackFree" href="#osStackFree">func osStackFree(s *mspan)</a>

```
searchKey: runtime.osStackFree
tags: [method private]
```

```Go
func osStackFree(s *mspan)
```

osStackFree undoes the effect of osStackAlloc before s is returned to the heap. 

### <a id="os_beforeExit" href="#os_beforeExit">func os_beforeExit()</a>

```
searchKey: runtime.os_beforeExit
tags: [function private]
```

```Go
func os_beforeExit()
```

os_beforeExit is called from os.Exit(0). 

### <a id="os_fastrand" href="#os_fastrand">func os_fastrand() uint32</a>

```
searchKey: runtime.os_fastrand
tags: [function private]
```

```Go
func os_fastrand() uint32
```

### <a id="os_runtime_args" href="#os_runtime_args">func os_runtime_args() []string</a>

```
searchKey: runtime.os_runtime_args
tags: [function private]
```

```Go
func os_runtime_args() []string
```

### <a id="os_sigpipe" href="#os_sigpipe">func os_sigpipe()</a>

```
searchKey: runtime.os_sigpipe
tags: [function private]
```

```Go
func os_sigpipe()
```

### <a id="osinit" href="#osinit">func osinit()</a>

```
searchKey: runtime.osinit
tags: [function private]
```

```Go
func osinit()
```

BSD interface for threading. 

### <a id="osyield" href="#osyield">func osyield()</a>

```
searchKey: runtime.osyield
tags: [function private]
```

```Go
func osyield()
```

### <a id="osyield_no_g" href="#osyield_no_g">func osyield_no_g()</a>

```
searchKey: runtime.osyield_no_g
tags: [function private]
```

```Go
func osyield_no_g()
```

### <a id="overLoadFactor" href="#overLoadFactor">func overLoadFactor(count int, B uint8) bool</a>

```
searchKey: runtime.overLoadFactor
tags: [method private]
```

```Go
func overLoadFactor(count int, B uint8) bool
```

overLoadFactor reports whether count items placed in 1<<B buckets is over loadFactor. 

### <a id="panicCheck1" href="#panicCheck1">func panicCheck1(pc uintptr, msg string)</a>

```
searchKey: runtime.panicCheck1
tags: [method private]
```

```Go
func panicCheck1(pc uintptr, msg string)
```

Check to make sure we can really generate a panic. If the panic was generated from the runtime, or from inside malloc, then convert to a throw of msg. pc should be the program counter of the compiler-generated code that triggered this panic. 

### <a id="panicCheck2" href="#panicCheck2">func panicCheck2(err string)</a>

```
searchKey: runtime.panicCheck2
tags: [method private]
```

```Go
func panicCheck2(err string)
```

Same as above, but calling from the runtime is allowed. 

Using this function is necessary for any panic that may be generated by runtime.sigpanic, since those are always called by the runtime. 

### <a id="panicIndex" href="#panicIndex">func panicIndex(x int, y int)</a>

```
searchKey: runtime.panicIndex
tags: [method private]
```

```Go
func panicIndex(x int, y int)
```

Implemented in assembly, as they take arguments in registers. Declared here to mark them as ABIInternal. 

### <a id="panicIndexU" href="#panicIndexU">func panicIndexU(x uint, y int)</a>

```
searchKey: runtime.panicIndexU
tags: [method private]
```

```Go
func panicIndexU(x uint, y int)
```

### <a id="panicSlice3Acap" href="#panicSlice3Acap">func panicSlice3Acap(x int, y int)</a>

```
searchKey: runtime.panicSlice3Acap
tags: [method private]
```

```Go
func panicSlice3Acap(x int, y int)
```

### <a id="panicSlice3AcapU" href="#panicSlice3AcapU">func panicSlice3AcapU(x uint, y int)</a>

```
searchKey: runtime.panicSlice3AcapU
tags: [method private]
```

```Go
func panicSlice3AcapU(x uint, y int)
```

### <a id="panicSlice3Alen" href="#panicSlice3Alen">func panicSlice3Alen(x int, y int)</a>

```
searchKey: runtime.panicSlice3Alen
tags: [method private]
```

```Go
func panicSlice3Alen(x int, y int)
```

### <a id="panicSlice3AlenU" href="#panicSlice3AlenU">func panicSlice3AlenU(x uint, y int)</a>

```
searchKey: runtime.panicSlice3AlenU
tags: [method private]
```

```Go
func panicSlice3AlenU(x uint, y int)
```

### <a id="panicSlice3B" href="#panicSlice3B">func panicSlice3B(x int, y int)</a>

```
searchKey: runtime.panicSlice3B
tags: [method private]
```

```Go
func panicSlice3B(x int, y int)
```

### <a id="panicSlice3BU" href="#panicSlice3BU">func panicSlice3BU(x uint, y int)</a>

```
searchKey: runtime.panicSlice3BU
tags: [method private]
```

```Go
func panicSlice3BU(x uint, y int)
```

### <a id="panicSlice3C" href="#panicSlice3C">func panicSlice3C(x int, y int)</a>

```
searchKey: runtime.panicSlice3C
tags: [method private]
```

```Go
func panicSlice3C(x int, y int)
```

### <a id="panicSlice3CU" href="#panicSlice3CU">func panicSlice3CU(x uint, y int)</a>

```
searchKey: runtime.panicSlice3CU
tags: [method private]
```

```Go
func panicSlice3CU(x uint, y int)
```

### <a id="panicSliceAcap" href="#panicSliceAcap">func panicSliceAcap(x int, y int)</a>

```
searchKey: runtime.panicSliceAcap
tags: [method private]
```

```Go
func panicSliceAcap(x int, y int)
```

### <a id="panicSliceAcapU" href="#panicSliceAcapU">func panicSliceAcapU(x uint, y int)</a>

```
searchKey: runtime.panicSliceAcapU
tags: [method private]
```

```Go
func panicSliceAcapU(x uint, y int)
```

### <a id="panicSliceAlen" href="#panicSliceAlen">func panicSliceAlen(x int, y int)</a>

```
searchKey: runtime.panicSliceAlen
tags: [method private]
```

```Go
func panicSliceAlen(x int, y int)
```

### <a id="panicSliceAlenU" href="#panicSliceAlenU">func panicSliceAlenU(x uint, y int)</a>

```
searchKey: runtime.panicSliceAlenU
tags: [method private]
```

```Go
func panicSliceAlenU(x uint, y int)
```

### <a id="panicSliceB" href="#panicSliceB">func panicSliceB(x int, y int)</a>

```
searchKey: runtime.panicSliceB
tags: [method private]
```

```Go
func panicSliceB(x int, y int)
```

### <a id="panicSliceBU" href="#panicSliceBU">func panicSliceBU(x uint, y int)</a>

```
searchKey: runtime.panicSliceBU
tags: [method private]
```

```Go
func panicSliceBU(x uint, y int)
```

### <a id="panicSliceConvert" href="#panicSliceConvert">func panicSliceConvert(x int, y int)</a>

```
searchKey: runtime.panicSliceConvert
tags: [method private]
```

```Go
func panicSliceConvert(x int, y int)
```

### <a id="panicdivide" href="#panicdivide">func panicdivide()</a>

```
searchKey: runtime.panicdivide
tags: [function private]
```

```Go
func panicdivide()
```

### <a id="panicdottypeE" href="#panicdottypeE">func panicdottypeE(have, want, iface *_type)</a>

```
searchKey: runtime.panicdottypeE
tags: [method private]
```

```Go
func panicdottypeE(have, want, iface *_type)
```

panicdottypeE is called when doing an e.(T) conversion and the conversion fails. have = the dynamic type we have. want = the static type we're trying to convert to. iface = the static type we're converting from. 

### <a id="panicdottypeI" href="#panicdottypeI">func panicdottypeI(have *itab, want, iface *_type)</a>

```
searchKey: runtime.panicdottypeI
tags: [method private]
```

```Go
func panicdottypeI(have *itab, want, iface *_type)
```

panicdottypeI is called when doing an i.(T) conversion and the conversion fails. Same args as panicdottypeE, but "have" is the dynamic itab we have. 

### <a id="panicfloat" href="#panicfloat">func panicfloat()</a>

```
searchKey: runtime.panicfloat
tags: [function private]
```

```Go
func panicfloat()
```

### <a id="panicmakeslicecap" href="#panicmakeslicecap">func panicmakeslicecap()</a>

```
searchKey: runtime.panicmakeslicecap
tags: [function private]
```

```Go
func panicmakeslicecap()
```

### <a id="panicmakeslicelen" href="#panicmakeslicelen">func panicmakeslicelen()</a>

```
searchKey: runtime.panicmakeslicelen
tags: [function private]
```

```Go
func panicmakeslicelen()
```

### <a id="panicmem" href="#panicmem">func panicmem()</a>

```
searchKey: runtime.panicmem
tags: [function private]
```

```Go
func panicmem()
```

### <a id="panicmemAddr" href="#panicmemAddr">func panicmemAddr(addr uintptr)</a>

```
searchKey: runtime.panicmemAddr
tags: [method private]
```

```Go
func panicmemAddr(addr uintptr)
```

### <a id="panicnildottype" href="#panicnildottype">func panicnildottype(want *_type)</a>

```
searchKey: runtime.panicnildottype
tags: [method private]
```

```Go
func panicnildottype(want *_type)
```

panicnildottype is called when doing a i.(T) conversion and the interface i is nil. want = the static type we're trying to convert to. 

### <a id="panicoverflow" href="#panicoverflow">func panicoverflow()</a>

```
searchKey: runtime.panicoverflow
tags: [function private]
```

```Go
func panicoverflow()
```

### <a id="panicshift" href="#panicshift">func panicshift()</a>

```
searchKey: runtime.panicshift
tags: [function private]
```

```Go
func panicshift()
```

### <a id="panicunsafeslicelen" href="#panicunsafeslicelen">func panicunsafeslicelen()</a>

```
searchKey: runtime.panicunsafeslicelen
tags: [function private]
```

```Go
func panicunsafeslicelen()
```

### <a id="panicwrap" href="#panicwrap">func panicwrap()</a>

```
searchKey: runtime.panicwrap
tags: [function private]
```

```Go
func panicwrap()
```

panicwrap generates a panic for a call to a wrapped value method with a nil pointer receiver. 

It is called from the generated wrapper code. 

### <a id="park_m" href="#park_m">func park_m(gp *g)</a>

```
searchKey: runtime.park_m
tags: [method private]
```

```Go
func park_m(gp *g)
```

park continuation on g0. 

### <a id="parkunlock_c" href="#parkunlock_c">func parkunlock_c(gp *g, lock unsafe.Pointer) bool</a>

```
searchKey: runtime.parkunlock_c
tags: [method private]
```

```Go
func parkunlock_c(gp *g, lock unsafe.Pointer) bool
```

### <a id="parsedebugvars" href="#parsedebugvars">func parsedebugvars()</a>

```
searchKey: runtime.parsedebugvars
tags: [function private]
```

```Go
func parsedebugvars()
```

### <a id="pcdatastart" href="#pcdatastart">func pcdatastart(f funcInfo, table uint32) uint32</a>

```
searchKey: runtime.pcdatastart
tags: [method private]
```

```Go
func pcdatastart(f funcInfo, table uint32) uint32
```

### <a id="pcdatavalue" href="#pcdatavalue">func pcdatavalue(f funcInfo, table uint32, targetpc uintptr, cache *pcvalueCache) int32</a>

```
searchKey: runtime.pcdatavalue
tags: [method private]
```

```Go
func pcdatavalue(f funcInfo, table uint32, targetpc uintptr, cache *pcvalueCache) int32
```

### <a id="pcdatavalue1" href="#pcdatavalue1">func pcdatavalue1(f funcInfo, table uint32, targetpc uintptr, cache *pcvalueCache, strict bool) int32</a>

```
searchKey: runtime.pcdatavalue1
tags: [method private]
```

```Go
func pcdatavalue1(f funcInfo, table uint32, targetpc uintptr, cache *pcvalueCache, strict bool) int32
```

### <a id="pcdatavalue2" href="#pcdatavalue2">func pcdatavalue2(f funcInfo, table uint32, targetpc uintptr) (int32, uintptr)</a>

```
searchKey: runtime.pcdatavalue2
tags: [method private]
```

```Go
func pcdatavalue2(f funcInfo, table uint32, targetpc uintptr) (int32, uintptr)
```

Like pcdatavalue, but also return the start PC of this PCData value. It doesn't take a cache. 

### <a id="pcvalue" href="#pcvalue">func pcvalue(f funcInfo, off uint32, targetpc uintptr, cache *pcvalueCache, strict bool) (int32, uintptr)</a>

```
searchKey: runtime.pcvalue
tags: [method private]
```

```Go
func pcvalue(f funcInfo, off uint32, targetpc uintptr, cache *pcvalueCache, strict bool) (int32, uintptr)
```

Returns the PCData value, and the PC where this value starts. TODO: the start PC is returned only when cache is nil. 

### <a id="pcvalueCacheKey" href="#pcvalueCacheKey">func pcvalueCacheKey(targetpc uintptr) uintptr</a>

```
searchKey: runtime.pcvalueCacheKey
tags: [method private]
```

```Go
func pcvalueCacheKey(targetpc uintptr) uintptr
```

pcvalueCacheKey returns the outermost index in a pcvalueCache to use for targetpc. It must be very cheap to calculate. For now, align to sys.PtrSize and reduce mod the number of entries. In practice, this appears to be fairly randomly and evenly distributed. 

### <a id="persistentalloc" href="#persistentalloc">func persistentalloc(size, align uintptr, sysStat *sysMemStat) unsafe.Pointer</a>

```
searchKey: runtime.persistentalloc
tags: [method private]
```

```Go
func persistentalloc(size, align uintptr, sysStat *sysMemStat) unsafe.Pointer
```

Wrapper around sysAlloc that can allocate small chunks. There is no associated free operation. Intended for things like function/type/debug-related persistent data. If align is 0, uses default align (currently 8). The returned memory will be zeroed. 

Consider marking persistentalloc'd types go:notinheap. 

### <a id="pidleput" href="#pidleput">func pidleput(_p_ *p)</a>

```
searchKey: runtime.pidleput
tags: [method private]
```

```Go
func pidleput(_p_ *p)
```

pidleput puts p to on the _Pidle list. 

This releases ownership of p. Once sched.lock is released it is no longer safe to use p. 

sched.lock must be held. 

May run during STW, so write barriers are not allowed. 

### <a id="pipe" href="#pipe">func pipe() (r, w int32, errno int32)</a>

```
searchKey: runtime.pipe
tags: [function private]
```

```Go
func pipe() (r, w int32, errno int32)
```

### <a id="pipe_trampoline" href="#pipe_trampoline">func pipe_trampoline()</a>

```
searchKey: runtime.pipe_trampoline
tags: [function private]
```

```Go
func pipe_trampoline()
```

### <a id="plugin_lastmoduleinit" href="#plugin_lastmoduleinit">func plugin_lastmoduleinit() (path string, syms map[string]interface{}, errstr string)</a>

```
searchKey: runtime.plugin_lastmoduleinit
tags: [function private]
```

```Go
func plugin_lastmoduleinit() (path string, syms map[string]interface{}, errstr string)
```

### <a id="pluginftabverify" href="#pluginftabverify">func pluginftabverify(md *moduledata)</a>

```
searchKey: runtime.pluginftabverify
tags: [method private]
```

```Go
func pluginftabverify(md *moduledata)
```

### <a id="pollFractionalWorkerExit" href="#pollFractionalWorkerExit">func pollFractionalWorkerExit() bool</a>

```
searchKey: runtime.pollFractionalWorkerExit
tags: [function private]
```

```Go
func pollFractionalWorkerExit() bool
```

pollFractionalWorkerExit reports whether a fractional mark worker should self-preempt. It assumes it is called from the fractional worker. 

### <a id="pollWork" href="#pollWork">func pollWork() bool</a>

```
searchKey: runtime.pollWork
tags: [function private]
```

```Go
func pollWork() bool
```

pollWork reports whether there is non-background work this P could be doing. This is a fairly lightweight check to be used for background work loops, like idle GC. It checks a subset of the conditions checked by the actual scheduler. 

### <a id="poll_runtime_Semacquire" href="#poll_runtime_Semacquire">func poll_runtime_Semacquire(addr *uint32)</a>

```
searchKey: runtime.poll_runtime_Semacquire
tags: [method private]
```

```Go
func poll_runtime_Semacquire(addr *uint32)
```

### <a id="poll_runtime_Semrelease" href="#poll_runtime_Semrelease">func poll_runtime_Semrelease(addr *uint32)</a>

```
searchKey: runtime.poll_runtime_Semrelease
tags: [method private]
```

```Go
func poll_runtime_Semrelease(addr *uint32)
```

### <a id="poll_runtime_isPollServerDescriptor" href="#poll_runtime_isPollServerDescriptor">func poll_runtime_isPollServerDescriptor(fd uintptr) bool</a>

```
searchKey: runtime.poll_runtime_isPollServerDescriptor
tags: [method private]
```

```Go
func poll_runtime_isPollServerDescriptor(fd uintptr) bool
```

poll_runtime_isPollServerDescriptor reports whether fd is a descriptor being used by netpoll. 

### <a id="poll_runtime_pollClose" href="#poll_runtime_pollClose">func poll_runtime_pollClose(pd *pollDesc)</a>

```
searchKey: runtime.poll_runtime_pollClose
tags: [method private]
```

```Go
func poll_runtime_pollClose(pd *pollDesc)
```

### <a id="poll_runtime_pollReset" href="#poll_runtime_pollReset">func poll_runtime_pollReset(pd *pollDesc, mode int) int</a>

```
searchKey: runtime.poll_runtime_pollReset
tags: [method private]
```

```Go
func poll_runtime_pollReset(pd *pollDesc, mode int) int
```

poll_runtime_pollReset, which is internal/poll.runtime_pollReset, prepares a descriptor for polling in mode, which is 'r' or 'w'. This returns an error code; the codes are defined above. 

### <a id="poll_runtime_pollServerInit" href="#poll_runtime_pollServerInit">func poll_runtime_pollServerInit()</a>

```
searchKey: runtime.poll_runtime_pollServerInit
tags: [function private]
```

```Go
func poll_runtime_pollServerInit()
```

### <a id="poll_runtime_pollSetDeadline" href="#poll_runtime_pollSetDeadline">func poll_runtime_pollSetDeadline(pd *pollDesc, d int64, mode int)</a>

```
searchKey: runtime.poll_runtime_pollSetDeadline
tags: [method private]
```

```Go
func poll_runtime_pollSetDeadline(pd *pollDesc, d int64, mode int)
```

### <a id="poll_runtime_pollUnblock" href="#poll_runtime_pollUnblock">func poll_runtime_pollUnblock(pd *pollDesc)</a>

```
searchKey: runtime.poll_runtime_pollUnblock
tags: [method private]
```

```Go
func poll_runtime_pollUnblock(pd *pollDesc)
```

### <a id="poll_runtime_pollWait" href="#poll_runtime_pollWait">func poll_runtime_pollWait(pd *pollDesc, mode int) int</a>

```
searchKey: runtime.poll_runtime_pollWait
tags: [method private]
```

```Go
func poll_runtime_pollWait(pd *pollDesc, mode int) int
```

poll_runtime_pollWait, which is internal/poll.runtime_pollWait, waits for a descriptor to be ready for reading or writing, according to mode, which is 'r' or 'w'. This returns an error code; the codes are defined above. 

### <a id="poll_runtime_pollWaitCanceled" href="#poll_runtime_pollWaitCanceled">func poll_runtime_pollWaitCanceled(pd *pollDesc, mode int)</a>

```
searchKey: runtime.poll_runtime_pollWaitCanceled
tags: [method private]
```

```Go
func poll_runtime_pollWaitCanceled(pd *pollDesc, mode int)
```

### <a id="preemptM" href="#preemptM">func preemptM(mp *m)</a>

```
searchKey: runtime.preemptM
tags: [method private]
```

```Go
func preemptM(mp *m)
```

preemptM sends a preemption request to mp. This request may be handled asynchronously and may be coalesced with other requests to the M. When the request is received, if the running G or P are marked for preemption and the goroutine is at an asynchronous safe-point, it will preempt the goroutine. It always atomically increments mp.preemptGen after handling a preemption request. 

### <a id="preemptPark" href="#preemptPark">func preemptPark(gp *g)</a>

```
searchKey: runtime.preemptPark
tags: [method private]
```

```Go
func preemptPark(gp *g)
```

preemptPark parks gp and puts it in _Gpreempted. 

### <a id="preemptall" href="#preemptall">func preemptall() bool</a>

```
searchKey: runtime.preemptall
tags: [function private]
```

```Go
func preemptall() bool
```

Tell all goroutines that they have been preempted and they should stop. This function is purely best-effort. It can fail to inform a goroutine if a processor just started running it. No locks need to be held. Returns true if preemption request was issued to at least one goroutine. 

### <a id="preemptone" href="#preemptone">func preemptone(_p_ *p) bool</a>

```
searchKey: runtime.preemptone
tags: [method private]
```

```Go
func preemptone(_p_ *p) bool
```

Tell the goroutine running on processor P to stop. This function is purely best-effort. It can incorrectly fail to inform the goroutine. It can inform the wrong goroutine. Even if it informs the correct goroutine, that goroutine might ignore the request if it is simultaneously executing newstack. No lock needs to be held. Returns true if preemption request was issued. The actual preemption will happen at some point in the future and will be indicated by the gp->status no longer being Grunning 

### <a id="prepGoExitFrame" href="#prepGoExitFrame">func prepGoExitFrame(sp uintptr)</a>

```
searchKey: runtime.prepGoExitFrame
tags: [method private]
```

```Go
func prepGoExitFrame(sp uintptr)
```

### <a id="prepareFreeWorkbufs" href="#prepareFreeWorkbufs">func prepareFreeWorkbufs()</a>

```
searchKey: runtime.prepareFreeWorkbufs
tags: [function private]
```

```Go
func prepareFreeWorkbufs()
```

prepareFreeWorkbufs moves busy workbuf spans to free list so they can be freed to the heap. This must only be called when all workbufs are on the empty list. 

### <a id="preprintpanics" href="#preprintpanics">func preprintpanics(p *_panic)</a>

```
searchKey: runtime.preprintpanics
tags: [method private]
```

```Go
func preprintpanics(p *_panic)
```

Call all Error and String methods before freezing the world. Used when crashing with panicking. 

### <a id="printAncestorTraceback" href="#printAncestorTraceback">func printAncestorTraceback(ancestor ancestorInfo)</a>

```
searchKey: runtime.printAncestorTraceback
tags: [method private]
```

```Go
func printAncestorTraceback(ancestor ancestorInfo)
```

printAncestorTraceback prints the traceback of the given ancestor. TODO: Unify this with gentraceback and CallersFrames. 

### <a id="printAncestorTracebackFuncInfo" href="#printAncestorTracebackFuncInfo">func printAncestorTracebackFuncInfo(f funcInfo, pc uintptr)</a>

```
searchKey: runtime.printAncestorTracebackFuncInfo
tags: [method private]
```

```Go
func printAncestorTracebackFuncInfo(f funcInfo, pc uintptr)
```

printAncestorTraceback prints the given function info at a given pc within an ancestor traceback. The precision of this info is reduced due to only have access to the pcs at the time of the caller goroutine being created. 

### <a id="printArgs" href="#printArgs">func printArgs(f funcInfo, argp unsafe.Pointer)</a>

```
searchKey: runtime.printArgs
tags: [method private]
```

```Go
func printArgs(f funcInfo, argp unsafe.Pointer)
```

printArgs prints function arguments in traceback. 

### <a id="printCgoTraceback" href="#printCgoTraceback">func printCgoTraceback(callers *cgoCallers)</a>

```
searchKey: runtime.printCgoTraceback
tags: [method private]
```

```Go
func printCgoTraceback(callers *cgoCallers)
```

cgoTraceback prints a traceback of callers. 

### <a id="printDebugLog" href="#printDebugLog">func printDebugLog()</a>

```
searchKey: runtime.printDebugLog
tags: [function private]
```

```Go
func printDebugLog()
```

printDebugLog prints the debug log. 

### <a id="printDebugLogPC" href="#printDebugLogPC">func printDebugLogPC(pc uintptr, returnPC bool)</a>

```
searchKey: runtime.printDebugLogPC
tags: [method private]
```

```Go
func printDebugLogPC(pc uintptr, returnPC bool)
```

printDebugLogPC prints a single symbolized PC. If returnPC is true, pc is a return PC that must first be converted to a call PC. 

### <a id="printOneCgoTraceback" href="#printOneCgoTraceback">func printOneCgoTraceback(pc uintptr, max int, arg *cgoSymbolizerArg) int</a>

```
searchKey: runtime.printOneCgoTraceback
tags: [method private]
```

```Go
func printOneCgoTraceback(pc uintptr, max int, arg *cgoSymbolizerArg) int
```

printOneCgoTraceback prints the traceback of a single cgo caller. This can print more than one line because of inlining. Returns the number of frames printed. 

### <a id="printScavTrace" href="#printScavTrace">func printScavTrace(gen uint32, released uintptr, forced bool)</a>

```
searchKey: runtime.printScavTrace
tags: [method private]
```

```Go
func printScavTrace(gen uint32, released uintptr, forced bool)
```

printScavTrace prints a scavenge trace line to standard error. 

released should be the amount of memory released since the last time this was called, and forced indicates whether the scavenge was forced by the application. 

### <a id="printany" href="#printany">func printany(i interface{})</a>

```
searchKey: runtime.printany
tags: [method private]
```

```Go
func printany(i interface{})
```

printany prints an argument passed to panic. If panic is called with a value that has a String or Error method, it has already been converted into a string by preprintpanics. 

### <a id="printanycustomtype" href="#printanycustomtype">func printanycustomtype(i interface{})</a>

```
searchKey: runtime.printanycustomtype
tags: [method private]
```

```Go
func printanycustomtype(i interface{})
```

### <a id="printbool" href="#printbool">func printbool(v bool)</a>

```
searchKey: runtime.printbool
tags: [method private]
```

```Go
func printbool(v bool)
```

### <a id="printcomplex" href="#printcomplex">func printcomplex(c complex128)</a>

```
searchKey: runtime.printcomplex
tags: [method private]
```

```Go
func printcomplex(c complex128)
```

### <a id="printcreatedby" href="#printcreatedby">func printcreatedby(gp *g)</a>

```
searchKey: runtime.printcreatedby
tags: [method private]
```

```Go
func printcreatedby(gp *g)
```

### <a id="printcreatedby1" href="#printcreatedby1">func printcreatedby1(f funcInfo, pc uintptr)</a>

```
searchKey: runtime.printcreatedby1
tags: [method private]
```

```Go
func printcreatedby1(f funcInfo, pc uintptr)
```

### <a id="printeface" href="#printeface">func printeface(e eface)</a>

```
searchKey: runtime.printeface
tags: [method private]
```

```Go
func printeface(e eface)
```

### <a id="printfloat" href="#printfloat">func printfloat(v float64)</a>

```
searchKey: runtime.printfloat
tags: [method private]
```

```Go
func printfloat(v float64)
```

### <a id="printhex" href="#printhex">func printhex(v uint64)</a>

```
searchKey: runtime.printhex
tags: [method private]
```

```Go
func printhex(v uint64)
```

### <a id="printiface" href="#printiface">func printiface(i iface)</a>

```
searchKey: runtime.printiface
tags: [method private]
```

```Go
func printiface(i iface)
```

### <a id="printint" href="#printint">func printint(v int64)</a>

```
searchKey: runtime.printint
tags: [method private]
```

```Go
func printint(v int64)
```

### <a id="printlock" href="#printlock">func printlock()</a>

```
searchKey: runtime.printlock
tags: [function private]
```

```Go
func printlock()
```

### <a id="printnl" href="#printnl">func printnl()</a>

```
searchKey: runtime.printnl
tags: [function private]
```

```Go
func printnl()
```

### <a id="printpanics" href="#printpanics">func printpanics(p *_panic)</a>

```
searchKey: runtime.printpanics
tags: [method private]
```

```Go
func printpanics(p *_panic)
```

Print all currently active panics. Used when crashing. Should only be called after preprintpanics. 

### <a id="printpointer" href="#printpointer">func printpointer(p unsafe.Pointer)</a>

```
searchKey: runtime.printpointer
tags: [method private]
```

```Go
func printpointer(p unsafe.Pointer)
```

### <a id="printslice" href="#printslice">func printslice(s []byte)</a>

```
searchKey: runtime.printslice
tags: [method private]
```

```Go
func printslice(s []byte)
```

### <a id="printsp" href="#printsp">func printsp()</a>

```
searchKey: runtime.printsp
tags: [function private]
```

```Go
func printsp()
```

### <a id="printstring" href="#printstring">func printstring(s string)</a>

```
searchKey: runtime.printstring
tags: [method private]
```

```Go
func printstring(s string)
```

### <a id="printuint" href="#printuint">func printuint(v uint64)</a>

```
searchKey: runtime.printuint
tags: [method private]
```

```Go
func printuint(v uint64)
```

### <a id="printuintptr" href="#printuintptr">func printuintptr(p uintptr)</a>

```
searchKey: runtime.printuintptr
tags: [method private]
```

```Go
func printuintptr(p uintptr)
```

### <a id="printunlock" href="#printunlock">func printunlock()</a>

```
searchKey: runtime.printunlock
tags: [function private]
```

```Go
func printunlock()
```

### <a id="procPin" href="#procPin">func procPin() int</a>

```
searchKey: runtime.procPin
tags: [function private]
```

```Go
func procPin() int
```

### <a id="procUnpin" href="#procUnpin">func procUnpin()</a>

```
searchKey: runtime.procUnpin
tags: [function private]
```

```Go
func procUnpin()
```

### <a id="procyield" href="#procyield">func procyield(cycles uint32)</a>

```
searchKey: runtime.procyield
tags: [method private]
```

```Go
func procyield(cycles uint32)
```

### <a id="profilealloc" href="#profilealloc">func profilealloc(mp *m, x unsafe.Pointer, size uintptr)</a>

```
searchKey: runtime.profilealloc
tags: [method private]
```

```Go
func profilealloc(mp *m, x unsafe.Pointer, size uintptr)
```

### <a id="pthread_attr_getstacksize" href="#pthread_attr_getstacksize">func pthread_attr_getstacksize(attr *pthreadattr, size *uintptr) int32</a>

```
searchKey: runtime.pthread_attr_getstacksize
tags: [method private]
```

```Go
func pthread_attr_getstacksize(attr *pthreadattr, size *uintptr) int32
```

### <a id="pthread_attr_getstacksize_trampoline" href="#pthread_attr_getstacksize_trampoline">func pthread_attr_getstacksize_trampoline()</a>

```
searchKey: runtime.pthread_attr_getstacksize_trampoline
tags: [function private]
```

```Go
func pthread_attr_getstacksize_trampoline()
```

### <a id="pthread_attr_init" href="#pthread_attr_init">func pthread_attr_init(attr *pthreadattr) int32</a>

```
searchKey: runtime.pthread_attr_init
tags: [method private]
```

```Go
func pthread_attr_init(attr *pthreadattr) int32
```

### <a id="pthread_attr_init_trampoline" href="#pthread_attr_init_trampoline">func pthread_attr_init_trampoline()</a>

```
searchKey: runtime.pthread_attr_init_trampoline
tags: [function private]
```

```Go
func pthread_attr_init_trampoline()
```

### <a id="pthread_attr_setdetachstate" href="#pthread_attr_setdetachstate">func pthread_attr_setdetachstate(attr *pthreadattr, state int) int32</a>

```
searchKey: runtime.pthread_attr_setdetachstate
tags: [method private]
```

```Go
func pthread_attr_setdetachstate(attr *pthreadattr, state int) int32
```

### <a id="pthread_attr_setdetachstate_trampoline" href="#pthread_attr_setdetachstate_trampoline">func pthread_attr_setdetachstate_trampoline()</a>

```
searchKey: runtime.pthread_attr_setdetachstate_trampoline
tags: [function private]
```

```Go
func pthread_attr_setdetachstate_trampoline()
```

### <a id="pthread_cond_init" href="#pthread_cond_init">func pthread_cond_init(c *pthreadcond, attr *pthreadcondattr) int32</a>

```
searchKey: runtime.pthread_cond_init
tags: [method private]
```

```Go
func pthread_cond_init(c *pthreadcond, attr *pthreadcondattr) int32
```

### <a id="pthread_cond_init_trampoline" href="#pthread_cond_init_trampoline">func pthread_cond_init_trampoline()</a>

```
searchKey: runtime.pthread_cond_init_trampoline
tags: [function private]
```

```Go
func pthread_cond_init_trampoline()
```

### <a id="pthread_cond_signal" href="#pthread_cond_signal">func pthread_cond_signal(c *pthreadcond) int32</a>

```
searchKey: runtime.pthread_cond_signal
tags: [method private]
```

```Go
func pthread_cond_signal(c *pthreadcond) int32
```

### <a id="pthread_cond_signal_trampoline" href="#pthread_cond_signal_trampoline">func pthread_cond_signal_trampoline()</a>

```
searchKey: runtime.pthread_cond_signal_trampoline
tags: [function private]
```

```Go
func pthread_cond_signal_trampoline()
```

### <a id="pthread_cond_timedwait_relative_np" href="#pthread_cond_timedwait_relative_np">func pthread_cond_timedwait_relative_np(c *pthreadcond, m *pthreadmutex, t *timespec) int32</a>

```
searchKey: runtime.pthread_cond_timedwait_relative_np
tags: [method private]
```

```Go
func pthread_cond_timedwait_relative_np(c *pthreadcond, m *pthreadmutex, t *timespec) int32
```

### <a id="pthread_cond_timedwait_relative_np_trampoline" href="#pthread_cond_timedwait_relative_np_trampoline">func pthread_cond_timedwait_relative_np_trampoline()</a>

```
searchKey: runtime.pthread_cond_timedwait_relative_np_trampoline
tags: [function private]
```

```Go
func pthread_cond_timedwait_relative_np_trampoline()
```

### <a id="pthread_cond_wait" href="#pthread_cond_wait">func pthread_cond_wait(c *pthreadcond, m *pthreadmutex) int32</a>

```
searchKey: runtime.pthread_cond_wait
tags: [method private]
```

```Go
func pthread_cond_wait(c *pthreadcond, m *pthreadmutex) int32
```

### <a id="pthread_cond_wait_trampoline" href="#pthread_cond_wait_trampoline">func pthread_cond_wait_trampoline()</a>

```
searchKey: runtime.pthread_cond_wait_trampoline
tags: [function private]
```

```Go
func pthread_cond_wait_trampoline()
```

### <a id="pthread_create" href="#pthread_create">func pthread_create(attr *pthreadattr, start uintptr, arg unsafe.Pointer) int32</a>

```
searchKey: runtime.pthread_create
tags: [method private]
```

```Go
func pthread_create(attr *pthreadattr, start uintptr, arg unsafe.Pointer) int32
```

### <a id="pthread_create_trampoline" href="#pthread_create_trampoline">func pthread_create_trampoline()</a>

```
searchKey: runtime.pthread_create_trampoline
tags: [function private]
```

```Go
func pthread_create_trampoline()
```

### <a id="pthread_kill" href="#pthread_kill">func pthread_kill(t pthread, sig uint32)</a>

```
searchKey: runtime.pthread_kill
tags: [method private]
```

```Go
func pthread_kill(t pthread, sig uint32)
```

### <a id="pthread_kill_trampoline" href="#pthread_kill_trampoline">func pthread_kill_trampoline()</a>

```
searchKey: runtime.pthread_kill_trampoline
tags: [function private]
```

```Go
func pthread_kill_trampoline()
```

### <a id="pthread_mutex_init" href="#pthread_mutex_init">func pthread_mutex_init(m *pthreadmutex, attr *pthreadmutexattr) int32</a>

```
searchKey: runtime.pthread_mutex_init
tags: [method private]
```

```Go
func pthread_mutex_init(m *pthreadmutex, attr *pthreadmutexattr) int32
```

### <a id="pthread_mutex_init_trampoline" href="#pthread_mutex_init_trampoline">func pthread_mutex_init_trampoline()</a>

```
searchKey: runtime.pthread_mutex_init_trampoline
tags: [function private]
```

```Go
func pthread_mutex_init_trampoline()
```

### <a id="pthread_mutex_lock" href="#pthread_mutex_lock">func pthread_mutex_lock(m *pthreadmutex) int32</a>

```
searchKey: runtime.pthread_mutex_lock
tags: [method private]
```

```Go
func pthread_mutex_lock(m *pthreadmutex) int32
```

### <a id="pthread_mutex_lock_trampoline" href="#pthread_mutex_lock_trampoline">func pthread_mutex_lock_trampoline()</a>

```
searchKey: runtime.pthread_mutex_lock_trampoline
tags: [function private]
```

```Go
func pthread_mutex_lock_trampoline()
```

### <a id="pthread_mutex_unlock" href="#pthread_mutex_unlock">func pthread_mutex_unlock(m *pthreadmutex) int32</a>

```
searchKey: runtime.pthread_mutex_unlock
tags: [method private]
```

```Go
func pthread_mutex_unlock(m *pthreadmutex) int32
```

### <a id="pthread_mutex_unlock_trampoline" href="#pthread_mutex_unlock_trampoline">func pthread_mutex_unlock_trampoline()</a>

```
searchKey: runtime.pthread_mutex_unlock_trampoline
tags: [function private]
```

```Go
func pthread_mutex_unlock_trampoline()
```

### <a id="pthread_self_trampoline" href="#pthread_self_trampoline">func pthread_self_trampoline()</a>

```
searchKey: runtime.pthread_self_trampoline
tags: [function private]
```

```Go
func pthread_self_trampoline()
```

### <a id="publicationBarrier" href="#publicationBarrier">func publicationBarrier()</a>

```
searchKey: runtime.publicationBarrier
tags: [function private]
```

```Go
func publicationBarrier()
```

publicationBarrier performs a store/store barrier (a "publication" or "export" barrier). Some form of synchronization is required between initializing an object and making that object accessible to another processor. Without synchronization, the initialization writes and the "publication" write may be reordered, allowing the other processor to follow the pointer and observe an uninitialized object. In general, higher-level synchronization should be used, such as locking or an atomic pointer write. publicationBarrier is for when those aren't an option, such as in the implementation of the memory manager. 

There's no corresponding barrier for the read side because the read side naturally has a data dependency order. All architectures that Go supports or seems likely to ever support automatically enforce data dependency ordering. 

### <a id="putCachedDlogger" href="#putCachedDlogger">func putCachedDlogger(l *dlogger) bool</a>

```
searchKey: runtime.putCachedDlogger
tags: [method private]
```

```Go
func putCachedDlogger(l *dlogger) bool
```

### <a id="putempty" href="#putempty">func putempty(b *workbuf)</a>

```
searchKey: runtime.putempty
tags: [method private]
```

```Go
func putempty(b *workbuf)
```

putempty puts a workbuf onto the work.empty list. Upon entry this goroutine owns b. The lfstack.push relinquishes ownership. 

### <a id="putfull" href="#putfull">func putfull(b *workbuf)</a>

```
searchKey: runtime.putfull
tags: [method private]
```

```Go
func putfull(b *workbuf)
```

putfull puts the workbuf on the work.full list for the GC. putfull accepts partially full buffers so the GC can avoid competing with the mutators for ownership of partially full buffers. 

### <a id="queuefinalizer" href="#queuefinalizer">func queuefinalizer(p unsafe.Pointer, fn *funcval, nret uintptr, fint *_type, ot *ptrtype)</a>

```
searchKey: runtime.queuefinalizer
tags: [method private]
```

```Go
func queuefinalizer(p unsafe.Pointer, fn *funcval, nret uintptr, fint *_type, ot *ptrtype)
```

### <a id="r4" href="#r4">func r4(p unsafe.Pointer) uintptr</a>

```
searchKey: runtime.r4
tags: [method private]
```

```Go
func r4(p unsafe.Pointer) uintptr
```

### <a id="r8" href="#r8">func r8(p unsafe.Pointer) uintptr</a>

```
searchKey: runtime.r8
tags: [method private]
```

```Go
func r8(p unsafe.Pointer) uintptr
```

### <a id="raceReadObjectPC" href="#raceReadObjectPC">func raceReadObjectPC(t *_type, addr unsafe.Pointer, callerpc, pc uintptr)</a>

```
searchKey: runtime.raceReadObjectPC
tags: [method private]
```

```Go
func raceReadObjectPC(t *_type, addr unsafe.Pointer, callerpc, pc uintptr)
```

### <a id="raceWriteObjectPC" href="#raceWriteObjectPC">func raceWriteObjectPC(t *_type, addr unsafe.Pointer, callerpc, pc uintptr)</a>

```
searchKey: runtime.raceWriteObjectPC
tags: [method private]
```

```Go
func raceWriteObjectPC(t *_type, addr unsafe.Pointer, callerpc, pc uintptr)
```

### <a id="raceacquire" href="#raceacquire">func raceacquire(addr unsafe.Pointer)</a>

```
searchKey: runtime.raceacquire
tags: [method private]
```

```Go
func raceacquire(addr unsafe.Pointer)
```

### <a id="raceacquirectx" href="#raceacquirectx">func raceacquirectx(racectx uintptr, addr unsafe.Pointer)</a>

```
searchKey: runtime.raceacquirectx
tags: [method private]
```

```Go
func raceacquirectx(racectx uintptr, addr unsafe.Pointer)
```

### <a id="raceacquireg" href="#raceacquireg">func raceacquireg(gp *g, addr unsafe.Pointer)</a>

```
searchKey: runtime.raceacquireg
tags: [method private]
```

```Go
func raceacquireg(gp *g, addr unsafe.Pointer)
```

### <a id="racectxend" href="#racectxend">func racectxend(racectx uintptr)</a>

```
searchKey: runtime.racectxend
tags: [method private]
```

```Go
func racectxend(racectx uintptr)
```

### <a id="racefingo" href="#racefingo">func racefingo()</a>

```
searchKey: runtime.racefingo
tags: [function private]
```

```Go
func racefingo()
```

### <a id="racefini" href="#racefini">func racefini()</a>

```
searchKey: runtime.racefini
tags: [function private]
```

```Go
func racefini()
```

### <a id="racefree" href="#racefree">func racefree(p unsafe.Pointer, sz uintptr)</a>

```
searchKey: runtime.racefree
tags: [method private]
```

```Go
func racefree(p unsafe.Pointer, sz uintptr)
```

### <a id="racegoend" href="#racegoend">func racegoend()</a>

```
searchKey: runtime.racegoend
tags: [function private]
```

```Go
func racegoend()
```

### <a id="racegostart" href="#racegostart">func racegostart(pc uintptr) uintptr</a>

```
searchKey: runtime.racegostart
tags: [method private]
```

```Go
func racegostart(pc uintptr) uintptr
```

### <a id="raceinit" href="#raceinit">func raceinit() (uintptr, uintptr)</a>

```
searchKey: runtime.raceinit
tags: [function private]
```

```Go
func raceinit() (uintptr, uintptr)
```

### <a id="racemalloc" href="#racemalloc">func racemalloc(p unsafe.Pointer, sz uintptr)</a>

```
searchKey: runtime.racemalloc
tags: [method private]
```

```Go
func racemalloc(p unsafe.Pointer, sz uintptr)
```

### <a id="racemapshadow" href="#racemapshadow">func racemapshadow(addr unsafe.Pointer, size uintptr)</a>

```
searchKey: runtime.racemapshadow
tags: [method private]
```

```Go
func racemapshadow(addr unsafe.Pointer, size uintptr)
```

### <a id="racenotify" href="#racenotify">func racenotify(c *hchan, idx uint, sg *sudog)</a>

```
searchKey: runtime.racenotify
tags: [method private]
```

```Go
func racenotify(c *hchan, idx uint, sg *sudog)
```

Notify the race detector of a send or receive involving buffer entry idx and a channel c or its communicating partner sg. This function handles the special case of c.elemsize==0. 

### <a id="raceproccreate" href="#raceproccreate">func raceproccreate() uintptr</a>

```
searchKey: runtime.raceproccreate
tags: [function private]
```

```Go
func raceproccreate() uintptr
```

### <a id="raceprocdestroy" href="#raceprocdestroy">func raceprocdestroy(ctx uintptr)</a>

```
searchKey: runtime.raceprocdestroy
tags: [method private]
```

```Go
func raceprocdestroy(ctx uintptr)
```

### <a id="racereadpc" href="#racereadpc">func racereadpc(addr unsafe.Pointer, callerpc, pc uintptr)</a>

```
searchKey: runtime.racereadpc
tags: [method private]
```

```Go
func racereadpc(addr unsafe.Pointer, callerpc, pc uintptr)
```

### <a id="racereadrangepc" href="#racereadrangepc">func racereadrangepc(addr unsafe.Pointer, sz, callerpc, pc uintptr)</a>

```
searchKey: runtime.racereadrangepc
tags: [method private]
```

```Go
func racereadrangepc(addr unsafe.Pointer, sz, callerpc, pc uintptr)
```

### <a id="racerelease" href="#racerelease">func racerelease(addr unsafe.Pointer)</a>

```
searchKey: runtime.racerelease
tags: [method private]
```

```Go
func racerelease(addr unsafe.Pointer)
```

### <a id="racereleaseacquire" href="#racereleaseacquire">func racereleaseacquire(addr unsafe.Pointer)</a>

```
searchKey: runtime.racereleaseacquire
tags: [method private]
```

```Go
func racereleaseacquire(addr unsafe.Pointer)
```

### <a id="racereleaseacquireg" href="#racereleaseacquireg">func racereleaseacquireg(gp *g, addr unsafe.Pointer)</a>

```
searchKey: runtime.racereleaseacquireg
tags: [method private]
```

```Go
func racereleaseacquireg(gp *g, addr unsafe.Pointer)
```

### <a id="racereleaseg" href="#racereleaseg">func racereleaseg(gp *g, addr unsafe.Pointer)</a>

```
searchKey: runtime.racereleaseg
tags: [method private]
```

```Go
func racereleaseg(gp *g, addr unsafe.Pointer)
```

### <a id="racereleasemerge" href="#racereleasemerge">func racereleasemerge(addr unsafe.Pointer)</a>

```
searchKey: runtime.racereleasemerge
tags: [method private]
```

```Go
func racereleasemerge(addr unsafe.Pointer)
```

### <a id="racereleasemergeg" href="#racereleasemergeg">func racereleasemergeg(gp *g, addr unsafe.Pointer)</a>

```
searchKey: runtime.racereleasemergeg
tags: [method private]
```

```Go
func racereleasemergeg(gp *g, addr unsafe.Pointer)
```

### <a id="racesync" href="#racesync">func racesync(c *hchan, sg *sudog)</a>

```
searchKey: runtime.racesync
tags: [method private]
```

```Go
func racesync(c *hchan, sg *sudog)
```

### <a id="racewritepc" href="#racewritepc">func racewritepc(addr unsafe.Pointer, callerpc, pc uintptr)</a>

```
searchKey: runtime.racewritepc
tags: [method private]
```

```Go
func racewritepc(addr unsafe.Pointer, callerpc, pc uintptr)
```

### <a id="racewriterangepc" href="#racewriterangepc">func racewriterangepc(addr unsafe.Pointer, sz, callerpc, pc uintptr)</a>

```
searchKey: runtime.racewriterangepc
tags: [method private]
```

```Go
func racewriterangepc(addr unsafe.Pointer, sz, callerpc, pc uintptr)
```

### <a id="raise" href="#raise">func raise(sig uint32)</a>

```
searchKey: runtime.raise
tags: [method private]
```

```Go
func raise(sig uint32)
```

### <a id="raise_trampoline" href="#raise_trampoline">func raise_trampoline()</a>

```
searchKey: runtime.raise_trampoline
tags: [function private]
```

```Go
func raise_trampoline()
```

### <a id="raisebadsignal" href="#raisebadsignal">func raisebadsignal(sig uint32, c *sigctxt)</a>

```
searchKey: runtime.raisebadsignal
tags: [method private]
```

```Go
func raisebadsignal(sig uint32, c *sigctxt)
```

raisebadsignal is called when a signal is received on a non-Go thread, and the Go program does not want to handle it (that is, the program has not called os/signal.Notify for the signal). 

### <a id="raiseproc" href="#raiseproc">func raiseproc(sig uint32)</a>

```
searchKey: runtime.raiseproc
tags: [method private]
```

```Go
func raiseproc(sig uint32)
```

### <a id="raiseproc_trampoline" href="#raiseproc_trampoline">func raiseproc_trampoline()</a>

```
searchKey: runtime.raiseproc_trampoline
tags: [function private]
```

```Go
func raiseproc_trampoline()
```

### <a id="rawbyteslice" href="#rawbyteslice">func rawbyteslice(size int) (b []byte)</a>

```
searchKey: runtime.rawbyteslice
tags: [method private]
```

```Go
func rawbyteslice(size int) (b []byte)
```

rawbyteslice allocates a new byte slice. The byte slice is not zeroed. 

### <a id="rawruneslice" href="#rawruneslice">func rawruneslice(size int) (b []rune)</a>

```
searchKey: runtime.rawruneslice
tags: [method private]
```

```Go
func rawruneslice(size int) (b []rune)
```

rawruneslice allocates a new rune slice. The rune slice is not zeroed. 

### <a id="rawstring" href="#rawstring">func rawstring(size int) (s string, b []byte)</a>

```
searchKey: runtime.rawstring
tags: [method private]
```

```Go
func rawstring(size int) (s string, b []byte)
```

rawstring allocates storage for a new string. The returned string and byte slice both refer to the same storage. The storage is not zeroed. Callers should use b to set the string contents and then drop b. 

### <a id="rawstringtmp" href="#rawstringtmp">func rawstringtmp(buf *tmpBuf, l int) (s string, b []byte)</a>

```
searchKey: runtime.rawstringtmp
tags: [method private]
```

```Go
func rawstringtmp(buf *tmpBuf, l int) (s string, b []byte)
```

### <a id="read" href="#read">func read(fd int32, p unsafe.Pointer, n int32) int32</a>

```
searchKey: runtime.read
tags: [method private]
```

```Go
func read(fd int32, p unsafe.Pointer, n int32) int32
```

### <a id="readGCStats" href="#readGCStats">func readGCStats(pauses *[]uint64)</a>

```
searchKey: runtime.readGCStats
tags: [method private]
```

```Go
func readGCStats(pauses *[]uint64)
```

### <a id="readGCStats_m" href="#readGCStats_m">func readGCStats_m(pauses *[]uint64)</a>

```
searchKey: runtime.readGCStats_m
tags: [method private]
```

```Go
func readGCStats_m(pauses *[]uint64)
```

readGCStats_m must be called on the system stack because it acquires the heap lock. See mheap for details. 

### <a id="readGOGC" href="#readGOGC">func readGOGC() int32</a>

```
searchKey: runtime.readGOGC
tags: [function private]
```

```Go
func readGOGC() int32
```

### <a id="readMetrics" href="#readMetrics">func readMetrics(samplesp unsafe.Pointer, len int, cap int)</a>

```
searchKey: runtime.readMetrics
tags: [method private]
```

```Go
func readMetrics(samplesp unsafe.Pointer, len int, cap int)
```

readMetrics is the implementation of runtime/metrics.Read. 

### <a id="readUnaligned32" href="#readUnaligned32">func readUnaligned32(p unsafe.Pointer) uint32</a>

```
searchKey: runtime.readUnaligned32
tags: [method private]
```

```Go
func readUnaligned32(p unsafe.Pointer) uint32
```

Note: These routines perform the read with a native endianness. 

### <a id="readUnaligned64" href="#readUnaligned64">func readUnaligned64(p unsafe.Pointer) uint64</a>

```
searchKey: runtime.readUnaligned64
tags: [method private]
```

```Go
func readUnaligned64(p unsafe.Pointer) uint64
```

### <a id="read_trampoline" href="#read_trampoline">func read_trampoline()</a>

```
searchKey: runtime.read_trampoline
tags: [function private]
```

```Go
func read_trampoline()
```

### <a id="readgstatus" href="#readgstatus">func readgstatus(gp *g) uint32</a>

```
searchKey: runtime.readgstatus
tags: [method private]
```

```Go
func readgstatus(gp *g) uint32
```

All reads and writes of g's status go through readgstatus, casgstatus castogscanstatus, casfrom_Gscanstatus. 

### <a id="readmemstats_m" href="#readmemstats_m">func readmemstats_m(stats *MemStats)</a>

```
searchKey: runtime.readmemstats_m
tags: [method private]
```

```Go
func readmemstats_m(stats *MemStats)
```

### <a id="readvarint" href="#readvarint">func readvarint(p []byte) (read uint32, val uint32)</a>

```
searchKey: runtime.readvarint
tags: [method private]
```

```Go
func readvarint(p []byte) (read uint32, val uint32)
```

readvarint reads a varint from p. 

### <a id="readvarintUnsafe" href="#readvarintUnsafe">func readvarintUnsafe(fd unsafe.Pointer) (uint32, unsafe.Pointer)</a>

```
searchKey: runtime.readvarintUnsafe
tags: [method private]
```

```Go
func readvarintUnsafe(fd unsafe.Pointer) (uint32, unsafe.Pointer)
```

readvarintUnsafe reads the uint32 in varint format starting at fd, and returns the uint32 and a pointer to the byte following the varint. 

There is a similar function runtime.readvarint, which takes a slice of bytes, rather than an unsafe pointer. These functions are duplicated, because one of the two use cases for the functions would get slower if the functions were combined. 

### <a id="ready" href="#ready">func ready(gp *g, traceskip int, next bool)</a>

```
searchKey: runtime.ready
tags: [method private]
```

```Go
func ready(gp *g, traceskip int, next bool)
```

Mark gp ready to run. 

### <a id="readyForScavenger" href="#readyForScavenger">func readyForScavenger()</a>

```
searchKey: runtime.readyForScavenger
tags: [function private]
```

```Go
func readyForScavenger()
```

readyForScavenger signals sysmon to wake the scavenger because there may be new work to do. 

There may be a significant delay between when this function runs and when the scavenger is kicked awake, but it may be safely invoked in contexts where wakeScavenger is unsafe to call directly. 

### <a id="readyWithTime" href="#readyWithTime">func readyWithTime(s *sudog, traceskip int)</a>

```
searchKey: runtime.readyWithTime
tags: [method private]
```

```Go
func readyWithTime(s *sudog, traceskip int)
```

### <a id="record" href="#record">func record(r *MemProfileRecord, b *bucket)</a>

```
searchKey: runtime.record
tags: [method private]
```

```Go
func record(r *MemProfileRecord, b *bucket)
```

Write b's data to r. 

### <a id="recordForPanic" href="#recordForPanic">func recordForPanic(b []byte)</a>

```
searchKey: runtime.recordForPanic
tags: [method private]
```

```Go
func recordForPanic(b []byte)
```

recordForPanic maintains a circular buffer of messages written by the runtime leading up to a process crash, allowing the messages to be extracted from a core dump. 

The text written during a process crash (following "panic" or "fatal error") is not saved, since the goroutine stacks will generally be readable from the runtime datastructures in the core file. 

### <a id="recordspan" href="#recordspan">func recordspan(vh unsafe.Pointer, p unsafe.Pointer)</a>

```
searchKey: runtime.recordspan
tags: [method private]
```

```Go
func recordspan(vh unsafe.Pointer, p unsafe.Pointer)
```

recordspan adds a newly allocated span to h.allspans. 

This only happens the first time a span is allocated from mheap.spanalloc (it is not called when a span is reused). 

Write barriers are disallowed here because it can be called from gcWork when allocating new workbufs. However, because it's an indirect call from the fixalloc initializer, the compiler can't see this. 

The heap lock must be held. 

### <a id="recovery" href="#recovery">func recovery(gp *g)</a>

```
searchKey: runtime.recovery
tags: [method private]
```

```Go
func recovery(gp *g)
```

Unwind the stack after a deferred function calls recover after a panic. Then arrange to continue running as though the caller of the deferred function returned normally. 

### <a id="recv" href="#recv">func recv(c *hchan, sg *sudog, ep unsafe.Pointer, unlockf func(), skip int)</a>

```
searchKey: runtime.recv
tags: [method private]
```

```Go
func recv(c *hchan, sg *sudog, ep unsafe.Pointer, unlockf func(), skip int)
```

recv processes a receive operation on a full channel c. There are 2 parts: 1) The value sent by the sender sg is put into the channel 

```
and the sender is woken up to go on its merry way.

```
2) The value received by the receiver (the current G) is 

```
written to ep.

```
For synchronous channels, both values are the same. For asynchronous channels, the receiver gets its data from the channel buffer and the sender's data is put in the channel buffer. Channel c must be full and locked. recv unlocks c with unlockf. sg must already be dequeued from c. A non-nil ep must point to the heap or the caller's stack. 

### <a id="recvDirect" href="#recvDirect">func recvDirect(t *_type, sg *sudog, dst unsafe.Pointer)</a>

```
searchKey: runtime.recvDirect
tags: [method private]
```

```Go
func recvDirect(t *_type, sg *sudog, dst unsafe.Pointer)
```

### <a id="reentersyscall" href="#reentersyscall">func reentersyscall(pc, sp uintptr)</a>

```
searchKey: runtime.reentersyscall
tags: [method private]
```

```Go
func reentersyscall(pc, sp uintptr)
```

The goroutine g is about to enter a system call. Record that it's not using the cpu anymore. This is called only from the go syscall library and cgocall, not from the low-level system calls used by the runtime. 

Entersyscall cannot split the stack: the save must make g->sched refer to the caller's stack segment, because entersyscall is going to return immediately after. 

Nothing entersyscall calls can split the stack either. We cannot safely move the stack during an active call to syscall, because we do not know which of the uintptr arguments are really pointers (back into the stack). In practice, this means that we make the fast path run through entersyscall doing no-split things, and the slow path has to use systemstack to run bigger things on the system stack. 

reentersyscall is the entry point used by cgo callbacks, where explicitly saved SP and PC are restored. This is needed when exitsyscall will be called from a function further up in the call stack than the parent, as g->syscallsp must always point to a valid stack frame. entersyscall below is the normal entry point for syscalls, which obtains the SP and PC from the caller. 

Syscall tracing: At the start of a syscall we emit traceGoSysCall to capture the stack trace. If the syscall does not block, that is it, we do not emit any other events. If the syscall blocks (that is, P is retaken), retaker emits traceGoSysBlock; when syscall returns we emit traceGoSysExit and when the goroutine starts running (potentially instantly, if exitsyscallfast returns true) we emit traceGoStart. To ensure that traceGoSysExit is emitted strictly after traceGoSysBlock, we remember current value of syscalltick in m (_g_.m.syscalltick = _g_.m.p.ptr().syscalltick), whoever emits traceGoSysBlock increments p.syscalltick afterwards; and we wait for the increment before emitting traceGoSysExit. Note that the increment is done even if tracing is not enabled, because tracing can be enabled in the middle of syscall. We don't want the wait to hang. 

### <a id="reflectOffsLock" href="#reflectOffsLock">func reflectOffsLock()</a>

```
searchKey: runtime.reflectOffsLock
tags: [function private]
```

```Go
func reflectOffsLock()
```

### <a id="reflectOffsUnlock" href="#reflectOffsUnlock">func reflectOffsUnlock()</a>

```
searchKey: runtime.reflectOffsUnlock
tags: [function private]
```

```Go
func reflectOffsUnlock()
```

### <a id="reflect_addReflectOff" href="#reflect_addReflectOff">func reflect_addReflectOff(ptr unsafe.Pointer) int32</a>

```
searchKey: runtime.reflect_addReflectOff
tags: [method private]
```

```Go
func reflect_addReflectOff(ptr unsafe.Pointer) int32
```

reflect_addReflectOff adds a pointer to the reflection offset lookup map. 

### <a id="reflect_chancap" href="#reflect_chancap">func reflect_chancap(c *hchan) int</a>

```
searchKey: runtime.reflect_chancap
tags: [method private]
```

```Go
func reflect_chancap(c *hchan) int
```

### <a id="reflect_chanclose" href="#reflect_chanclose">func reflect_chanclose(c *hchan)</a>

```
searchKey: runtime.reflect_chanclose
tags: [method private]
```

```Go
func reflect_chanclose(c *hchan)
```

### <a id="reflect_chanlen" href="#reflect_chanlen">func reflect_chanlen(c *hchan) int</a>

```
searchKey: runtime.reflect_chanlen
tags: [method private]
```

```Go
func reflect_chanlen(c *hchan) int
```

### <a id="reflect_chanrecv" href="#reflect_chanrecv">func reflect_chanrecv(c *hchan, nb bool, elem unsafe.Pointer) (selected bool, received bool)</a>

```
searchKey: runtime.reflect_chanrecv
tags: [method private]
```

```Go
func reflect_chanrecv(c *hchan, nb bool, elem unsafe.Pointer) (selected bool, received bool)
```

### <a id="reflect_chansend" href="#reflect_chansend">func reflect_chansend(c *hchan, elem unsafe.Pointer, nb bool) (selected bool)</a>

```
searchKey: runtime.reflect_chansend
tags: [method private]
```

```Go
func reflect_chansend(c *hchan, elem unsafe.Pointer, nb bool) (selected bool)
```

### <a id="reflect_gcbits" href="#reflect_gcbits">func reflect_gcbits(x interface{}) []byte</a>

```
searchKey: runtime.reflect_gcbits
tags: [method private]
```

```Go
func reflect_gcbits(x interface{}) []byte
```

gcbits returns the GC type info for x, for testing. The result is the bitmap entries (0 or 1), one entry per byte. 

### <a id="reflect_ifaceE2I" href="#reflect_ifaceE2I">func reflect_ifaceE2I(inter *interfacetype, e eface, dst *iface)</a>

```
searchKey: runtime.reflect_ifaceE2I
tags: [method private]
```

```Go
func reflect_ifaceE2I(inter *interfacetype, e eface, dst *iface)
```

### <a id="reflect_mapaccess" href="#reflect_mapaccess">func reflect_mapaccess(t *maptype, h *hmap, key unsafe.Pointer) unsafe.Pointer</a>

```
searchKey: runtime.reflect_mapaccess
tags: [method private]
```

```Go
func reflect_mapaccess(t *maptype, h *hmap, key unsafe.Pointer) unsafe.Pointer
```

### <a id="reflect_mapassign" href="#reflect_mapassign">func reflect_mapassign(t *maptype, h *hmap, key unsafe.Pointer, elem unsafe.Pointer)</a>

```
searchKey: runtime.reflect_mapassign
tags: [method private]
```

```Go
func reflect_mapassign(t *maptype, h *hmap, key unsafe.Pointer, elem unsafe.Pointer)
```

### <a id="reflect_mapdelete" href="#reflect_mapdelete">func reflect_mapdelete(t *maptype, h *hmap, key unsafe.Pointer)</a>

```
searchKey: runtime.reflect_mapdelete
tags: [method private]
```

```Go
func reflect_mapdelete(t *maptype, h *hmap, key unsafe.Pointer)
```

### <a id="reflect_mapiterelem" href="#reflect_mapiterelem">func reflect_mapiterelem(it *hiter) unsafe.Pointer</a>

```
searchKey: runtime.reflect_mapiterelem
tags: [method private]
```

```Go
func reflect_mapiterelem(it *hiter) unsafe.Pointer
```

### <a id="reflect_mapiterkey" href="#reflect_mapiterkey">func reflect_mapiterkey(it *hiter) unsafe.Pointer</a>

```
searchKey: runtime.reflect_mapiterkey
tags: [method private]
```

```Go
func reflect_mapiterkey(it *hiter) unsafe.Pointer
```

### <a id="reflect_mapiternext" href="#reflect_mapiternext">func reflect_mapiternext(it *hiter)</a>

```
searchKey: runtime.reflect_mapiternext
tags: [method private]
```

```Go
func reflect_mapiternext(it *hiter)
```

### <a id="reflect_maplen" href="#reflect_maplen">func reflect_maplen(h *hmap) int</a>

```
searchKey: runtime.reflect_maplen
tags: [method private]
```

```Go
func reflect_maplen(h *hmap) int
```

### <a id="reflect_memclrNoHeapPointers" href="#reflect_memclrNoHeapPointers">func reflect_memclrNoHeapPointers(ptr unsafe.Pointer, n uintptr)</a>

```
searchKey: runtime.reflect_memclrNoHeapPointers
tags: [method private]
```

```Go
func reflect_memclrNoHeapPointers(ptr unsafe.Pointer, n uintptr)
```

### <a id="reflect_memmove" href="#reflect_memmove">func reflect_memmove(to, from unsafe.Pointer, n uintptr)</a>

```
searchKey: runtime.reflect_memmove
tags: [method private]
```

```Go
func reflect_memmove(to, from unsafe.Pointer, n uintptr)
```

### <a id="reflect_resolveNameOff" href="#reflect_resolveNameOff">func reflect_resolveNameOff(ptrInModule unsafe.Pointer, off int32) unsafe.Pointer</a>

```
searchKey: runtime.reflect_resolveNameOff
tags: [method private]
```

```Go
func reflect_resolveNameOff(ptrInModule unsafe.Pointer, off int32) unsafe.Pointer
```

reflect_resolveNameOff resolves a name offset from a base pointer. 

### <a id="reflect_resolveTextOff" href="#reflect_resolveTextOff">func reflect_resolveTextOff(rtype unsafe.Pointer, off int32) unsafe.Pointer</a>

```
searchKey: runtime.reflect_resolveTextOff
tags: [method private]
```

```Go
func reflect_resolveTextOff(rtype unsafe.Pointer, off int32) unsafe.Pointer
```

reflect_resolveTextOff resolves a function pointer offset from a base type. 

### <a id="reflect_resolveTypeOff" href="#reflect_resolveTypeOff">func reflect_resolveTypeOff(rtype unsafe.Pointer, off int32) unsafe.Pointer</a>

```
searchKey: runtime.reflect_resolveTypeOff
tags: [method private]
```

```Go
func reflect_resolveTypeOff(rtype unsafe.Pointer, off int32) unsafe.Pointer
```

reflect_resolveTypeOff resolves an *rtype offset from a base type. 

### <a id="reflect_rselect" href="#reflect_rselect">func reflect_rselect(cases []runtimeSelect) (int, bool)</a>

```
searchKey: runtime.reflect_rselect
tags: [method private]
```

```Go
func reflect_rselect(cases []runtimeSelect) (int, bool)
```

### <a id="reflect_typedmemclr" href="#reflect_typedmemclr">func reflect_typedmemclr(typ *_type, ptr unsafe.Pointer)</a>

```
searchKey: runtime.reflect_typedmemclr
tags: [method private]
```

```Go
func reflect_typedmemclr(typ *_type, ptr unsafe.Pointer)
```

### <a id="reflect_typedmemclrpartial" href="#reflect_typedmemclrpartial">func reflect_typedmemclrpartial(typ *_type, ptr unsafe.Pointer, off, size uintptr)</a>

```
searchKey: runtime.reflect_typedmemclrpartial
tags: [method private]
```

```Go
func reflect_typedmemclrpartial(typ *_type, ptr unsafe.Pointer, off, size uintptr)
```

### <a id="reflect_typedmemmove" href="#reflect_typedmemmove">func reflect_typedmemmove(typ *_type, dst, src unsafe.Pointer)</a>

```
searchKey: runtime.reflect_typedmemmove
tags: [method private]
```

```Go
func reflect_typedmemmove(typ *_type, dst, src unsafe.Pointer)
```

### <a id="reflect_typedmemmovepartial" href="#reflect_typedmemmovepartial">func reflect_typedmemmovepartial(typ *_type, dst, src unsafe.Pointer, off, size uintptr)</a>

```
searchKey: runtime.reflect_typedmemmovepartial
tags: [method private]
```

```Go
func reflect_typedmemmovepartial(typ *_type, dst, src unsafe.Pointer, off, size uintptr)
```

typedmemmovepartial is like typedmemmove but assumes that dst and src point off bytes into the value and only copies size bytes. off must be a multiple of sys.PtrSize. 

### <a id="reflect_typedslicecopy" href="#reflect_typedslicecopy">func reflect_typedslicecopy(elemType *_type, dst, src slice) int</a>

```
searchKey: runtime.reflect_typedslicecopy
tags: [method private]
```

```Go
func reflect_typedslicecopy(elemType *_type, dst, src slice) int
```

### <a id="reflect_typehash" href="#reflect_typehash">func reflect_typehash(t *_type, p unsafe.Pointer, h uintptr) uintptr</a>

```
searchKey: runtime.reflect_typehash
tags: [method private]
```

```Go
func reflect_typehash(t *_type, p unsafe.Pointer, h uintptr) uintptr
```

### <a id="reflect_typelinks" href="#reflect_typelinks">func reflect_typelinks() ([]unsafe.Pointer, [][]int32)</a>

```
searchKey: runtime.reflect_typelinks
tags: [function private]
```

```Go
func reflect_typelinks() ([]unsafe.Pointer, [][]int32)
```

### <a id="reflect_unsafe_New" href="#reflect_unsafe_New">func reflect_unsafe_New(typ *_type) unsafe.Pointer</a>

```
searchKey: runtime.reflect_unsafe_New
tags: [method private]
```

```Go
func reflect_unsafe_New(typ *_type) unsafe.Pointer
```

### <a id="reflect_unsafe_NewArray" href="#reflect_unsafe_NewArray">func reflect_unsafe_NewArray(typ *_type, n int) unsafe.Pointer</a>

```
searchKey: runtime.reflect_unsafe_NewArray
tags: [method private]
```

```Go
func reflect_unsafe_NewArray(typ *_type, n int) unsafe.Pointer
```

### <a id="reflectcall" href="#reflectcall">func reflectcall(stackArgsType *_type, fn, stackArgs unsafe.Pointer, stackArgsSize, stackRetOffset, frameSize uint32, regArgs *abi.RegArgs)</a>

```
searchKey: runtime.reflectcall
tags: [method private]
```

```Go
func reflectcall(stackArgsType *_type, fn, stackArgs unsafe.Pointer, stackArgsSize, stackRetOffset, frameSize uint32, regArgs *abi.RegArgs)
```

reflectcall calls fn with arguments described by stackArgs, stackArgsSize, frameSize, and regArgs. 

Arguments passed on the stack and space for return values passed on the stack must be laid out at the space pointed to by stackArgs (with total length stackArgsSize) according to the ABI. 

stackRetOffset must be some value <= stackArgsSize that indicates the offset within stackArgs where the return value space begins. 

frameSize is the total size of the argument frame at stackArgs and must therefore be >= stackArgsSize. It must include additional space for spilling register arguments for stack growth and preemption. 

TODO(mknyszek): Once we don't need the additional spill space, remove frameSize, since frameSize will be redundant with stackArgsSize. 

Arguments passed in registers must be laid out in regArgs according to the ABI. regArgs will hold any return values passed in registers after the call. 

reflectcall copies stack arguments from stackArgs to the goroutine stack, and then copies back stackArgsSize-stackRetOffset bytes back to the return space in stackArgs once fn has completed. It also "unspills" argument registers from regArgs before calling fn, and spills them back into regArgs immediately following the call to fn. If there are results being returned on the stack, the caller should pass the argument frame type as stackArgsType so that reflectcall can execute appropriate write barriers during the copy. 

reflectcall expects regArgs.ReturnIsPtr to be populated indicating which registers on the return path will contain Go pointers. It will then store these pointers in regArgs.Ptrs such that they are visible to the GC. 

Package reflect passes a frame type. In package runtime, there is only one call that copies results back, in callbackWrap in syscall_windows.go, and it does NOT pass a frame type, meaning there are no write barriers invoked. See that call site for justification. 

Package reflect accesses this symbol through a linkname. 

Arguments passed through to reflectcall do not escape. The type is used only in a very limited callee of reflectcall, the stackArgs are copied, and regArgs is only used in the reflectcall frame. 

### <a id="reflectcallSave" href="#reflectcallSave">func reflectcallSave(p *_panic, fn, arg unsafe.Pointer, argsize uint32)</a>

```
searchKey: runtime.reflectcallSave
tags: [method private]
```

```Go
func reflectcallSave(p *_panic, fn, arg unsafe.Pointer, argsize uint32)
```

reflectcallSave calls reflectcall after saving the caller's pc and sp in the panic record. This allows the runtime to return to the Goexit defer processing loop, in the unusual case where the Goexit may be bypassed by a successful recover. 

This is marked as a wrapper by the compiler so it doesn't appear in tracebacks. 

### <a id="reflectcallmove" href="#reflectcallmove">func reflectcallmove(typ *_type, dst, src unsafe.Pointer, size uintptr, regs *abi.RegArgs)</a>

```
searchKey: runtime.reflectcallmove
tags: [method private]
```

```Go
func reflectcallmove(typ *_type, dst, src unsafe.Pointer, size uintptr, regs *abi.RegArgs)
```

reflectcallmove is invoked by reflectcall to copy the return values out of the stack and into the heap, invoking the necessary write barriers. dst, src, and size describe the return value area to copy. typ describes the entire frame (not just the return values). typ may be nil, which indicates write barriers are not needed. 

It must be nosplit and must only call nosplit functions because the stack map of reflectcall is wrong. 

### <a id="reflectlite_chanlen" href="#reflectlite_chanlen">func reflectlite_chanlen(c *hchan) int</a>

```
searchKey: runtime.reflectlite_chanlen
tags: [method private]
```

```Go
func reflectlite_chanlen(c *hchan) int
```

### <a id="reflectlite_ifaceE2I" href="#reflectlite_ifaceE2I">func reflectlite_ifaceE2I(inter *interfacetype, e eface, dst *iface)</a>

```
searchKey: runtime.reflectlite_ifaceE2I
tags: [method private]
```

```Go
func reflectlite_ifaceE2I(inter *interfacetype, e eface, dst *iface)
```

### <a id="reflectlite_maplen" href="#reflectlite_maplen">func reflectlite_maplen(h *hmap) int</a>

```
searchKey: runtime.reflectlite_maplen
tags: [method private]
```

```Go
func reflectlite_maplen(h *hmap) int
```

### <a id="reflectlite_resolveNameOff" href="#reflectlite_resolveNameOff">func reflectlite_resolveNameOff(ptrInModule unsafe.Pointer, off int32) unsafe.Pointer</a>

```
searchKey: runtime.reflectlite_resolveNameOff
tags: [method private]
```

```Go
func reflectlite_resolveNameOff(ptrInModule unsafe.Pointer, off int32) unsafe.Pointer
```

reflectlite_resolveNameOff resolves a name offset from a base pointer. 

### <a id="reflectlite_resolveTypeOff" href="#reflectlite_resolveTypeOff">func reflectlite_resolveTypeOff(rtype unsafe.Pointer, off int32) unsafe.Pointer</a>

```
searchKey: runtime.reflectlite_resolveTypeOff
tags: [method private]
```

```Go
func reflectlite_resolveTypeOff(rtype unsafe.Pointer, off int32) unsafe.Pointer
```

reflectlite_resolveTypeOff resolves an *rtype offset from a base type. 

### <a id="reflectlite_typedmemmove" href="#reflectlite_typedmemmove">func reflectlite_typedmemmove(typ *_type, dst, src unsafe.Pointer)</a>

```
searchKey: runtime.reflectlite_typedmemmove
tags: [method private]
```

```Go
func reflectlite_typedmemmove(typ *_type, dst, src unsafe.Pointer)
```

### <a id="reflectlite_unsafe_New" href="#reflectlite_unsafe_New">func reflectlite_unsafe_New(typ *_type) unsafe.Pointer</a>

```
searchKey: runtime.reflectlite_unsafe_New
tags: [method private]
```

```Go
func reflectlite_unsafe_New(typ *_type) unsafe.Pointer
```

### <a id="releaseLockRank" href="#releaseLockRank">func releaseLockRank(rank lockRank)</a>

```
searchKey: runtime.releaseLockRank
tags: [method private]
```

```Go
func releaseLockRank(rank lockRank)
```

This function may be called in nosplit context and thus must be nosplit. 

### <a id="releaseSudog" href="#releaseSudog">func releaseSudog(s *sudog)</a>

```
searchKey: runtime.releaseSudog
tags: [method private]
```

```Go
func releaseSudog(s *sudog)
```

### <a id="releasem" href="#releasem">func releasem(mp *m)</a>

```
searchKey: runtime.releasem
tags: [method private]
```

```Go
func releasem(mp *m)
```

### <a id="removefinalizer" href="#removefinalizer">func removefinalizer(p unsafe.Pointer)</a>

```
searchKey: runtime.removefinalizer
tags: [method private]
```

```Go
func removefinalizer(p unsafe.Pointer)
```

Removes the finalizer (if any) from the object p. 

### <a id="resetForSleep" href="#resetForSleep">func resetForSleep(gp *g, ut unsafe.Pointer) bool</a>

```
searchKey: runtime.resetForSleep
tags: [method private]
```

```Go
func resetForSleep(gp *g, ut unsafe.Pointer) bool
```

resetForSleep is called after the goroutine is parked for timeSleep. We can't call resettimer in timeSleep itself because if this is a short sleep and there are many goroutines then the P can wind up running the timer function, goroutineReady, before the goroutine has been parked. 

### <a id="resetTimer" href="#resetTimer">func resetTimer(t *timer, when int64) bool</a>

```
searchKey: runtime.resetTimer
tags: [method private]
```

```Go
func resetTimer(t *timer, when int64) bool
```

resetTimer resets an inactive timer, adding it to the heap. Reports whether the timer was modified before it was run. 

### <a id="resetspinning" href="#resetspinning">func resetspinning()</a>

```
searchKey: runtime.resetspinning
tags: [function private]
```

```Go
func resetspinning()
```

### <a id="resettimer" href="#resettimer">func resettimer(t *timer, when int64) bool</a>

```
searchKey: runtime.resettimer
tags: [method private]
```

```Go
func resettimer(t *timer, when int64) bool
```

resettimer resets the time when a timer should fire. If used for an inactive timer, the timer will become active. This should be called instead of addtimer if the timer value has been, or may have been, used previously. Reports whether the timer was modified before it was run. 

### <a id="restoreGsignalStack" href="#restoreGsignalStack">func restoreGsignalStack(st *gsignalStack)</a>

```
searchKey: runtime.restoreGsignalStack
tags: [method private]
```

```Go
func restoreGsignalStack(st *gsignalStack)
```

restoreGsignalStack restores the gsignal stack to the value it had before entering the signal handler. 

### <a id="resumeG" href="#resumeG">func resumeG(state suspendGState)</a>

```
searchKey: runtime.resumeG
tags: [method private]
```

```Go
func resumeG(state suspendGState)
```

resumeG undoes the effects of suspendG, allowing the suspended goroutine to continue from its current safe-point. 

### <a id="retake" href="#retake">func retake(now int64) uint32</a>

```
searchKey: runtime.retake
tags: [method private]
```

```Go
func retake(now int64) uint32
```

### <a id="retpolineAX" href="#retpolineAX">func retpolineAX()</a>

```
searchKey: runtime.retpolineAX
tags: [function private]
```

```Go
func retpolineAX()
```

Retpolines, used by -spectre=ret flag in cmd/asm, cmd/compile. 

### <a id="retpolineBP" href="#retpolineBP">func retpolineBP()</a>

```
searchKey: runtime.retpolineBP
tags: [function private]
```

```Go
func retpolineBP()
```

### <a id="retpolineBX" href="#retpolineBX">func retpolineBX()</a>

```
searchKey: runtime.retpolineBX
tags: [function private]
```

```Go
func retpolineBX()
```

### <a id="retpolineCX" href="#retpolineCX">func retpolineCX()</a>

```
searchKey: runtime.retpolineCX
tags: [function private]
```

```Go
func retpolineCX()
```

### <a id="retpolineDI" href="#retpolineDI">func retpolineDI()</a>

```
searchKey: runtime.retpolineDI
tags: [function private]
```

```Go
func retpolineDI()
```

### <a id="retpolineDX" href="#retpolineDX">func retpolineDX()</a>

```
searchKey: runtime.retpolineDX
tags: [function private]
```

```Go
func retpolineDX()
```

### <a id="retpolineR10" href="#retpolineR10">func retpolineR10()</a>

```
searchKey: runtime.retpolineR10
tags: [function private]
```

```Go
func retpolineR10()
```

### <a id="retpolineR11" href="#retpolineR11">func retpolineR11()</a>

```
searchKey: runtime.retpolineR11
tags: [function private]
```

```Go
func retpolineR11()
```

### <a id="retpolineR12" href="#retpolineR12">func retpolineR12()</a>

```
searchKey: runtime.retpolineR12
tags: [function private]
```

```Go
func retpolineR12()
```

### <a id="retpolineR13" href="#retpolineR13">func retpolineR13()</a>

```
searchKey: runtime.retpolineR13
tags: [function private]
```

```Go
func retpolineR13()
```

### <a id="retpolineR14" href="#retpolineR14">func retpolineR14()</a>

```
searchKey: runtime.retpolineR14
tags: [function private]
```

```Go
func retpolineR14()
```

### <a id="retpolineR15" href="#retpolineR15">func retpolineR15()</a>

```
searchKey: runtime.retpolineR15
tags: [function private]
```

```Go
func retpolineR15()
```

### <a id="retpolineR8" href="#retpolineR8">func retpolineR8()</a>

```
searchKey: runtime.retpolineR8
tags: [function private]
```

```Go
func retpolineR8()
```

### <a id="retpolineR9" href="#retpolineR9">func retpolineR9()</a>

```
searchKey: runtime.retpolineR9
tags: [function private]
```

```Go
func retpolineR9()
```

### <a id="retpolineSI" href="#retpolineSI">func retpolineSI()</a>

```
searchKey: runtime.retpolineSI
tags: [function private]
```

```Go
func retpolineSI()
```

### <a id="return0" href="#return0">func return0()</a>

```
searchKey: runtime.return0
tags: [function private]
```

```Go
func return0()
```

return0 is a stub used to return 0 from deferproc. It is called at the very end of deferproc to signal the calling Go function that it should not jump to deferreturn. in asm_*.s 

### <a id="round2" href="#round2">func round2(x int32) int32</a>

```
searchKey: runtime.round2
tags: [method private]
```

```Go
func round2(x int32) int32
```

round x up to a power of 2. 

### <a id="roundupsize" href="#roundupsize">func roundupsize(size uintptr) uintptr</a>

```
searchKey: runtime.roundupsize
tags: [method private]
```

```Go
func roundupsize(size uintptr) uintptr
```

Returns size of the memory block that mallocgc will allocate if you ask for the size. 

### <a id="rt0_go" href="#rt0_go">func rt0_go()</a>

```
searchKey: runtime.rt0_go
tags: [function private]
```

```Go
func rt0_go()
```

### <a id="runGCProg" href="#runGCProg">func runGCProg(prog, trailer, dst *byte, size int) uintptr</a>

```
searchKey: runtime.runGCProg
tags: [method private]
```

```Go
func runGCProg(prog, trailer, dst *byte, size int) uintptr
```

runGCProg executes the GC program prog, and then trailer if non-nil, writing to dst with entries of the given size. If size == 1, dst is a 1-bit pointer mask laid out moving forward from dst. If size == 2, dst is the 2-bit heap bitmap, and writes move backward starting at dst (because the heap bitmap does). In this case, the caller guarantees that only whole bytes in dst need to be written. 

runGCProg returns the number of 1- or 2-bit entries written to memory. 

### <a id="runOneTimer" href="#runOneTimer">func runOneTimer(pp *p, t *timer, now int64)</a>

```
searchKey: runtime.runOneTimer
tags: [method private]
```

```Go
func runOneTimer(pp *p, t *timer, now int64)
```

runOneTimer runs a single timer. The caller must have locked the timers for pp. This will temporarily unlock the timers while running the timer function. 

### <a id="runOpenDeferFrame" href="#runOpenDeferFrame">func runOpenDeferFrame(gp *g, d *_defer) bool</a>

```
searchKey: runtime.runOpenDeferFrame
tags: [method private]
```

```Go
func runOpenDeferFrame(gp *g, d *_defer) bool
```

runOpenDeferFrame runs the active open-coded defers in the frame specified by d. It normally processes all active defers in the frame, but stops immediately if a defer does a successful recover. It returns true if there are no remaining defers to run in the frame. 

### <a id="runSafePointFn" href="#runSafePointFn">func runSafePointFn()</a>

```
searchKey: runtime.runSafePointFn
tags: [function private]
```

```Go
func runSafePointFn()
```

runSafePointFn runs the safe point function, if any, for this P. This should be called like 

```
if getg().m.p.runSafePointFn != 0 {
    runSafePointFn()
}

```
runSafePointFn must be checked on any transition in to _Pidle or _Psyscall to avoid a race where forEachP sees that the P is running just before the P goes into _Pidle/_Psyscall and neither forEachP nor the P run the safe-point function. 

### <a id="runfinq" href="#runfinq">func runfinq()</a>

```
searchKey: runtime.runfinq
tags: [function private]
```

```Go
func runfinq()
```

This is the goroutine that runs all of the finalizers 

### <a id="runqempty" href="#runqempty">func runqempty(_p_ *p) bool</a>

```
searchKey: runtime.runqempty
tags: [method private]
```

```Go
func runqempty(_p_ *p) bool
```

runqempty reports whether _p_ has no Gs on its local run queue. It never returns true spuriously. 

### <a id="runqgrab" href="#runqgrab">func runqgrab(_p_ *p, batch *[256]guintptr, batchHead uint32, stealRunNextG bool) uint32</a>

```
searchKey: runtime.runqgrab
tags: [method private]
```

```Go
func runqgrab(_p_ *p, batch *[256]guintptr, batchHead uint32, stealRunNextG bool) uint32
```

Grabs a batch of goroutines from _p_'s runnable queue into batch. Batch is a ring buffer starting at batchHead. Returns number of grabbed goroutines. Can be executed by any P. 

### <a id="runqput" href="#runqput">func runqput(_p_ *p, gp *g, next bool)</a>

```
searchKey: runtime.runqput
tags: [method private]
```

```Go
func runqput(_p_ *p, gp *g, next bool)
```

runqput tries to put g on the local runnable queue. If next is false, runqput adds g to the tail of the runnable queue. If next is true, runqput puts g in the _p_.runnext slot. If the run queue is full, runnext puts g on the global queue. Executed only by the owner P. 

### <a id="runqputbatch" href="#runqputbatch">func runqputbatch(pp *p, q *gQueue, qsize int)</a>

```
searchKey: runtime.runqputbatch
tags: [method private]
```

```Go
func runqputbatch(pp *p, q *gQueue, qsize int)
```

runqputbatch tries to put all the G's on q on the local runnable queue. If the queue is full, they are put on the global queue; in that case this will temporarily acquire the scheduler lock. Executed only by the owner P. 

### <a id="runqputslow" href="#runqputslow">func runqputslow(_p_ *p, gp *g, h, t uint32) bool</a>

```
searchKey: runtime.runqputslow
tags: [method private]
```

```Go
func runqputslow(_p_ *p, gp *g, h, t uint32) bool
```

Put g and a batch of work from local runnable queue on global queue. Executed only by the owner P. 

### <a id="runtime_debug_WriteHeapDump" href="#runtime_debug_WriteHeapDump">func runtime_debug_WriteHeapDump(fd uintptr)</a>

```
searchKey: runtime.runtime_debug_WriteHeapDump
tags: [method private]
```

```Go
func runtime_debug_WriteHeapDump(fd uintptr)
```

### <a id="runtime_debug_freeOSMemory" href="#runtime_debug_freeOSMemory">func runtime_debug_freeOSMemory()</a>

```
searchKey: runtime.runtime_debug_freeOSMemory
tags: [function private]
```

```Go
func runtime_debug_freeOSMemory()
```

### <a id="runtime_expandFinalInlineFrame" href="#runtime_expandFinalInlineFrame">func runtime_expandFinalInlineFrame(stk []uintptr) []uintptr</a>

```
searchKey: runtime.runtime_expandFinalInlineFrame
tags: [method private]
```

```Go
func runtime_expandFinalInlineFrame(stk []uintptr) []uintptr
```

runtime_expandFinalInlineFrame expands the final pc in stk to include all "callers" if pc is inline. 

### <a id="runtime_getProfLabel" href="#runtime_getProfLabel">func runtime_getProfLabel() unsafe.Pointer</a>

```
searchKey: runtime.runtime_getProfLabel
tags: [function private]
```

```Go
func runtime_getProfLabel() unsafe.Pointer
```

### <a id="runtime_goroutineProfileWithLabels" href="#runtime_goroutineProfileWithLabels">func runtime_goroutineProfileWithLabels(p []StackRecord, labels []unsafe.Pointer) (n int, ok bool)</a>

```
searchKey: runtime.runtime_goroutineProfileWithLabels
tags: [method private]
```

```Go
func runtime_goroutineProfileWithLabels(p []StackRecord, labels []unsafe.Pointer) (n int, ok bool)
```

### <a id="runtime_pprof_readProfile" href="#runtime_pprof_readProfile">func runtime_pprof_readProfile() ([]uint64, []unsafe.Pointer, bool)</a>

```
searchKey: runtime.runtime_pprof_readProfile
tags: [function private]
```

```Go
func runtime_pprof_readProfile() ([]uint64, []unsafe.Pointer, bool)
```

readProfile, provided to runtime/pprof, returns the next chunk of binary CPU profiling stack trace data, blocking until data is available. If profiling is turned off and all the profile data accumulated while it was on has been returned, readProfile returns eof=true. The caller must save the returned data and tags before calling readProfile again. 

### <a id="runtime_pprof_runtime_cyclesPerSecond" href="#runtime_pprof_runtime_cyclesPerSecond">func runtime_pprof_runtime_cyclesPerSecond() int64</a>

```
searchKey: runtime.runtime_pprof_runtime_cyclesPerSecond
tags: [function private]
```

```Go
func runtime_pprof_runtime_cyclesPerSecond() int64
```

### <a id="runtime_setProfLabel" href="#runtime_setProfLabel">func runtime_setProfLabel(labels unsafe.Pointer)</a>

```
searchKey: runtime.runtime_setProfLabel
tags: [method private]
```

```Go
func runtime_setProfLabel(labels unsafe.Pointer)
```

### <a id="runtimer" href="#runtimer">func runtimer(pp *p, now int64) int64</a>

```
searchKey: runtime.runtimer
tags: [method private]
```

```Go
func runtimer(pp *p, now int64) int64
```

runtimer examines the first timer in timers. If it is ready based on now, it runs the timer and removes or updates it. Returns 0 if it ran a timer, -1 if there are no more timers, or the time when the first timer should run. The caller must have locked the timers for pp. If a timer is run, this will temporarily unlock the timers. 

### <a id="save" href="#save">func save(pc, sp uintptr)</a>

```
searchKey: runtime.save
tags: [method private]
```

```Go
func save(pc, sp uintptr)
```

save updates getg().sched to refer to pc and sp so that a following gogo will restore pc and sp. 

save must not have write barriers because invoking a write barrier can clobber getg().sched. 

### <a id="saveAncestors" href="#saveAncestors">func saveAncestors(callergp *g) *[]ancestorInfo</a>

```
searchKey: runtime.saveAncestors
tags: [method private]
```

```Go
func saveAncestors(callergp *g) *[]ancestorInfo
```

saveAncestors copies previous ancestors of the given caller g and includes infor for the current caller into a new set of tracebacks for a g being created. 

### <a id="saveblockevent" href="#saveblockevent">func saveblockevent(cycles, rate int64, skip int, which bucketType)</a>

```
searchKey: runtime.saveblockevent
tags: [method private]
```

```Go
func saveblockevent(cycles, rate int64, skip int, which bucketType)
```

### <a id="saveg" href="#saveg">func saveg(pc, sp uintptr, gp *g, r *StackRecord)</a>

```
searchKey: runtime.saveg
tags: [method private]
```

```Go
func saveg(pc, sp uintptr, gp *g, r *StackRecord)
```

### <a id="sbrk0" href="#sbrk0">func sbrk0() uintptr</a>

```
searchKey: runtime.sbrk0
tags: [function private]
```

```Go
func sbrk0() uintptr
```

sbrk0 returns the current process brk, or 0 if not implemented. 

### <a id="scanConservative" href="#scanConservative">func scanConservative(b, n uintptr, ptrmask *uint8, gcw *gcWork, state *stackScanState)</a>

```
searchKey: runtime.scanConservative
tags: [method private]
```

```Go
func scanConservative(b, n uintptr, ptrmask *uint8, gcw *gcWork, state *stackScanState)
```

scanConservative scans block [b, b+n) conservatively, treating any pointer-like value in the block as a pointer. 

If ptrmask != nil, only words that are marked in ptrmask are considered as potential pointers. 

If state != nil, it's assumed that [b, b+n) is a block in the stack and may contain pointers to stack objects. 

### <a id="scanblock" href="#scanblock">func scanblock(b0, n0 uintptr, ptrmask *uint8, gcw *gcWork, stk *stackScanState)</a>

```
searchKey: runtime.scanblock
tags: [method private]
```

```Go
func scanblock(b0, n0 uintptr, ptrmask *uint8, gcw *gcWork, stk *stackScanState)
```

scanblock scans b as scanobject would, but using an explicit pointer bitmap instead of the heap bitmap. 

This is used to scan non-heap roots, so it does not update gcw.bytesMarked or gcw.scanWork. 

If stk != nil, possible stack pointers are also reported to stk.putPtr. 

### <a id="scanframeworker" href="#scanframeworker">func scanframeworker(frame *stkframe, state *stackScanState, gcw *gcWork)</a>

```
searchKey: runtime.scanframeworker
tags: [method private]
```

```Go
func scanframeworker(frame *stkframe, state *stackScanState, gcw *gcWork)
```

Scan a stack frame: local variables and function arguments/results. 

### <a id="scanobject" href="#scanobject">func scanobject(b uintptr, gcw *gcWork)</a>

```
searchKey: runtime.scanobject
tags: [method private]
```

```Go
func scanobject(b uintptr, gcw *gcWork)
```

scanobject scans the object starting at b, adding pointers to gcw. b must point to the beginning of a heap object or an oblet. scanobject consults the GC bitmap for the pointer mask and the spans for the size of the object. 

### <a id="scanstack" href="#scanstack">func scanstack(gp *g, gcw *gcWork)</a>

```
searchKey: runtime.scanstack
tags: [method private]
```

```Go
func scanstack(gp *g, gcw *gcWork)
```

scanstack scans gp's stack, greying all pointers found on the stack. 

scanstack will also shrink the stack if it is safe to do so. If it is not, it schedules a stack shrink for the next synchronous safe point. 

scanstack is marked go:systemstack because it must not be preempted while using a workbuf. 

### <a id="scavengeSleep" href="#scavengeSleep">func scavengeSleep(ns int64) int64</a>

```
searchKey: runtime.scavengeSleep
tags: [method private]
```

```Go
func scavengeSleep(ns int64) int64
```

scavengeSleep attempts to put the scavenger to sleep for ns. 

Note that this function should only be called by the scavenger. 

The scavenger may be woken up earlier by a pacing change, and it may not go to sleep at all if there's a pending pacing change. 

Returns the amount of time actually slept. 

### <a id="schedEnableUser" href="#schedEnableUser">func schedEnableUser(enable bool)</a>

```
searchKey: runtime.schedEnableUser
tags: [method private]
```

```Go
func schedEnableUser(enable bool)
```

schedEnableUser enables or disables the scheduling of user goroutines. 

This does not stop already running user goroutines, so the caller should first stop the world when disabling user goroutines. 

### <a id="schedEnabled" href="#schedEnabled">func schedEnabled(gp *g) bool</a>

```
searchKey: runtime.schedEnabled
tags: [method private]
```

```Go
func schedEnabled(gp *g) bool
```

schedEnabled reports whether gp should be scheduled. It returns false is scheduling of gp is disabled. 

sched.lock must be held. 

### <a id="schedinit" href="#schedinit">func schedinit()</a>

```
searchKey: runtime.schedinit
tags: [function private]
```

```Go
func schedinit()
```

The bootstrap sequence is: 

```
call osinit
call schedinit
make & queue new G
call runtime·mstart

```
The new G calls runtime·main. 

### <a id="schedtrace" href="#schedtrace">func schedtrace(detailed bool)</a>

```
searchKey: runtime.schedtrace
tags: [method private]
```

```Go
func schedtrace(detailed bool)
```

### <a id="schedule" href="#schedule">func schedule()</a>

```
searchKey: runtime.schedule
tags: [function private]
```

```Go
func schedule()
```

One round of scheduler: find a runnable goroutine and execute it. Never returns. 

### <a id="selectgo" href="#selectgo">func selectgo(cas0 *scase, order0 *uint16, pc0 *uintptr, nsends, nrecvs int, block bool) (int, bool)</a>

```
searchKey: runtime.selectgo
tags: [method private]
```

```Go
func selectgo(cas0 *scase, order0 *uint16, pc0 *uintptr, nsends, nrecvs int, block bool) (int, bool)
```

selectgo implements the select statement. 

cas0 points to an array of type [ncases]scase, and order0 points to an array of type [2*ncases]uint16 where ncases must be <= 65536. Both reside on the goroutine's stack (regardless of any escaping in selectgo). 

For race detector builds, pc0 points to an array of type [ncases]uintptr (also on the stack); for other builds, it's set to nil. 

selectgo returns the index of the chosen scase, which matches the ordinal position of its respective select{recv,send,default} call. Also, if the chosen scase was a receive operation, it reports whether a value was received. 

### <a id="selectnbrecv" href="#selectnbrecv">func selectnbrecv(elem unsafe.Pointer, c *hchan) (selected, received bool)</a>

```
searchKey: runtime.selectnbrecv
tags: [method private]
```

```Go
func selectnbrecv(elem unsafe.Pointer, c *hchan) (selected, received bool)
```

compiler implements 

```
select {
case v, ok = <-c:
	... foo
default:
	... bar
}

```
as 

```
if selected, ok = selectnbrecv(&v, c); selected {
	... foo
} else {
	... bar
}

```
### <a id="selectnbsend" href="#selectnbsend">func selectnbsend(c *hchan, elem unsafe.Pointer) (selected bool)</a>

```
searchKey: runtime.selectnbsend
tags: [method private]
```

```Go
func selectnbsend(c *hchan, elem unsafe.Pointer) (selected bool)
```

compiler implements 

```
select {
case c <- v:
	... foo
default:
	... bar
}

```
as 

```
if selectnbsend(c, v) {
	... foo
} else {
	... bar
}

```
### <a id="selectsetpc" href="#selectsetpc">func selectsetpc(pc *uintptr)</a>

```
searchKey: runtime.selectsetpc
tags: [method private]
```

```Go
func selectsetpc(pc *uintptr)
```

### <a id="sellock" href="#sellock">func sellock(scases []scase, lockorder []uint16)</a>

```
searchKey: runtime.sellock
tags: [method private]
```

```Go
func sellock(scases []scase, lockorder []uint16)
```

### <a id="selparkcommit" href="#selparkcommit">func selparkcommit(gp *g, _ unsafe.Pointer) bool</a>

```
searchKey: runtime.selparkcommit
tags: [method private]
```

```Go
func selparkcommit(gp *g, _ unsafe.Pointer) bool
```

### <a id="selunlock" href="#selunlock">func selunlock(scases []scase, lockorder []uint16)</a>

```
searchKey: runtime.selunlock
tags: [method private]
```

```Go
func selunlock(scases []scase, lockorder []uint16)
```

### <a id="semacquire" href="#semacquire">func semacquire(addr *uint32)</a>

```
searchKey: runtime.semacquire
tags: [method private]
```

```Go
func semacquire(addr *uint32)
```

Called from runtime. 

### <a id="semacquire1" href="#semacquire1">func semacquire1(addr *uint32, lifo bool, profile semaProfileFlags, skipframes int)</a>

```
searchKey: runtime.semacquire1
tags: [method private]
```

```Go
func semacquire1(addr *uint32, lifo bool, profile semaProfileFlags, skipframes int)
```

### <a id="semacreate" href="#semacreate">func semacreate(mp *m)</a>

```
searchKey: runtime.semacreate
tags: [method private]
```

```Go
func semacreate(mp *m)
```

### <a id="semasleep" href="#semasleep">func semasleep(ns int64) int32</a>

```
searchKey: runtime.semasleep
tags: [method private]
```

```Go
func semasleep(ns int64) int32
```

### <a id="semawakeup" href="#semawakeup">func semawakeup(mp *m)</a>

```
searchKey: runtime.semawakeup
tags: [method private]
```

```Go
func semawakeup(mp *m)
```

### <a id="semrelease" href="#semrelease">func semrelease(addr *uint32)</a>

```
searchKey: runtime.semrelease
tags: [method private]
```

```Go
func semrelease(addr *uint32)
```

### <a id="semrelease1" href="#semrelease1">func semrelease1(addr *uint32, handoff bool, skipframes int)</a>

```
searchKey: runtime.semrelease1
tags: [method private]
```

```Go
func semrelease1(addr *uint32, handoff bool, skipframes int)
```

### <a id="send" href="#send">func send(c *hchan, sg *sudog, ep unsafe.Pointer, unlockf func(), skip int)</a>

```
searchKey: runtime.send
tags: [method private]
```

```Go
func send(c *hchan, sg *sudog, ep unsafe.Pointer, unlockf func(), skip int)
```

send processes a send operation on an empty channel c. The value ep sent by the sender is copied to the receiver sg. The receiver is then woken up to go on its merry way. Channel c must be empty and locked.  send unlocks c with unlockf. sg must already be dequeued from c. ep must be non-nil and point to the heap or the caller's stack. 

### <a id="sendDirect" href="#sendDirect">func sendDirect(t *_type, sg *sudog, src unsafe.Pointer)</a>

```
searchKey: runtime.sendDirect
tags: [method private]
```

```Go
func sendDirect(t *_type, sg *sudog, src unsafe.Pointer)
```

### <a id="setCheckmark" href="#setCheckmark">func setCheckmark(obj, base, off uintptr, mbits markBits) bool</a>

```
searchKey: runtime.setCheckmark
tags: [method private]
```

```Go
func setCheckmark(obj, base, off uintptr, mbits markBits) bool
```

setCheckmark throws if marking object is a checkmarks violation, and otherwise sets obj's checkmark. It returns true if obj was already checkmarked. 

### <a id="setGCPercent" href="#setGCPercent">func setGCPercent(in int32) (out int32)</a>

```
searchKey: runtime.setGCPercent
tags: [method private]
```

```Go
func setGCPercent(in int32) (out int32)
```

### <a id="setGCPhase" href="#setGCPhase">func setGCPhase(x uint32)</a>

```
searchKey: runtime.setGCPhase
tags: [method private]
```

```Go
func setGCPhase(x uint32)
```

### <a id="setGNoWB" href="#setGNoWB">func setGNoWB(gp **g, new *g)</a>

```
searchKey: runtime.setGNoWB
tags: [method private]
```

```Go
func setGNoWB(gp **g, new *g)
```

setGNoWB performs *gp = new without a write barrier. For times when it's impractical to use a guintptr. 

### <a id="setGsignalStack" href="#setGsignalStack">func setGsignalStack(st *stackt, old *gsignalStack)</a>

```
searchKey: runtime.setGsignalStack
tags: [method private]
```

```Go
func setGsignalStack(st *stackt, old *gsignalStack)
```

setGsignalStack sets the gsignal stack of the current m to an alternate signal stack returned from the sigaltstack system call. It saves the old values in *old for use by restoreGsignalStack. This is used when handling a signal if non-Go code has set the alternate signal stack. 

### <a id="setMNoWB" href="#setMNoWB">func setMNoWB(mp **m, new *m)</a>

```
searchKey: runtime.setMNoWB
tags: [method private]
```

```Go
func setMNoWB(mp **m, new *m)
```

setMNoWB performs *mp = new without a write barrier. For times when it's impractical to use an muintptr. 

### <a id="setMaxStack" href="#setMaxStack">func setMaxStack(in int) (out int)</a>

```
searchKey: runtime.setMaxStack
tags: [method private]
```

```Go
func setMaxStack(in int) (out int)
```

### <a id="setMaxThreads" href="#setMaxThreads">func setMaxThreads(in int) (out int)</a>

```
searchKey: runtime.setMaxThreads
tags: [method private]
```

```Go
func setMaxThreads(in int) (out int)
```

### <a id="setNonblock" href="#setNonblock">func setNonblock(fd int32)</a>

```
searchKey: runtime.setNonblock
tags: [method private]
```

```Go
func setNonblock(fd int32)
```

### <a id="setPanicOnFault" href="#setPanicOnFault">func setPanicOnFault(new bool) (old bool)</a>

```
searchKey: runtime.setPanicOnFault
tags: [method private]
```

```Go
func setPanicOnFault(new bool) (old bool)
```

### <a id="setProcessCPUProfiler" href="#setProcessCPUProfiler">func setProcessCPUProfiler(hz int32)</a>

```
searchKey: runtime.setProcessCPUProfiler
tags: [method private]
```

```Go
func setProcessCPUProfiler(hz int32)
```

setProcessCPUProfiler is called when the profiling timer changes. It is called with prof.lock held. hz is the new timer, and is 0 if profiling is being disabled. Enable or disable the signal as required for -buildmode=c-archive. 

### <a id="setSignalstackSP" href="#setSignalstackSP">func setSignalstackSP(s *stackt, sp uintptr)</a>

```
searchKey: runtime.setSignalstackSP
tags: [method private]
```

```Go
func setSignalstackSP(s *stackt, sp uintptr)
```

setSignaltstackSP sets the ss_sp field of a stackt. 

### <a id="setThreadCPUProfiler" href="#setThreadCPUProfiler">func setThreadCPUProfiler(hz int32)</a>

```
searchKey: runtime.setThreadCPUProfiler
tags: [method private]
```

```Go
func setThreadCPUProfiler(hz int32)
```

setThreadCPUProfiler makes any thread-specific changes required to implement profiling at a rate of hz. No changes required on Unix systems. 

### <a id="setTraceback" href="#setTraceback">func setTraceback(level string)</a>

```
searchKey: runtime.setTraceback
tags: [method private]
```

```Go
func setTraceback(level string)
```

### <a id="setcpuprofilerate" href="#setcpuprofilerate">func setcpuprofilerate(hz int32)</a>

```
searchKey: runtime.setcpuprofilerate
tags: [method private]
```

```Go
func setcpuprofilerate(hz int32)
```

setcpuprofilerate sets the CPU profiling rate to hz times per second. If hz <= 0, setcpuprofilerate turns off CPU profiling. 

### <a id="setg" href="#setg">func setg(gg *g)</a>

```
searchKey: runtime.setg
tags: [method private]
```

```Go
func setg(gg *g)
```

### <a id="setitimer" href="#setitimer">func setitimer(mode int32, new, old *itimerval)</a>

```
searchKey: runtime.setitimer
tags: [method private]
```

```Go
func setitimer(mode int32, new, old *itimerval)
```

### <a id="setitimer_trampoline" href="#setitimer_trampoline">func setitimer_trampoline()</a>

```
searchKey: runtime.setitimer_trampoline
tags: [function private]
```

```Go
func setitimer_trampoline()
```

### <a id="setprofilebucket" href="#setprofilebucket">func setprofilebucket(p unsafe.Pointer, b *bucket)</a>

```
searchKey: runtime.setprofilebucket
tags: [method private]
```

```Go
func setprofilebucket(p unsafe.Pointer, b *bucket)
```

Set the heap profile bucket associated with addr to b. 

### <a id="setsig" href="#setsig">func setsig(i uint32, fn uintptr)</a>

```
searchKey: runtime.setsig
tags: [method private]
```

```Go
func setsig(i uint32, fn uintptr)
```

### <a id="setsigsegv" href="#setsigsegv">func setsigsegv(pc uintptr)</a>

```
searchKey: runtime.setsigsegv
tags: [method private]
```

```Go
func setsigsegv(pc uintptr)
```

setsigsegv is used on darwin/arm64 to fake a segmentation fault. 

This is exported via linkname to assembly in runtime/cgo. 

### <a id="setsigstack" href="#setsigstack">func setsigstack(i uint32)</a>

```
searchKey: runtime.setsigstack
tags: [method private]
```

```Go
func setsigstack(i uint32)
```

### <a id="settls" href="#settls">func settls()</a>

```
searchKey: runtime.settls
tags: [function private]
```

```Go
func settls()
```

Called from assembly only; declared for go vet. 

### <a id="shade" href="#shade">func shade(b uintptr)</a>

```
searchKey: runtime.shade
tags: [method private]
```

```Go
func shade(b uintptr)
```

Shade the object if it isn't already. The object is not nil and known to be in the heap. Preemption must be disabled. 

### <a id="shouldPushSigpanic" href="#shouldPushSigpanic">func shouldPushSigpanic(gp *g, pc, lr uintptr) bool</a>

```
searchKey: runtime.shouldPushSigpanic
tags: [method private]
```

```Go
func shouldPushSigpanic(gp *g, pc, lr uintptr) bool
```

shouldPushSigpanic reports whether pc should be used as sigpanic's return PC (pushing a frame for the call). Otherwise, it should be left alone so that LR is used as sigpanic's return PC, effectively replacing the top-most frame with sigpanic. This is used by preparePanic. 

### <a id="showframe" href="#showframe">func showframe(f funcInfo, gp *g, firstFrame bool, funcID, childID funcID) bool</a>

```
searchKey: runtime.showframe
tags: [method private]
```

```Go
func showframe(f funcInfo, gp *g, firstFrame bool, funcID, childID funcID) bool
```

showframe reports whether the frame with the given characteristics should be printed during a traceback. 

### <a id="showfuncinfo" href="#showfuncinfo">func showfuncinfo(f funcInfo, firstFrame bool, funcID, childID funcID) bool</a>

```
searchKey: runtime.showfuncinfo
tags: [method private]
```

```Go
func showfuncinfo(f funcInfo, firstFrame bool, funcID, childID funcID) bool
```

showfuncinfo reports whether a function with the given characteristics should be printed during a traceback. 

### <a id="shrinkstack" href="#shrinkstack">func shrinkstack(gp *g)</a>

```
searchKey: runtime.shrinkstack
tags: [method private]
```

```Go
func shrinkstack(gp *g)
```

Maybe shrink the stack being used by gp. 

gp must be stopped and we must own its stack. It may be in _Grunning, but only if this is our own user G. 

### <a id="siftdownTimer" href="#siftdownTimer">func siftdownTimer(t []*timer, i int)</a>

```
searchKey: runtime.siftdownTimer
tags: [method private]
```

```Go
func siftdownTimer(t []*timer, i int)
```

### <a id="siftupTimer" href="#siftupTimer">func siftupTimer(t []*timer, i int)</a>

```
searchKey: runtime.siftupTimer
tags: [method private]
```

```Go
func siftupTimer(t []*timer, i int)
```

### <a id="sigInitIgnored" href="#sigInitIgnored">func sigInitIgnored(s uint32)</a>

```
searchKey: runtime.sigInitIgnored
tags: [method private]
```

```Go
func sigInitIgnored(s uint32)
```

sigInitIgnored marks the signal as already ignored. This is called at program start by initsig. In a shared library initsig is called by libpreinit, so the runtime may not be initialized yet. 

### <a id="sigInstallGoHandler" href="#sigInstallGoHandler">func sigInstallGoHandler(sig uint32) bool</a>

```
searchKey: runtime.sigInstallGoHandler
tags: [method private]
```

```Go
func sigInstallGoHandler(sig uint32) bool
```

### <a id="sigNotOnStack" href="#sigNotOnStack">func sigNotOnStack(sig uint32)</a>

```
searchKey: runtime.sigNotOnStack
tags: [method private]
```

```Go
func sigNotOnStack(sig uint32)
```

This is called if we receive a signal when there is a signal stack but we are not on it. This can only happen if non-Go code called sigaction without setting the SS_ONSTACK flag. 

### <a id="sigNoteSetup" href="#sigNoteSetup">func sigNoteSetup(*note)</a>

```
searchKey: runtime.sigNoteSetup
tags: [method private]
```

```Go
func sigNoteSetup(*note)
```

sigNoteSetup initializes an async-signal-safe note. 

The current implementation of notes on Darwin is not async-signal-safe, because the functions pthread_mutex_lock, pthread_cond_signal, and pthread_mutex_unlock, called by semawakeup, are not async-signal-safe. There is only one case where we need to wake up a note from a signal handler: the sigsend function. The signal handler code does not require all the features of notes: it does not need to do a timed wait. This is a separate implementation of notes, based on a pipe, that does not support timed waits but is async-signal-safe. 

### <a id="sigNoteSleep" href="#sigNoteSleep">func sigNoteSleep(*note)</a>

```
searchKey: runtime.sigNoteSleep
tags: [method private]
```

```Go
func sigNoteSleep(*note)
```

sigNoteSleep waits for a note created by sigNoteSetup to be woken. 

### <a id="sigNoteWakeup" href="#sigNoteWakeup">func sigNoteWakeup(*note)</a>

```
searchKey: runtime.sigNoteWakeup
tags: [method private]
```

```Go
func sigNoteWakeup(*note)
```

sigNoteWakeup wakes up a thread sleeping on a note created by sigNoteSetup. 

### <a id="sigRecvPrepareForFixup" href="#sigRecvPrepareForFixup">func sigRecvPrepareForFixup()</a>

```
searchKey: runtime.sigRecvPrepareForFixup
tags: [function private]
```

```Go
func sigRecvPrepareForFixup()
```

sigRecvPrepareForFixup is used to temporarily wake up the signal_recv() running thread while it is blocked waiting for the arrival of a signal. If it causes the thread to wake up, the sig.state travels through this sequence: sigReceiving -> sigFixup -> sigIdle -> sigReceiving and resumes. (This is only called while GC is disabled.) 

### <a id="sigaction" href="#sigaction">func sigaction(sig uint32, new *usigactiont, old *usigactiont)</a>

```
searchKey: runtime.sigaction
tags: [method private]
```

```Go
func sigaction(sig uint32, new *usigactiont, old *usigactiont)
```

### <a id="sigaction_trampoline" href="#sigaction_trampoline">func sigaction_trampoline()</a>

```
searchKey: runtime.sigaction_trampoline
tags: [function private]
```

```Go
func sigaction_trampoline()
```

### <a id="sigaddset" href="#sigaddset">func sigaddset(mask *sigset, i int)</a>

```
searchKey: runtime.sigaddset
tags: [method private]
```

```Go
func sigaddset(mask *sigset, i int)
```

### <a id="sigaltstack" href="#sigaltstack">func sigaltstack(new *stackt, old *stackt)</a>

```
searchKey: runtime.sigaltstack
tags: [method private]
```

```Go
func sigaltstack(new *stackt, old *stackt)
```

### <a id="sigaltstack_trampoline" href="#sigaltstack_trampoline">func sigaltstack_trampoline()</a>

```
searchKey: runtime.sigaltstack_trampoline
tags: [function private]
```

```Go
func sigaltstack_trampoline()
```

### <a id="sigblock" href="#sigblock">func sigblock(exiting bool)</a>

```
searchKey: runtime.sigblock
tags: [method private]
```

```Go
func sigblock(exiting bool)
```

sigblock blocks signals in the current thread's signal mask. This is used to block signals while setting up and tearing down g when a non-Go thread calls a Go function. When a thread is exiting we use the sigsetAllExiting value, otherwise the OS specific definition of sigset_all is used. This is nosplit and nowritebarrierrec because it is called by needm which may be called on a non-Go thread with no g available. 

### <a id="sigdelset" href="#sigdelset">func sigdelset(mask *sigset, i int)</a>

```
searchKey: runtime.sigdelset
tags: [method private]
```

```Go
func sigdelset(mask *sigset, i int)
```

### <a id="sigdisable" href="#sigdisable">func sigdisable(sig uint32)</a>

```
searchKey: runtime.sigdisable
tags: [method private]
```

```Go
func sigdisable(sig uint32)
```

sigdisable disables the Go signal handler for the signal sig. It is only called while holding the os/signal.handlers lock, via os/signal.disableSignal and signal_disable. 

### <a id="sigenable" href="#sigenable">func sigenable(sig uint32)</a>

```
searchKey: runtime.sigenable
tags: [method private]
```

```Go
func sigenable(sig uint32)
```

sigenable enables the Go signal handler to catch the signal sig. It is only called while holding the os/signal.handlers lock, via os/signal.enableSignal and signal_enable. 

### <a id="sigfwd" href="#sigfwd">func sigfwd(fn uintptr, sig uint32, info *siginfo, ctx unsafe.Pointer)</a>

```
searchKey: runtime.sigfwd
tags: [method private]
```

```Go
func sigfwd(fn uintptr, sig uint32, info *siginfo, ctx unsafe.Pointer)
```

### <a id="sigfwdgo" href="#sigfwdgo">func sigfwdgo(sig uint32, info *siginfo, ctx unsafe.Pointer) bool</a>

```
searchKey: runtime.sigfwdgo
tags: [method private]
```

```Go
func sigfwdgo(sig uint32, info *siginfo, ctx unsafe.Pointer) bool
```

Determines if the signal should be handled by Go and if not, forwards the signal to the handler that was installed before Go's. Returns whether the signal was forwarded. This is called by the signal handler, and the world may be stopped. 

### <a id="sighandler" href="#sighandler">func sighandler(sig uint32, info *siginfo, ctxt unsafe.Pointer, gp *g)</a>

```
searchKey: runtime.sighandler
tags: [method private]
```

```Go
func sighandler(sig uint32, info *siginfo, ctxt unsafe.Pointer, gp *g)
```

sighandler is invoked when a signal occurs. The global g will be set to a gsignal goroutine and we will be running on the alternate signal stack. The parameter g will be the value of the global g when the signal occurred. The sig, info, and ctxt parameters are from the system signal handler: they are the parameters passed when the SA is passed to the sigaction system call. 

The garbage collector may have stopped the world, so write barriers are not allowed. 

### <a id="sigignore" href="#sigignore">func sigignore(sig uint32)</a>

```
searchKey: runtime.sigignore
tags: [method private]
```

```Go
func sigignore(sig uint32)
```

sigignore ignores the signal sig. It is only called while holding the os/signal.handlers lock, via os/signal.ignoreSignal and signal_ignore. 

### <a id="sigismember" href="#sigismember">func sigismember(mask *sigset, i int) bool</a>

```
searchKey: runtime.sigismember
tags: [method private]
```

```Go
func sigismember(mask *sigset, i int) bool
```

### <a id="signalDuringFork" href="#signalDuringFork">func signalDuringFork(sig uint32)</a>

```
searchKey: runtime.signalDuringFork
tags: [method private]
```

```Go
func signalDuringFork(sig uint32)
```

signalDuringFork is called if we receive a signal while doing a fork. We do not want signals at that time, as a signal sent to the process group may be delivered to the child process, causing confusion. This should never be called, because we block signals across the fork; this function is just a safety check. See issue 18600 for background. 

### <a id="signalM" href="#signalM">func signalM(mp *m, sig int)</a>

```
searchKey: runtime.signalM
tags: [method private]
```

```Go
func signalM(mp *m, sig int)
```

### <a id="signalWaitUntilIdle" href="#signalWaitUntilIdle">func signalWaitUntilIdle()</a>

```
searchKey: runtime.signalWaitUntilIdle
tags: [function private]
```

```Go
func signalWaitUntilIdle()
```

signalWaitUntilIdle waits until the signal delivery mechanism is idle. This is used to ensure that we do not drop a signal notification due to a race between disabling a signal and receiving a signal. This assumes that signal delivery has already been disabled for the signal(s) in question, and here we are just waiting to make sure that all the signals have been delivered to the user channels by the os/signal package. 

### <a id="signal_disable" href="#signal_disable">func signal_disable(s uint32)</a>

```
searchKey: runtime.signal_disable
tags: [method private]
```

```Go
func signal_disable(s uint32)
```

Must only be called from a single goroutine at a time. 

### <a id="signal_enable" href="#signal_enable">func signal_enable(s uint32)</a>

```
searchKey: runtime.signal_enable
tags: [method private]
```

```Go
func signal_enable(s uint32)
```

Must only be called from a single goroutine at a time. 

### <a id="signal_ignore" href="#signal_ignore">func signal_ignore(s uint32)</a>

```
searchKey: runtime.signal_ignore
tags: [method private]
```

```Go
func signal_ignore(s uint32)
```

Must only be called from a single goroutine at a time. 

### <a id="signal_ignored" href="#signal_ignored">func signal_ignored(s uint32) bool</a>

```
searchKey: runtime.signal_ignored
tags: [method private]
```

```Go
func signal_ignored(s uint32) bool
```

Checked by signal handlers. 

### <a id="signal_recv" href="#signal_recv">func signal_recv() uint32</a>

```
searchKey: runtime.signal_recv
tags: [function private]
```

```Go
func signal_recv() uint32
```

Called to receive the next queued signal. Must only be called from a single goroutine at a time. 

### <a id="signalstack" href="#signalstack">func signalstack(s *stack)</a>

```
searchKey: runtime.signalstack
tags: [method private]
```

```Go
func signalstack(s *stack)
```

signalstack sets the current thread's alternate signal stack to s. 

### <a id="signame" href="#signame">func signame(sig uint32) string</a>

```
searchKey: runtime.signame
tags: [method private]
```

```Go
func signame(sig uint32) string
```

### <a id="sigpanic" href="#sigpanic">func sigpanic()</a>

```
searchKey: runtime.sigpanic
tags: [function private]
```

```Go
func sigpanic()
```

sigpanic turns a synchronous signal into a run-time panic. If the signal handler sees a synchronous panic, it arranges the stack to look like the function where the signal occurred called sigpanic, sets the signal's PC value to sigpanic, and returns from the signal handler. The effect is that the program will act as though the function that got the signal simply called sigpanic instead. 

This must NOT be nosplit because the linker doesn't know where sigpanic calls can be injected. 

The signal handler must not inject a call to sigpanic if getg().throwsplit, since sigpanic may need to grow the stack. 

This is exported via linkname to assembly in runtime/cgo. 

### <a id="sigpanic0" href="#sigpanic0">func sigpanic0()</a>

```
searchKey: runtime.sigpanic0
tags: [function private]
```

```Go
func sigpanic0()
```

Injected by the signal handler for panicking signals. Initializes any registers that have fixed meaning at calls but are scratch in bodies and calls sigpanic. On many platforms it just jumps to sigpanic. 

### <a id="sigpipe" href="#sigpipe">func sigpipe()</a>

```
searchKey: runtime.sigpipe
tags: [function private]
```

```Go
func sigpipe()
```

### <a id="sigprocmask" href="#sigprocmask">func sigprocmask(how uint32, new *sigset, old *sigset)</a>

```
searchKey: runtime.sigprocmask
tags: [method private]
```

```Go
func sigprocmask(how uint32, new *sigset, old *sigset)
```

### <a id="sigprocmask_trampoline" href="#sigprocmask_trampoline">func sigprocmask_trampoline()</a>

```
searchKey: runtime.sigprocmask_trampoline
tags: [function private]
```

```Go
func sigprocmask_trampoline()
```

### <a id="sigprof" href="#sigprof">func sigprof(pc, sp, lr uintptr, gp *g, mp *m)</a>

```
searchKey: runtime.sigprof
tags: [method private]
```

```Go
func sigprof(pc, sp, lr uintptr, gp *g, mp *m)
```

Called if we receive a SIGPROF signal. Called by the signal handler, may run during STW. 

### <a id="sigprofNonGo" href="#sigprofNonGo">func sigprofNonGo()</a>

```
searchKey: runtime.sigprofNonGo
tags: [function private]
```

```Go
func sigprofNonGo()
```

sigprofNonGo is called if we receive a SIGPROF signal on a non-Go thread, and the signal handler collected a stack trace in sigprofCallers. When this is called, sigprofCallersUse will be non-zero. g is nil, and what we can do is very limited. 

### <a id="sigprofNonGoPC" href="#sigprofNonGoPC">func sigprofNonGoPC(pc uintptr)</a>

```
searchKey: runtime.sigprofNonGoPC
tags: [method private]
```

```Go
func sigprofNonGoPC(pc uintptr)
```

sigprofNonGoPC is called when a profiling signal arrived on a non-Go thread and we have a single PC value, not a stack trace. g is nil, and what we can do is very limited. 

### <a id="sigsave" href="#sigsave">func sigsave(p *sigset)</a>

```
searchKey: runtime.sigsave
tags: [method private]
```

```Go
func sigsave(p *sigset)
```

sigsave saves the current thread's signal mask into *p. This is used to preserve the non-Go signal mask when a non-Go thread calls a Go function. This is nosplit and nowritebarrierrec because it is called by needm which may be called on a non-Go thread with no g available. 

### <a id="sigsend" href="#sigsend">func sigsend(s uint32) bool</a>

```
searchKey: runtime.sigsend
tags: [method private]
```

```Go
func sigsend(s uint32) bool
```

sigsend delivers a signal from sighandler to the internal signal delivery queue. It reports whether the signal was sent. If not, the caller typically crashes the program. It runs from the signal handler, so it's limited in what it can do. 

### <a id="sigtramp" href="#sigtramp">func sigtramp()</a>

```
searchKey: runtime.sigtramp
tags: [function private]
```

```Go
func sigtramp()
```

sigtramp is the callback from libc when a signal is received. It is called with the C calling convention. 

### <a id="sigtrampgo" href="#sigtrampgo">func sigtrampgo(sig uint32, info *siginfo, ctx unsafe.Pointer)</a>

```
searchKey: runtime.sigtrampgo
tags: [method private]
```

```Go
func sigtrampgo(sig uint32, info *siginfo, ctx unsafe.Pointer)
```

sigtrampgo is called from the signal handler function, sigtramp, written in assembly code. This is called by the signal handler, and the world may be stopped. 

It must be nosplit because getg() is still the G that was running (if any) when the signal was delivered, but it's (usually) called on the gsignal stack. Until this switches the G to gsignal, the stack bounds check won't work. 

### <a id="slicebytetostring" href="#slicebytetostring">func slicebytetostring(buf *tmpBuf, ptr *byte, n int) (str string)</a>

```
searchKey: runtime.slicebytetostring
tags: [method private]
```

```Go
func slicebytetostring(buf *tmpBuf, ptr *byte, n int) (str string)
```

slicebytetostring converts a byte slice to a string. It is inserted by the compiler into generated code. ptr is a pointer to the first element of the slice; n is the length of the slice. Buf is a fixed-size buffer for the result, it is not nil if the result does not escape. 

### <a id="slicebytetostringtmp" href="#slicebytetostringtmp">func slicebytetostringtmp(ptr *byte, n int) (str string)</a>

```
searchKey: runtime.slicebytetostringtmp
tags: [method private]
```

```Go
func slicebytetostringtmp(ptr *byte, n int) (str string)
```

slicebytetostringtmp returns a "string" referring to the actual []byte bytes. 

Callers need to ensure that the returned string will not be used after the calling goroutine modifies the original slice or synchronizes with another goroutine. 

The function is only called when instrumenting and otherwise intrinsified by the compiler. 

Some internal compiler optimizations use this function. - Used for m[T1{... Tn{..., string(k), ...} ...}] and m[string(k)] 

```
where k is []byte, T1 to Tn is a nesting of struct and array literals.

```
- Used for "<"+string(b)+">" concatenation where b is []byte. - Used for string(b)=="foo" comparison where b is []byte. 

### <a id="slicecopy" href="#slicecopy">func slicecopy(toPtr unsafe.Pointer, toLen int, fromPtr unsafe.Pointer, fromLen int, width uintptr) int</a>

```
searchKey: runtime.slicecopy
tags: [method private]
```

```Go
func slicecopy(toPtr unsafe.Pointer, toLen int, fromPtr unsafe.Pointer, fromLen int, width uintptr) int
```

slicecopy is used to copy from a string or slice of pointerless elements into a slice. 

### <a id="slicerunetostring" href="#slicerunetostring">func slicerunetostring(buf *tmpBuf, a []rune) string</a>

```
searchKey: runtime.slicerunetostring
tags: [method private]
```

```Go
func slicerunetostring(buf *tmpBuf, a []rune) string
```

### <a id="spanHasNoSpecials" href="#spanHasNoSpecials">func spanHasNoSpecials(s *mspan)</a>

```
searchKey: runtime.spanHasNoSpecials
tags: [method private]
```

```Go
func spanHasNoSpecials(s *mspan)
```

spanHasNoSpecials marks a span as having no specials in the arena bitmap. 

### <a id="spanHasSpecials" href="#spanHasSpecials">func spanHasSpecials(s *mspan)</a>

```
searchKey: runtime.spanHasSpecials
tags: [method private]
```

```Go
func spanHasSpecials(s *mspan)
```

spanHasSpecials marks a span as having specials in the arena bitmap. 

### <a id="spillArgs" href="#spillArgs">func spillArgs()</a>

```
searchKey: runtime.spillArgs
tags: [function private]
```

```Go
func spillArgs()
```

Used by reflectcall and the reflect package. 

Spills/loads arguments in registers to/from an internal/abi.RegArgs respectively. Does not follow the Go ABI. 

### <a id="stackOverflow" href="#stackOverflow">func stackOverflow(x *byte)</a>

```
searchKey: runtime.stackOverflow
tags: [method private]
```

```Go
func stackOverflow(x *byte)
```

### <a id="stackcache_clear" href="#stackcache_clear">func stackcache_clear(c *mcache)</a>

```
searchKey: runtime.stackcache_clear
tags: [method private]
```

```Go
func stackcache_clear(c *mcache)
```

### <a id="stackcacherefill" href="#stackcacherefill">func stackcacherefill(c *mcache, order uint8)</a>

```
searchKey: runtime.stackcacherefill
tags: [method private]
```

```Go
func stackcacherefill(c *mcache, order uint8)
```

stackcacherefill/stackcacherelease implement a global pool of stack segments. The pool is required to prevent unlimited growth of per-thread caches. 

### <a id="stackcacherelease" href="#stackcacherelease">func stackcacherelease(c *mcache, order uint8)</a>

```
searchKey: runtime.stackcacherelease
tags: [method private]
```

```Go
func stackcacherelease(c *mcache, order uint8)
```

### <a id="stackcheck" href="#stackcheck">func stackcheck()</a>

```
searchKey: runtime.stackcheck
tags: [function private]
```

```Go
func stackcheck()
```

stackcheck checks that SP is in range [g->stack.lo, g->stack.hi). 

### <a id="stackfree" href="#stackfree">func stackfree(stk stack)</a>

```
searchKey: runtime.stackfree
tags: [method private]
```

```Go
func stackfree(stk stack)
```

stackfree frees an n byte stack allocation at stk. 

stackfree must run on the system stack because it uses per-P resources and must not split the stack. 

### <a id="stackinit" href="#stackinit">func stackinit()</a>

```
searchKey: runtime.stackinit
tags: [function private]
```

```Go
func stackinit()
```

### <a id="stacklog2" href="#stacklog2">func stacklog2(n uintptr) int</a>

```
searchKey: runtime.stacklog2
tags: [method private]
```

```Go
func stacklog2(n uintptr) int
```

stacklog2 returns ⌊log_2(n)⌋. 

### <a id="stackpoolfree" href="#stackpoolfree">func stackpoolfree(x gclinkptr, order uint8)</a>

```
searchKey: runtime.stackpoolfree
tags: [method private]
```

```Go
func stackpoolfree(x gclinkptr, order uint8)
```

Adds stack x to the free pool. Must be called with stackpool[order].item.mu held. 

### <a id="startCheckmarks" href="#startCheckmarks">func startCheckmarks()</a>

```
searchKey: runtime.startCheckmarks
tags: [function private]
```

```Go
func startCheckmarks()
```

startCheckmarks prepares for the checkmarks phase. 

The world must be stopped. 

### <a id="startTemplateThread" href="#startTemplateThread">func startTemplateThread()</a>

```
searchKey: runtime.startTemplateThread
tags: [function private]
```

```Go
func startTemplateThread()
```

startTemplateThread starts the template thread if it is not already running. 

The calling thread must itself be in a known-good state. 

### <a id="startTheWorld" href="#startTheWorld">func startTheWorld()</a>

```
searchKey: runtime.startTheWorld
tags: [function private]
```

```Go
func startTheWorld()
```

startTheWorld undoes the effects of stopTheWorld. 

### <a id="startTheWorldGC" href="#startTheWorldGC">func startTheWorldGC()</a>

```
searchKey: runtime.startTheWorldGC
tags: [function private]
```

```Go
func startTheWorldGC()
```

startTheWorldGC undoes the effects of stopTheWorldGC. 

### <a id="startTheWorldWithSema" href="#startTheWorldWithSema">func startTheWorldWithSema(emitTraceEvent bool) int64</a>

```
searchKey: runtime.startTheWorldWithSema
tags: [method private]
```

```Go
func startTheWorldWithSema(emitTraceEvent bool) int64
```

### <a id="startTimer" href="#startTimer">func startTimer(t *timer)</a>

```
searchKey: runtime.startTimer
tags: [method private]
```

```Go
func startTimer(t *timer)
```

startTimer adds t to the timer heap. 

### <a id="startlockedm" href="#startlockedm">func startlockedm(gp *g)</a>

```
searchKey: runtime.startlockedm
tags: [method private]
```

```Go
func startlockedm(gp *g)
```

Schedules the locked m to run the locked gp. May run during STW, so write barriers are not allowed. 

### <a id="startm" href="#startm">func startm(_p_ *p, spinning bool)</a>

```
searchKey: runtime.startm
tags: [method private]
```

```Go
func startm(_p_ *p, spinning bool)
```

Schedules some M to run the p (creates an M if necessary). If p==nil, tries to get an idle P, if no idle P's does nothing. May run with m.p==nil, so write barriers are not allowed. If spinning is set, the caller has incremented nmspinning and startm will either decrement nmspinning or set m.spinning in the newly started M. 

Callers passing a non-nil P must call from a non-preemptible context. See comment on acquirem below. 

Must not have write barriers because this may be called without a P. 

### <a id="startpanic_m" href="#startpanic_m">func startpanic_m() bool</a>

```
searchKey: runtime.startpanic_m
tags: [function private]
```

```Go
func startpanic_m() bool
```

startpanic_m prepares for an unrecoverable panic. 

It returns true if panic messages should be printed, or false if the runtime is in bad shape and should just print stacks. 

It must not have write barriers even though the write barrier explicitly ignores writes once dying > 0. Write barriers still assume that g.m.p != nil, and this function may not have P in some contexts (e.g. a panic in a signal handler for a signal sent to an M with no P). 

### <a id="step" href="#step">func step(p []byte, pc *uintptr, val *int32, first bool) (newp []byte, ok bool)</a>

```
searchKey: runtime.step
tags: [method private]
```

```Go
func step(p []byte, pc *uintptr, val *int32, first bool) (newp []byte, ok bool)
```

step advances to the next pc, value pair in the encoded table. 

### <a id="stopTheWorld" href="#stopTheWorld">func stopTheWorld(reason string)</a>

```
searchKey: runtime.stopTheWorld
tags: [method private]
```

```Go
func stopTheWorld(reason string)
```

stopTheWorld stops all P's from executing goroutines, interrupting all goroutines at GC safe points and records reason as the reason for the stop. On return, only the current goroutine's P is running. stopTheWorld must not be called from a system stack and the caller must not hold worldsema. The caller must call startTheWorld when other P's should resume execution. 

stopTheWorld is safe for multiple goroutines to call at the same time. Each will execute its own stop, and the stops will be serialized. 

This is also used by routines that do stack dumps. If the system is in panic or being exited, this may not reliably stop all goroutines. 

### <a id="stopTheWorldGC" href="#stopTheWorldGC">func stopTheWorldGC(reason string)</a>

```
searchKey: runtime.stopTheWorldGC
tags: [method private]
```

```Go
func stopTheWorldGC(reason string)
```

stopTheWorldGC has the same effect as stopTheWorld, but blocks until the GC is not running. It also blocks a GC from starting until startTheWorldGC is called. 

### <a id="stopTheWorldWithSema" href="#stopTheWorldWithSema">func stopTheWorldWithSema()</a>

```
searchKey: runtime.stopTheWorldWithSema
tags: [function private]
```

```Go
func stopTheWorldWithSema()
```

stopTheWorldWithSema is the core implementation of stopTheWorld. The caller is responsible for acquiring worldsema and disabling preemption first and then should stopTheWorldWithSema on the system stack: 

```
semacquire(&worldsema, 0)
m.preemptoff = "reason"
systemstack(stopTheWorldWithSema)

```
When finished, the caller must either call startTheWorld or undo these three operations separately: 

```
m.preemptoff = ""
systemstack(startTheWorldWithSema)
semrelease(&worldsema)

```
It is allowed to acquire worldsema once and then execute multiple startTheWorldWithSema/stopTheWorldWithSema pairs. Other P's are able to execute between successive calls to startTheWorldWithSema and stopTheWorldWithSema. Holding worldsema causes any other goroutines invoking stopTheWorld to block. 

### <a id="stopTimer" href="#stopTimer">func stopTimer(t *timer) bool</a>

```
searchKey: runtime.stopTimer
tags: [method private]
```

```Go
func stopTimer(t *timer) bool
```

stopTimer stops a timer. It reports whether t was stopped before being run. 

### <a id="stoplockedm" href="#stoplockedm">func stoplockedm()</a>

```
searchKey: runtime.stoplockedm
tags: [function private]
```

```Go
func stoplockedm()
```

Stops execution of the current m that is locked to a g until the g is runnable again. Returns with acquired P. 

### <a id="stopm" href="#stopm">func stopm()</a>

```
searchKey: runtime.stopm
tags: [function private]
```

```Go
func stopm()
```

Stops execution of the current m until new work is available. Returns with acquired P. 

### <a id="strequal" href="#strequal">func strequal(p, q unsafe.Pointer) bool</a>

```
searchKey: runtime.strequal
tags: [method private]
```

```Go
func strequal(p, q unsafe.Pointer) bool
```

### <a id="strhash" href="#strhash">func strhash(p unsafe.Pointer, h uintptr) uintptr</a>

```
searchKey: runtime.strhash
tags: [method private]
```

```Go
func strhash(p unsafe.Pointer, h uintptr) uintptr
```

### <a id="strhashFallback" href="#strhashFallback">func strhashFallback(a unsafe.Pointer, h uintptr) uintptr</a>

```
searchKey: runtime.strhashFallback
tags: [method private]
```

```Go
func strhashFallback(a unsafe.Pointer, h uintptr) uintptr
```

### <a id="stringDataOnStack" href="#stringDataOnStack">func stringDataOnStack(s string) bool</a>

```
searchKey: runtime.stringDataOnStack
tags: [method private]
```

```Go
func stringDataOnStack(s string) bool
```

stringDataOnStack reports whether the string's data is stored on the current goroutine's stack. 

### <a id="stringHash" href="#stringHash">func stringHash(s string, seed uintptr) uintptr</a>

```
searchKey: runtime.stringHash
tags: [method private]
```

```Go
func stringHash(s string, seed uintptr) uintptr
```

Testing adapters for hash quality tests (see hash_test.go) 

### <a id="stringtoslicebyte" href="#stringtoslicebyte">func stringtoslicebyte(buf *tmpBuf, s string) []byte</a>

```
searchKey: runtime.stringtoslicebyte
tags: [method private]
```

```Go
func stringtoslicebyte(buf *tmpBuf, s string) []byte
```

### <a id="stringtoslicerune" href="#stringtoslicerune">func stringtoslicerune(buf *[tmpStringBufSize]rune, s string) []rune</a>

```
searchKey: runtime.stringtoslicerune
tags: [method private]
```

```Go
func stringtoslicerune(buf *[tmpStringBufSize]rune, s string) []rune
```

### <a id="subtract1" href="#subtract1">func subtract1(p *byte) *byte</a>

```
searchKey: runtime.subtract1
tags: [method private]
```

```Go
func subtract1(p *byte) *byte
```

subtract1 returns the byte pointer p-1. 

nosplit because it is used during write barriers and must not be preempted. 

### <a id="subtractb" href="#subtractb">func subtractb(p *byte, n uintptr) *byte</a>

```
searchKey: runtime.subtractb
tags: [method private]
```

```Go
func subtractb(p *byte, n uintptr) *byte
```

subtractb returns the byte pointer p-n. 

### <a id="sweepone" href="#sweepone">func sweepone() uintptr</a>

```
searchKey: runtime.sweepone
tags: [function private]
```

```Go
func sweepone() uintptr
```

sweepone sweeps some unswept heap span and returns the number of pages returned to the heap, or ^uintptr(0) if there was nothing to sweep. 

### <a id="sync_atomic_CompareAndSwapPointer" href="#sync_atomic_CompareAndSwapPointer">func sync_atomic_CompareAndSwapPointer(ptr *unsafe.Pointer, old, new unsafe.Pointer) bool</a>

```
searchKey: runtime.sync_atomic_CompareAndSwapPointer
tags: [method private]
```

```Go
func sync_atomic_CompareAndSwapPointer(ptr *unsafe.Pointer, old, new unsafe.Pointer) bool
```

### <a id="sync_atomic_CompareAndSwapUintptr" href="#sync_atomic_CompareAndSwapUintptr">func sync_atomic_CompareAndSwapUintptr(ptr *uintptr, old, new uintptr) bool</a>

```
searchKey: runtime.sync_atomic_CompareAndSwapUintptr
tags: [method private]
```

```Go
func sync_atomic_CompareAndSwapUintptr(ptr *uintptr, old, new uintptr) bool
```

### <a id="sync_atomic_StorePointer" href="#sync_atomic_StorePointer">func sync_atomic_StorePointer(ptr *unsafe.Pointer, new unsafe.Pointer)</a>

```
searchKey: runtime.sync_atomic_StorePointer
tags: [method private]
```

```Go
func sync_atomic_StorePointer(ptr *unsafe.Pointer, new unsafe.Pointer)
```

### <a id="sync_atomic_StoreUintptr" href="#sync_atomic_StoreUintptr">func sync_atomic_StoreUintptr(ptr *uintptr, new uintptr)</a>

```
searchKey: runtime.sync_atomic_StoreUintptr
tags: [method private]
```

```Go
func sync_atomic_StoreUintptr(ptr *uintptr, new uintptr)
```

### <a id="sync_atomic_SwapPointer" href="#sync_atomic_SwapPointer">func sync_atomic_SwapPointer(ptr *unsafe.Pointer, new unsafe.Pointer) unsafe.Pointer</a>

```
searchKey: runtime.sync_atomic_SwapPointer
tags: [method private]
```

```Go
func sync_atomic_SwapPointer(ptr *unsafe.Pointer, new unsafe.Pointer) unsafe.Pointer
```

### <a id="sync_atomic_SwapUintptr" href="#sync_atomic_SwapUintptr">func sync_atomic_SwapUintptr(ptr *uintptr, new uintptr) uintptr</a>

```
searchKey: runtime.sync_atomic_SwapUintptr
tags: [method private]
```

```Go
func sync_atomic_SwapUintptr(ptr *uintptr, new uintptr) uintptr
```

### <a id="sync_atomic_runtime_procPin" href="#sync_atomic_runtime_procPin">func sync_atomic_runtime_procPin() int</a>

```
searchKey: runtime.sync_atomic_runtime_procPin
tags: [function private]
```

```Go
func sync_atomic_runtime_procPin() int
```

### <a id="sync_atomic_runtime_procUnpin" href="#sync_atomic_runtime_procUnpin">func sync_atomic_runtime_procUnpin()</a>

```
searchKey: runtime.sync_atomic_runtime_procUnpin
tags: [function private]
```

```Go
func sync_atomic_runtime_procUnpin()
```

### <a id="sync_fastrand" href="#sync_fastrand">func sync_fastrand() uint32</a>

```
searchKey: runtime.sync_fastrand
tags: [function private]
```

```Go
func sync_fastrand() uint32
```

### <a id="sync_nanotime" href="#sync_nanotime">func sync_nanotime() int64</a>

```
searchKey: runtime.sync_nanotime
tags: [function private]
```

```Go
func sync_nanotime() int64
```

### <a id="sync_runtime_Semacquire" href="#sync_runtime_Semacquire">func sync_runtime_Semacquire(addr *uint32)</a>

```
searchKey: runtime.sync_runtime_Semacquire
tags: [method private]
```

```Go
func sync_runtime_Semacquire(addr *uint32)
```

### <a id="sync_runtime_SemacquireMutex" href="#sync_runtime_SemacquireMutex">func sync_runtime_SemacquireMutex(addr *uint32, lifo bool, skipframes int)</a>

```
searchKey: runtime.sync_runtime_SemacquireMutex
tags: [method private]
```

```Go
func sync_runtime_SemacquireMutex(addr *uint32, lifo bool, skipframes int)
```

### <a id="sync_runtime_Semrelease" href="#sync_runtime_Semrelease">func sync_runtime_Semrelease(addr *uint32, handoff bool, skipframes int)</a>

```
searchKey: runtime.sync_runtime_Semrelease
tags: [method private]
```

```Go
func sync_runtime_Semrelease(addr *uint32, handoff bool, skipframes int)
```

### <a id="sync_runtime_canSpin" href="#sync_runtime_canSpin">func sync_runtime_canSpin(i int) bool</a>

```
searchKey: runtime.sync_runtime_canSpin
tags: [method private]
```

```Go
func sync_runtime_canSpin(i int) bool
```

Active spinning for sync.Mutex. 

### <a id="sync_runtime_doSpin" href="#sync_runtime_doSpin">func sync_runtime_doSpin()</a>

```
searchKey: runtime.sync_runtime_doSpin
tags: [function private]
```

```Go
func sync_runtime_doSpin()
```

### <a id="sync_runtime_procPin" href="#sync_runtime_procPin">func sync_runtime_procPin() int</a>

```
searchKey: runtime.sync_runtime_procPin
tags: [function private]
```

```Go
func sync_runtime_procPin() int
```

### <a id="sync_runtime_procUnpin" href="#sync_runtime_procUnpin">func sync_runtime_procUnpin()</a>

```
searchKey: runtime.sync_runtime_procUnpin
tags: [function private]
```

```Go
func sync_runtime_procUnpin()
```

### <a id="sync_runtime_registerPoolCleanup" href="#sync_runtime_registerPoolCleanup">func sync_runtime_registerPoolCleanup(f func())</a>

```
searchKey: runtime.sync_runtime_registerPoolCleanup
tags: [method private]
```

```Go
func sync_runtime_registerPoolCleanup(f func())
```

### <a id="sync_throw" href="#sync_throw">func sync_throw(s string)</a>

```
searchKey: runtime.sync_throw
tags: [method private]
```

```Go
func sync_throw(s string)
```

### <a id="syncadjustsudogs" href="#syncadjustsudogs">func syncadjustsudogs(gp *g, used uintptr, adjinfo *adjustinfo) uintptr</a>

```
searchKey: runtime.syncadjustsudogs
tags: [method private]
```

```Go
func syncadjustsudogs(gp *g, used uintptr, adjinfo *adjustinfo) uintptr
```

syncadjustsudogs adjusts gp's sudogs and copies the part of gp's stack they refer to while synchronizing with concurrent channel operations. It returns the number of bytes of stack copied. 

### <a id="sysAlloc" href="#sysAlloc">func sysAlloc(n uintptr, sysStat *sysMemStat) unsafe.Pointer</a>

```
searchKey: runtime.sysAlloc
tags: [method private]
```

```Go
func sysAlloc(n uintptr, sysStat *sysMemStat) unsafe.Pointer
```

Don't split the stack as this function may be invoked without a valid G, which prevents us from allocating more stack. 

### <a id="sysFault" href="#sysFault">func sysFault(v unsafe.Pointer, n uintptr)</a>

```
searchKey: runtime.sysFault
tags: [method private]
```

```Go
func sysFault(v unsafe.Pointer, n uintptr)
```

### <a id="sysFree" href="#sysFree">func sysFree(v unsafe.Pointer, n uintptr, sysStat *sysMemStat)</a>

```
searchKey: runtime.sysFree
tags: [method private]
```

```Go
func sysFree(v unsafe.Pointer, n uintptr, sysStat *sysMemStat)
```

Don't split the stack as this function may be invoked without a valid G, which prevents us from allocating more stack. 

### <a id="sysHugePage" href="#sysHugePage">func sysHugePage(v unsafe.Pointer, n uintptr)</a>

```
searchKey: runtime.sysHugePage
tags: [method private]
```

```Go
func sysHugePage(v unsafe.Pointer, n uintptr)
```

### <a id="sysMap" href="#sysMap">func sysMap(v unsafe.Pointer, n uintptr, sysStat *sysMemStat)</a>

```
searchKey: runtime.sysMap
tags: [method private]
```

```Go
func sysMap(v unsafe.Pointer, n uintptr, sysStat *sysMemStat)
```

### <a id="sysReserve" href="#sysReserve">func sysReserve(v unsafe.Pointer, n uintptr) unsafe.Pointer</a>

```
searchKey: runtime.sysReserve
tags: [method private]
```

```Go
func sysReserve(v unsafe.Pointer, n uintptr) unsafe.Pointer
```

### <a id="sysReserveAligned" href="#sysReserveAligned">func sysReserveAligned(v unsafe.Pointer, size, align uintptr) (unsafe.Pointer, uintptr)</a>

```
searchKey: runtime.sysReserveAligned
tags: [method private]
```

```Go
func sysReserveAligned(v unsafe.Pointer, size, align uintptr) (unsafe.Pointer, uintptr)
```

sysReserveAligned is like sysReserve, but the returned pointer is aligned to align bytes. It may reserve either n or n+align bytes, so it returns the size that was reserved. 

### <a id="sysUnused" href="#sysUnused">func sysUnused(v unsafe.Pointer, n uintptr)</a>

```
searchKey: runtime.sysUnused
tags: [method private]
```

```Go
func sysUnused(v unsafe.Pointer, n uintptr)
```

### <a id="sysUsed" href="#sysUsed">func sysUsed(v unsafe.Pointer, n uintptr)</a>

```
searchKey: runtime.sysUsed
tags: [method private]
```

```Go
func sysUsed(v unsafe.Pointer, n uintptr)
```

### <a id="sysargs" href="#sysargs">func sysargs(argc int32, argv **byte)</a>

```
searchKey: runtime.sysargs
tags: [method private]
```

```Go
func sysargs(argc int32, argv **byte)
```

### <a id="syscall" href="#syscall">func syscall()</a>

```
searchKey: runtime.syscall
tags: [function private]
```

```Go
func syscall()
```

### <a id="syscall6" href="#syscall6">func syscall6()</a>

```
searchKey: runtime.syscall6
tags: [function private]
```

```Go
func syscall6()
```

### <a id="syscall6X" href="#syscall6X">func syscall6X()</a>

```
searchKey: runtime.syscall6X
tags: [function private]
```

```Go
func syscall6X()
```

### <a id="syscallNoErr" href="#syscallNoErr">func syscallNoErr()</a>

```
searchKey: runtime.syscallNoErr
tags: [function private]
```

```Go
func syscallNoErr()
```

### <a id="syscallPtr" href="#syscallPtr">func syscallPtr()</a>

```
searchKey: runtime.syscallPtr
tags: [function private]
```

```Go
func syscallPtr()
```

### <a id="syscallX" href="#syscallX">func syscallX()</a>

```
searchKey: runtime.syscallX
tags: [function private]
```

```Go
func syscallX()
```

### <a id="syscall_Exit" href="#syscall_Exit">func syscall_Exit(code int)</a>

```
searchKey: runtime.syscall_Exit
tags: [method private]
```

```Go
func syscall_Exit(code int)
```

### <a id="syscall_Getpagesize" href="#syscall_Getpagesize">func syscall_Getpagesize() int</a>

```
searchKey: runtime.syscall_Getpagesize
tags: [function private]
```

```Go
func syscall_Getpagesize() int
```

### <a id="syscall_cgocaller" href="#syscall_cgocaller">func syscall_cgocaller(fn unsafe.Pointer, args ...uintptr) uintptr</a>

```
searchKey: runtime.syscall_cgocaller
tags: [method private]
```

```Go
func syscall_cgocaller(fn unsafe.Pointer, args ...uintptr) uintptr
```

wrapper for syscall package to call cgocall for libc (cgo) calls. 

### <a id="syscall_rawSyscall" href="#syscall_rawSyscall">func syscall_rawSyscall(fn, a1, a2, a3 uintptr) (r1, r2, err uintptr)</a>

```
searchKey: runtime.syscall_rawSyscall
tags: [method private]
```

```Go
func syscall_rawSyscall(fn, a1, a2, a3 uintptr) (r1, r2, err uintptr)
```

### <a id="syscall_rawSyscall6" href="#syscall_rawSyscall6">func syscall_rawSyscall6(fn, a1, a2, a3, a4, a5, a6 uintptr) (r1, r2, err uintptr)</a>

```
searchKey: runtime.syscall_rawSyscall6
tags: [method private]
```

```Go
func syscall_rawSyscall6(fn, a1, a2, a3, a4, a5, a6 uintptr) (r1, r2, err uintptr)
```

### <a id="syscall_runtime_AfterExec" href="#syscall_runtime_AfterExec">func syscall_runtime_AfterExec()</a>

```
searchKey: runtime.syscall_runtime_AfterExec
tags: [function private]
```

```Go
func syscall_runtime_AfterExec()
```

Called from syscall package after Exec. 

### <a id="syscall_runtime_AfterFork" href="#syscall_runtime_AfterFork">func syscall_runtime_AfterFork()</a>

```
searchKey: runtime.syscall_runtime_AfterFork
tags: [function private]
```

```Go
func syscall_runtime_AfterFork()
```

Called from syscall package after fork in parent. 

### <a id="syscall_runtime_AfterForkInChild" href="#syscall_runtime_AfterForkInChild">func syscall_runtime_AfterForkInChild()</a>

```
searchKey: runtime.syscall_runtime_AfterForkInChild
tags: [function private]
```

```Go
func syscall_runtime_AfterForkInChild()
```

Called from syscall package after fork in child. It resets non-sigignored signals to the default handler, and restores the signal mask in preparation for the exec. 

Because this might be called during a vfork, and therefore may be temporarily sharing address space with the parent process, this must not change any global variables or calling into C code that may do so. 

### <a id="syscall_runtime_BeforeExec" href="#syscall_runtime_BeforeExec">func syscall_runtime_BeforeExec()</a>

```
searchKey: runtime.syscall_runtime_BeforeExec
tags: [function private]
```

```Go
func syscall_runtime_BeforeExec()
```

Called from syscall package before Exec. 

### <a id="syscall_runtime_BeforeFork" href="#syscall_runtime_BeforeFork">func syscall_runtime_BeforeFork()</a>

```
searchKey: runtime.syscall_runtime_BeforeFork
tags: [function private]
```

```Go
func syscall_runtime_BeforeFork()
```

Called from syscall package before fork. 

### <a id="syscall_runtime_doAllThreadsSyscall" href="#syscall_runtime_doAllThreadsSyscall">func syscall_runtime_doAllThreadsSyscall(fn func(bool) bool)</a>

```
searchKey: runtime.syscall_runtime_doAllThreadsSyscall
tags: [method private]
```

```Go
func syscall_runtime_doAllThreadsSyscall(fn func(bool) bool)
```

syscall_runtime_doAllThreadsSyscall serializes Go execution and executes a specified fn() call on all m's. 

The boolean argument to fn() indicates whether the function's return value will be consulted or not. That is, fn(true) should return true if fn() succeeds, and fn(true) should return false if it failed. When fn(false) is called, its return status will be ignored. 

syscall_runtime_doAllThreadsSyscall first invokes fn(true) on a single, coordinating, m, and only if it returns true does it go on to invoke fn(false) on all of the other m's known to the process. 

### <a id="syscall_runtime_envs" href="#syscall_runtime_envs">func syscall_runtime_envs() []string</a>

```
searchKey: runtime.syscall_runtime_envs
tags: [function private]
```

```Go
func syscall_runtime_envs() []string
```

### <a id="syscall_setenv_c" href="#syscall_setenv_c">func syscall_setenv_c(k string, v string)</a>

```
searchKey: runtime.syscall_setenv_c
tags: [method private]
```

```Go
func syscall_setenv_c(k string, v string)
```

Update the C environment if cgo is loaded. Called from syscall.Setenv. 

### <a id="syscall_syscall" href="#syscall_syscall">func syscall_syscall(fn, a1, a2, a3 uintptr) (r1, r2, err uintptr)</a>

```
searchKey: runtime.syscall_syscall
tags: [method private]
```

```Go
func syscall_syscall(fn, a1, a2, a3 uintptr) (r1, r2, err uintptr)
```

### <a id="syscall_syscall6" href="#syscall_syscall6">func syscall_syscall6(fn, a1, a2, a3, a4, a5, a6 uintptr) (r1, r2, err uintptr)</a>

```
searchKey: runtime.syscall_syscall6
tags: [method private]
```

```Go
func syscall_syscall6(fn, a1, a2, a3, a4, a5, a6 uintptr) (r1, r2, err uintptr)
```

### <a id="syscall_syscall6X" href="#syscall_syscall6X">func syscall_syscall6X(fn, a1, a2, a3, a4, a5, a6 uintptr) (r1, r2, err uintptr)</a>

```
searchKey: runtime.syscall_syscall6X
tags: [method private]
```

```Go
func syscall_syscall6X(fn, a1, a2, a3, a4, a5, a6 uintptr) (r1, r2, err uintptr)
```

### <a id="syscall_syscallPtr" href="#syscall_syscallPtr">func syscall_syscallPtr(fn, a1, a2, a3 uintptr) (r1, r2, err uintptr)</a>

```
searchKey: runtime.syscall_syscallPtr
tags: [method private]
```

```Go
func syscall_syscallPtr(fn, a1, a2, a3 uintptr) (r1, r2, err uintptr)
```

### <a id="syscall_syscallX" href="#syscall_syscallX">func syscall_syscallX(fn, a1, a2, a3 uintptr) (r1, r2, err uintptr)</a>

```
searchKey: runtime.syscall_syscallX
tags: [method private]
```

```Go
func syscall_syscallX(fn, a1, a2, a3 uintptr) (r1, r2, err uintptr)
```

### <a id="syscall_unsetenv_c" href="#syscall_unsetenv_c">func syscall_unsetenv_c(k string)</a>

```
searchKey: runtime.syscall_unsetenv_c
tags: [method private]
```

```Go
func syscall_unsetenv_c(k string)
```

Update the C environment if cgo is loaded. Called from syscall.unsetenv. 

### <a id="sysctl" href="#sysctl">func sysctl(mib *uint32, miblen uint32, oldp *byte, oldlenp *uintptr, newp *byte, newlen uintptr) int32</a>

```
searchKey: runtime.sysctl
tags: [method private]
```

```Go
func sysctl(mib *uint32, miblen uint32, oldp *byte, oldlenp *uintptr, newp *byte, newlen uintptr) int32
```

### <a id="sysctl_trampoline" href="#sysctl_trampoline">func sysctl_trampoline()</a>

```
searchKey: runtime.sysctl_trampoline
tags: [function private]
```

```Go
func sysctl_trampoline()
```

### <a id="sysctlbyname" href="#sysctlbyname">func sysctlbyname(name *byte, oldp *byte, oldlenp *uintptr, newp *byte, newlen uintptr) int32</a>

```
searchKey: runtime.sysctlbyname
tags: [method private]
```

```Go
func sysctlbyname(name *byte, oldp *byte, oldlenp *uintptr, newp *byte, newlen uintptr) int32
```

### <a id="sysctlbynameInt32" href="#sysctlbynameInt32">func sysctlbynameInt32(name []byte) (int32, int32)</a>

```
searchKey: runtime.sysctlbynameInt32
tags: [method private]
```

```Go
func sysctlbynameInt32(name []byte) (int32, int32)
```

### <a id="sysctlbyname_trampoline" href="#sysctlbyname_trampoline">func sysctlbyname_trampoline()</a>

```
searchKey: runtime.sysctlbyname_trampoline
tags: [function private]
```

```Go
func sysctlbyname_trampoline()
```

### <a id="sysmon" href="#sysmon">func sysmon()</a>

```
searchKey: runtime.sysmon
tags: [function private]
```

```Go
func sysmon()
```

Always runs without a P, so write barriers are not allowed. 

### <a id="systemstack" href="#systemstack">func systemstack(fn func())</a>

```
searchKey: runtime.systemstack
tags: [method private]
```

```Go
func systemstack(fn func())
```

systemstack runs fn on a system stack. If systemstack is called from the per-OS-thread (g0) stack, or if systemstack is called from the signal handling (gsignal) stack, systemstack calls fn directly and returns. Otherwise, systemstack is being called from the limited stack of an ordinary goroutine. In this case, systemstack switches to the per-OS-thread stack, calls fn, and switches back. It is common to use a func literal as the argument, in order to share inputs and outputs with the code around the call to system stack: 

```
... set up y ...
systemstack(func() {
	x = bigcall(y)
})
... use x ...

```
### <a id="systemstack_switch" href="#systemstack_switch">func systemstack_switch()</a>

```
searchKey: runtime.systemstack_switch
tags: [function private]
```

```Go
func systemstack_switch()
```

### <a id="templateThread" href="#templateThread">func templateThread()</a>

```
searchKey: runtime.templateThread
tags: [function private]
```

```Go
func templateThread()
```

templateThread is a thread in a known-good state that exists solely to start new threads in known-good states when the calling thread may not be in a good state. 

Many programs never need this, so templateThread is started lazily when we first enter a state that might lead to running on a thread in an unknown state. 

templateThread runs on an M without a P, so it must not have write barriers. 

### <a id="testAtomic64" href="#testAtomic64">func testAtomic64()</a>

```
searchKey: runtime.testAtomic64
tags: [function private]
```

```Go
func testAtomic64()
```

### <a id="testdefersizes" href="#testdefersizes">func testdefersizes()</a>

```
searchKey: runtime.testdefersizes
tags: [function private]
```

```Go
func testdefersizes()
```

Ensure that defer arg sizes that map to the same defer size class also map to the same malloc size class. 

### <a id="throw" href="#throw">func throw(s string)</a>

```
searchKey: runtime.throw
tags: [method private]
```

```Go
func throw(s string)
```

### <a id="tickspersecond" href="#tickspersecond">func tickspersecond() int64</a>

```
searchKey: runtime.tickspersecond
tags: [function private]
```

```Go
func tickspersecond() int64
```

Note: Called by runtime/pprof in addition to runtime code. 

### <a id="timeHistogramMetricsBuckets" href="#timeHistogramMetricsBuckets">func timeHistogramMetricsBuckets() []float64</a>

```
searchKey: runtime.timeHistogramMetricsBuckets
tags: [function private]
```

```Go
func timeHistogramMetricsBuckets() []float64
```

timeHistogramMetricsBuckets generates a slice of boundaries for the timeHistogram. These boundaries are represented in seconds, not nanoseconds like the timeHistogram represents durations. 

### <a id="timeSleep" href="#timeSleep">func timeSleep(ns int64)</a>

```
searchKey: runtime.timeSleep
tags: [method private]
```

```Go
func timeSleep(ns int64)
```

timeSleep puts the current goroutine to sleep for at least ns nanoseconds. 

### <a id="time_now" href="#time_now">func time_now() (sec int64, nsec int32, mono int64)</a>

```
searchKey: runtime.time_now
tags: [function private]
```

```Go
func time_now() (sec int64, nsec int32, mono int64)
```

### <a id="timediv" href="#timediv">func timediv(v int64, div int32, rem *int32) int32</a>

```
searchKey: runtime.timediv
tags: [method private]
```

```Go
func timediv(v int64, div int32, rem *int32) int32
```

Poor mans 64-bit division. This is a very special function, do not use it if you are not sure what you are doing. int64 division is lowered into _divv() call on 386, which does not fit into nosplit functions. Handles overflow in a time-specific manner. This keeps us within no-split stack limits on 32-bit processors. 

### <a id="tooManyOverflowBuckets" href="#tooManyOverflowBuckets">func tooManyOverflowBuckets(noverflow uint16, B uint8) bool</a>

```
searchKey: runtime.tooManyOverflowBuckets
tags: [method private]
```

```Go
func tooManyOverflowBuckets(noverflow uint16, B uint8) bool
```

tooManyOverflowBuckets reports whether noverflow buckets is too many for a map with 1<<B buckets. Note that most of these overflow buckets must be in sparse use; if use was dense, then we'd have already triggered regular map growth. 

### <a id="tophash" href="#tophash">func tophash(hash uintptr) uint8</a>

```
searchKey: runtime.tophash
tags: [method private]
```

```Go
func tophash(hash uintptr) uint8
```

tophash calculates the tophash value for hash. 

### <a id="totaldefersize" href="#totaldefersize">func totaldefersize(siz uintptr) uintptr</a>

```
searchKey: runtime.totaldefersize
tags: [method private]
```

```Go
func totaldefersize(siz uintptr) uintptr
```

total size of memory block for defer with arg size sz 

### <a id="traceAppend" href="#traceAppend">func traceAppend(buf []byte, v uint64) []byte</a>

```
searchKey: runtime.traceAppend
tags: [method private]
```

```Go
func traceAppend(buf []byte, v uint64) []byte
```

traceAppend appends v to buf in little-endian-base-128 encoding. 

### <a id="traceEvent" href="#traceEvent">func traceEvent(ev byte, skip int, args ...uint64)</a>

```
searchKey: runtime.traceEvent
tags: [method private]
```

```Go
func traceEvent(ev byte, skip int, args ...uint64)
```

traceEvent writes a single event to trace buffer, flushing the buffer if necessary. ev is event type. If skip > 0, write current stack id as the last argument (skipping skip top frames). If skip = 0, this event type should contain a stack, but we don't want to collect and remember it for this particular call. 

### <a id="traceEventLocked" href="#traceEventLocked">func traceEventLocked(extraBytes int, mp *m, pid int32, bufp *traceBufPtr, ev byte, skip int, args ...uint64)</a>

```
searchKey: runtime.traceEventLocked
tags: [method private]
```

```Go
func traceEventLocked(extraBytes int, mp *m, pid int32, bufp *traceBufPtr, ev byte, skip int, args ...uint64)
```

### <a id="traceFullQueue" href="#traceFullQueue">func traceFullQueue(buf traceBufPtr)</a>

```
searchKey: runtime.traceFullQueue
tags: [method private]
```

```Go
func traceFullQueue(buf traceBufPtr)
```

traceFullQueue queues buf into queue of full buffers. 

### <a id="traceGCDone" href="#traceGCDone">func traceGCDone()</a>

```
searchKey: runtime.traceGCDone
tags: [function private]
```

```Go
func traceGCDone()
```

### <a id="traceGCMarkAssistDone" href="#traceGCMarkAssistDone">func traceGCMarkAssistDone()</a>

```
searchKey: runtime.traceGCMarkAssistDone
tags: [function private]
```

```Go
func traceGCMarkAssistDone()
```

### <a id="traceGCMarkAssistStart" href="#traceGCMarkAssistStart">func traceGCMarkAssistStart()</a>

```
searchKey: runtime.traceGCMarkAssistStart
tags: [function private]
```

```Go
func traceGCMarkAssistStart()
```

### <a id="traceGCSTWDone" href="#traceGCSTWDone">func traceGCSTWDone()</a>

```
searchKey: runtime.traceGCSTWDone
tags: [function private]
```

```Go
func traceGCSTWDone()
```

### <a id="traceGCSTWStart" href="#traceGCSTWStart">func traceGCSTWStart(kind int)</a>

```
searchKey: runtime.traceGCSTWStart
tags: [method private]
```

```Go
func traceGCSTWStart(kind int)
```

### <a id="traceGCStart" href="#traceGCStart">func traceGCStart()</a>

```
searchKey: runtime.traceGCStart
tags: [function private]
```

```Go
func traceGCStart()
```

### <a id="traceGCSweepDone" href="#traceGCSweepDone">func traceGCSweepDone()</a>

```
searchKey: runtime.traceGCSweepDone
tags: [function private]
```

```Go
func traceGCSweepDone()
```

### <a id="traceGCSweepSpan" href="#traceGCSweepSpan">func traceGCSweepSpan(bytesSwept uintptr)</a>

```
searchKey: runtime.traceGCSweepSpan
tags: [method private]
```

```Go
func traceGCSweepSpan(bytesSwept uintptr)
```

traceGCSweepSpan traces the sweep of a single page. 

This may be called outside a traceGCSweepStart/traceGCSweepDone pair; however, it will not emit any trace events in this case. 

### <a id="traceGCSweepStart" href="#traceGCSweepStart">func traceGCSweepStart()</a>

```
searchKey: runtime.traceGCSweepStart
tags: [function private]
```

```Go
func traceGCSweepStart()
```

traceGCSweepStart prepares to trace a sweep loop. This does not emit any events until traceGCSweepSpan is called. 

traceGCSweepStart must be paired with traceGCSweepDone and there must be no preemption points between these two calls. 

### <a id="traceGoCreate" href="#traceGoCreate">func traceGoCreate(newg *g, pc uintptr)</a>

```
searchKey: runtime.traceGoCreate
tags: [method private]
```

```Go
func traceGoCreate(newg *g, pc uintptr)
```

### <a id="traceGoEnd" href="#traceGoEnd">func traceGoEnd()</a>

```
searchKey: runtime.traceGoEnd
tags: [function private]
```

```Go
func traceGoEnd()
```

### <a id="traceGoPark" href="#traceGoPark">func traceGoPark(traceEv byte, skip int)</a>

```
searchKey: runtime.traceGoPark
tags: [method private]
```

```Go
func traceGoPark(traceEv byte, skip int)
```

### <a id="traceGoPreempt" href="#traceGoPreempt">func traceGoPreempt()</a>

```
searchKey: runtime.traceGoPreempt
tags: [function private]
```

```Go
func traceGoPreempt()
```

### <a id="traceGoSched" href="#traceGoSched">func traceGoSched()</a>

```
searchKey: runtime.traceGoSched
tags: [function private]
```

```Go
func traceGoSched()
```

### <a id="traceGoStart" href="#traceGoStart">func traceGoStart()</a>

```
searchKey: runtime.traceGoStart
tags: [function private]
```

```Go
func traceGoStart()
```

### <a id="traceGoSysBlock" href="#traceGoSysBlock">func traceGoSysBlock(pp *p)</a>

```
searchKey: runtime.traceGoSysBlock
tags: [method private]
```

```Go
func traceGoSysBlock(pp *p)
```

### <a id="traceGoSysCall" href="#traceGoSysCall">func traceGoSysCall()</a>

```
searchKey: runtime.traceGoSysCall
tags: [function private]
```

```Go
func traceGoSysCall()
```

### <a id="traceGoSysExit" href="#traceGoSysExit">func traceGoSysExit(ts int64)</a>

```
searchKey: runtime.traceGoSysExit
tags: [method private]
```

```Go
func traceGoSysExit(ts int64)
```

### <a id="traceGoUnpark" href="#traceGoUnpark">func traceGoUnpark(gp *g, skip int)</a>

```
searchKey: runtime.traceGoUnpark
tags: [method private]
```

```Go
func traceGoUnpark(gp *g, skip int)
```

### <a id="traceGomaxprocs" href="#traceGomaxprocs">func traceGomaxprocs(procs int32)</a>

```
searchKey: runtime.traceGomaxprocs
tags: [method private]
```

```Go
func traceGomaxprocs(procs int32)
```

### <a id="traceHeapAlloc" href="#traceHeapAlloc">func traceHeapAlloc()</a>

```
searchKey: runtime.traceHeapAlloc
tags: [function private]
```

```Go
func traceHeapAlloc()
```

### <a id="traceHeapGoal" href="#traceHeapGoal">func traceHeapGoal()</a>

```
searchKey: runtime.traceHeapGoal
tags: [function private]
```

```Go
func traceHeapGoal()
```

### <a id="traceProcFree" href="#traceProcFree">func traceProcFree(pp *p)</a>

```
searchKey: runtime.traceProcFree
tags: [method private]
```

```Go
func traceProcFree(pp *p)
```

traceProcFree frees trace buffer associated with pp. 

### <a id="traceProcStart" href="#traceProcStart">func traceProcStart()</a>

```
searchKey: runtime.traceProcStart
tags: [function private]
```

```Go
func traceProcStart()
```

### <a id="traceProcStop" href="#traceProcStop">func traceProcStop(pp *p)</a>

```
searchKey: runtime.traceProcStop
tags: [method private]
```

```Go
func traceProcStop(pp *p)
```

### <a id="traceReleaseBuffer" href="#traceReleaseBuffer">func traceReleaseBuffer(pid int32)</a>

```
searchKey: runtime.traceReleaseBuffer
tags: [method private]
```

```Go
func traceReleaseBuffer(pid int32)
```

traceReleaseBuffer releases a buffer previously acquired with traceAcquireBuffer. 

### <a id="traceStackID" href="#traceStackID">func traceStackID(mp *m, buf []uintptr, skip int) uint64</a>

```
searchKey: runtime.traceStackID
tags: [method private]
```

```Go
func traceStackID(mp *m, buf []uintptr, skip int) uint64
```

### <a id="trace_userLog" href="#trace_userLog">func trace_userLog(id uint64, category, message string)</a>

```
searchKey: runtime.trace_userLog
tags: [method private]
```

```Go
func trace_userLog(id uint64, category, message string)
```

### <a id="trace_userRegion" href="#trace_userRegion">func trace_userRegion(id, mode uint64, name string)</a>

```
searchKey: runtime.trace_userRegion
tags: [method private]
```

```Go
func trace_userRegion(id, mode uint64, name string)
```

### <a id="trace_userTaskCreate" href="#trace_userTaskCreate">func trace_userTaskCreate(id, parentID uint64, taskType string)</a>

```
searchKey: runtime.trace_userTaskCreate
tags: [method private]
```

```Go
func trace_userTaskCreate(id, parentID uint64, taskType string)
```

### <a id="trace_userTaskEnd" href="#trace_userTaskEnd">func trace_userTaskEnd(id uint64)</a>

```
searchKey: runtime.trace_userTaskEnd
tags: [method private]
```

```Go
func trace_userTaskEnd(id uint64)
```

### <a id="tracealloc" href="#tracealloc">func tracealloc(p unsafe.Pointer, size uintptr, typ *_type)</a>

```
searchKey: runtime.tracealloc
tags: [method private]
```

```Go
func tracealloc(p unsafe.Pointer, size uintptr, typ *_type)
```

### <a id="traceback" href="#traceback">func traceback(pc, sp, lr uintptr, gp *g)</a>

```
searchKey: runtime.traceback
tags: [method private]
```

```Go
func traceback(pc, sp, lr uintptr, gp *g)
```

### <a id="traceback1" href="#traceback1">func traceback1(pc, sp, lr uintptr, gp *g, flags uint)</a>

```
searchKey: runtime.traceback1
tags: [method private]
```

```Go
func traceback1(pc, sp, lr uintptr, gp *g, flags uint)
```

### <a id="tracebackCgoContext" href="#tracebackCgoContext">func tracebackCgoContext(pcbuf *uintptr, printing bool, ctxt uintptr, n, max int) int</a>

```
searchKey: runtime.tracebackCgoContext
tags: [method private]
```

```Go
func tracebackCgoContext(pcbuf *uintptr, printing bool, ctxt uintptr, n, max int) int
```

tracebackCgoContext handles tracing back a cgo context value, from the context argument to setCgoTraceback, for the gentraceback function. It returns the new value of n. 

### <a id="tracebackHexdump" href="#tracebackHexdump">func tracebackHexdump(stk stack, frame *stkframe, bad uintptr)</a>

```
searchKey: runtime.tracebackHexdump
tags: [method private]
```

```Go
func tracebackHexdump(stk stack, frame *stkframe, bad uintptr)
```

tracebackHexdump hexdumps part of stk around frame.sp and frame.fp for debugging purposes. If the address bad is included in the hexdumped range, it will mark it as well. 

### <a id="tracebackdefers" href="#tracebackdefers">func tracebackdefers(gp *g, callback func(*stkframe, unsafe.Pointer) bool, v unsafe.Pointer)</a>

```
searchKey: runtime.tracebackdefers
tags: [method private]
```

```Go
func tracebackdefers(gp *g, callback func(*stkframe, unsafe.Pointer) bool, v unsafe.Pointer)
```

Traceback over the deferred function calls. Report them like calls that have been invoked but not started executing yet. 

### <a id="tracebackothers" href="#tracebackothers">func tracebackothers(me *g)</a>

```
searchKey: runtime.tracebackothers
tags: [method private]
```

```Go
func tracebackothers(me *g)
```

### <a id="tracebacktrap" href="#tracebacktrap">func tracebacktrap(pc, sp, lr uintptr, gp *g)</a>

```
searchKey: runtime.tracebacktrap
tags: [method private]
```

```Go
func tracebacktrap(pc, sp, lr uintptr, gp *g)
```

tracebacktrap is like traceback but expects that the PC and SP were obtained from a trap, not from gp->sched or gp->syscallpc/gp->syscallsp or getcallerpc/getcallersp. Because they are from a trap instead of from a saved pair, the initial PC must not be rewound to the previous instruction. (All the saved pairs record a PC that is a return address, so we rewind it into the CALL instruction.) If gp.m.libcall{g,pc,sp} information is available, it uses that information in preference to the pc/sp/lr passed in. 

### <a id="tracefree" href="#tracefree">func tracefree(p unsafe.Pointer, size uintptr)</a>

```
searchKey: runtime.tracefree
tags: [method private]
```

```Go
func tracefree(p unsafe.Pointer, size uintptr)
```

### <a id="tracegc" href="#tracegc">func tracegc()</a>

```
searchKey: runtime.tracegc
tags: [function private]
```

```Go
func tracegc()
```

### <a id="typeBitsBulkBarrier" href="#typeBitsBulkBarrier">func typeBitsBulkBarrier(typ *_type, dst, src, size uintptr)</a>

```
searchKey: runtime.typeBitsBulkBarrier
tags: [method private]
```

```Go
func typeBitsBulkBarrier(typ *_type, dst, src, size uintptr)
```

typeBitsBulkBarrier executes a write barrier for every pointer that would be copied from [src, src+size) to [dst, dst+size) by a memmove using the type bitmap to locate those pointer slots. 

The type typ must correspond exactly to [src, src+size) and [dst, dst+size). dst, src, and size must be pointer-aligned. The type typ must have a plain bitmap, not a GC program. The only use of this function is in channel sends, and the 64 kB channel element limit takes care of this for us. 

Must not be preempted because it typically runs right before memmove, and the GC must observe them as an atomic action. 

Callers must perform cgo checks if writeBarrier.cgo. 

### <a id="typedmemclr" href="#typedmemclr">func typedmemclr(typ *_type, ptr unsafe.Pointer)</a>

```
searchKey: runtime.typedmemclr
tags: [method private]
```

```Go
func typedmemclr(typ *_type, ptr unsafe.Pointer)
```

typedmemclr clears the typed memory at ptr with type typ. The memory at ptr must already be initialized (and hence in type-safe state). If the memory is being initialized for the first time, see memclrNoHeapPointers. 

If the caller knows that typ has pointers, it can alternatively call memclrHasPointers. 

### <a id="typedmemmove" href="#typedmemmove">func typedmemmove(typ *_type, dst, src unsafe.Pointer)</a>

```
searchKey: runtime.typedmemmove
tags: [method private]
```

```Go
func typedmemmove(typ *_type, dst, src unsafe.Pointer)
```

typedmemmove copies a value of type t to dst from src. Must be nosplit, see #16026. 

TODO: Perfect for go:nosplitrec since we can't have a safe point anywhere in the bulk barrier or memmove. 

### <a id="typedslicecopy" href="#typedslicecopy">func typedslicecopy(typ *_type, dstPtr unsafe.Pointer, dstLen int, srcPtr unsafe.Pointer, srcLen int) int</a>

```
searchKey: runtime.typedslicecopy
tags: [method private]
```

```Go
func typedslicecopy(typ *_type, dstPtr unsafe.Pointer, dstLen int, srcPtr unsafe.Pointer, srcLen int) int
```

### <a id="typehash" href="#typehash">func typehash(t *_type, p unsafe.Pointer, h uintptr) uintptr</a>

```
searchKey: runtime.typehash
tags: [method private]
```

```Go
func typehash(t *_type, p unsafe.Pointer, h uintptr) uintptr
```

typehash computes the hash of the object of type t at address p. h is the seed. This function is seldom used. Most maps use for hashing either fixed functions (e.g. f32hash) or compiler-generated functions (e.g. for a type like struct { x, y string }). This implementation is slower but more general and is used for hashing interface types (called from interhash or nilinterhash, above) or for hashing in maps generated by reflect.MapOf (reflect_typehash, below). Note: this function must match the compiler generated functions exactly. See issue 37716. 

### <a id="typelinksinit" href="#typelinksinit">func typelinksinit()</a>

```
searchKey: runtime.typelinksinit
tags: [function private]
```

```Go
func typelinksinit()
```

typelinksinit scans the types from extra modules and builds the moduledata typemap used to de-duplicate type pointers. 

### <a id="typesEqual" href="#typesEqual">func typesEqual(t, v *_type, seen map[_typePair]struct{}) bool</a>

```
searchKey: runtime.typesEqual
tags: [method private]
```

```Go
func typesEqual(t, v *_type, seen map[_typePair]struct{}) bool
```

typesEqual reports whether two types are equal. 

Everywhere in the runtime and reflect packages, it is assumed that there is exactly one *_type per Go type, so that pointer equality can be used to test if types are equal. There is one place that breaks this assumption: buildmode=shared. In this case a type can appear as two different pieces of memory. This is hidden from the runtime and reflect package by the per-module typemap built in typelinksinit. It uses typesEqual to map types from later modules back into earlier ones. 

Only typelinksinit needs this function. 

### <a id="unblocksig" href="#unblocksig">func unblocksig(sig uint32)</a>

```
searchKey: runtime.unblocksig
tags: [method private]
```

```Go
func unblocksig(sig uint32)
```

unblocksig removes sig from the current thread's signal mask. This is nosplit and nowritebarrierrec because it is called from dieFromSignal, which can be called by sigfwdgo while running in the signal handler, on the signal stack, with no g available. 

### <a id="unexportedPanicForTesting" href="#unexportedPanicForTesting">func unexportedPanicForTesting(b []byte, i int) byte</a>

```
searchKey: runtime.unexportedPanicForTesting
tags: [method private]
```

```Go
func unexportedPanicForTesting(b []byte, i int) byte
```

### <a id="unimplemented" href="#unimplemented">func unimplemented(name string)</a>

```
searchKey: runtime.unimplemented
tags: [method private]
```

```Go
func unimplemented(name string)
```

### <a id="unlock" href="#unlock">func unlock(l *mutex)</a>

```
searchKey: runtime.unlock
tags: [method private]
```

```Go
func unlock(l *mutex)
```

### <a id="unlock2" href="#unlock2">func unlock2(l *mutex)</a>

```
searchKey: runtime.unlock2
tags: [method private]
```

```Go
func unlock2(l *mutex)
```

We might not be holding a p in this code. 

### <a id="unlockOSThread" href="#unlockOSThread">func unlockOSThread()</a>

```
searchKey: runtime.unlockOSThread
tags: [function private]
```

```Go
func unlockOSThread()
```

### <a id="unlockWithRank" href="#unlockWithRank">func unlockWithRank(l *mutex)</a>

```
searchKey: runtime.unlockWithRank
tags: [method private]
```

```Go
func unlockWithRank(l *mutex)
```

### <a id="unlockextra" href="#unlockextra">func unlockextra(mp *m)</a>

```
searchKey: runtime.unlockextra
tags: [method private]
```

```Go
func unlockextra(mp *m)
```

### <a id="unminit" href="#unminit">func unminit()</a>

```
searchKey: runtime.unminit
tags: [function private]
```

```Go
func unminit()
```

Called from dropm to undo the effect of an minit. 

### <a id="unminitSignals" href="#unminitSignals">func unminitSignals()</a>

```
searchKey: runtime.unminitSignals
tags: [function private]
```

```Go
func unminitSignals()
```

unminitSignals is called from dropm, via unminit, to undo the effect of calling minit on a non-Go thread. 

### <a id="unreachableMethod" href="#unreachableMethod">func unreachableMethod()</a>

```
searchKey: runtime.unreachableMethod
tags: [function private]
```

```Go
func unreachableMethod()
```

The linker redirects a reference of a method that it determined unreachable to a reference to this function, so it will throw if ever called. 

### <a id="unsafeslice" href="#unsafeslice">func unsafeslice(et *_type, len int)</a>

```
searchKey: runtime.unsafeslice
tags: [method private]
```

```Go
func unsafeslice(et *_type, len int)
```

### <a id="unsafeslice64" href="#unsafeslice64">func unsafeslice64(et *_type, len64 int64)</a>

```
searchKey: runtime.unsafeslice64
tags: [method private]
```

```Go
func unsafeslice64(et *_type, len64 int64)
```

### <a id="unspillArgs" href="#unspillArgs">func unspillArgs()</a>

```
searchKey: runtime.unspillArgs
tags: [function private]
```

```Go
func unspillArgs()
```

### <a id="unwindm" href="#unwindm">func unwindm(restore *bool)</a>

```
searchKey: runtime.unwindm
tags: [method private]
```

```Go
func unwindm(restore *bool)
```

### <a id="updateTimer0When" href="#updateTimer0When">func updateTimer0When(pp *p)</a>

```
searchKey: runtime.updateTimer0When
tags: [method private]
```

```Go
func updateTimer0When(pp *p)
```

updateTimer0When sets the P's timer0When field. The caller must have locked the timers for pp. 

### <a id="updateTimerModifiedEarliest" href="#updateTimerModifiedEarliest">func updateTimerModifiedEarliest(pp *p, nextwhen int64)</a>

```
searchKey: runtime.updateTimerModifiedEarliest
tags: [method private]
```

```Go
func updateTimerModifiedEarliest(pp *p, nextwhen int64)
```

updateTimerModifiedEarliest updates the recorded nextwhen field of the earlier timerModifiedEarier value. The timers for pp will not be locked. 

### <a id="updateTimerPMask" href="#updateTimerPMask">func updateTimerPMask(pp *p)</a>

```
searchKey: runtime.updateTimerPMask
tags: [method private]
```

```Go
func updateTimerPMask(pp *p)
```

updateTimerPMask clears pp's timer mask if it has no timers on its heap. 

Ideally, the timer mask would be kept immediately consistent on any timer operations. Unfortunately, updating a shared global data structure in the timer hot path adds too much overhead in applications frequently switching between no timers and some timers. 

As a compromise, the timer mask is updated only on pidleget / pidleput. A running P (returned by pidleget) may add a timer at any time, so its mask must be set. An idle P (passed to pidleput) cannot add new timers while idle, so if it has no timers at that time, its mask may be cleared. 

Thus, we get the following effects on timer-stealing in findrunnable: 

* Idle Ps with no timers when they go idle are never checked in findrunnable 

```
(for work- or timer-stealing; this is the ideal case).

```
* Running Ps must always be checked. * Idle Ps whose timers are stolen must continue to be checked until they run 

```
again, even after timer expiration.

```
When the P starts running again, the mask should be set, as a timer may be added at any time. 

TODO(prattmic): Additional targeted updates may improve the above cases. e.g., updating the mask when stealing a timer. 

### <a id="updatememstats" href="#updatememstats">func updatememstats()</a>

```
searchKey: runtime.updatememstats
tags: [function private]
```

```Go
func updatememstats()
```

Updates the memstats structure. 

The world must be stopped. 

### <a id="usesLibcall" href="#usesLibcall">func usesLibcall() bool</a>

```
searchKey: runtime.usesLibcall
tags: [function private]
```

```Go
func usesLibcall() bool
```

usesLibcall indicates whether this runtime performs system calls via libcall. 

### <a id="usleep" href="#usleep">func usleep(usec uint32)</a>

```
searchKey: runtime.usleep
tags: [method private]
```

```Go
func usleep(usec uint32)
```

### <a id="usleep_no_g" href="#usleep_no_g">func usleep_no_g(usec uint32)</a>

```
searchKey: runtime.usleep_no_g
tags: [method private]
```

```Go
func usleep_no_g(usec uint32)
```

### <a id="usleep_trampoline" href="#usleep_trampoline">func usleep_trampoline()</a>

```
searchKey: runtime.usleep_trampoline
tags: [function private]
```

```Go
func usleep_trampoline()
```

### <a id="verifyTimerHeap" href="#verifyTimerHeap">func verifyTimerHeap(pp *p)</a>

```
searchKey: runtime.verifyTimerHeap
tags: [method private]
```

```Go
func verifyTimerHeap(pp *p)
```

verifyTimerHeap verifies that the timer heap is in a valid state. This is only for debugging, and is only called if verifyTimers is true. The caller must have locked the timers. 

### <a id="waitForSigusr1Callback" href="#waitForSigusr1Callback">func waitForSigusr1Callback(gp *g) bool</a>

```
searchKey: runtime.waitForSigusr1Callback
tags: [method private]
```

```Go
func waitForSigusr1Callback(gp *g) bool
```

waitForSigusr1Callback is called from the signal handler during WaitForSigusr1. It must not have write barriers because there may not be a P. 

### <a id="wakeNetPoller" href="#wakeNetPoller">func wakeNetPoller(when int64)</a>

```
searchKey: runtime.wakeNetPoller
tags: [method private]
```

```Go
func wakeNetPoller(when int64)
```

wakeNetPoller wakes up the thread sleeping in the network poller if it isn't going to wake up before the when argument; or it wakes an idle P to service timers and the network poller if there isn't one already. 

### <a id="wakeScavenger" href="#wakeScavenger">func wakeScavenger()</a>

```
searchKey: runtime.wakeScavenger
tags: [function private]
```

```Go
func wakeScavenger()
```

wakeScavenger immediately unparks the scavenger if necessary. 

May run without a P, but it may allocate, so it must not be called on any allocation path. 

mheap_.lock, scavenge.lock, and sched.lock must not be held. 

### <a id="wakep" href="#wakep">func wakep()</a>

```
searchKey: runtime.wakep
tags: [function private]
```

```Go
func wakep()
```

Tries to add one more P to execute G's. Called when a G is made runnable (newproc, ready). 

### <a id="walltime" href="#walltime">func walltime() (int64, int32)</a>

```
searchKey: runtime.walltime
tags: [function private]
```

```Go
func walltime() (int64, int32)
```

### <a id="walltime_trampoline" href="#walltime_trampoline">func walltime_trampoline()</a>

```
searchKey: runtime.walltime_trampoline
tags: [function private]
```

```Go
func walltime_trampoline()
```

### <a id="wantAsyncPreempt" href="#wantAsyncPreempt">func wantAsyncPreempt(gp *g) bool</a>

```
searchKey: runtime.wantAsyncPreempt
tags: [method private]
```

```Go
func wantAsyncPreempt(gp *g) bool
```

wantAsyncPreempt returns whether an asynchronous preemption is queued for gp. 

### <a id="wbBufFlush" href="#wbBufFlush">func wbBufFlush(dst *uintptr, src uintptr)</a>

```
searchKey: runtime.wbBufFlush
tags: [method private]
```

```Go
func wbBufFlush(dst *uintptr, src uintptr)
```

wbBufFlush flushes the current P's write barrier buffer to the GC workbufs. It is passed the slot and value of the write barrier that caused the flush so that it can implement cgocheck. 

This must not have write barriers because it is part of the write barrier implementation. 

This and everything it calls must be nosplit because 1) the stack contains untyped slots from gcWriteBarrier and 2) there must not be a GC safe point between the write barrier test in the caller and flushing the buffer. 

TODO: A "go:nosplitrec" annotation would be perfect for this. 

### <a id="wbBufFlush1" href="#wbBufFlush1">func wbBufFlush1(_p_ *p)</a>

```
searchKey: runtime.wbBufFlush1
tags: [method private]
```

```Go
func wbBufFlush1(_p_ *p)
```

wbBufFlush1 flushes p's write barrier buffer to the GC work queue. 

This must not have write barriers because it is part of the write barrier implementation, so this may lead to infinite loops or buffer corruption. 

This must be non-preemptible because it uses the P's workbuf. 

### <a id="wirep" href="#wirep">func wirep(_p_ *p)</a>

```
searchKey: runtime.wirep
tags: [method private]
```

```Go
func wirep(_p_ *p)
```

wirep is the first step of acquirep, which actually associates the current M to _p_. This is broken out so we can disallow write barriers for this part, since we don't yet have a P. 

### <a id="worldStarted" href="#worldStarted">func worldStarted()</a>

```
searchKey: runtime.worldStarted
tags: [function private]
```

```Go
func worldStarted()
```

### <a id="worldStopped" href="#worldStopped">func worldStopped()</a>

```
searchKey: runtime.worldStopped
tags: [function private]
```

```Go
func worldStopped()
```

### <a id="write" href="#write">func write(fd uintptr, p unsafe.Pointer, n int32) int32</a>

```
searchKey: runtime.write
tags: [method private]
```

```Go
func write(fd uintptr, p unsafe.Pointer, n int32) int32
```

write must be nosplit on Windows (see write1) 

### <a id="write1" href="#write1">func write1(fd uintptr, p unsafe.Pointer, n int32) int32</a>

```
searchKey: runtime.write1
tags: [method private]
```

```Go
func write1(fd uintptr, p unsafe.Pointer, n int32) int32
```

### <a id="writeErr" href="#writeErr">func writeErr(b []byte)</a>

```
searchKey: runtime.writeErr
tags: [method private]
```

```Go
func writeErr(b []byte)
```

### <a id="write_trampoline" href="#write_trampoline">func write_trampoline()</a>

```
searchKey: runtime.write_trampoline
tags: [function private]
```

```Go
func write_trampoline()
```

### <a id="writeheapdump_m" href="#writeheapdump_m">func writeheapdump_m(fd uintptr, m *MemStats)</a>

```
searchKey: runtime.writeheapdump_m
tags: [method private]
```

```Go
func writeheapdump_m(fd uintptr, m *MemStats)
```

### <a id="_ExternalCode" href="#_ExternalCode">func _ExternalCode()</a>

```
searchKey: runtime._ExternalCode
tags: [function private]
```

```Go
func _ExternalCode()
```

### <a id="_GC" href="#_GC">func _GC()</a>

```
searchKey: runtime._GC
tags: [function private]
```

```Go
func _GC()
```

### <a id="_LostExternalCode" href="#_LostExternalCode">func _LostExternalCode()</a>

```
searchKey: runtime._LostExternalCode
tags: [function private]
```

```Go
func _LostExternalCode()
```

### <a id="_LostSIGPROFDuringAtomic64" href="#_LostSIGPROFDuringAtomic64">func _LostSIGPROFDuringAtomic64()</a>

```
searchKey: runtime._LostSIGPROFDuringAtomic64
tags: [function private]
```

```Go
func _LostSIGPROFDuringAtomic64()
```

### <a id="_System" href="#_System">func _System()</a>

```
searchKey: runtime._System
tags: [function private]
```

```Go
func _System()
```

### <a id="_VDSO" href="#_VDSO">func _VDSO()</a>

```
searchKey: runtime._VDSO
tags: [function private]
```

```Go
func _VDSO()
```

### <a id="_cgo_panic_internal" href="#_cgo_panic_internal">func _cgo_panic_internal(p *byte)</a>

```
searchKey: runtime._cgo_panic_internal
tags: [method private]
```

```Go
func _cgo_panic_internal(p *byte)
```

